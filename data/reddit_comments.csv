brand,comment_id,text,subreddit,created_utc,score
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,51
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,71
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,7
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,5
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-5
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,40
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,13
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,28
Intel,notfij9,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,10
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,8
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,1
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,17
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,10
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,2
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,10
Intel,nouozrl,I’m assuming the caveat to posts like this is “running a version of windows/linux”,hardware,2025-11-14 18:41:25,13
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,22
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,1
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,11
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-8
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,12
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,5
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,12
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,7
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,19
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,15
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,10
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,69
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,42
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,7
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-6
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-3
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-5
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,47
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,17
Intel,nnntnit,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,4
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,22
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,8
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,9
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-4
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-5
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,3
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,23
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,12
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,8
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,6
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,0
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-9
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,24
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,7
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,10
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,7
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,13
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,18
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,20
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-9
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,11
Intel,nnh18hs,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,9
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,8
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,9
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,6
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,7
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,9
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,13
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,6
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-3
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-5
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,5
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,9
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,5
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,13
Intel,nngrskq,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,11
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,11
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,8
Intel,nnhryps,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,15
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,11
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,4
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,12
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,8
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,14
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,8
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,16
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,3
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,17
Intel,nnguuaj,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,5
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-9
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,4
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then …,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,7
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,5
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,8
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,0
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,7
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,8
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,5
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,10
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,8
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,15
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,8
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,3
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,6
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,0
Intel,nnhohos,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,11
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,10
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,8
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,5
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,5
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,13
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,7
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,5
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,7
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,1
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,5
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-6
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,7
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,1
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,4
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,8
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,9
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,3
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,6
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,30
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,57
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,16
Intel,nmn2kpy,Fine wine,hardware,2025-11-02 02:36:42,1
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-23
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,13
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,25
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,8
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,30
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,13
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,4
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,21
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,15
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,2
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,2
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,8
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,30
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,3
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-3
Intel,nis0ezd,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,13
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,17
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,5
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,38
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,29
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,15
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,29
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,8
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,11
Intel,nii5519,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,12
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,3
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,17
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,4
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,8
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,5
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,5
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,8
Intel,niixhey,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",hardware,2025-10-09 01:35:25,71
Intel,nij1t7l,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,hardware,2025-10-09 02:02:14,35
Intel,nijk1dw,that's insane vram density,hardware,2025-10-09 04:05:01,12
Intel,nijb3xq,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",hardware,2025-10-09 02:59:53,16
Intel,niiwt1u,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 01:31:12,2
Intel,nimo4dq,Is there a fork of chrome that runs on gpus,hardware,2025-10-09 17:24:22,2
Intel,nindfe4,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",hardware,2025-10-09 19:31:05,2
Intel,nikln6u,but is that faster than a single 5090?,hardware,2025-10-09 10:10:09,2
Intel,nik40r8,Is this enough VRAM for modern gaming?,hardware,2025-10-09 07:06:00,-2
Intel,nil0agd,Nvidia: ill commit s------e,hardware,2025-10-09 12:07:51,-8
Intel,nil8gj0,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",hardware,2025-10-09 12:59:34,19
Intel,no4mb2m,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",hardware,2025-11-10 16:22:54,1
Intel,nikh8oe,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",hardware,2025-10-09 09:25:55,-15
Intel,nij5zhc,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",hardware,2025-10-09 02:27:37,44
Intel,niml0xn,I don't think servers are supposed to stay idle for long.,hardware,2025-10-09 17:09:18,2
Intel,niqkqmc,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",hardware,2025-10-10 07:57:46,2
Intel,nijeto3,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,hardware,2025-10-09 03:25:25,41
Intel,nilc2tl,Could that make it very cost effective for any particular use cases?,hardware,2025-10-09 13:21:04,6
Intel,nim2kjx,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",hardware,2025-10-09 15:38:17,4
Intel,nij8lcp,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,hardware,2025-10-09 02:43:33,27
Intel,nijhbrl,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",hardware,2025-10-09 03:44:18,3
Intel,nilfpel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",hardware,2025-10-09 13:41:48,6
Intel,nin9mev,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",hardware,2025-10-09 19:11:33,3
Intel,nijt662,At least its a human hallucination and not AI hallucination.,hardware,2025-10-09 05:21:24,19
Intel,nik407y,Can also be bad translation.,hardware,2025-10-09 07:05:52,2
Intel,nijj2tk,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",hardware,2025-10-09 03:57:40,17
Intel,nilgnxj,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",hardware,2025-10-09 13:47:09,2
Intel,nili8e7,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",hardware,2025-10-09 13:55:46,4
Intel,niokcbz,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,hardware,2025-10-09 23:22:33,12
Intel,nilzeoj,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",hardware,2025-10-09 15:22:43,26
Intel,nime2oj,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",hardware,2025-10-09 16:35:12,19
Intel,nilf97b,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 13:39:16,1
Intel,nim7yeq,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,hardware,2025-10-09 16:04:46,-3
Intel,nilnsw1,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",hardware,2025-10-09 14:24:56,-25
Intel,nionxww,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,hardware,2025-10-09 23:43:45,5
Intel,nimlaz8,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",hardware,2025-10-09 17:10:41,4
Intel,nimhv2z,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,hardware,2025-10-09 16:53:51,5
Intel,nimkx2o,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",hardware,2025-10-09 17:08:47,22
Intel,nin877l,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,hardware,2025-10-09 19:04:21,8
Intel,nir42up,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,hardware,2025-10-10 11:05:57,6
Intel,nirvt3d,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,hardware,2025-10-10 14:00:42,2
Intel,nixqbsp,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",hardware,2025-10-11 13:25:20,1
Intel,nimkw5p,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,hardware,2025-10-09 17:08:39,6
Intel,nilrz1f,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",hardware,2025-10-09 14:45:58,21
Intel,nilwkhh,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",hardware,2025-10-09 15:08:45,10
Intel,nioq01t,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,hardware,2025-10-09 23:56:01,4
Intel,nirvexf,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",hardware,2025-10-10 13:58:40,4
Intel,nimlpyk,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",hardware,2025-10-09 17:12:42,3
Intel,nimm20x,Celestial was based on Xe3p.,hardware,2025-10-09 17:14:19,7
Intel,niokiba,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",hardware,2025-10-09 23:23:33,1
Intel,niy2dcz,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,hardware,2025-10-11 14:38:03,3
Intel,nimmzhl,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,hardware,2025-10-09 17:18:48,15
Intel,nirvxip,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",hardware,2025-10-10 14:01:21,3
Intel,nin3vrr,That's not what my colleague's say,hardware,2025-10-09 18:42:31,2
Intel,nilz2fy,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,hardware,2025-10-09 15:21:03,25
Intel,nim3npl,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",hardware,2025-10-09 15:43:35,-11
Intel,ninrarq,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",hardware,2025-10-09 20:40:49,-7
Intel,niy1tvt,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",hardware,2025-10-11 14:35:03,1
Intel,nj09dos,Xe3p is a significant architectural advancement says Tom Petersen.,hardware,2025-10-11 21:46:07,2
Intel,nim48wq,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",hardware,2025-10-09 15:46:27,-14
Intel,nim6va1,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",hardware,2025-10-09 15:59:21,8
Intel,nimks6h,"So much buzzwords, yet it sounds like a stroke.  You need help.",hardware,2025-10-09 17:08:07,7
Intel,nip4uap,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",hardware,2025-10-10 01:23:47,3
Intel,nim4t3o,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",hardware,2025-10-09 15:49:13,14
Intel,nino52j,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",hardware,2025-10-09 20:24:55,-2
Intel,nirwlsr,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,hardware,2025-10-10 14:04:53,1
Intel,ninor5p,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",hardware,2025-10-09 20:27:56,-7
Intel,ninre0y,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",hardware,2025-10-09 20:41:15,6
Intel,nixpud1,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,hardware,2025-10-11 13:22:13,1
Intel,ninu5gl,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",hardware,2025-10-09 20:55:03,-1
Intel,nio14ia,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",hardware,2025-10-09 21:31:45,7
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,32
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,36
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,11
Intel,nfy52qu,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,hardware,2025-09-24 13:53:35,11
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,9
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,3
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,2
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,2
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,46
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,5
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,9
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,5
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-8
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,10
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,9
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,5
Intel,ng3cb6o,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",hardware,2025-09-25 08:18:28,3
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,7
Intel,nfy9qc1,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",hardware,2025-09-24 14:17:24,4
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,12
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,9
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,nfyowfp,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",hardware,2025-09-24 15:31:47,4
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,35
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,74
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,13
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,13
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,34
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,11
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,17
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,6
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,32
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,5
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,11
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,5
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-20
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,5
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,24
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,14
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,8
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,4
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,4
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,3
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,3
Intel,nfbo7bs,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,hardware,2025-09-20 22:33:22,139
Intel,nfb2xr8,Far more than I expected them to come out at. Damn.,hardware,2025-09-20 20:35:14,18
Intel,nfarr9a,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",hardware,2025-09-20 19:36:31,60
Intel,nfbz1kb,I’m getting one when it releases in Australia,hardware,2025-09-20 23:37:15,4
Intel,nfj2y23,Thats great and all but when will there be stock? (Canada),hardware,2025-09-22 02:00:50,2
Intel,ng8k9a2,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,hardware,2025-09-26 02:15:59,1
Intel,ngid0f0,I'm disappointed.  My order was canceled,hardware,2025-09-27 17:12:34,1
Intel,nfaolmx,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-20 19:19:41,1
Intel,nfbxe5d,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",hardware,2025-09-20 23:27:31,135
Intel,nfc739o,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,hardware,2025-09-21 00:26:10,19
Intel,nfateyp,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",hardware,2025-09-20 19:45:24,203
Intel,nfb479l,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",hardware,2025-09-20 20:41:48,79
Intel,nfb166b,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",hardware,2025-09-20 20:26:03,38
Intel,nfb3pdb,The 3090 does not have ECC on it's VRAM nor certified drivers,hardware,2025-09-20 20:39:13,16
Intel,nfc4ak9,Totally not worth it.,hardware,2025-09-21 00:08:54,-3
Intel,nfb6hdv,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,hardware,2025-09-20 20:53:44,-9
Intel,nfd2k0s,"I'm more looking forward to the B50, but obviously local pricing is everything.",hardware,2025-09-21 03:53:22,7
Intel,njp8t1o,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",hardware,2025-10-15 22:34:30,1
Intel,nfbgzuw,"It has SR-IOV, certified drivers and other professional features...",hardware,2025-09-20 21:51:30,23
Intel,nfc15d7,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",hardware,2025-09-20 23:49:42,8
Intel,nfdv26x,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",hardware,2025-09-21 08:04:03,14
Intel,nfbzlhb,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,hardware,2025-09-20 23:40:29,-14
Intel,nfeqn5s,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",hardware,2025-09-21 12:41:54,6
Intel,nfauwnn,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",hardware,2025-09-20 19:53:22,21
Intel,nfau9ib,Do not underestimate the lack of CUDA.,hardware,2025-09-20 19:49:57,27
Intel,nfdmqe2,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",hardware,2025-09-21 06:45:03,2
Intel,nfbbfys,A used 3090 is only $100 more.,hardware,2025-09-20 21:20:29,1
Intel,nfauhs7,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",hardware,2025-09-20 19:51:10,-18
Intel,nfc1012,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,hardware,2025-09-20 23:48:50,-6
Intel,nfb5jdp,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,hardware,2025-09-20 20:48:47,29
Intel,nfc0fwm,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",hardware,2025-09-20 23:45:29,16
Intel,nfbovre,Atlas 300i duo,hardware,2025-09-20 22:37:31,-3
Intel,nfb510h,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",hardware,2025-09-20 20:46:06,27
Intel,nfb3ro9,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",hardware,2025-09-20 20:39:33,7
Intel,nfc234i,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,hardware,2025-09-20 23:55:21,5
Intel,njpmmsp,Yeah it was very scummy imo,hardware,2025-10-15 23:55:13,1
Intel,nfcbz1s,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",hardware,2025-09-21 00:56:35,1
Intel,nfc72xn,It was meant to be a joke. Not so funny I guess.,hardware,2025-09-21 00:26:06,5
Intel,nfek88y,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",hardware,2025-09-21 11:57:29,11
Intel,nfsgdep,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",hardware,2025-09-23 16:15:20,2
Intel,nfd2naj,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,hardware,2025-09-21 03:54:03,11
Intel,nfc0nds,Why does this matter?,hardware,2025-09-20 23:46:45,20
Intel,nfv9260,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",hardware,2025-09-24 00:45:10,2
Intel,nfb63fj,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,hardware,2025-09-20 20:51:42,42
Intel,nfbbr35,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",hardware,2025-09-20 21:22:13,9
Intel,nfbedl2,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",hardware,2025-09-20 21:36:51,8
Intel,nfbifxk,Used market is freaking insane. It is better to grab new or open box.,hardware,2025-09-20 21:59:41,4
Intel,nfcx3qi,Over here used 3090s are sold for 500-600€.,hardware,2025-09-21 03:13:43,1
Intel,nfbbgrb,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",hardware,2025-09-20 21:20:36,30
Intel,nfhd7ci,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,hardware,2025-09-21 20:19:33,1
Intel,nfb5o6f,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,hardware,2025-09-20 20:49:29,22
Intel,nfayuhx,the super gpus are not expected to release soon?,hardware,2025-09-20 20:14:00,9
Intel,nfcl0od,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",hardware,2025-09-21 01:53:39,14
Intel,nfcm0px,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",hardware,2025-09-21 02:00:02,8
Intel,nfc9w3x,Used 4070 Supers dont sell for nearly $700+ on the low dude.,hardware,2025-09-21 00:43:40,4
Intel,nfco0nb,Yes running business of used cards is how its done.....,hardware,2025-09-21 02:12:43,2
Intel,nfdimrg,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,hardware,2025-09-21 06:07:29,8
Intel,nfbr125,Typically run at 250W though to be fair.,hardware,2025-09-20 22:50:29,6
Intel,nfc1wom,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",hardware,2025-09-20 23:54:16,-9
Intel,nfbgadn,not on the Vram like professional cards,hardware,2025-09-20 21:47:32,7
Intel,nfdjdro,Q4.,hardware,2025-09-21 06:14:16,2
Intel,nfdm9xu,I thought it was funny ¯\_(ツ)_/¯,hardware,2025-09-21 06:40:53,2
Intel,nfjxn7s,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,hardware,2025-09-22 05:47:02,3
Intel,nfddcxx,"Yeah, apparently a year’s worth of it",hardware,2025-09-21 05:20:13,-5
Intel,nfcew77,Because they're super late to the party.,hardware,2025-09-21 01:14:55,-9
Intel,nfb6fud,"Wow, 800-900USD after 5 years is more than I would have expected.",hardware,2025-09-20 20:53:31,17
Intel,nfdc18j,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",hardware,2025-09-21 05:08:46,5
Intel,nfc1e0n,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,hardware,2025-09-20 23:51:10,-7
Intel,nfc1j2x,For what exactly do they need vram without cuda ?,hardware,2025-09-20 23:52:00,-5
Intel,nfbq925,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",hardware,2025-09-20 22:45:52,-11
Intel,nfksagq,Gamers are not niche. They are 20 billion a year business.,hardware,2025-09-22 10:46:43,-1
Intel,nfbohl3,"False, they’re releasing in december or jan. So about 3 months from now",hardware,2025-09-20 22:35:06,5
Intel,nfbxm75,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",hardware,2025-09-20 23:28:51,12
Intel,nfbksrd,"The 3090 Ti did, but the standard 3090 did not.",hardware,2025-09-20 22:13:21,9
Intel,nfjz0lo,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",hardware,2025-09-22 05:59:38,3
Intel,nfdem0c,"If intel felt they could have released this safely earlier, they would have",hardware,2025-09-21 05:31:12,5
Intel,nfb6vit,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,hardware,2025-09-20 20:55:46,24
Intel,nfc975k,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",hardware,2025-09-21 00:39:26,1
Intel,nfg0b8g,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,hardware,2025-09-21 16:40:24,3
Intel,nfcmei6,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,hardware,2025-09-21 02:02:24,9
Intel,nfc3jog,"rendering, working on big BIM / CAD models, medical imaging,...",hardware,2025-09-21 00:04:18,21
Intel,nfc3y1b,CUDA isnt the only backend used by AI frameworks,hardware,2025-09-21 00:06:45,24
Intel,nfde0a9,"I use opencl for doing gpgpu simulations, this card would be great for it",hardware,2025-09-21 05:25:54,2
Intel,nfbqi8f,You mean your 0 profile history because you have it private?,hardware,2025-09-20 22:47:22,10
Intel,nfd62l7,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",hardware,2025-09-21 04:20:02,0
Intel,nfbqms5,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,hardware,2025-09-20 22:48:07,-1
Intel,nfksqka,Spoken like a true gamer.,hardware,2025-09-22 10:50:25,1
Intel,nfbt6h6,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,hardware,2025-09-20 23:03:03,2
Intel,nfcz630,You still get about 85% performance compared to stock settings.,hardware,2025-09-21 03:28:33,2
Intel,nfdr09z,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",hardware,2025-09-21 07:24:57,-3
Intel,nfbvai4,What are they doing that's making them money? Or are theu selling the compute somehow?,hardware,2025-09-20 23:15:17,19
Intel,nfc1jx5,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",hardware,2025-09-20 23:52:08,-10
Intel,nfc9f6k,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",hardware,2025-09-21 00:40:47,-1
Intel,nfkntvr,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",hardware,2025-09-22 10:07:27,3
Intel,nfcsv0e,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",hardware,2025-09-21 02:44:26,4
Intel,nfdll3m,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,hardware,2025-09-21 06:34:32,1
Intel,nfks66x,It is the only functional one.,hardware,2025-09-22 10:45:44,1
Intel,nfbqlfx,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",hardware,2025-09-20 22:47:54,-6
Intel,nfd6ez2,0 people are using autodesk with an intel arc gpu lmao,hardware,2025-09-21 04:22:47,-1
Intel,nfbriv2,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",hardware,2025-09-20 22:53:25,0
Intel,nfkvz03,"Oh no, im part of 60% of world populatioin. how terrible.",hardware,2025-09-22 11:15:48,0
Intel,nfd2pwi,Which is 36% higher performance per watt.       ... to be fair.,hardware,2025-09-21 03:54:35,4
Intel,nfdsvbs,The reason they didn't is because it would have been worse to release it earlier,hardware,2025-09-21 07:43:03,0
Intel,nfbxbg7,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",hardware,2025-09-20 23:27:04,15
Intel,nfbw0b9,Software Development,hardware,2025-09-20 23:19:26,-4
Intel,nfc44n9,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,hardware,2025-09-21 00:07:53,8
Intel,nfcbku1,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:54:09,-1
Intel,nfcbqrf,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:55:09,-4
Intel,nfkwldk,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",hardware,2025-09-22 11:20:33,1
Intel,nfcv5fj,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",hardware,2025-09-21 03:00:04,7
Intel,nfeidh5,Which ones?,hardware,2025-09-21 11:43:23,1
Intel,nfcjfv3,Private profile = Complete troll.  You're not an exception to this rule.,hardware,2025-09-21 01:43:42,13
Intel,nfbs5c3,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,hardware,2025-09-20 22:57:03,-3
Intel,nfdu2bk,"Right man, my point is that it shouldn’t have been",hardware,2025-09-21 07:54:31,-1
Intel,nfbxo4k,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,hardware,2025-09-20 23:29:11,11
Intel,nfcdfer,"Hell, one of them was just the cooler without the actual GPU.",hardware,2025-09-21 01:05:34,12
Intel,nfcde1y,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",hardware,2025-09-21 01:05:20,-2
Intel,nfgcxg6,for example every local image and video generation system I've seen.,hardware,2025-09-21 17:38:32,3
Intel,nfbsbxa,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",hardware,2025-09-20 22:58:06,2
Intel,nfc0g5t,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",hardware,2025-09-20 23:45:32,-1
Intel,nfducqd,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,hardware,2025-09-21 07:57:16,3
Intel,nfd4nt4,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",hardware,2025-09-21 04:09:11,5
Intel,nfh4t9j,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,hardware,2025-09-21 19:41:40,1
Intel,nfd0war,And the one directly under that was the actual PCB...,hardware,2025-09-21 03:41:08,6
Intel,nfgecr2,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,hardware,2025-09-21 17:44:40,0
Intel,nfksgbg,Propaganda is 90% of chinas economic output though.,hardware,2025-09-22 10:48:04,0
Intel,ne6ahg8,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",hardware,2025-09-14 14:46:52,185
Intel,ne5xw93,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,hardware,2025-09-14 13:39:18,220
Intel,ne6669f,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",hardware,2025-09-14 14:24:15,60
Intel,ne680ly,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",hardware,2025-09-14 14:34:03,57
Intel,ne6hcg6,L1 techs had a great feature on these.,hardware,2025-09-14 15:21:17,23
Intel,ne62jka,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",hardware,2025-09-14 14:04:39,30
Intel,ne8mbnv,1:4 ratio of FP64 performance is a pleasant surprise,hardware,2025-09-14 21:19:44,10
Intel,ne6anee,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",hardware,2025-09-14 14:47:44,14
Intel,ne65b1l,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",hardware,2025-09-14 14:19:36,20
Intel,ne6rl68,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",hardware,2025-09-14 16:11:13,4
Intel,ne5wmh6,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-14 13:32:02,-3
Intel,ne6nm7f,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,hardware,2025-09-14 15:52:11,-22
Intel,ne7zdif,what's up my Neweggâs,hardware,2025-09-14 19:32:23,93
Intel,ne95tlq,My favorite game  Fallout Neweggas,hardware,2025-09-14 23:05:48,25
Intel,ne9ba6q,It also sounds like an ikea lamp name or something,hardware,2025-09-14 23:35:36,7
Intel,ne7p3l1,Geriau nei Bilas Gatesas,hardware,2025-09-14 18:44:05,3
Intel,ne6o1us,I gotta spell Neweggâs?!?,hardware,2025-09-14 15:54:14,1
Intel,ne6355p,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",hardware,2025-09-14 14:07:57,71
Intel,ne7314f,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",hardware,2025-09-14 17:04:11,7
Intel,ne8v3zo,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,hardware,2025-09-14 22:05:58,17
Intel,ne6kilx,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",hardware,2025-09-14 15:37:09,-73
Intel,ne7d8mq,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,hardware,2025-09-14 17:49:31,22
Intel,ne6eqfv,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",hardware,2025-09-14 15:08:14,-8
Intel,ne70sx3,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",hardware,2025-09-14 16:54:03,6
Intel,nealgik,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",hardware,2025-09-15 04:31:20,9
Intel,ne6c0of,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",hardware,2025-09-14 14:54:44,44
Intel,ne6dpqm,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",hardware,2025-09-14 15:03:08,23
Intel,ne6yi39,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,hardware,2025-09-14 16:43:33,10
Intel,nebc25u,ECC memory.,hardware,2025-09-15 08:46:16,1
Intel,neb16s5,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",hardware,2025-09-15 06:53:42,3
Intel,neaqhtz,Oh when they get enough enterprise customers they will definitely charge licensing fees,hardware,2025-09-15 05:14:05,3
Intel,ne70g18,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,hardware,2025-09-14 16:52:26,18
Intel,ne878lu,Lmao,hardware,2025-09-14 20:08:06,13
Intel,nebbyvw,Komentarą skaitai per tapšnoklį ar vaizduoklį?,hardware,2025-09-15 08:45:18,1
Intel,ne66mwj,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,hardware,2025-09-14 14:26:44,73
Intel,ne6xv28,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",hardware,2025-09-14 16:40:31,5
Intel,ne8a75b,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",hardware,2025-09-14 20:21:25,3
Intel,ne95ql6,Vulcan does fine with inferencing.,hardware,2025-09-14 23:05:20,3
Intel,nfekr88,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",hardware,2025-09-21 12:01:22,1
Intel,neavc5w,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,hardware,2025-09-15 05:57:49,4
Intel,ne8ur5d,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",hardware,2025-09-14 22:03:59,18
Intel,neprgo9,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",hardware,2025-09-17 14:56:51,0
Intel,ne6jp74,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,hardware,2025-09-14 15:33:02,40
Intel,ne8fips,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,hardware,2025-09-14 20:46:12,14
Intel,ne8eldw,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",hardware,2025-09-14 20:41:52,9
Intel,ne93icy,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",hardware,2025-09-14 22:52:49,2
Intel,nealnf3,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",hardware,2025-09-15 04:32:57,12
Intel,ne6dmlw,I recognize those as words...,hardware,2025-09-14 15:02:42,5
Intel,ne7cbc2,I think it was obvious I was being facetious.,hardware,2025-09-14 17:45:29,-2
Intel,ne8jx5n,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,hardware,2025-09-14 21:07:34,30
Intel,ne6qvax,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",hardware,2025-09-14 16:07:47,21
Intel,ne6p0f3,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,hardware,2025-09-14 15:58:50,3
Intel,ne8kqkn,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",hardware,2025-09-14 21:11:39,11
Intel,ne95wnj,Have you tried GPU paravirtualization?,hardware,2025-09-14 23:06:16,1
Intel,nebgla3,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",hardware,2025-09-15 09:33:31,3
Intel,ne8r5hr,They need GPU IP for two things: client and AI. Anything else is expendable.,hardware,2025-09-14 21:44:37,5
Intel,ne9x6oh,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",hardware,2025-09-15 01:47:23,8
Intel,ne6f8e1,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",hardware,2025-09-14 15:10:44,34
Intel,neanu9y,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,hardware,2025-09-15 04:51:08,12
Intel,ne6s65g,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,hardware,2025-09-14 16:13:57,7
Intel,neaqr14,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",hardware,2025-09-15 05:16:19,2
Intel,neb03p4,Pathetic 1:64 ratio of FP64 flops,hardware,2025-09-15 06:43:15,2
Intel,neb28v9,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,hardware,2025-09-15 07:03:44,2
Intel,ne705od,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",hardware,2025-09-14 16:51:06,17
Intel,nea091d,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",hardware,2025-09-15 02:05:50,3
Intel,nebghm1,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",hardware,2025-09-15 09:32:29,3
Intel,necmv9o,my guy sr-iov is a type of gpu paravirtualization.,hardware,2025-09-15 14:23:28,1
Intel,nenukwd,The Asrock b60s are $599,hardware,2025-09-17 06:30:55,1
Intel,ne90ue7,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,hardware,2025-09-14 22:37:50,2
Intel,ne8u0oh,"One use case for ECC, is when the data is critical and can’t be lost.",hardware,2025-09-14 21:59:59,10
Intel,nea1lfo,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,hardware,2025-09-15 02:14:04,11
Intel,nebc61d,ECC should be in literally all memory.,hardware,2025-09-15 08:47:24,1
Intel,ne70ox2,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",hardware,2025-09-14 16:53:32,-10
Intel,ne6xml5,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,hardware,2025-09-14 16:39:25,8
Intel,nee4dd7,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",hardware,2025-09-15 18:42:34,2
Intel,neozcix,"Where can you get them? And are they for sale yet, or pre-orders, or...?",hardware,2025-09-17 12:27:00,1
Intel,ne9vlfz,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",hardware,2025-09-15 01:38:00,8
Intel,ne9xfff,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",hardware,2025-09-15 01:48:50,6
Intel,neav264,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,hardware,2025-09-15 05:55:13,2
Intel,neaiu6t,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",hardware,2025-09-15 04:10:17,11
Intel,ne78lg0,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",hardware,2025-09-14 17:29:09,15
Intel,neea3nk,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",hardware,2025-09-15 19:10:25,5
Intel,ne79g56,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,hardware,2025-09-14 17:32:53,9
Intel,ne79riz,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",hardware,2025-09-14 17:34:16,-13
Intel,ne8go27,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",hardware,2025-09-14 20:51:40,9
Intel,ne92tmq,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",hardware,2025-09-14 22:49:00,-7
Intel,nd4rusz,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",hardware,2025-09-08 18:55:24,81
Intel,nd5dm3n,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,hardware,2025-09-08 20:41:18,24
Intel,nd8r4il,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",hardware,2025-09-09 10:54:51,10
Intel,nd57awu,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,hardware,2025-09-08 20:11:04,28
Intel,nd5z0zb,Too late for me I already went with a 9060xt but hell I had dreamt of it!,hardware,2025-09-08 22:32:44,5
Intel,nd4ofdy,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",hardware,2025-09-08 18:38:32,3
Intel,nd5mkse,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,hardware,2025-09-08 21:25:37,3
Intel,nd8g10o,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,hardware,2025-09-09 09:13:47,2
Intel,nd4ety2,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-08 17:51:42,1
Intel,ndaptcd,Depending on the price I might give it a shot.,hardware,2025-09-09 17:16:31,1
Intel,nd7euko,Love it going to get one if I can scratch some money together,hardware,2025-09-09 03:38:15,1
Intel,nd5tii5,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,hardware,2025-09-08 22:02:05,-3
Intel,nd6wdjt,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",hardware,2025-09-09 01:46:51,0
Intel,nd4lt4f,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,hardware,2025-09-08 18:25:36,-12
Intel,nd5ec1f,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",hardware,2025-09-08 20:44:45,-8
Intel,nd50mkd,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,hardware,2025-09-08 19:38:23,18
Intel,nd7icfg,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",hardware,2025-09-09 04:02:28,-1
Intel,nd91aqv,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",hardware,2025-09-09 12:07:11,7
Intel,nd5nmhj,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",hardware,2025-09-08 21:31:01,20
Intel,nd50bqm,I have a B580 and the driver seems pretty stable to me at this point.,hardware,2025-09-08 19:36:56,39
Intel,nd4xpjv,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",hardware,2025-09-08 19:24:08,10
Intel,nd5kt0f,my b580 has been stable,hardware,2025-09-08 21:16:33,7
Intel,nd7ucd7,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",hardware,2025-09-09 05:39:29,2
Intel,nd86r6j,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",hardware,2025-09-09 07:37:52,2
Intel,nd7ihdv,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,hardware,2025-09-09 04:03:27,3
Intel,nd4pva0,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,hardware,2025-09-08 18:45:36,13
Intel,nd4qkrx,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",hardware,2025-09-08 18:49:07,20
Intel,nd57ffs,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,hardware,2025-09-08 20:11:40,38
Intel,nd5dyvw,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",hardware,2025-09-08 20:43:00,2
Intel,nd63tmr,Where do you find such information?,hardware,2025-09-08 23:00:34,16
Intel,nd648nd,Source: I made it the fk up,hardware,2025-09-08 23:03:03,7
Intel,nd55qwm,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",hardware,2025-09-08 20:03:30,18
Intel,nd4z315,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",hardware,2025-09-08 19:30:52,12
Intel,nd7nnb6,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,hardware,2025-09-09 04:42:33,44
Intel,nd6d7u0,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",hardware,2025-09-08 23:55:26,14
Intel,nd6ck88,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",hardware,2025-09-08 23:51:40,7
Intel,nd5oln7,Try Mechwarrior 5: Clans on high and say there are no problems again.,hardware,2025-09-08 21:36:04,-13
Intel,nd6bnfg,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",hardware,2025-09-08 23:46:18,14
Intel,nd89f2e,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,hardware,2025-09-09 08:05:10,11
Intel,nd6cup4,"It doesn't even run, it crashes left and right on an Arc.",hardware,2025-09-08 23:53:20,-5
Intel,nd6drxa,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",hardware,2025-09-08 23:58:41,3
Intel,nd6en4h,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",hardware,2025-09-09 00:03:40,-1
Intel,nc8cvxo,SR-IOV at that price. Who cares about anything else.,hardware,2025-09-03 17:55:15,164
Intel,nc9qxn7,Intel would be stupid to axe there Graphic card division if this proves to be successful.,hardware,2025-09-03 22:00:54,92
Intel,nc9seh0,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",hardware,2025-09-03 22:08:49,19
Intel,ncc2zjm,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,hardware,2025-09-04 07:35:40,15
Intel,nc98dp9,"This is exciting, definitely looking forward to the b60 as well.",hardware,2025-09-03 20:28:14,24
Intel,nc8kczs,"Obligatory ""Intel is exiting the GPU business any moment now"".",hardware,2025-09-03 18:32:12,83
Intel,nc8gp00,how hard are tehse to actually buy?,hardware,2025-09-03 18:14:07,17
Intel,ncaibo5,"Buying one, this is impressive",hardware,2025-09-04 00:36:23,14
Intel,nicvml9,"if compared by price to performance ratio,  ARC B50 is slower than RTX 5060 in terms of price and performance",hardware,2025-10-08 02:29:55,1
Intel,nca78up,Its better than a 1.5 year old bottom of the range card....well done i guess.,hardware,2025-09-03 23:32:04,-23
Intel,nca1tmp,Better than NVIDIA? lol .... oooookay,hardware,2025-09-03 23:01:43,-31
Intel,nc8e96z,"Haven't seen the video, but I'm already buying one if that's the case",hardware,2025-09-03 18:01:47,50
Intel,ncafc02,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,hardware,2025-09-04 00:18:35,33
Intel,ncbdpjm,Intel’s “MOAAAAAR CORES” in the GPU space???,hardware,2025-09-04 03:54:14,8
Intel,ncde8b8,what is that? SR-IOV?,hardware,2025-09-04 13:38:29,6
Intel,nccuj5h,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,hardware,2025-09-04 11:42:43,3
Intel,ncbojo9,16 GB VRAM too,hardware,2025-09-04 05:20:40,4
Intel,nc9ulp7,They will eventually axe it.,hardware,2025-09-03 22:20:53,20
Intel,nccun4t,Instead of axing it maybe spin it off like AMD did with Global Foundries?,hardware,2025-09-04 11:43:28,3
Intel,ncbkppb,And if it isn’t successful?,hardware,2025-09-04 04:48:20,1
Intel,ncbs3pt,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,hardware,2025-09-04 05:52:01,10
Intel,nc9di83,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",hardware,2025-09-03 20:52:30,15
Intel,nc95it5,I think it would be really stupid for them to do so.,hardware,2025-09-03 20:14:44,44
Intel,nc8hjt4,You can preorder from newegg now. They ship later this month.,hardware,2025-09-03 18:18:26,34
Intel,nc8xqpv,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",hardware,2025-09-03 19:37:13,12
Intel,nce6pwg,Same. I put my preorder in. Plan to put it into one of my sff builds.,hardware,2025-09-04 15:55:20,3
Intel,ndhamhf,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,hardware,2025-09-10 17:12:23,0
Intel,ncdy79x,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,hardware,2025-09-04 15:15:31,11
Intel,ncdy1t2,It quite literally is. Watch the fucking video.,hardware,2025-09-04 15:14:51,11
Intel,nc8xenf,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",hardware,2025-09-03 19:35:34,31
Intel,ncc2mdm,What does AMD have in this product segment?,hardware,2025-09-04 07:31:59,3
Intel,nd8jouo,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 09:50:11,0
Intel,nd8rcrq,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 10:56:38,0
Intel,ncdf0ln,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",hardware,2025-09-04 13:42:39,12
Intel,nccz58k,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,hardware,2025-09-04 12:12:40,32
Intel,ncegs0k,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,hardware,2025-09-04 16:42:52,3
Intel,ncenqdx,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,hardware,2025-09-04 17:15:36,3
Intel,nc9xu5x,Sadly Intel has a recent history of making poor life choices.,hardware,2025-09-03 22:39:04,55
Intel,ncaqsmf,"Maybe it's just me, but this reads as AI generated.",hardware,2025-09-04 01:26:28,13
Intel,ncar7n9,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",hardware,2025-09-04 01:28:53,2
Intel,nce2wy8,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",hardware,2025-09-04 15:37:44,-17
Intel,nceqap4,Radeon Pro V710 and you can't even buy it retail.,hardware,2025-09-04 17:27:25,10
Intel,ncdfanm,thanks 🙏,hardware,2025-09-04 13:44:06,2
Intel,ncd1hnr,Because intel shareholders are super short sighted.,hardware,2025-09-04 12:26:50,33
Intel,nc9zg8o,this comment is the weirdest version of 'corporations are people' that i've encountered,hardware,2025-09-03 22:48:13,14
Intel,ncb4jkb,Lmao seriously the formatting and the amount of bolded words just screams AI,hardware,2025-09-04 02:50:42,7
Intel,ncar9b0,It's AI generated in your mind,hardware,2025-09-04 01:29:09,-8
Intel,ncaz7h1,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",hardware,2025-09-04 02:16:51,2
Intel,ncee75r,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,hardware,2025-09-04 16:30:17,14
Intel,ncel88g,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,hardware,2025-09-04 17:03:58,8
Intel,ncemu0g,"Yes, compare an Arc Pro to a GeForce, totally the same market.",hardware,2025-09-04 17:11:26,6
Intel,nchqdw5,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",hardware,2025-09-05 03:02:19,1
Intel,ncdxuek,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",hardware,2025-09-04 15:13:52,11
Intel,nca5gwf,The weirdest version was Citizens United,hardware,2025-09-03 23:22:10,21
Intel,ncbz15z,Did you not figure out why they’re bolded?,hardware,2025-09-04 06:56:47,2
Intel,nci2bk5,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,hardware,2025-09-05 04:23:02,4
Intel,ncifqdy,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",hardware,2025-09-05 06:14:10,9
Intel,ncbjmt2,"if corps are people, they should be allowed to vote, right ?",hardware,2025-09-04 04:39:39,2
Intel,ncbjx8i,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",hardware,2025-09-04 04:41:58,6
Intel,ncd1vg1,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",hardware,2025-09-04 12:29:05,4
Intel,ner1h1d,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",hardware,2025-09-17 18:36:40,53
Intel,ner49pn,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",hardware,2025-09-17 18:50:20,75
Intel,ner8xmt,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),hardware,2025-09-17 19:12:31,17
Intel,nerwnif,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,hardware,2025-09-17 21:04:08,37
Intel,ner54hk,6 wide e core holy shit,hardware,2025-09-17 18:54:23,28
Intel,neqzzpm,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",hardware,2025-09-17 18:29:21,40
Intel,neugekl,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",hardware,2025-09-18 06:51:14,5
Intel,netnyfa,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",hardware,2025-09-18 03:04:06,9
Intel,newpjmq,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",hardware,2025-09-18 15:59:37,4
Intel,nes030p,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",hardware,2025-09-17 21:21:03,10
Intel,nergppp,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",hardware,2025-09-17 19:49:30,17
Intel,nf0sz1w,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),hardware,2025-09-19 05:27:21,2
Intel,nerudqn,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",hardware,2025-09-17 20:53:36,1
Intel,nezl76m,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,hardware,2025-09-19 00:45:37,1
Intel,nf76uxj,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,hardware,2025-09-20 05:23:42,1
Intel,nftq0t1,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",hardware,2025-09-23 19:51:55,1
Intel,ng9npz2,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,hardware,2025-09-26 07:28:53,1
Intel,njvb52i,"The snapdragon 8 elite gen 5 still beats it though , and handily at that. Unless I am wrong",hardware,2025-10-16 21:51:22,1
Intel,nerrr9i,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",hardware,2025-09-17 20:41:23,-4
Intel,nerpzuu,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",hardware,2025-09-17 20:33:10,-9
Intel,nfc55oq,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",hardware,2025-09-21 00:14:10,-1
Intel,neqz076,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:24:39,-10
Intel,neqyojo,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:23:10,-14
Intel,ner4der,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",hardware,2025-09-17 18:50:49,25
Intel,netwfkn,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,hardware,2025-09-18 04:02:47,17
Intel,neu40ew,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,hardware,2025-09-18 05:00:56,7
Intel,nfntq8k,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,hardware,2025-09-22 20:34:30,2
Intel,ner599v,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",hardware,2025-09-17 18:55:01,25
Intel,ner97f2,4 minutes ago. Damn. I should have waited. I'll post it!,hardware,2025-09-17 19:13:49,14
Intel,nerym62,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",hardware,2025-09-17 21:13:39,22
Intel,ner5ngo,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",hardware,2025-09-17 18:56:53,20
Intel,nesz0ua,What does 6 wide mean? What units?,hardware,2025-09-18 00:36:10,1
Intel,nesjazw,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",hardware,2025-09-17 23:06:05,9
Intel,ner9dq0,>	beating out even AMD  Was that one really surprising?,hardware,2025-09-17 19:14:39,2
Intel,nes24jj,Is that Metal vs Optix or Metal vs Cuda?,hardware,2025-09-17 21:31:31,1
Intel,neumrwz,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",hardware,2025-09-18 07:52:53,8
Intel,neu4z8x,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,hardware,2025-09-18 05:08:55,5
Intel,newyqf1,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,hardware,2025-09-18 16:43:46,4
Intel,nfd8klj,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",hardware,2025-09-21 04:40:04,2
Intel,netw9xh,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,hardware,2025-09-18 04:01:38,5
Intel,nerkv5x,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",hardware,2025-09-17 20:09:09,17
Intel,nf0tzxz,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",hardware,2025-09-19 05:35:43,1
Intel,nery3xl,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,hardware,2025-09-17 21:11:11,16
Intel,nezsmw1,There's a guy on twitter who does the microbenchmarking for them.,hardware,2025-09-19 01:28:21,2
Intel,nf79l31,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",hardware,2025-09-20 05:47:18,1
Intel,nfd8co2,With most GPU tasks you are never ALU limited.,hardware,2025-09-21 04:38:19,1
Intel,nl1tnnh,"8 elite gen 5 loses in ST perf, only matches in MT perf at same power, wins slightly in GPU perf, loses in GPU RT perf. It doesn't beat it handily by any metric.",hardware,2025-10-24 00:05:07,1
Intel,nerth3n,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",hardware,2025-09-17 20:49:22,17
Intel,netxg4n,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,hardware,2025-09-18 04:10:12,4
Intel,nerrcls,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",hardware,2025-09-17 20:39:28,15
Intel,neu4jji,in the end what matters is the sicion you can buy so you can compare them.,hardware,2025-09-18 05:05:19,4
Intel,nf0v0x1,Please tell us more about how we can never compare AMD vs Intel chips by your logic,hardware,2025-09-19 05:44:15,2
Intel,nfcfd2t,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",hardware,2025-09-21 01:17:53,1
Intel,nf0v37m,Bad bot,hardware,2025-09-19 05:44:47,1
Intel,ner7ckc,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",hardware,2025-09-17 19:04:59,7
Intel,nernd6v,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",hardware,2025-09-17 20:20:52,7
Intel,nhafpsx,>Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp  No,hardware,2025-10-02 01:16:15,1
Intel,nexnqt0,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",hardware,2025-09-18 18:41:54,5
Intel,ner8bhp,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",hardware,2025-09-17 19:09:33,8
Intel,nes3cj4,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",hardware,2025-09-17 21:37:52,17
Intel,neu4a08,That would be amazing. Id love to see them put some hbm there too,hardware,2025-09-18 05:03:07,1
Intel,neuqao2,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,hardware,2025-09-18 08:28:29,1
Intel,nfjr9p6,The m5 ultra should be on par with a 4090?,hardware,2025-09-22 04:50:52,1
Intel,nerh62x,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),hardware,2025-09-17 19:51:36,12
Intel,nevbky1,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",hardware,2025-09-18 11:35:22,5
Intel,nesz8nh,The decoder,hardware,2025-09-18 00:37:26,10
Intel,neuonvq,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",hardware,2025-09-18 08:11:51,13
Intel,ner9wud,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",hardware,2025-09-17 19:17:13,22
Intel,nes2dxt,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",hardware,2025-09-17 21:32:51,7
Intel,neuej06,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",hardware,2025-09-18 06:33:54,1
Intel,newy5n1,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,hardware,2025-09-18 16:41:01,1
Intel,nexio2d,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",hardware,2025-09-18 18:17:21,6
Intel,ngmjqfa,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,hardware,2025-09-28 10:08:16,1
Intel,nez7dv6,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",hardware,2025-09-18 23:23:49,2
Intel,nf0vdfo,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,hardware,2025-09-19 05:47:13,2
Intel,nes028x,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",hardware,2025-09-17 21:20:56,-2
Intel,nf7bmfd,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,hardware,2025-09-20 06:05:07,1
Intel,nfdgjzr,Maybe the improvement is from the new matmul units?,hardware,2025-09-21 05:48:36,1
Intel,nerzs8z,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",hardware,2025-09-17 21:19:32,5
Intel,nesrzwb,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",hardware,2025-09-17 23:55:18,1
Intel,nery8a6,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",hardware,2025-09-17 21:11:46,1
Intel,neu0m6n,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",hardware,2025-09-18 04:34:04,1
Intel,nfcgk7w,I'm sorry you had to take time away from ripping off others' content to correct my mistake,hardware,2025-09-21 01:25:24,-1
Intel,ner89fq,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",hardware,2025-09-17 19:09:17,7
Intel,nerolok,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",hardware,2025-09-17 20:26:36,10
Intel,nezdr68,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,hardware,2025-09-19 00:01:23,7
Intel,nes52e4,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,hardware,2025-09-17 21:46:45,16
Intel,ner8qa3,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",hardware,2025-09-17 19:11:31,17
Intel,neu2ajk,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,hardware,2025-09-18 04:47:07,3
Intel,neskg6l,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,hardware,2025-09-17 23:12:34,5
Intel,neun1xa,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,hardware,2025-09-18 07:55:36,4
Intel,neuqz81,The M4 Max already uses a 512 bit bus. Does it not?,hardware,2025-09-18 08:35:23,5
Intel,neri14i,Its technically not a true 9 wide core. I think its 3+3+3.,hardware,2025-09-17 19:55:40,15
Intel,nes6z1y,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,hardware,2025-09-17 21:56:51,12
Intel,neu4ctl,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,hardware,2025-09-18 05:03:46,2
Intel,nerdt8m,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",hardware,2025-09-17 19:35:44,1
Intel,neuqhpy,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",hardware,2025-09-18 08:30:29,6
Intel,ney7oxo,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",hardware,2025-09-18 20:16:47,5
Intel,ngmxbr9,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",hardware,2025-09-28 12:07:27,2
Intel,nes14p7,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,hardware,2025-09-17 21:26:24,8
Intel,nf7c0uo,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),hardware,2025-09-20 06:08:45,1
Intel,nes4cv8,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",hardware,2025-09-17 21:43:06,9
Intel,nes0tlj,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,hardware,2025-09-17 21:24:50,1
Intel,nes018z,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,hardware,2025-09-17 21:20:48,1
Intel,nerz3p1,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",hardware,2025-09-17 21:16:05,4
Intel,nfcj3et,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,hardware,2025-09-21 01:41:28,2
Intel,nerbuxh,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",hardware,2025-09-17 19:26:30,7
Intel,nerrlad,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",hardware,2025-09-17 20:40:36,4
Intel,nf8dztb,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",hardware,2025-09-20 11:59:43,3
Intel,neu482t,It does but the Macs are limited in other ways (memory speed among other things),hardware,2025-09-18 05:02:41,3
Intel,netuj31,It wasn't for a lack of trying.,hardware,2025-09-18 03:49:13,7
Intel,nf2fm06,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,hardware,2025-09-19 13:24:45,2
Intel,neuu5xp,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",hardware,2025-09-18 09:07:21,2
Intel,netvlsr,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",hardware,2025-09-18 03:56:47,4
Intel,nerhwya,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,hardware,2025-09-17 19:55:07,1
Intel,newfwdv,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",hardware,2025-09-18 15:14:05,2
Intel,ngnf1x5,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,hardware,2025-09-28 14:00:56,2
Intel,nesav03,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",hardware,2025-09-17 22:18:15,5
Intel,nesu2qs,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",hardware,2025-09-18 00:07:30,3
Intel,netwip5,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,hardware,2025-09-18 04:03:25,2
Intel,nes5hmu,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,hardware,2025-09-17 21:48:58,3
Intel,nes2dlk,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",hardware,2025-09-17 21:32:48,1
Intel,nes1r2v,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",hardware,2025-09-17 21:29:35,1
Intel,nes2xxk,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",hardware,2025-09-17 21:35:43,2
Intel,nfcjm81,You copied and pasted someone else's data and now you're acting like a hero,hardware,2025-09-21 01:44:49,-1
Intel,net2bpw,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",hardware,2025-09-18 00:55:13,6
Intel,neru5qx,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",hardware,2025-09-17 20:52:33,4
Intel,nf8k821,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,hardware,2025-09-20 12:42:15,1
Intel,neu5yj1,one will think the massive vram capacity just override the disadvantages.,hardware,2025-09-18 05:17:08,1
Intel,ney6vfr,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",hardware,2025-09-18 20:12:58,4
Intel,ngpzxjn,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",hardware,2025-09-28 21:29:23,2
Intel,nevepm7,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",hardware,2025-09-18 11:56:17,1
Intel,nes29nv,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",hardware,2025-09-17 21:32:15,3
Intel,netbw6q,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,hardware,2025-09-18 01:50:19,0
Intel,nfckcjc,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",hardware,2025-09-21 01:49:24,2
Intel,neuiclx,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",hardware,2025-09-18 07:09:23,2
Intel,nescl7s,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",hardware,2025-09-17 22:28:00,3
Intel,nf8puxr,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",hardware,2025-09-20 13:16:40,2
Intel,nfb2xd7,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",hardware,2025-09-20 20:35:11,1
Intel,neu6scx,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,hardware,2025-09-18 05:24:15,5
Intel,nf3x91p,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",hardware,2025-09-19 17:45:47,1
Intel,ngq1nfa,Impressive and thanks for enlightening me.,hardware,2025-09-28 21:38:03,1
Intel,nes2pt3,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,hardware,2025-09-17 21:34:33,3
Intel,nete2h5,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",hardware,2025-09-18 02:02:53,1
Intel,nfckhxr,I posted links to reviews. I didn't copy the entire data for my own post,hardware,2025-09-21 01:50:21,0
Intel,net132z,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",hardware,2025-09-18 00:48:07,-1
Intel,nfb1vqa,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",hardware,2025-09-20 20:29:44,1
Intel,nfbgkad,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",hardware,2025-09-20 21:49:05,1
Intel,nfd97ff,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",hardware,2025-09-21 04:45:15,1
Intel,nfclcjn,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",hardware,2025-09-21 01:55:46,2
Intel,nev2rja,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",hardware,2025-09-18 10:28:02,5
Intel,nfcmlir,Enjoy your scratch-prone iPhone!,hardware,2025-09-21 02:03:39,1
Intel,ng0a1u9,Your one brain cell is trying really hard. See if you can wake up its buddy.,hardware,2025-09-24 20:06:41,1
Intel,noclqcf,"Arc B580 is a good budget card and leagues better then a 3050, just note it will probably struggle on high settings on 1440p. On tomshardware it scored 42.6 FPS on average for 1440p Ultra Rast which included God of War Ragnarök in there game selection.   If you want to play high settings consistently without frame gen at 1440p, I would spend the extra $100 for the 9060XT 16GB. It scored 40% higher then the B580, and cost 40% more. So you're getting same value wise, for price to performance 1440p Ultra.  Also note, Intel GPU's are still new and have the most vary performance with time,  so I would actually imagine the B580 is a bit better then the recorded benchmark I'm refrencing from.    [https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025)",buildapc,2025-11-11 21:27:29,2
Intel,nocm5b2,"The B580 will about 90% faster (assuming no CPU bottleneck). But, if you can afford it, the Radeon RX 9060 XT 16GB is about 160% faster and can usually be found for about $350 in the US.",buildapc,2025-11-11 21:29:38,1
Intel,nocvm6w,"Depends on pricing, the B580 is a decent deal in the UK, you can get it here for £200 and a choice of a £60 game out of four options including BF6 so a no brainer for budget builders. Anymore than £200 or the dollar equivalent id just jump to a 5060 or better yet 9060XT 16gb. There will be some CPU bottleneck, but you can get alot of performance out of itself still.",buildapc,2025-11-11 22:18:42,1
Intel,np47jm8,b580 has no problem in 1080p gamings,buildapc,2025-11-16 08:26:17,1
Intel,np4hv89,thank u bro this was all i needed to know🙏🏿🙏🏿,buildapc,2025-11-16 10:11:16,1
Intel,npcbr4y,I don't wish to be the bearer of bad news but without some really good deals it is going to be tough.   A ryzen 3600(x) or 5600(x) used and a used B450 (Or other AM4 board) might be able to be found locally around $100.,buildapc,2025-11-17 16:48:54,2
Intel,npcd2ju,"I wouldn’t bother with the 2600x, because by the next series of game releases you’re upgrading it all again. The 2600x is also way more memory sensitive than the 5600, so the 2400mhz ram wouldn’t really influence performance.  If possible, I’d look for a used 5600/b550 board, but b450 will also work.",buildapc,2025-11-17 16:55:21,1
Intel,npd2it0,"Once you get into that price range and going used, you're kind of just looking for good deals than exact components.  Those games are very CPU demanding so try to get the best CPU you can.",buildapc,2025-11-17 18:58:55,1
Intel,npchkhb,All other components are good for arc raiders in mid/high quality or bf in low quality? Even cooler and power supply?,buildapc,2025-11-17 17:17:35,1
Intel,npcr63v,"Arc raiders, yes, bf6, no, your gpu is just below the minimum spec, but maybe if you play at 720p it will work.",buildapc,2025-11-17 18:04:29,1
Intel,npbyhif,Sounds like a power supply issue,buildapc,2025-11-17 15:43:38,3
Intel,npbxip4,I had something similar for a build I helped a buddy with. Very similar behavior and it ended up being the PSU (purchased new from BB for the build). Would launch furmark and crash everytime.,buildapc,2025-11-17 15:38:49,1
Intel,npc11l1,Power supply,buildapc,2025-11-17 15:56:21,1
Intel,npdpfl6,Needs more Powah!,buildapc,2025-11-17 20:54:04,1
Intel,noqgm2l,Have you checked the temperatures of your CPU and GPU?,buildapc,2025-11-14 01:27:57,1
Intel,noqh31w,I was running NZXT and the thermals were within normal range. I stopped running it to cut down on CPU load,buildapc,2025-11-14 01:30:46,1
Intel,noqhinw,How long ago did you stop running it? Was it before or after the crashing started?,buildapc,2025-11-14 01:33:23,1
Intel,noqhujs,After it started crashing,buildapc,2025-11-14 01:35:22,1
Intel,noed9yz,"sn5000 is a better drive for cheaper, msi A550BN 40 more watts for the same price, 5600 comes with stock cooler,  [https://ca.pcpartpicker.com/list/ktWhVF](https://ca.pcpartpicker.com/list/ktWhVF)",buildapc,2025-11-12 03:35:13,2
Intel,noehwhj,"So in the same budget I got a 9060xt and threw it in https://www.newegg.com/YAWYORE-Gaming-Desktop-PC/BrandSubCat/ID-225264-3742  This pre built that is pretty much plug and play, the ram isn't fantastic is the biggest downside but is right in your budget and can do 1440p, if you get the 9060xt on sale and buy some Newegg gift cards for the free cash you can cut down your cost as well.  So looking at this I didn't take into account your monitor cost, this would end up going a little over but   if you get the 9060xt on sale it ends up being <$100 Canadian difference, with a Newegg gift card offsetting a large chunk of that",buildapc,2025-11-12 04:06:36,1
Intel,non26v3,"Are you currently experiencing significant CPU bottlenecking or plan on upgrading the GPU in the near future? If the answer to both questions is no, upgrading everything but the GPU is not going to significantly improve gaming performance, and it's kind of a waste of money since your gaming performance will still be limited by the 3070.  Since you do plan on a future GPU upgrade and/or are CPU bottlenecked significantly currently, the upgrade makes more sense, but 3 years is a long time to wait on a gpu upgrade, especially if you're not currently CPU bound.",buildapc,2025-11-13 14:54:44,1
Intel,nonbutt,I’m hard-capped around 70-80 FPS.   In-game/overlay telemetry: FPS: ~80 GPU: ~49% usage CPU: ~93% usage  Based on my tests I am experiencing CPU bottlebecking.  Have updated the post :-)..,buildapc,2025-11-13 15:43:44,1
Intel,nonemeo,"It certainly does seem like you're CPU limited, at least for that game.   A cpu/mobo/ram upgrade wouldn't give you that much more performance, though, since the GPU is still at 93% usage, but it would likely improve 1% lows any any stuttering if you're experiencing it currently.",buildapc,2025-11-13 15:57:06,1
Intel,noni8j3,Can You put some more words into why such an upgrade wouldn’t fix it? My GPU usage is around 49% as opposed to my CPU around 93%,buildapc,2025-11-13 16:14:43,1
Intel,nonj23n,"Sorry I swapped CPU and GPU usage. You do seem to have a significant CPU bottleneck. Upgrading the CPU alone could significantly improve performance for that game, since the GPU is only at 49% currently, assuming nothing else is limiting it's performance such as Vsync or a set FPS limit.   [https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html](https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html)",buildapc,2025-11-13 16:18:43,2
Intel,nn549eq,"Get the b580, 12gb vram is more futureproof. Their drivers are getting better but if you want stability then go for 3060",buildapc,2025-11-04 23:03:03,1
Intel,nn4zgy5,"I strongly recommend NOT getting an 8GB GPU brand new in 2025.   I was the same too years ago when I gotten a GTX 1050 laptop with 2GB, thinking I don't need 4GB because I was a ""lightweight gamer"". I was dead wrong. My interests changed, my standards changed, the world changed. I don't regret getting a 1050 because that was all I could afford at that time, but I absolutely regretted not getting the 4GB variant.  Back to your question, if you're interested in older games and are concerned of B580 driver issues, get the 3060 12GB instead. It is quite a bit slower, but it can run just about anything, from ancient games like Mirror's Edge (where PhysX support really shows) all the way to newer VRAM-hungry titles like Cyberpunk and CK3 which will easily exceed 8GB VRAM even at just 1080p.",buildapc,2025-11-04 22:37:32,0
Intel,nn53qhe,"Thank you for your answer, will consider RTX 3060 12gb also, goes at same price as ARC and cheaper than Rx 9060 8g",buildapc,2025-11-04 23:00:10,1
Intel,no5qxw5,https://www.newegg.com/sgpc-mini-itx-k-lite-series-spcc-tempered-glass-cases-white-k77-lite/p/2AM-05KM-00009  There are many other SFFPC cases with handles.,buildapc,2025-11-10 19:41:17,1
Intel,no5th4b,"Much Appreciated! Some cool cases there, thank you.   [https://www.newegg.com/pccooler-mini-itx-spcc-steel-cases-white-k101-mesh-wh-3f/p/2AM-05KE-00012?Item=9SIAHCTKGH6658](https://www.newegg.com/pccooler-mini-itx-spcc-steel-cases-white-k101-mesh-wh-3f/p/2AM-05KE-00012?Item=9SIAHCTKGH6658)  This one look really cool. Not sure how I'd fit a full size PSU in there though, lol  These",buildapc,2025-11-10 19:53:59,1
Intel,no5uidb,> Not sure how I'd fit a full size PSU in there though  You don't....  > PSU Supports: 100~125mm SFX,buildapc,2025-11-10 19:59:12,1
Intel,np5gc6j,Probably 2070 super for DLSS,buildapc,2025-11-16 14:45:21,3
Intel,np5gnqc,I'd get the 3060 Ti.,buildapc,2025-11-16 14:47:15,1
Intel,np5ljka,"Strictly price/performance, the RTX 2060 is the best value GPU. But there’s other important areas to consider.  Firstly, I wouldn’t bother with the radeon cards in your position. At this tier of GPU you’ll need the upscaling, and RDNA 1/2 are stuck on FSR 3. They also won’t be getting any new features from now on either.  I wouldn’t look at Intel cards either. They have improved, but still have some work to do regarding drivers.  So between your Nvidia options, I recommend the RTX 3060 12gb. It’s decently fast, has 12gb of VRAM, and has access to the best upscaler. It will also be supported longer than most of the cards here.",buildapc,2025-11-16 15:15:27,1
Intel,np5qb3c,"I’ve found gputiful super useful for comparing all those options. If you're looking for the best price/performance, check out their benchmarks for the 2060 vs the 6600 or 5700 XT   good luck!",buildapc,2025-11-16 15:41:22,1
Intel,np5sydc,"For price to performance, a used Rx 6600 is quite literally known for that. I’m not sure if it was this subreddit but if you search on google there was a Reddit post where someone made a chart of the best FPS per Dollar and in there the 6600 and 6650xt from the used market were one of the most efficient.",buildapc,2025-11-16 15:55:26,1
Intel,np5hecv,what is dlss and do the other cards not have it?,buildapc,2025-11-16 14:51:37,1
Intel,np5hajx,does it make a big difference over the card or could i do a cheaper one too on 1080p?,buildapc,2025-11-16 14:50:59,1
Intel,np5mf3m,but the 3060 ti 8gb has better performance,buildapc,2025-11-16 15:20:17,1
Intel,np5lik8,"Nvidia feature ""Deep learning super sampling"", basically using AI, a lower resolution is scaled to higher one, giving you more performance with minimal quality loss. It was first introduced on the RTX 2000 series.",buildapc,2025-11-16 15:15:18,1
Intel,npa91aa,I don't understand your question.,buildapc,2025-11-17 07:53:11,1
Intel,np705df,"You’re the one who said it’s pretty expensive. My recommendation was based on that, so I thought the extra €40 would matter to you. If that’s not the case anymore then go ahead.",buildapc,2025-11-16 19:34:29,1
Intel,noa1y7v,This is a 1440p build.,buildapc,2025-11-11 13:39:05,10
Intel,noa0mkd,If your budget is enough just go with that.,buildapc,2025-11-11 13:31:01,4
Intel,noa7z2t,Cooler comes with paste you don’t need to buy extra.  The extra fans may also be unnecessary.,buildapc,2025-11-11 14:14:55,4
Intel,noasq0g,My build is Ryzen 5700x and rtx 3080. I get 120 fps at 1440p high settings with dlss.,buildapc,2025-11-11 16:04:08,3
Intel,noarw7g,"This is an extremely capable 1440p build even with ray tracing. In Arc Raiders on 1080p you are going to be severely cpu bound, but I wouldn’t expect it to drop under 120-144 fps. You could probably max out Arc Raiders at 4k and get 80 fps with this build.",buildapc,2025-11-11 16:00:09,2
Intel,noayq9k,"This is a monster at 1080p, definitely upgrade to 1440p when your budget allows, I wouldn’t cheap out for a bad 1440p monitor, save for a nice one.  Have fun with the new build!",buildapc,2025-11-11 16:33:03,2
Intel,noi6in8,"cpu bottleneck, but you'll run at 130+ all the time even on 1440p high settings.",buildapc,2025-11-12 19:12:38,2
Intel,no9noha,I run a lower end card and cpu and run 2K at 75 fps on arc. That’s more than enough for 1080p you could easily hit over 100 with that build.,buildapc,2025-11-11 12:00:45,1
Intel,no9sr4y,Ryzen 7600x + Rtx 4070 = All settings HIGH + DLLS (130fps with drops to 110fps). Smoooth game.,buildapc,2025-11-11 12:39:33,1
Intel,no9vmpt,"Ryzen 5 5600, 6650xt, 32gb ram, native 1080p mid to high settings gets 120fps. The game runs really well.",buildapc,2025-11-11 12:59:18,1
Intel,no9xvmi,"Arc Raiders is really really well optimised compared to most other (non-indie) modern games.  If you're looking for the best value right now specifically for Arc Raiders and not concerned about future games, get the cheapest 16gb 5060ti or 9060xt.  9070xt is what I'm running (with a 7800x3d) and I almost never drop below 150fps on 1440p.",buildapc,2025-11-11 13:13:54,1
Intel,no9ycyd,If this fits your budget you will probably be able to do better at 1080p. I have a RTX 2070 and run at 1080p medium settings 100+fps,buildapc,2025-11-11 13:16:57,1
Intel,noai4er,"If you're happy to use FSR4 which I don't see why you wouldn't this will be 120FPS+ on epic settings with raytracing on in ARC, at 1440p.  Source, have a 5800x3d and 9070XT and that's the performance I get. Rock solid with no dips.",buildapc,2025-11-11 15:11:29,1
Intel,noar7zi,for arc raiders that's a 4K build.,buildapc,2025-11-11 15:56:55,1
Intel,noa25jg,"Yeah for the future I want to go 1440p but can’t afford that as well currently, so I am wondering if it is good for 1080p before I make the jump to 1440p in a few months? I am terrible with all tech lol",buildapc,2025-11-11 13:40:19,1
Intel,noa8b4x,Someone told me to get fans lol not sure why just added them,buildapc,2025-11-11 14:16:54,1
Intel,noaav9r,Have you got any ideas for the case cause I’m lost lol I took a look and had no clue,buildapc,2025-11-11 14:31:33,1
Intel,noat4gy,"Ahh ok, nice at least I know it will be able to cope then. Sorry completely useless with pcs but I see how much cheaper building them can be as my original old pc was a pre built and cost me roughly the same as this one lol",buildapc,2025-11-11 16:06:05,1
Intel,noasaj4,Yeah my plan is to upgrade to 1440p in the coming months but I just can’t afford that on top of a new build right now. So staying at 1080p going for medium to high graphics and decent fps is my goal for now.,buildapc,2025-11-11 16:02:04,1
Intel,noaz6c8,Cheers man really looking forward to it,buildapc,2025-11-11 16:35:12,2
Intel,noi93ue,Would look to upgrade cpu to x3d in future but it’s double the price rn of the 9600x,buildapc,2025-11-12 19:25:28,1
Intel,no9q5fk,"Thanks man, yeah im not looking for anything to look super fancy just looking more for the FPS boost and if extra graphics come with then lovely haha",buildapc,2025-11-11 12:20:13,2
Intel,no9tazx,Really! Ok that’s made me rethink the build,buildapc,2025-11-11 12:43:27,1
Intel,no9w71m,"Thanks for letting me know man, my current rig is an i5-9400f and a 2060 and it just struggles way too much. It was a pre build I bought like 5 years ago",buildapc,2025-11-11 13:03:02,2
Intel,no9y564,Would you say the 9600x is worth it I’ve heard mixed opinions,buildapc,2025-11-11 13:15:35,2
Intel,no9yoco,You think this would be good for 1080p and a future upgrade to 1440p,buildapc,2025-11-11 13:18:58,1
Intel,noalxpt,"Yeah I’m not part of this nvidia or amd fan club I just want to get the most from my money lol, I’m no enthusiast I just want to play games well for a good price",buildapc,2025-11-11 15:30:58,1
Intel,noaregg,for reference i play arc raiders at 4k medium dlss quality on a 3070 and typically get 80-110 fps.,buildapc,2025-11-11 15:57:47,1
Intel,noav4p4,This will be high or max settings for EVERY game on 1080p,buildapc,2025-11-11 16:15:40,3
Intel,noa6lfb,"This is overkill for 1080p, so yes, you'll be all set for 1440p.",buildapc,2025-11-11 14:06:40,2
Intel,noatdr5,"The case you picked is fine, the two fans on the side are 'reverse blade' so that's where the intake/fresh air comes in.  You can forego the extra fans for now.  For not much more you can get a 2 TB SSD:  https://uk.pcpartpicker.com/product/VYqrxr/kioxia-exceria-plus-g3-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-lsd10z002tg8",buildapc,2025-11-11 16:07:19,1
Intel,noattrc,Get the paste just in case imo. It's only 7 bucks and I liked it better than the one that came with the cooler.,buildapc,2025-11-11 16:09:26,0
Intel,noasogz,"Honestly I would max it out at 1080p with these specs to lessen the load on the cpu, could even end up with higher fps than on medium settings",buildapc,2025-11-11 16:03:56,2
Intel,noazpe5,"Also my buddy’s have been trying to convince me to play arc raiders, would you say it’s worth it? It looks pretty cool",buildapc,2025-11-11 16:37:47,1
Intel,noji0bj,"9600x is great, I'm using it on my main desktop. Ryzen 5 9600x, rx 6700xt, I get 140-180 fps on arc raiders. so you're good.",buildapc,2025-11-12 23:16:58,2
Intel,noa1oiy,"How much more is the 9600x? If it's not much more I'd go for it due to lower temps, being newer, and better bench marks.",buildapc,2025-11-11 13:37:27,1
Intel,no9znox,"The 9070xt is usually comparable to the 5070ti, so I'm pretty sure 1440p will utilize the card even better than 1080p with comfortable high settings.",buildapc,2025-11-11 13:25:01,1
Intel,noashqb,Ahh ok that’s good to know at least it means it can cope with the 1080p I throw at it lol. My current rig is struggling big time,buildapc,2025-11-11 16:03:02,1
Intel,noavbg4,Love to hear that tbh it’s a nice feeling after having to load every game on the lowest possible graphics lol,buildapc,2025-11-11 16:16:34,4
Intel,noatrad,"Ahh nice to know, cheers man",buildapc,2025-11-11 16:09:07,1
Intel,noau3l4,Too much paste is bad right? You need like the perfect amount I have heard. I also assume mixing paste is a no no,buildapc,2025-11-11 16:10:44,1
Intel,noasv80,See these are the things I don’t know about and I’m glad people can bring that to light for me. Completely unaware this would be the case so thank you for letting me know!,buildapc,2025-11-11 16:04:50,1
Intel,nob0cxf,I mean at the price point of £30 it to me is a no brainer. Half the price of most games these days and such a good community. Never had interest in playing extraction shooters until this came out and can honestly say I’m hooked. I have the feeling of being a kid again just wanting to get home from work and play all evening (sad I know),buildapc,2025-11-11 16:40:58,1
Intel,noa1rlf,It’s about £30 more than the 7600x,buildapc,2025-11-11 13:37:59,2
Intel,no9zyr2,Ok I am defo gonna want to upgrade to 1440p but just not in the budget currently with the new build,buildapc,2025-11-11 13:26:56,1
Intel,noau623,what components are in your rig right now?,buildapc,2025-11-11 16:11:03,1
Intel,noayz1t,"This gpu can do some proper 4k with upscaling. So you are good for 1440p. Also Thermal paste comes with a cpu cooler, no need to buy it. Check how many fans you have with that case, i think it has some included.",buildapc,2025-11-11 16:34:15,3
Intel,noawdrc,"I painted my paste onto the CPU evenly. There's a few ways you can do it. Check YouTube for guides. GamerNexus did a video where he goes nuts with thermal paste and it still worked just fine. Although, you would never do that haha.",buildapc,2025-11-11 16:21:42,1
Intel,noazgwl,"I can also recommend checking out how much a 7800x3d costs in your area, where I’m from the difference between it and the 9600X you’ve chosen is probably about 100 GBP if I convert it. It will provide quite higher FPS in cpu bound scenarios (which on a 1080p screen with a 9070xt isnt going to be uncommon), but once u upgrade to 1440p the difference isnt going to be as noticeable. Just something else I thought I’d share",buildapc,2025-11-11 16:36:38,1
Intel,noa3234,If it's not breaking the bank for you then I'd do 9600x,buildapc,2025-11-11 13:45:42,2
Intel,noaupug,"I’m running a pre built i5-9400f and 2060. Stock cooler and one fan in the case, pretty sure the rest is pretty budget friendly parts even though it costed me like £1300 lol",buildapc,2025-11-11 16:13:41,1
Intel,nob00bp,"It jumps to about £350 for me so quite a large jump and would mean I’d have to scale some other bits back on the pc to fit it in the budget, unless you think that is something I should do?",buildapc,2025-11-11 16:39:15,1
Intel,noa3hbk,Thanks man,buildapc,2025-11-11 13:48:12,2
Intel,nob1imh,"Hey, don’t take my opinion as fact, I’m just a dude on the internet and its your money :D You should probably ask other people as well and do a bit of research yourself, but it really mainly comes down to the games you are going to be playing. If you are going to mainly be playing shooters, more esports focused games it could be well worth it to downgrade the gpu to a 9060 xt 16gb and get an x3d cpu. If you are going to be playing equal parts AAA single player games, equal parts multiplayer shooters, then in my opinion it’s probably not worth it. Like I said tho, just my thoughts   On another note 350 GBP is quite steep for a 7800x3d and is quite a bit more than what I can get it for (I’m in Europe, converted comes out to about 290 GBP). The 9600x is a mighty fine cpu tho, will do just fine",buildapc,2025-11-11 16:46:37,2
Intel,nob1pti,Cheers man I will look into it,buildapc,2025-11-11 16:47:35,1
Intel,nob2epz,"I was looking at the 7 series, I can see a Ryzen 5 7600x3d for £280 would that be a better cpu for occasional games like arc raiders but predominantly competitive shooters like valorant etc",buildapc,2025-11-11 16:51:00,1
Intel,nob2t89,Most places I can see talking about it say the X3D is superior for gaming at both 1080p and 1440p so may have to find a way to squeeze that in,buildapc,2025-11-11 16:53:00,1
Intel,noc4zka,"Yea the 7600x3d is quite similar to the 7800x3d its probably within 5-10% of it at most. Honestly you’ve balanced your build quite well in terms of price to performance, I can’t see anything immediately obvious that you can cut back on. Don’t get me wrong tho, the 9600X is also a damn fast cpu and will more than likely get you 300-400 fps in valorant, cs2 etc if you play at competitive settings",buildapc,2025-11-11 20:01:20,2
Intel,nouqezi,7800x3d   9800x3d if money isnt an issue,buildapc,2025-11-14 18:48:26,35
Intel,nouo21o,"I don’t really know anything about Arc Raiders, maybe it’s a really CPU intense game and worth the upgrade but for most games a CPU upgrade won’t help much at 4K",buildapc,2025-11-14 18:36:48,41
Intel,nous21v,Here you can see the 7600X3D benchmarked against the 9800X3D: https://gamersnexus.net/cpus/amds-silent-launch-ryzen-5-7600x3d-cpu-review-benchmarks-vs-7800x3d-5700x3d-9800x3d,buildapc,2025-11-14 18:56:29,5
Intel,now24d1,"Find a used 5700x3d or 5800x3d at a reasonable price if you can. I have a 5800x3d and arc raiders runs locked at 144fps constantly on my 5080 on max settings. The 5080 isn't that much faster than the 5070ti for arc raiders, which is actually pretty well optimized. It kind of sounds like you may have a separate issue than the cpu honestly.",buildapc,2025-11-14 23:00:25,3
Intel,nouz2dp,5070ti+ 9800x3d here my fps are great in arc raiders with this no problem at any time,buildapc,2025-11-14 19:32:14,2
Intel,nov50wi,"Check if your GPU is hitting 100% first. If it's not you have a CPU bottleneck.  After that I'd look at tuning up your 5800X+RAM. It's unlikely to push you over 120fps, but depending how untuned it is now, you might see 90-100.",buildapc,2025-11-14 20:03:12,1
Intel,novaxkp,"On Arc Raiders with 4080 I’m mostly locked 120 FPS 4k DLSS Q/B with 7800x3d, 7600x3d should be the same.  I’d get 7800x3d if it’s like within $50, 9800x3d is definitely heavy into diminishing returns",buildapc,2025-11-14 20:34:36,1
Intel,nove9az,"7800X3D oem \\tray edition . These going for same money as 7500\\7600x3d . I have very similar cpu to yours in laptop (7435hs) , recently sold 5800X3D PC . I can guarantee you,non x3d vs x3d is like a night vs day in games",buildapc,2025-11-14 20:52:12,1
Intel,novffl4,Why not just look up benchmarks on YouTube for the game you are playing. Try different or similar configurations to see what other people are getting at 4k.,buildapc,2025-11-14 20:58:23,1
Intel,novvv2b,I recently upgraded to a 9800X3D and 9070XT. I game at 1440p and get anywhere between 120 and 150 fps in Arc Raiders.  I’m not sure if it’s particularly cpu heavy but I also play Star Citizen which benefits from a chunkier CPU.,buildapc,2025-11-14 22:25:12,1
Intel,now093t,"I have made the switch, 5800x3d to 9800x3D on 1440p with a 7900xtx.The performance gains are noticeable.   But I would not sacrifice cores for it. Take the 9800x3d",buildapc,2025-11-14 22:49:40,1
Intel,now2qlj,"Here's a good comparison of the 5800x3D, 7800x3D, and 9800x3D for multiple games, OP.  11:18 is where he covers 4k.  [https://www.youtube.com/watch?v=Zi7LtyQVPdM&t=819s](https://www.youtube.com/watch?v=Zi7LtyQVPdM&t=819s)",buildapc,2025-11-14 23:04:03,1
Intel,nowgbn2,I got a 7800x3d for €250 shipped from aliexpress; works perfectly fine! Even have -30 pbo on all cores. Dunno what the situation is to get something similar in the states,buildapc,2025-11-15 00:28:53,1
Intel,nowp8nt,Could just wait for the 9850x3d,buildapc,2025-11-15 01:25:33,1
Intel,nozou6s,"Well technically you’ve already achieved your goal here haven’t you? You say you would really like to hit at least 120Hz locked and you had already said your monitor is an 120Hz display? Hence you’re golden.  What you meant to say is that you would really like to hit a locked 120fps, not 120Hz.   There’s a difference there - one is how much your GPU is basically able to produce on your monitor at any given time.   The other is the refresh rate of your monitor/how fast your monitor itself can refresh the image at any time.",buildapc,2025-11-15 15:34:40,1
Intel,nozwdmx,"I haven't seen my 9800x3d go above 60% usage except during loading screens, 1440p with 9070xt",buildapc,2025-11-15 16:14:10,1
Intel,np1ljl2,I play arc raiders with FG2x on my setup 5070ti r9 9900x3d and I get around 160-180 fps on native 4k,buildapc,2025-11-15 21:39:31,1
Intel,noup6wh,"Check your GPU utilization in games. If it's close to 100%, then a CPU upgrade isn't going to help you.",buildapc,2025-11-14 18:42:24,1
Intel,noup2iy,"Even with DLSS, I'd assume you may potentially benefit more from a GPU upgrade for the cost of AM5 platform upgrade playing at 4k, considering you're considering going up to 144Hz display (I'm assuming 4k?)",buildapc,2025-11-14 18:41:48,0
Intel,nov5n00,"350 plus tax vs 530 plus tax?  If you don't overclock, 7600x3d.  There's no reason to go 9800x3d unless you overclock and can see the cache related bottleneck.  With OC zen4 cache is still locked, while 9800x3d can go higher than 5.7ghz 3d vcache, and a lot of games, even when not CPU freq bottlenecked, is l3 cache bottlenecked",buildapc,2025-11-14 20:06:24,0
Intel,novla3r,"As is the case with probably 99.9% of all posts like this.   It's simple, and it comes down to ""if the difference in cost between the two options is not a deal breaker for you, why not?"".",buildapc,2025-11-14 21:28:51,11
Intel,now2aqg,"This is not correct, but gets thrown around a LOT.  CPU most definitely makes a difference in 4k.  Especially regarding 1% lows.  Avg FPS also increases.  This is one of the best examples showing multiple titles comparing 5800x3D, 7800x3D, and 9800x3D all at 4k.  CP2077 sees a 19.5% gain on 1% lows and 13% gain on avg FPS comparing the 5800x3D to 9800x3D.  [https://www.youtube.com/watch?v=Zi7LtyQVPdM&t=819s](https://www.youtube.com/watch?v=Zi7LtyQVPdM&t=819s)",buildapc,2025-11-14 23:01:28,32
Intel,nouoalh,"I mentioned in most games i’m using DLSS, so not really running anything in native 4k for the most part especially modern games",buildapc,2025-11-14 18:37:58,7
Intel,nov6dpa,I doubt OP is CPU limited in Arc.,buildapc,2025-11-14 20:10:20,0
Intel,now71oi,To be fair the 3d are much faster than standard 5800x. I can’t really find any at a reasonable price. But it would help me avoid having to upgrade the entire system.,buildapc,2025-11-14 23:29:53,1
Intel,nouz7gg,I think the 5800X is holding me back. Do you game in 4k?,buildapc,2025-11-14 19:32:57,1
Intel,novc6yb,"Same setup, arcraiders works fine on Epic settings at 4k, FPS is usually 110-140.  My monitor caps at 120 so im happy with it",buildapc,2025-11-14 20:41:17,1
Intel,nov8uw6,4k dlss balanced  80-90% GPU  70-80% CPU   4k dlaa  95% gpu 50-60% cpu  4k native 95% gpu 60-80% cpu  2k native  70% gpu 80% cpu,buildapc,2025-11-14 20:23:29,1
Intel,nozrjve,"Yea i’ve edited my post yesterday to say 120hz locked, cause I realized I do get pretty close to the max refresh anyways. I think i’m nitpicking here and it’s probably not worth the money for an entire system upgrade but, i’m still on the fence. It would cost me like $700 total and if i’m lucky I can make $300-400 reselling my current hardware.",buildapc,2025-11-15 15:48:55,1
Intel,np1m068,The more I think of it i’m not even sure if it’s worth me doing a whole system upgrade for $700 since my TV is capped 120hz.   Do you think a smaller 27” 1440p or 32” 4K oled with higher refresh would be a better use of my money?,buildapc,2025-11-15 21:42:01,1
Intel,novc8dn,4k dlss balanced 80-90% GPU 70-80% CPU  4k dlaa 95% gpu 50-60% cpu  4k native 95% gpu 60-80% cpu  2k native 70% gpu 80% cpu,buildapc,2025-11-14 20:41:30,2
Intel,noupvc3,"I just upgraded to the 5070Ti, as I feel like upscaling is pretty much required in every modern game, I didn’t feel like the 5080 was worth the price jump and a 5090 just seems a bit overkill when i’m fine using up scaling.   If I did get a monitor it would probably be 1440p OLED",buildapc,2025-11-14 18:45:46,0
Intel,nov6pyj,"Yes, thanks",buildapc,2025-11-14 20:12:08,2
Intel,noy8wgm,"This was my biggest uplift going from 5600 to 7800x3D. My highest fps are rarely higher than they were before, but rarely does it feel like any game stutters or has slowdowns now",buildapc,2025-11-15 08:48:40,3
Intel,now37p4,Ho with the 7600x3d. The extra bandwidth and 3d cashe only gives an extra 10% fps boost. Dollar per fps its not really worth it shelling out. It'll run Arc raiders perfectly fine. The game is incredibly optimized.,buildapc,2025-11-14 23:06:51,3
Intel,novas31,"I have a i5-12600kf and 3060 12gb and get 100-120fps in Arc Raiders. If you’re running a 5070ti and 5800X you may be CPU bound. If your 5070ti has 8gb VRAM, could also be an issue but cannot find any 8gb models myself  Just to give you benchmarks on another system",buildapc,2025-11-14 20:33:47,2
Intel,nouzl6a,No in 2k. It could be yes i was playing games like rust and it was with my old i5 12400f aswell not really great but the 9800x3d i perfect like it should be for the price:D,buildapc,2025-11-14 19:34:55,1
Intel,novfusp,I had your setup as well. The 5800x was causing 1% spikes often enough to notice in games. Switching to 9800x3d on 4k I well worth it since actual FPS and 1% are close together,buildapc,2025-11-14 21:00:38,1
Intel,now24id,9800x3d?,buildapc,2025-11-14 23:00:26,1
Intel,novav9z,"Yeah sounds like the CPU is holding it back, just not by a lot. Have you OC'd CPU/RAM at all",buildapc,2025-11-14 20:34:16,1
Intel,np1mnlj,"Personal opinion but I bet a lot of ppl would agree with me, if you are going for higher Rez with a 5070ti on your build just go for 4k bro, I can say with absolute certainty I’m never going back to anything less than 4k now that I’ve experienced it. Since you already got a 5070ti on your build I would definitely say a 4k screen with higher refresh rate is a shout, more than your cpu rn",buildapc,2025-11-15 21:45:34,1
Intel,np1njl0,"Bro mb you already have a 4k screen hahahaha, I’m ngl to you tv is nice for some games, but a proper monitor is much better, but considering u already got a 4k screen, if you are happy with it go for the cpu. But personally if it were me I’d go for a 4k monitor with higher refresh rate, I’m currently using the curved Alienware 34’ 240hz and it has been amazing",buildapc,2025-11-15 21:50:26,1
Intel,novcqvq,"At 4K a CPU upgrade is going to make a very tiny difference. 1440p might give you a small upgrade. In general, I don't think it's worth the cost.",buildapc,2025-11-14 20:44:15,3
Intel,noura05,"Ahh. Tbh 5800x is a beast CPU. Unless you feel the need or want to upgrade or see CPU at 100% utilization, not sure if upgrading CPU will help you.",buildapc,2025-11-14 18:52:38,0
Intel,npdrp17,"Same here, I experienced the same a couple gens ago.  I had upgraded my GPU from a 5700xt to a 6900xt, but I was still running my 3600 CPU.  I was initially upset with my performance gain with the 6900xt.  Then I upgraded to a 5800x3D, I felt that upgrade more than the GPU.  Not as bad, but I'm dealing with similar performance degradation on my 5080FE, mostly in 1% lows.  I upgraded over the summer, and it is performing well.  I can see my 5800x3D struggling on graphic intensive games though.  I'm currently waiting for a really good deal to change mobo, RAM, and go to a 9800x3D.  If not, I plan to wait for Zen 6 chips to release next year.",buildapc,2025-11-17 21:05:36,1
Intel,now6wb2,You’re right but I also use it for music production so I might notice the core / thread count drop.. might have to go with the 7800x3d $450 or the 9800x3d. $530. Still gonna need ram too. Thinking about it more i’m not sure it’s worth spending all this money,buildapc,2025-11-14 23:28:58,4
Intel,novc6pj,4k dlss balanced 80-90% GPU 70-80% CPU  4k dlaa 95% gpu 50-60% cpu  4k native 95% gpu 60-80% cpu  2k native 70% gpu 80% cpu,buildapc,2025-11-14 20:41:15,3
Intel,now4fg5,Yep.  I did the nvidia apps recommended settings which were pretty much max.,buildapc,2025-11-14 23:14:04,1
Intel,np1tr6n,"Not at all, I think I could get away with it a bit",buildapc,2025-11-15 22:25:47,1
Intel,np1p1fg,Yeah it just sometimes the TV is pretty big and can give me a headache. It’s especially good for single player controller games but sometimes I think when i’m playing a lot of mostly MNK games i’d benefit greatly from a smaller 32” 4k display and more than 120fps.,buildapc,2025-11-15 21:58:46,1
Intel,nouscd1,Maybe I should just get a monitor lol,buildapc,2025-11-14 18:57:54,1
Intel,nov0lwy,5800x is not a beast gaming cpu anymore. I was on a 5700x which is only like 2-5% slower and upgrading to a 7800x3d quite literally doubled my fps in BF6.,buildapc,2025-11-14 19:40:13,1
Intel,noy0zjg,I think the 7800 would be the better idea over the 7600 if you dont wanna get the 9800,buildapc,2025-11-15 07:27:50,1
Intel,novo640,Just get the 7600x3d  Realistically sounds like all you need is a 7600(x)/9600x though,buildapc,2025-11-14 21:43:56,1
Intel,np1u3tv,"Yeah you totally could, OCing is very safe these days. Stability is very much at risk but it's extremely difficult to brick your stuff. And aosy every chip has some headroom",buildapc,2025-11-15 22:27:51,1
Intel,np1tbxr,"I absolutely agree with you, tv is good for a laid back controller experience, but for mouse and keyboard games go with a smaller screen for sure",buildapc,2025-11-15 22:23:18,1
Intel,nov1bkx,You will absolutely get a noticeable performance boost by going for any of the AM5 x3d chips. Only reason to really avoid upgrading imo would be if you can't find a ram kit that hasn't skyrocketed in price yet.,buildapc,2025-11-14 19:43:57,1
Intel,nousfq4,"Honestly, that's a solid choice with your freshly upgraded rig.",buildapc,2025-11-14 18:58:22,0
Intel,nov0xlv,You play at 4k too?,buildapc,2025-11-14 19:41:56,0
Intel,noy477x,"I'd personally also go with a cheaper AM5 right now and then upgrade to a Zen 7 high-end CPU in 2028 or 2029. I think this way you're squeezing the most value out of your mobo + RAM platform long term , especially at the current RAM prices.",buildapc,2025-11-15 08:00:07,1
Intel,nov1qcf,I have to make sure I emphasize that my display is 4K but playing with DLSS balanced or performance which is 1080p internal res,buildapc,2025-11-14 19:46:05,1
Intel,nov4645,No but i have a 5060 which is half as fast as his 5070ti so he should see large gains unless he's actually maxed on the gpu utilization already which i doubt.,buildapc,2025-11-14 19:58:42,0
Intel,nov2ncv,"I understand. But upscaling still does require power, (I understand you just upgraded GPU, so GPU is not something you'd upgrade) so a stronger GPU COULD be beneficial as well.   As mentioned by another redditor, you really just take a look at your CPU and GPU utilization and then go from there to figure out what might be the best path forward FOR YOU. Because to go to AM5, on top of CPU cost, you'll need a new mobo and the now crazy priced RAM.",buildapc,2025-11-14 19:50:49,1
Intel,nov4tus,"Considering he's using 4k120Hz monitor (I guess technically a TV, but a display nonetheless), and even with DLSS, that's a high bar. I don't know how CPU dependent arc raiders is, but I know BF6 REALLY is, and that's probably why you saw such a huge jump. If arc raiders is, then he may see it.  As I said, I wouldn't be so quick to determine that he'll get the same jump you did with CPU upgrade without knowing his utilization, which is exactly what  I communicated when I said OP should see if CPU is at 100% utilization.",buildapc,2025-11-14 20:02:10,1
Intel,nov8cuc,Didn't mean to imply OP would double his frames or even close to that. Was just pointing out that the 5800x isn't really a high end chip anymore and that upgrading it would make sense.,buildapc,2025-11-14 20:20:49,1
Intel,novcc0s,4k dlss balanced 80-90% GPU 70-80% CPU  4k dlaa 95% gpu 50-60% cpu  4k native 95% gpu 60-80% cpu  2k native 70% gpu 80% cpu,buildapc,2025-11-14 20:42:03,1
Intel,novjyvs,Since you usually play balanced i'd deff say go for the cpu upgrade. You should gain 20+ frames and have significantly better 1% lows. What's the cost for a 7800x3d for you since you only mentioned 9800 & 7600 in the post?,buildapc,2025-11-14 21:22:08,1
Intel,noywm13,Looks good to me. Provided pricing right and all that jazz. I'm a big believe in value-for-money.,buildapc,2025-11-15 12:40:16,2
Intel,np046z1,"The NV3 often uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN5000, TeamGroup G50, Patriot P400 Lite, or Klevv CRAS C910.",buildapc,2025-11-15 16:55:19,2
Intel,np1mfed,"Thanks for including possible replacements, i'll look into it.",buildapc,2025-11-15 21:44:19,1
Intel,noqh19w,"If your GPU usage is significantly less than 100%, you aren't getting all the performance you paid for.  In this case, since you're capping your FPS, that isn't at all surprising.",buildapc,2025-11-14 01:30:29,36
Intel,noqge65,"Yes, that's normal and expected. What is your FPS cap set to? What is your resolution?",buildapc,2025-11-14 01:26:38,14
Intel,noqh1eh,"The answer is how much you want to use your GPU. IMO, I paid for the whole GPU so I'm gonna use the whole GPU. There's no danger in it.",buildapc,2025-11-14 01:30:30,14
Intel,noqhqkt,I have a 5090 and get to 98% utilization max on arc raiders. You're perfectly fine buddy.,buildapc,2025-11-14 01:34:41,8
Intel,noqjjfh,When it comes to “health” all you really need to do is make sure the thermals are good and clean the dust once or twice in a year. Avoid bloatware and malware and so on but that’s just default advice,buildapc,2025-11-14 01:45:35,2
Intel,noqze32,"You paid 100% of the price, you're getting 100% of what the gpu can give.  You scored!    So long as your temps are good, enjoy your gpu!",buildapc,2025-11-14 03:20:31,2
Intel,noqshns,That's great it means your CPU is not holding back or limiting your GPU. Or what the kids call these days a bottleneck lol.,buildapc,2025-11-14 02:38:26,2
Intel,nors8gn,Some games I play my gpu and cpu are running at 100% all the time. It's fine,buildapc,2025-11-14 07:03:32,1
Intel,nos13n4,"Tell me your CPU and I will let you know if you are likely to have a CPU issue without capping your fps. Since you are capping your fps and have some GPU headroom left over, that is perfectly fine and actually better for frame stability.",buildapc,2025-11-14 08:29:55,1
Intel,nos19jd,"Don't mind the snarky comments. Your concern makes sense since there are other tech things where running things maxed might affect longevity, like fast charging phone batters 0-100 and running own 0 again over and over.  GPUs are designed to run maxed out while gaming over the lifetime of the unit and doesn't really lose performance over time. What you do need to look after is maintaining adequate cooling, as overheating it and wider hot/cold cycles is what causes wear. That just means making sure the case is well ventilated and dusting it out occasionally.",buildapc,2025-11-14 08:31:38,1
Intel,noskyku,"Doesn't matter. Mine sit at 11% utilization while actively gaming. Its just a 3090 that i capped to 230W on my headless sunshine host, i game with moonlight from any of my screen doing 1080p with 60fps cap.",buildapc,2025-11-14 11:42:52,1
Intel,nosq846,"The video that girl said she would send you, and you fell for,  you will probably max out your CPU and GPU usage.  Careful where you get your material.",buildapc,2025-11-14 12:23:25,1
Intel,noqmqwa,"Sorry, I'm asking about a lot of things, I'm building a PC, here I see that it's better for the GPU to be used as much as possible, right? I want to buy a 16GB 5060ti and I think the Ryzen 5 9600x is good, it's true that I rely a lot on the AI ​​information and it says that having this processor would be too much, I mean it would be too much CPU and I wouldn't be using the CPU at 100% but what I see here is that the thing is to use the GPU at 100% which will give me more than enough CPU so to speak or maybe the AI ​​is I'm exaggerating about it being too much CPU, that's why I'm asking.",buildapc,2025-11-14 02:04:28,1
Intel,nos4zwl,what kind of sick frame rates do you get up to with that beast?  I’m on 5080 (slight oc) so I’m curious lol,buildapc,2025-11-14 09:09:37,1
Intel,nowjqsz,I have a 9800x3d,buildapc,2025-11-15 00:49:58,1
Intel,nor1oe1,"bro, stop asking AI and go research, whatever the AI is telling you is utter bullshit, a cpu and a gpu both do different things but also work together to do certain things. Please go research, there’s no such thing as “too much cpu” you’re either bottlenecked by one or the other, not the other way around",buildapc,2025-11-14 03:35:15,5
Intel,nor6ypc,For some great advice regarding picking parts check out the relevant subs.  Buildapc Pcmasterrace  And this website [Part picker ](https://uk.pcpartpicker.com/),buildapc,2025-11-14 04:10:12,1
Intel,nos5r8i,165 capped at 3440x1440.,buildapc,2025-11-14 09:17:24,2
Intel,nos67su,Ah you are not running at 4K. I see!,buildapc,2025-11-14 09:22:05,2
Intel,nnhi631,"It is a good entry-level build. Make sure to enable Resizable BAR in BIOS and install latest Arc driver, as those will help.   The P3 Plus is a QLC, DRAM-less drive, so if prices are close then consider getting a WD SN770, Crucial P5 Plus or Samsung 980 Pro for better performance.  Add two front intakes and that will help manage temps better.  850W is overkill for the build, but if you are aiming to upgrade parts in the future then that's fine.",buildapc,2025-11-06 21:06:10,3
Intel,nnhhz0z,It's perfect for it.  I'd go even cheaper or wait for some AM4 mobo and cpu combos on black friday. i've seen them as cheap as $110 for the 5500.  That combo will play pretty much any game out now and for the near future in 1080p pretty much maxed.,buildapc,2025-11-06 21:05:12,1
Intel,nnhi8yy,The thing is u don't need a 850W PSU 750w or even 650w would be sufficient and u could put more money into the CPU.  But If it's for the future proofing it's okay then.,buildapc,2025-11-06 21:06:34,1
Intel,nnj1phg,"Get the Lexar NM790 instead, it is a far better SSD (TLC with higher endurance) for not a whole lot of extra money.   Also, I would recommend a PSU from a better known brand like MSI or Corsair. I know someone with this PSU who is experiencing loud coil whine issues.",buildapc,2025-11-07 02:16:17,1
Intel,nnhj3le,Anything significantly cheaper than the Montech Century II (more than $20 cheaper) is significantly worse quality. It's not worth the cost savings to get a worse power supply.,buildapc,2025-11-06 21:10:44,1
Intel,nou1hyn,"I'm waiting on them to make something that can replace my 3070ti, the minimum performance I want for me to justify an upgrade would be a 9070xt / 5070ti  Intel, i'm waiting, and i'm willing, so show me what you can do.",pcmasterrace,2025-11-14 16:44:15,236
Intel,nouewiw,https://preview.redd.it/l09xx3bhg91g1.jpeg?width=1846&format=pjpg&auto=webp&s=85c81cc5c09ff61412c6b72e85e82902ce185990  Does it count?,pcmasterrace,2025-11-14 17:51:49,66
Intel,nou1xr6,"I would've got the b580, but it only has 8 lanes.  So I got the 9060xt.  I'm playing on a gen 3 system, so l would loose a bit of performance.",pcmasterrace,2025-11-14 16:46:25,46
Intel,nou3l2x,"I was considering getting an Arc, but it didn't seem like a good fit for an AM4 board. It was also kinda too low end to be a worthwhile upgrade from my previous card, while the 9070 XT was one.",pcmasterrace,2025-11-14 16:54:36,11
Intel,nou14lr,"Not touched arc yet, waiting for them to compete at the mid-high end before I take a serious look",pcmasterrace,2025-11-14 16:42:25,51
Intel,nouqmp5,"""yo intel arc is fucking horrendous""   - xXDÆTHZlay3erXx  (RTX 6000 PRO, 9950X3D,MSI GODLIKE X870E, 96GB 8200mhz)",pcmasterrace,2025-11-14 18:49:29,20
Intel,nou58od,"Got a B580 at a great price, small upgrade from my 6700XT which went to another build. Great 1440p performance paired with my 5900X.",pcmasterrace,2025-11-14 17:02:53,13
Intel,noukr29,The $250 price point is compelling to me for the specs.   But does it handle games well? That's what I want to know; $250 is a pretty big purchase for me right now.,pcmasterrace,2025-11-14 18:20:36,4
Intel,nouxqvu,"I mean i sort of do? My ultra 7 155h laptop has an arc igpu, if that counts",pcmasterrace,2025-11-14 19:25:25,5
Intel,nouynui,"They don’t make anything powerful enough for what I want out of a gaming PC but they’ve made strides and if they keep it up, I could see them continuing to improve, maybe having xx60 competitors in a couple of years.",pcmasterrace,2025-11-14 19:30:08,3
Intel,nouzy8h,"A770 Owner here. Was between 4060-7600XT-A770 this year, video games was a second focus but wanted to go more for productivity (in a budget). 4060 was fine, but the 8gb VRAM make me go for the other 2 options, mainly if I get to edit very big things in high res., it would be hard to do.  And between 7600XT and A770... The A770 gets 4070 performance on video edition, and Intel codes are far better than AMD. Gaming performance they are almost the same... So far very happy with my purchase.  Since the start of the year, doing the same workflow, with driver updates I got to shave a total of 5 minutes on each video render. At the start was 20m, rn is 15m with the same type of edition. And driver bugs? 0, enever needed to reinstall drivers.",pcmasterrace,2025-11-14 19:36:47,3
Intel,noudbdg,"I love having more competition but I'm not touching it until they make a high end model, I just got a 5080",pcmasterrace,2025-11-14 17:43:58,5
Intel,nouhrp7,"Had a A770 and it was great, only reaon I moved away was VR. If they could support VR I would go back. Prices are better, it worked really well, the software was great and dev support was amazing.  Halo Master Chief Collection wasn't running well. Raised a support ticket and in a month they fixed it.",pcmasterrace,2025-11-14 18:05:56,5
Intel,nougnx2,My sons PC has an A770. Never really had any issues with it at all.,pcmasterrace,2025-11-14 18:00:22,2
Intel,nousic2,"I have an Arc A750, and I'm pretty happy with it so far. I mainly play older games so I can get 60-100fps on 1440p ultra.",pcmasterrace,2025-11-14 18:58:44,2
Intel,nov0g45,"Been using B580 with 4K monitor for a while now. 7500F as CPU. With modern upscaling, it is very capable.",pcmasterrace,2025-11-14 19:39:24,2
Intel,nov24ks,I use one for an AV1,pcmasterrace,2025-11-14 19:48:07,2
Intel,nov4dno,"I use an A770 16GB for gaming on the 4K TV. Mostly older or lighter games, but in Forza Horizon 5 with the help of XESS there's no problem with high-ultra settings in 4K. hitting rock solid 60fps",pcmasterrace,2025-11-14 19:59:49,2
Intel,nov63z8,I have a A770 and I'd say it performs about on par with my 3070 for most games I play. Honestly interested in seeing what more they can do with better hardware design and continued work on improving their drivers.,pcmasterrace,2025-11-14 20:08:53,2
Intel,nov9db5,Plex server,pcmasterrace,2025-11-14 20:26:14,2
Intel,novcztj,"A770, really liked it for it's price point, even though it was a pain at first. Solid at 1440p.   I'll get the b780 if it ever releases.",pcmasterrace,2025-11-14 20:45:33,2
Intel,novffom,I've got the A770 16 gig and it seems fine.  Hard to tell with games if it's my GPU or the game being next gen ready. I came from a gtx 970 so it felt like a massive upgrade. I remember Balder's gate being real low fps in act 1 on the 970 but very decent on the A770 at 1440. Hunt showdown also gets 90+ fps in busy areas but has had issues dropping to very choppy fps when things are happening close. With Hunt it's very hard to tell what the issue is as it has frequent issues that effect many gpus at the same time. Like a friend playing on amd had different issues at the same time as I did. Honestly couldn't tell you if I've turned the graphics up from low since I had that issue. A770 runs armored core 4 really well to the point that I haven't had to mess with any settings or check fps.,pcmasterrace,2025-11-14 20:58:24,2
Intel,novgcrp,"Been using my A770 since it launched. It's been great for a majority of the time, but the beginning was rough.",pcmasterrace,2025-11-14 21:03:19,2
Intel,now1pyd,The a750/770 wasn't bad for gaming and neither is the b570/580  I use an a370 for transcoding video and it does a great job.,pcmasterrace,2025-11-14 22:58:04,2
Intel,nowgg77,"I have a b580, and I absolutely love it. It is the perfect entry level 1440p card.",pcmasterrace,2025-11-15 00:29:39,2
Intel,nowo6qd,![gif](giphy|kSlJtVrqxDYKk|downsized),pcmasterrace,2025-11-15 01:18:39,2
Intel,nowrfek,"i have a b580. i just hate nvidia (and also use linux).  nvidia on linux is a pain in the ass. its getting better but i cba. i bought it a couple months ago. it was either amd or intel arc and i decided to just go for it.      Intel arc on linux is quite literally just plug and play. I can't express how much I love this card. i hope intel stays in the gpu market, but seeing recent news the situation seems dire.",pcmasterrace,2025-11-15 01:39:59,2
Intel,noxbpi3,B580 in my guest computer. Haven't had any issues with it playing all the LAN games my cousin and I play together every week.,pcmasterrace,2025-11-15 03:53:49,2
Intel,nou7hcw,"Been using an A770 for over two years! I definitely had my fair share of issues, especially early on, but over all it's been great",pcmasterrace,2025-11-14 17:14:13,4
Intel,noua0ne,i'm using [intel arc](https://www.reddit.com/r/pcmasterrace/comments/1ouj4en/im_glad_i_use_a_mesh_case/) and i'm very satisfied with it,pcmasterrace,2025-11-14 17:27:06,4
Intel,nou0czb,"I've got two, an A310 and a DG1.  I have yet to get the DG1 working though.",pcmasterrace,2025-11-14 16:38:37,2
Intel,nou923w,"Here, just finished my build yesterday.",pcmasterrace,2025-11-14 17:22:15,2
Intel,noucikm,A770 16GB LE here.,pcmasterrace,2025-11-14 17:39:54,2
Intel,noucmps,woulda bought it if not for vram alone,pcmasterrace,2025-11-14 17:40:29,2
Intel,noudt24,If they had a card that could compete with Nvidia and AMD high end I would consider it. At the current performance tier with B580 as the best card it doesn't interest me.,pcmasterrace,2025-11-14 17:46:27,2
Intel,noufuyl,"I love to not buy Nvidia, just as soon as someone makes a better product",pcmasterrace,2025-11-14 17:56:26,2
Intel,nouf9we,Had a a770 in my Linux but sadly I had to many issues getting it to play certain games and running it in my main  rig which at the time had a 6700xt it ran worse than the 6700xt so now it’s just sitting in it box not sure what to do with it,pcmasterrace,2025-11-14 17:53:36,1
Intel,noufd34,is it good for rendering  ? like blender/unreal  stuff  ?,pcmasterrace,2025-11-14 17:54:02,1
Intel,nouha1a,At the very least it is damn good value for money.,pcmasterrace,2025-11-14 18:03:28,1
Intel,nouhmux,"I don't use it, but I appreciate there being more options available for people.",pcmasterrace,2025-11-14 18:05:15,1
Intel,nouinri,It's okay at best,pcmasterrace,2025-11-14 18:10:22,1
Intel,noulper,B580 at 1080 so far so good. Can lock 60fps ARC Raiders max settings or drop down from max a level and 120fps,pcmasterrace,2025-11-14 18:25:14,1
Intel,noumgg9,"I picked up a B580 for stoopid cheap secondhand and am using it to learn about local LLMs on my linux rig for future home automation. Works fairly decent, but my benchmark options are limited on linux for now. It runs a bit warm at idle without an undervolt (55° - 60° C).",pcmasterrace,2025-11-14 18:28:55,1
Intel,noumrkp,If they ever release a single fan battlemage card akin to the a380 I'd buy one for my sub 5L build for sure... Tempted as it is by the sparkle a380 but I keep hoping for better especially since the b50 pro is a thing.,pcmasterrace,2025-11-14 18:30:26,1
Intel,noumv3o,"Not me really, irs rare to find one here also",pcmasterrace,2025-11-14 18:30:54,1
Intel,noutyoj,It’s great for a budget build but I wanted something higher-end so I went for the 9070XT,pcmasterrace,2025-11-14 19:06:06,1
Intel,nouuj5w,I wish I could. But my friends have outspoken this idea from me. Ended up with a laptop and RTX card in it.,pcmasterrace,2025-11-14 19:09:00,1
Intel,nouxv8r,For me the problem is that Intel's best offering is about as fast as 4060. I got no use for that.,pcmasterrace,2025-11-14 19:26:04,1
Intel,nouy2kj,"I would swap to ARC, but the rest of my system needs an upgrade first. I don't even have ReBAR support.",pcmasterrace,2025-11-14 19:27:07,1
Intel,nouyskv,"Was looking at an intel one of them for a long time, until I realized most FEA & CFD software suites require nvidia graphics cards and so that killed that idea.",pcmasterrace,2025-11-14 19:30:49,1
Intel,nov17pf,I am interested but I don't think it is worth replacing my 2080 Super.,pcmasterrace,2025-11-14 19:43:23,1
Intel,nov5zlw,lol,pcmasterrace,2025-11-14 20:08:15,1
Intel,nov669r,"I just got an a380 for 120 and put it in my plex server.  Love it, barely spins up for 4K to 1080p transcodes including HDR to SDR mapping.  Makes zero noise.",pcmasterrace,2025-11-14 20:09:14,1
Intel,nov6uk6,"Yes, got a B580 for MSRP, and put it into my custom Steam Machine!",pcmasterrace,2025-11-14 20:12:48,1
Intel,nov8i0x,"I mean, is it? While the price to performance *seems* to be good, from what I’ve heard, the drivers kind of suck, and it doesn’t really work well with actual budget CPUs.  It’s good on paper, but it’s not really a mature product yet. I’d guess that by the next generation, they’d be more caught up with the competition at that price range.",pcmasterrace,2025-11-14 20:21:35,1
Intel,nov8u9z,"I have one in my jellyfin and its been amazing doing 3 transcodes at a time and not sucking up too much power, along with some misc server tasks like trickplay. Looking into the b580 for my personal computer currently. I've heard people say the drivers aren't good but personally I havent really had a problem with them.",pcmasterrace,2025-11-14 20:23:24,1
Intel,novb3gi,"I have an A770 in my rig, genuinely never had a problem unless it was Fallout 3",pcmasterrace,2025-11-14 20:35:28,1
Intel,novcv5j,I had a B580 near launch. Was my replacement for a 1080ti at the time. It was totally fine for my use case. I did upgrade to a 4080 later but that was more due to my use case changing (built a sim racing rig with a large format 4k TV) than the performance of the card itself. I was also quite partial to the understated looks of the card,pcmasterrace,2025-11-14 20:44:52,1
Intel,novda5p,"Ive used plenty of Arcs and have enjoyed the performance and compatability.  The 1 weird thing is in one older game I played, the game defaulted to my PCs AMD iGPU, and wouldnt run on the Arc without disabling the iGPU in the BIOS.   Probably just an issue with the engine only knowing to expect an AMD/Nvidia GPU",pcmasterrace,2025-11-14 20:47:03,1
Intel,novddt1,You think it would be the cheapest option to make a steam machine equivalent?,pcmasterrace,2025-11-14 20:47:34,1
Intel,nove9o5,I had the B580 for 4 months then switched to a 5070.,pcmasterrace,2025-11-14 20:52:15,1
Intel,novehw4,I downgraded from a 6950xt to a B580. honestly don't notice much if any difference in the games i play and the power consumption is signifcantly less.,pcmasterrace,2025-11-14 20:53:27,1
Intel,novfb5r,"b580 user here. paired with a 5700x3d.  not much to complain about when it comes to gaming performance. HOWEVER, video encoding is an absolute no-no with the b580. (streaming on discord or recording with hardware encoder in obs absolutely kills gaming performance.)",pcmasterrace,2025-11-14 20:57:45,1
Intel,novfitp,"I have one in my shop computer.....I have a gpu from all three in use. A 6800 in my wifes rig, I have a 4080, and the shop has the 580! Guess where all the gaming happens?",pcmasterrace,2025-11-14 20:58:52,1
Intel,novg2sv,I don't run arc on my personal system (have a 7800XT so it'd be a downgrade) but I build a budget system for my little brother using the B580 and he seems to like it,pcmasterrace,2025-11-14 21:01:50,1
Intel,novg7gh,"Upgraded from 1070 to b580, and have been very pleased with it. I mainly play dota, arpgs and do some photo editing in lightroom, and thought the b580 was the most priceworthy option for me!",pcmasterrace,2025-11-14 21:02:32,1
Intel,novgf1j,"We do. Arc 380. Best thing you can use for a Plex server.   Side note. I can't get DLSS to work on Battlefield 6. Before, it would hard crash. Now it just turns everything greyscale red. But XeSS works.",pcmasterrace,2025-11-14 21:03:39,1
Intel,novgndc,I would have happily gotten one if I didn't have a card more powerful already. I'm a big fan of what intel is doing so far in the GPU space.,pcmasterrace,2025-11-14 21:04:53,1
Intel,novhdz3,friend uses it on our private emby server for transcoding.,pcmasterrace,2025-11-14 21:08:46,1
Intel,novhqic,Decent choice for some price points but I wouldn't call it good good. It's definetely good for the market though. Waiting for some mid-high end moves,pcmasterrace,2025-11-14 21:10:36,1
Intel,novi5gj,I would 100% do it if I were in a situation of needing a 250$ GPU. Sadly Arc doesn't cover any other price category except if you wanna buy an old-gen card,pcmasterrace,2025-11-14 21:12:47,1
Intel,noviqo8,"A380 used here for my Plex Server, perfomance is incredible for transcoding media, especially given the price.",pcmasterrace,2025-11-14 21:15:49,1
Intel,novl32r,"i would gotten them but can't find any, only used one and even then, still pretty expensive since they are shipped from overseas, not in stores",pcmasterrace,2025-11-14 21:27:50,1
Intel,novlrhh,Intel cards aren't a bad deal for entry level rigs and I hear battlemage is much improved from the A generation cards.,pcmasterrace,2025-11-14 21:31:23,1
Intel,novmnsz,It’s a good graphic card. I have the A750. Runs oblivion remastered decently well at 1080p,pcmasterrace,2025-11-14 21:36:03,1
Intel,novnhsm,I got the b580 for my wife and it’s great she plays flight sim asserts corsa and I got the free bf6 from the deal which also runs great on b580,pcmasterrace,2025-11-14 21:40:24,1
Intel,novqpw6,"Speaking of, I'm planning to get B580 to replace aging 5700XT for mostly encoding to HEVC and light gaming on 1440p (Paradox games like EU5, CK3, Vic3, maybe F1, or other sports games, nothing competitive). Budget is 250 eur up to most 400 eur. For reference, I can get B580 for 250-270 easily, RTX 5060Ti 16GB for at least 430, so it's out of scope.  For US people, this is with VAT 20% included",pcmasterrace,2025-11-14 21:57:10,1
Intel,novrbgs,"I'd like to get one, but I'm waiting for it to be better than my 7900XT.",pcmasterrace,2025-11-14 22:00:21,1
Intel,novrmdm,I have one I bought to leave sealed as a collectible. I did buy another A770LE for personal use but sold it to a friend.,pcmasterrace,2025-11-14 22:01:58,1
Intel,novs58t,I got the Intel A580 last year at microcenter for $90 ☺️ I only game at 1080p medium setting,pcmasterrace,2025-11-14 22:04:47,1
Intel,novu6fo,"Just built my first pc with an arc b580 12gb and I'm loving it, been playing both flat-screen and vr and it rips",pcmasterrace,2025-11-14 22:15:53,1
Intel,novujep,"As soon as they optimize the performance in dx 11 and lower. Efficiency of battlemage is nice but drivers need the aforementioned tinkering. For now planning to go 4070, solid midrange choice and better than 5000 series (proper cuda, hotspot sensor is a must, dont care about dlss or framegen because i never use these).",pcmasterrace,2025-11-14 22:17:53,1
Intel,novus6m,It's fcking terrible.,pcmasterrace,2025-11-14 22:19:14,1
Intel,novutdc,Fantastic transcoding cards for servers.,pcmasterrace,2025-11-14 22:19:25,1
Intel,novv9ko,My 4070ti scoffs.,pcmasterrace,2025-11-14 22:21:55,1
Intel,novwdxj,"I considered for my latest build, upgrading from a 980ti.  But I was concerned about support and drivers and ended up finding a 9070 at msrp instead.  I would definitely consider Intel GPU's in the future though as they mature.",pcmasterrace,2025-11-14 22:28:06,1
Intel,novyq41,"Technically I use one, but not for gaming. I run a PleX server on a dedicated PC and I use an Arc A380. It's a beast for that usage, especially at that price point.",pcmasterrace,2025-11-14 22:41:04,1
Intel,novzhwv,Not me but I welcome choice in the gpu space. Sick of nvidia and amds pricing,pcmasterrace,2025-11-14 22:45:24,1
Intel,now139s,I want one for av1 encoding pbad. But other than it sitting in my server for that one use case it wouldn’t get used.,pcmasterrace,2025-11-14 22:54:29,1
Intel,now5fw5,Bad compared to a 5090. Sure.  Bad for price to performance. No.  My main problem with arc is that its still early days. If they make another generation assuming the rnd team survives lay-offs I might pick one up.,pcmasterrace,2025-11-14 23:20:09,1
Intel,now5pqe,"I don't. I'm all AMD, but I do like what they're bringing to the market. Would love to see them push out something to compete with the high end from Nvidia or AMD.",pcmasterrace,2025-11-14 23:21:47,1
Intel,now68w8,"I have one in my server, best media engine for transcoding on the market, for real! And linux support too!",pcmasterrace,2025-11-14 23:25:01,1
Intel,now7v0e,"I sometimes see the B570 at 250 USD (including import taxes and stuff where I live), but since I'm on AM4 I'm not sure if it's a good upgrade when I can save 50 USD more for a 5050",pcmasterrace,2025-11-14 23:34:56,1
Intel,now9shu,"I've only used the Arc 140m for a little bit, but it was shockingly good.",pcmasterrace,2025-11-14 23:46:57,1
Intel,nowagkq,"I use Intel gpu, just happens to be iGpu of the i3 2nd Gen (always thought it was 3rd but it's 2nd just found out today",pcmasterrace,2025-11-14 23:51:45,1
Intel,nowatqg,"I have a B580 Steel Legend, a super cool card, especially for budget builds. I paired it with a 5600x and I can run pretty much anything.",pcmasterrace,2025-11-14 23:54:27,1
Intel,nowbuqc,I current have the Intel Arc A770 LE. It's been running fairly well and hasn't caused too much problems with it. I def look forward to more software updates and see how well it'll perform overtime through its driver development.,pcmasterrace,2025-11-15 00:01:29,1
Intel,nowc713,Nobody was selling it when I build my current pc and I didn't feel like smuggling one trough the border,pcmasterrace,2025-11-15 00:03:39,1
Intel,nowcavy,They got to make something at at least the 5080 level for me to even consider an Intel card. What they have now can't handle native 4K.,pcmasterrace,2025-11-15 00:04:20,1
Intel,nowcn93,"If i ever build a pc, i will go for arc",pcmasterrace,2025-11-15 00:06:29,1
Intel,nowdfr6,"Technically yes. Not in my main gaming rig, but my Claw is Arc.",pcmasterrace,2025-11-15 00:11:19,1
Intel,nowdh8e,"I've had my 750 in a proxmox server doing passthrough for various things since it came out. When I first got it, I had lots of driver issues of course but that got smoothed out.",pcmasterrace,2025-11-15 00:11:34,1
Intel,nowgwc9,its ok,pcmasterrace,2025-11-15 00:32:23,1
Intel,nowh8v3,Great if you don’t plan to do anything other than game and don’t need hard hitting production equipment.,pcmasterrace,2025-11-15 00:34:30,1
Intel,nowhh2z,"I've got one, but it is used primarily to transcode videos on my jellyfin server.",pcmasterrace,2025-11-15 00:35:52,1
Intel,nowio4t,Have a ARC A770 16gb in my gaming server. One of these days I'll have to take it out and throw it into one of my other rigs to see how it does gaming.  https://imgur.com/gallery/CsqR093,pcmasterrace,2025-11-15 00:43:12,1
Intel,nowk3pt,I put one in my dads PC .  He only plays 10 year old games and it works perfectly.  No issues with it ever.,pcmasterrace,2025-11-15 00:52:15,1
Intel,nowk5iu,https://preview.redd.it/ctca68qjjb1g1.jpeg?width=9248&format=pjpg&auto=webp&s=797dea53fd3cf9096881d0a22fc85aab978d821a  My bros system but i guess it still counts.,pcmasterrace,2025-11-15 00:52:35,1
Intel,nowp9vg,I wanted to but scored a deal on a 6700 for 160 so couldn't pass it up,pcmasterrace,2025-11-15 01:25:46,1
Intel,nowpz9d,I got a cheap one as a secondary GPU to drive my secondary monitors,pcmasterrace,2025-11-15 01:30:26,1
Intel,nowtcb8,"Used A770 le 16Gb for about 2 years, very good gpu. Heating a bit  too much and too fast in 4K gaming, managed to deliver 4K high quality at 30-60 fps in most of the games. Now switched to Radeon RX 9070XT , this one doesn’t struggle with 4K towards 120 fps, and the ARC770 Sitting in her box for resting.",pcmasterrace,2025-11-15 01:52:19,1
Intel,nowx4kl,The CPU overhead issue on Intel still seems way to bad to consider it over AMDs entry level options instead imo.   It's great if you have a high end CPU but kinda falls apart with a CPU you'd expect to pair with it.,pcmasterrace,2025-11-15 02:16:51,1
Intel,nowxjod,When my 3080ti starts showing up in the bottom quarter of performance graphs I’ll look then. But it’ll be hard to justify enriching Jensen anymore. AMD has my vote.,pcmasterrace,2025-11-15 02:19:32,1
Intel,nowxt9a,I don't yet. I really want to though!,pcmasterrace,2025-11-15 02:21:15,1
Intel,nowxuss,This meme format is mid at best,pcmasterrace,2025-11-15 02:21:31,1
Intel,nowyyz5,It the lanes are only 8 a piece can I run two Arcs,pcmasterrace,2025-11-15 02:28:43,1
Intel,nox0ih5,"I used an a770 for a while, I liked it but it just barely wasn’t enough for the quality level and target fps in games I wanted to play, so I went back to the older more powerful card I had before. The hdr on the arc card was very pretty though.",pcmasterrace,2025-11-15 02:38:38,1
Intel,nox0mrg,"Grabbed the b580 for 235, replacing my 1660s, absolute world of difference, coincidentally my monitor is not 144hz as i previously thought... its 165hz lol rdr2 is amazing 90fps at 1440p, bf6 same story.",pcmasterrace,2025-11-15 02:39:24,1
Intel,nox0p9l,"I love my Bifrost A770 16gb, but I wanted to play vr and scored a refurbed 3080ti Fe for $450……..",pcmasterrace,2025-11-15 02:39:52,1
Intel,nox25pe,I was considering getting one to upgrade from my 3060 12gb when i had it. Got lucky and scored a 4070 for cheap or i wouldve bought one,pcmasterrace,2025-11-15 02:49:07,1
Intel,nox2age,"I built a couch gaming pc and im currently using my old 1080 in it, im really hoping they release the B770 so i can buy it to support arc. Imagine running AMD CPU with Intel GPU lmao",pcmasterrace,2025-11-15 02:49:57,1
Intel,nox2xwf,I love my b580,pcmasterrace,2025-11-15 02:54:10,1
Intel,nox5a2m,https://preview.redd.it/6yd6j8wx7c1g1.jpeg?width=4624&format=pjpg&auto=webp&s=d0a6420d49f4b717483a033444d22db1573743ce,pcmasterrace,2025-11-15 03:09:30,1
Intel,nox6odi,Next card I buy will be an Intel GPU for sure. Nvidia is pretty scummy and there's no way I'm putting an AMD anything in my computer lol  EDIT: I'm still very happy with my RTX 3070. It's still more than enough for the games I like.,pcmasterrace,2025-11-15 03:18:58,1
Intel,nox7jt3,"I recently bought a laptop with the Ultra 7 155H in it and that includes the Intel Arc iGPU. It also has a dedicated RTX 3050Ti, I have been surprised that leaving the RTX disabled I'm still able to emulate PS2 and 360 (Xenia) perfectly. I only use the RTX for heavier Steam games due to heat output and power consumption.  Also, when encoding 1080p video h.265/MP4 container in Handbrake using hardware acceleration the Arc is only 60-90 frames per second slower than the RTX.   I've been impressed I may check out Arc desktop options.",pcmasterrace,2025-11-15 03:24:59,1
Intel,nox7p81,A750. 👍,pcmasterrace,2025-11-15 03:26:00,1
Intel,nox9smj,Intel ARC A310 in truenas for AV1 encoding.,pcmasterrace,2025-11-15 03:40:33,1
Intel,noxecqy,"Not at the moment, but probably in the near future.   Need some affordable AV1",pcmasterrace,2025-11-15 04:12:54,1
Intel,noxfkj2,I have a Sparkle Titan b580 in my server for Plex. I’ve never used it for gaming.  Technically I use Arc in my MSI Claw 8 AI+. Works great in that.,pcmasterrace,2025-11-15 04:22:00,1
Intel,noxhvmz,it kicks ass in my pley server A380,pcmasterrace,2025-11-15 04:39:42,1
Intel,noxjk0l,If they can hit high end Nvidia gpus then sure. I'm not a mid range boy,pcmasterrace,2025-11-15 04:52:22,1
Intel,noxkax3,Was actually looking at their GPUs yesterday. Performances are quite good for the price. Maybe they should focus on GPUs more? Their Ultra chips dont come close to AMD.,pcmasterrace,2025-11-15 04:58:09,1
Intel,noxkt59,"Threw one in my son's computer, was a good bit cheaper than the equivalent Nvidia card and still a bit cheaper than the AMD.  Doom TDA is the most intensive game he plays and it has no problem with it so that's good enough for me.",pcmasterrace,2025-11-15 05:02:05,1
Intel,noxlgpy,"Have a b580 & so far so good. Bf6, cyberpunk, sm2, EfT, etc all run great.   I'm not frame chasing, as long as the games I play run with no stutter lags & look good I'm happy. I don't need to see the absolute detailed zit on a character's face in their stubble beard at 4k ultra settings at 196fps.   Would definitely like to see what a beefier card could be if they made one. You get a solid amount of vram, low price, & no burning cable. I hope it has a 1080ti lifecycle. But also waiting to see how much the C, D, etc improve.",pcmasterrace,2025-11-15 05:07:16,1
Intel,noxqa0y,A750 in a second build. Great for 1080p in most games,pcmasterrace,2025-11-15 05:47:07,1
Intel,noxvbxc,It's an amazing value. I used one after my 4090 and was kinda shocked how well it played COD and Warzone.,pcmasterrace,2025-11-15 06:33:07,1
Intel,noy0ero,"Got an Arc B580 for the price/performance ratio, coming from an RX 5700 XT. Was a good bit of an upgrade in a lot of newer games. Honestly think it's a great card for that purpose.  Unfortunately, I ran into poor performance that was a no-go for me. Yakuza Kiwami had a huge issue (ran at like 50-55 fps with horrible lows that made it jittery, while my 5700 XT easily pushed 144, which makes sense, it's basically a PS3 era engine with better assets). From what I can tell, 0, Kiwami 2, and YLAD all had the same issue, and they got fixed over the course of a few years. The problem with that is that 0 was reported on the forum like 2 years before it was fixed. I was in the middle of a playthrough and couldn't just wait for reasonable, playable performance. I even tried DXVK and got a big boost (like 90-100ish), but it was still very unstable.  So you really do have to beware of older games still, despite the drivers having improved a lot and being overall solid. Some other DX11 titles ran perfectly fine though.  OpenGL performance? Forget it. Necesse uses Java and OpenGL and I managed about 70fps before using Mesa for Windows to get a boost. Still wasn't ideal compared to my 5700 XT, once again (and AMD isn't known for OpenGL performance on Windows lmao).  So overall, I think it's a great piece of hardware with mostly solid drivers, but it's not good enough for me on the driver side and I don't want to wait. It lasted 2 weeks before I returned it and got a 9060 XT 16GB (which is again a good bit of an upgrade from the B580, so the extra cost doesn't hurt as bad. I've been waiting for so long for an upgrade that made sense to me).  This was all in the middle of last month. Got my 9060 XT on the 30th.  I'd also like to note that they did alleviate most of the CPU overhead issues recently, so my 5600G wasn't an issue in games that did run well, like The Finals (actually ran extremely well with maxed out settings, and XeSS is actually really good on the Arc cards).",pcmasterrace,2025-11-15 07:22:01,1
Intel,noy563j,"If I haven't got the RTX4060 LP, I would definitely vuy the B580",pcmasterrace,2025-11-15 08:09:50,1
Intel,noy7fmv,"I used a B580 as a media machine GPU for a bit.  The encoding was a little jittery when starting a file or stream, but smoothed out acceptably after 2-5 seconds of play.",pcmasterrace,2025-11-15 08:33:16,1
Intel,noyik65,Idk if an arc b580 would be good for me since Im still running a i7 6700k with a z170 mobo. But I have an Rx 580 that I've been wanting to upgrade from.   I fell like I need a full system upgrade to take advantage of any significant GPU upgrades.,pcmasterrace,2025-11-15 10:30:28,1
Intel,noyou2p,I have it in my NAS and for plex transcoding its a godsent. It just works.  Do be honest for my main rig it will not be an option for a long time. NVidia has with Cuda just to much stuff that i use daily.   AMD is years behind sadly.,pcmasterrace,2025-11-15 11:33:34,1
Intel,noz0hrm,I'd buy any companies gpu if its good price and good performance.   But I feel like Ibwould be a beta tester at this point.,pcmasterrace,2025-11-15 13:08:46,1
Intel,noz2vp3,Arc team reporting!,pcmasterrace,2025-11-15 13:24:26,1
Intel,nozgcu9,Replaced my 5700xt with a b580.   Performance is equal or slightly better at 1440p ultrawide.  The 5700xt was running out of VRAM in some games though and I was having very frequent driver crashes too.  The b580 has actually been a smoother experience.  The 5700xt has moved into a pc that had a 1050ti so it loves on being a very capable 1080p card,pcmasterrace,2025-11-15 14:47:29,1
Intel,nozgiex,I use the *word* arc. Is that the same thing?,pcmasterrace,2025-11-15 14:48:23,1
Intel,nozmzx0,I love these posts. Seeing the joker instantly tells me not to take it serious.,pcmasterrace,2025-11-15 15:24:45,1
Intel,nozorff,"Personally just not there yet for me, what they've offered has been interesting and I think intel should continue developing and release discrete gpus because I do believe they have a chance of rivaling amd and nividia for a share of the market. If they only keep improving on this",pcmasterrace,2025-11-15 15:34:17,1
Intel,nozrv0h,"Absolutely good, but also absolutely not great. I'd probably still rather just use an AMD card/APU FOR 1080p only because of driver optimization.",pcmasterrace,2025-11-15 15:50:31,1
Intel,noztt0j,They are great entry level gpus.  They are NOT great GPUs,pcmasterrace,2025-11-15 16:00:35,1
Intel,nozy1e6,"Planning on getting the Intel Arc Pro 16 GB for my server, for some video encoding. Such a monster for a little 70w card with 16 GB VRAM.",pcmasterrace,2025-11-15 16:22:55,1
Intel,nozzdwx,"I friend got an arc A750 for his PC. I think benchmarks showed around similar performance to a 3060 but I could be wrong. Anyways, it was only like... 270 bucks brand new. Looks fuckin sick too.",pcmasterrace,2025-11-15 16:30:05,1
Intel,np092sq,"My system is running i5 13600k and A770 (sparkle titan) for almost 3 years. Ngl, the GPU does exactly what I needed - video rendering in Premiere Pro and casual gaming. The drivers has been improved significantly.   Though, currently I'm not thinking about any upgrades, I might upgrade later - to newer GPUs from Intel.",pcmasterrace,2025-11-15 17:21:02,1
Intel,np0gu5i,"Been using the A750 since launch, got it for free during the Scavenger Hunt they held. It's been good so far, only recently showing signs of struggle as I'd like to enjoy Borderlands 4",pcmasterrace,2025-11-15 18:01:38,1
Intel,np1lqn4,Didn’t NVDA recently buy up a massive stake in INTC? Intel Arc is unlikely to survive the next 5-6 years.,pcmasterrace,2025-11-15 21:40:35,1
Intel,np6jp7y,"arc a770, it crashes alot",pcmasterrace,2025-11-16 18:14:14,1
Intel,np87kwm,It would have been good if Intel didn't scrap the idea for something better than the B580 because if we were to have the B750/770 there could have been a more affordable option to compete against the RX 9070 and RTX 5070 but of course Intel got rid of that notion and we would have to wait for the next series of GPUs if they do plan on making one which would most likely compete against the RX 9060 XT and RTX 5060 Ti 16GB cards.,pcmasterrace,2025-11-16 23:22:04,1
Intel,nou8hqj,"Hell yea it is, my MSI Claw 8 is killing it out here.",pcmasterrace,2025-11-14 17:19:23,1
Intel,noubmju,"I have three across two rigs. They may not the best, but I can do everything I want at pretty good settings and barely cracked $500 for all.",pcmasterrace,2025-11-14 17:35:20,1
Intel,noucdkf,"The B580 is scratching an itch I cannot even begin to describe in adequate terms.  It is the default card for my 2-gamer builds now.  Since it's an 8x card, any consumer grade motherboard with two x16 slots that split the primary slot lanes when both are populated ultimately does not suffer a dip in performance.  Plus the linux friendly nature of the cards means that I can pack a double-bazzite PC in my bag and take anywhere.  It makes road trips and game nights significantly easier.",pcmasterrace,2025-11-14 17:39:12,1
Intel,nov90fw,come on this is a stupid question... everyone knows 99% of reddit is on 5090's. Hadn't you seen the Nvidia subreddit,pcmasterrace,2025-11-14 20:24:20,1
Intel,nouh0v0,Driver issues and compatibility in games frightens me. On paper they look great for the price but I'm worried.,pcmasterrace,2025-11-14 18:02:11,0
Intel,noutr0u,Good how? And for what? We talking mid gaming here?,pcmasterrace,2025-11-14 19:05:00,0
Intel,nov1f4q,Okay ngl this made me laugh so hard all of a sudden 🤣🙏🏻,pcmasterrace,2025-11-14 19:44:29,0
Intel,novu58e,Intel Arc Raiders?,pcmasterrace,2025-11-14 22:15:42,0
Intel,np2943u,I have b580 12gb gpu and amd is no go black screen central. Intel has no issues like those amd cards are trash.,pcmasterrace,2025-11-15 23:57:52,0
Intel,noufhb6,The odds of someone responding to this post that has an Intel Arc gpu is near zero. They have less than 1% market share and 0.16% of Steam users.,pcmasterrace,2025-11-14 17:54:35,-8
Intel,nough5r,![gif](giphy|1zSz5MVw4zKg0|downsized),pcmasterrace,2025-11-14 17:59:26,-2
Intel,nowh22v,![gif](giphy|iibEPf8xEDTedJcDJr),pcmasterrace,2025-11-15 00:33:20,45
Intel,now2cht,How about replacing 5090 without the flammable connector?,pcmasterrace,2025-11-14 23:01:45,16
Intel,novr9up,Me still happy with my 2080ti:,pcmasterrace,2025-11-14 22:00:07,20
Intel,nowbekd,Similar situation. 6800xt. If Intel can provide a meaningful upgrade to my 6800xt I'll seriously consider them. I've zero loyalty as every card I've bought has made me happy and been a genuine upgrade.,pcmasterrace,2025-11-14 23:58:33,4
Intel,npbm2yq,"Same, other than that I have a 3060ti. Budget cards are great, but i already have something better and im not willing to downgrade",pcmasterrace,2025-11-17 14:38:44,2
Intel,nownpmn,"The 3070Ti is barely 4 years old... thanks to the 8GBs, it was obsolete on its release. It could have had so much potential with 16 or 24GB",pcmasterrace,2025-11-15 01:15:31,2
Intel,nox3tga,"Same. Offer me good performance per price, enough vram, and solid drivers and my money is now yours.",pcmasterrace,2025-11-15 02:59:50,1
Intel,noxbbtu,Bmg g31 is going to be released q1. 2026. It will be anywhere from 32 to 44 xe2 cores.  I'm speculating that price tag is going to be around $500 USD.  Will most likely have 16 to 24 gigs of vram.  The next discrete cards after that will be xe3p. Those cards are currently insane. Testing just started for them. Most likely release will be 6 months after Nova lake.,pcmasterrace,2025-11-15 03:51:08,1
Intel,np8welp,Considering Nvidea bought a share of intel i would be surprised if ARC isnt dead in the water,pcmasterrace,2025-11-17 01:47:56,1
Intel,nou3bb3,Intel partnered with Nvidia to drive their APU's.  I don't think a new ARC will come out.,pcmasterrace,2025-11-14 16:53:16,-80
Intel,novxw1c,Newsflash: it isn't going to happen. AMD and nvidia are very much ahead. Intel isn't going to make up in a year what amd and nvidia have been doing for decades.,pcmasterrace,2025-11-14 22:36:24,-10
Intel,nov5era,Lossless scaling?,pcmasterrace,2025-11-14 20:05:12,28
Intel,now8c9p,"https://preview.redd.it/48776rh76b1g1.jpeg?width=2992&format=pjpg&auto=webp&s=9acb9751d120d4eecab4bd4cf1b532fcb37589b9  I feel you, great encoder so the main gpu can run free",pcmasterrace,2025-11-14 23:37:55,10
Intel,novu2un,Its in danger.,pcmasterrace,2025-11-14 22:15:21,4
Intel,nowny3f,Is the A310 connected to the chipset? I had that idea as well (with an Arc Pro A40) but i have already 3 PCie Cards in my system and the space is getting thight...,pcmasterrace,2025-11-15 01:17:05,1
Intel,noyw3h2,Adorable,pcmasterrace,2025-11-15 12:36:08,1
Intel,novi2av,"It only matters if you run out of VRAM. Like there's a 3-4% difference otherwise between x8 or x16. My 5060 Ti is running at 3.0 no problem, just don't overflow the VRAM.",pcmasterrace,2025-11-14 21:12:19,14
Intel,nougv4r,"lose, not loose",pcmasterrace,2025-11-14 18:01:22,25
Intel,nou8r23,PCIe 3.0? You're basically losing no performance on a 16x interface with the 9060XT. Even a 5090 only loses about 2%.  VRAM limited cards on an 8x or 4x interface though... Oof...,pcmasterrace,2025-11-14 17:20:41,9
Intel,novn56r,Lmo rhere's like a 1% difference at most in 99% of gamws dude,pcmasterrace,2025-11-14 21:38:34,4
Intel,nowwug8,"I wanted one, but couldn't find them at MSRP. I bought a 6800 instead.",pcmasterrace,2025-11-15 02:15:03,1
Intel,nou6cqs,"Honestly they're excellent for budget gaming PCs. Hella good performance for their prices.   For high end, though? I agree with you there.",pcmasterrace,2025-11-14 17:08:29,9
Intel,noua1tj,"I've got one paired with a 5950x and it runs great.  I'd like to see a b or c series 770+. But I do 1440 gaming, and I've been able to run anything I've thrown at it at 60+ fps.",pcmasterrace,2025-11-14 17:27:16,4
Intel,nouapa5,are you not bottlenecked by the 3900x in a lot of titles?,pcmasterrace,2025-11-14 17:30:36,1
Intel,nou2zjx,"Used the B580, even if it only has eight lanes, it still gives me some hella good performance at its price point.   $250-270 and I can play 1440p 60fps, that's all I could really ask for.",pcmasterrace,2025-11-14 16:51:38,28
Intel,now6g2q,I really wanted Amd and Intel to come out with a competitor that sits in the same category as the 5080.... AMD was rumored to have one but that never seemed to materialize.,pcmasterrace,2025-11-14 23:26:13,1
Intel,novr4tx,That’s never happening so I guess have fun waiting lol.,pcmasterrace,2025-11-14 21:59:23,0
Intel,now2jc0,If you look at the content in this sub you'd think 50% of people run a 5090/6000 Pro.  Bit funny cause even among members of this sub I doubt it's over 5%,pcmasterrace,2025-11-14 23:02:53,8
Intel,nov7fc8,"with their fake msrp/stock on release, and the overhead/driver issues, this gpu was DOA. Even if they completely resolve these problems, it's late, you can get way better products now at this tier, with way better support and resell value.",pcmasterrace,2025-11-14 20:15:52,-15
Intel,novkavh,"Ahh same, I really really wanted a B580 but they were not available in my country so I bought a used 6700XT instead.",pcmasterrace,2025-11-14 21:23:50,5
Intel,noy2nob,TIL the B580 has better performance than the 6700XT,pcmasterrace,2025-11-15 07:44:31,1
Intel,nov0ew3,"I'd say yes! I paired it with an i5-13400F and ran Forza Horizon 5 at 4K60, Space Marine 2 at 1440p60, Helldivers 2 at 4K40/1440p60, and Doom: The Dark Ages at 4K40/1440p60.   Settings were all at ultra, only had dynamic resolution set for Space Marine 2.",pcmasterrace,2025-11-14 19:39:13,2
Intel,nox1l3e,"It eats whatever i throw at it, got it for 235 from b&h photo, also came with bf6 a game i was never going to buy at full price that i now get to play at peak popularity.",pcmasterrace,2025-11-15 02:45:28,1
Intel,np0f42w,"I play Avowed on a 1440p ultrawide, no complaints here",pcmasterrace,2025-11-15 17:52:52,1
Intel,nowcx9k,"It does, it's arc",pcmasterrace,2025-11-15 00:08:12,1
Intel,nov01vh,"I heard that using Virtual Desktop is the best way right now.   However, I think someone who wants to use VR is likely not going to get an Arc GPU anyway due to the more budget nature of the GPU line.",pcmasterrace,2025-11-14 19:37:19,3
Intel,novgpcg,Yeah I made a support ticket about old Minecraft versions having rendering bugs with its old version of OpenGL and they fixed it also.,pcmasterrace,2025-11-14 21:05:10,1
Intel,nougrls,"For someone windows shopping, what kind of issues did you have?",pcmasterrace,2025-11-14 18:00:52,1
Intel,np07b2x,I’m on a regular 2080 and feel the same way.  Still hoping for a B770 release and that it would be a worthwhile upgrade.,pcmasterrace,2025-11-15 17:11:42,1
Intel,nov9oyg,"I'd say it's good with i5s, they just seriously improved the CPU overhead performance with i5 and i7.",pcmasterrace,2025-11-14 20:27:59,1
Intel,novgnwg,I do wonder if this is a problem with CPUs that don't have iGPUs.,pcmasterrace,2025-11-14 21:04:57,1
Intel,novgitf,Probably better due to the higher VRAM and clock speed.,pcmasterrace,2025-11-14 21:04:13,1
Intel,noy3m9q,"Actually, they just fixed that recently. Used an i5-14400F and it ran absolutely pristine.",pcmasterrace,2025-11-15 07:54:13,1
Intel,noxwf88,Fun thing: try and see if you can use lossless scaling and combine the rtx with the igpu.,pcmasterrace,2025-11-15 06:43:29,2
Intel,nozcetc,"Is amd years behind or just blocked out of the game.  Nvidia holds the market because of cuda, and cuda is proprietary to nvidia and very old tech so almost all programs are designed to take advantage of cuda.  Amd is not allowed to make a cuda equivalent and developers dont want to write their code for something new.    Thats not years of behind, thats a monopoly.",pcmasterrace,2025-11-15 14:23:49,0
Intel,np1of6b,"They're partnering up for integrated graphics because, let's face it, Nvidia doesn't do well in the CPU market.",pcmasterrace,2025-11-15 21:55:18,1
Intel,nouxbw7,"Budget gaming. I was able to play some good games from 2024-2025 at 1440p 60FPS with ultra graphics settings.   Also good for competition. Everyone complains that Nvidia is overpriced and AMD is underwhelming, but then where does Intel sit in the hate train?",pcmasterrace,2025-11-14 19:23:16,3
Intel,noupy00,"I'd wager the vast vast majority of those Arc users are enthusiasts, and those people are more likely to be on PC forums like this",pcmasterrace,2025-11-14 18:46:07,8
Intel,novmd4h,Common RTX 4090 L,pcmasterrace,2025-11-14 21:34:30,3
Intel,nox7q7g,Intel C990KS or Intel C990XE does have a ring to it ngl.,pcmasterrace,2025-11-15 03:26:11,3
Intel,noxmn5m,I bought a 5090. I undervolted and overclocked it and get a 3% performance boost while using about half the power of stock.  The connector issues are exaggerated and easily avoided.,pcmasterrace,2025-11-15 05:16:31,-5
Intel,novxo7v,me living with a turbine 2 ft away from me playing ck3 with my 2080 super:,pcmasterrace,2025-11-14 22:35:13,10
Intel,nowft0v,Me still happy with my rtx4000.,pcmasterrace,2025-11-15 00:25:43,1
Intel,nowqbe4,"Still using my 1080, it's a lil trooper",pcmasterrace,2025-11-15 01:32:40,1
Intel,nox73nc,"Not saying that my 3070ti isn't a good card, I'm playing in 1440p and in most situations it crushes it, but there are those couple of times where I find the performance lacking.",pcmasterrace,2025-11-15 03:21:53,1
Intel,np1vwyh,"I can understand that without judgement. I only just replaced my GTX 1080 a few weeks ago. Had that thing running for over 7 years and it's still got enough life left in it to go into a secondary computer for a few more years of service.  I don't know if I just don't tend to play demanding games or what but for example, I was still rocking out 60+ fps in Cyberpunk without turning the graphics settings down all that much.",pcmasterrace,2025-11-15 22:38:30,1
Intel,nox6tse,Same here. 6800xt for the last 3 years and still no issues with gaming at 1440 high in most games so I'm good until something is significantly better at a reasonable price.,pcmasterrace,2025-11-15 03:20:00,3
Intel,noyr6cv,Name a game and I’ve played it in 1440p on a 3070ti. Why do you people thing 8 gigs of vram is basically useless😭😭,pcmasterrace,2025-11-15 11:55:06,9
Intel,nowv7z7,Even just 12GB like the 4070 would have made it last longer   Same for the 3080,pcmasterrace,2025-11-15 02:04:35,2
Intel,noy0l9t,16 and 24 gigs wasted on it as the core can barely utilize 12gigs without shitting the bed.,pcmasterrace,2025-11-15 07:23:49,1
Intel,nouaj8j,"Unless intel confirms the cancellation of ARC officially as a whole, then that means pretty much nothing.",pcmasterrace,2025-11-14 17:29:43,43
Intel,nov1lib,"Maybe you don't get the reason for your downvotes, but at least one of them is that their cpu and gpu departments are completely separate, no that's why there was a big disconnect between Intel cpu and gpu quality, it's essentially 2 different companies sharing intel's name",pcmasterrace,2025-11-14 19:45:23,4
Intel,novma4m,Is that on paper? No? Then it doesn’t exist.,pcmasterrace,2025-11-14 21:34:04,2
Intel,now2vfm,"Intel doesn't compete at the high end, if anything it just takes even more market share from amd so idk why Nvidia would stop it.",pcmasterrace,2025-11-14 23:04:50,2
Intel,nowy981,"The partnership is because, as much as people in this sub argue otherwise, the age of large APUs has begun. They're likely not gonna catch on in DIY desktop, but for laptop and mini-PC's, giant APUs with large iGPUs are going to likely replace entry level dGPU.   So the partnership allows Nvidia to, instead of selling a GPU chip to a motherboard OEM to be soldered, will now sell that chip to Intel to be integrated into the SoC package as a chiplet.   It doesn't mean Arc is dead. It means Nvidia is ensuring they aren't squeezed out of this new emerging market",pcmasterrace,2025-11-15 02:24:06,1
Intel,noulj42,I'm aware,pcmasterrace,2025-11-14 18:24:22,1
Intel,nowr7ia,This is not how it works,pcmasterrace,2025-11-15 01:38:34,4
Intel,nows8mb,"By your logic, Tesla shouldn't have had the success it has now since Ford has been making cars for a lot longer.",pcmasterrace,2025-11-15 01:45:13,1
Intel,noy0n67,Oh intel pretty much can.,pcmasterrace,2025-11-15 07:24:20,1
Intel,nove8sk,Or video encoding. I've considered getting an A350 for AV1 OBS recordings,pcmasterrace,2025-11-14 20:52:07,23
Intel,novil9h,"Nope. Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",pcmasterrace,2025-11-14 21:15:03,5
Intel,nowvpgi,"While being strangled, JK but yeah slight airflow decrease.",pcmasterrace,2025-11-15 02:07:42,2
Intel,noy45ml,"It's not A310, it's A380 (although it's power limited to 43W) and it's not connected to the chipset, but directly to the CPU. I've picked up motherboard that has two large PCIe slots running at PCIe gen 4 speed, connected directly to the CPU, exactly for this reason. 2nd slot uses 4 lanes though, but it's more than enough for 3440x1440 170Hz uncompressed video.",pcmasterrace,2025-11-15 07:59:41,2
Intel,novk38z,Exactly.,pcmasterrace,2025-11-14 21:22:45,1
Intel,novunxn,Loosen up,pcmasterrace,2025-11-14 22:18:34,5
Intel,nouaj9w,"read the comment again, slowly this time",pcmasterrace,2025-11-14 17:29:44,-10
Intel,nozfzop,"I got the 8gb, so I might run low on Vram in the future.  So it might be a bigger difference when you runout of Vram.",pcmasterrace,2025-11-15 14:45:18,1
Intel,noyruun,"I found in the UK, the 8gb models were £240, while the 16gb were selling above £300 and I assume msrp.  The b580s were always cheap though, last I was aware they were going under msrp at £215 for some models.  And the b570s under £180.",pcmasterrace,2025-11-15 12:01:00,1
Intel,nox04e6,Would it be good for CAD modeling?,pcmasterrace,2025-11-15 02:36:07,1
Intel,noujxb8,"maybe a slight bottleneck in 1080p, but nothing to worry about.",pcmasterrace,2025-11-14 18:16:33,1
Intel,nove0mw,"It seems like I'm still more bound by the GPU and maybe something else. I can see in MangoHUD in e.g. Tiy Tina's Wonderlands my GPU at 60-70% and my CPU only at 15-20%.  The Ryzen 9 3900X might not be the newest CPU anymore, but it's also not slow. It's a 12-core (24 thread) CPU, which boosts up to 4,6GHz",pcmasterrace,2025-11-14 20:50:55,1
Intel,nougf04,"For this dumbdumb, what do you mean 8 lanes?",pcmasterrace,2025-11-14 17:59:09,10
Intel,now9wmb,Yeah great value for sure,pcmasterrace,2025-11-14 23:47:47,2
Intel,noxb7hg,Which is good enough for 98% of gamers,pcmasterrace,2025-11-15 03:50:15,1
Intel,novxrxz,"I mean I have a 7900xtx so I'm fine as is, but it would benefit everyone having some actual competition in the high end",pcmasterrace,2025-11-14 22:35:47,1
Intel,now75ly,"According to the Steam hardware survey we all run mid level cards. So yeah, not enough pro 6000s out there for the users here.   All joking aside... I really want to try Intel Arc one day.  I was planning a build for my younger brother, and I'm really considering a B580.... In my market Intel is really price competitive vs Nvidia, specially in the mid level cards like the 5060. I also considered the MSI Claw but it apparently sucked for another reason.",pcmasterrace,2025-11-14 23:30:33,4
Intel,novee48,That all got fixed in the first month,pcmasterrace,2025-11-14 20:52:54,7
Intel,noviykm,Its really funny you felt compelled to comment this as a reply to this specific comment and not as a comment as a whole,pcmasterrace,2025-11-14 21:16:57,3
Intel,novpl2s,Like?,pcmasterrace,2025-11-14 21:51:14,2
Intel,np0yp7m,"Not exactly better, but there was some performance increase compared to the 6700XT. In scenarios I could use Intel XeSS instead of AMD FSR, I got more frames with increased image quality.  For comparison, I paid for a brand new B580 the same price of a used 6700XT. It was a no-brainer and I am not suffering from CPU bottleneck, which isn't a big issue since the .8xxx drivers.",pcmasterrace,2025-11-15 19:33:21,2
Intel,nowh8v4,"Woo! I love that thing, its so good for an igpu",pcmasterrace,2025-11-15 00:34:30,2
Intel,now2iws,It's not that's its budget. It's the support full stop. They are not supporting a use case that many use. They are throwing away sales.,pcmasterrace,2025-11-14 23:02:48,2
Intel,nouoveh,"It was early adopter teething issues mainly. I haven't had any issues recently, Intel has fixed A LOT of stuff over time. My main source of complaint now is just that their driver software is lacking some features compared to my previous Radeon card. I'd rather Intel focus on perfecting the basics rather than implementing a bunch of fancy driver features though",pcmasterrace,2025-11-14 18:40:49,5
Intel,nowsky0,A lot of the early issues was with older games. Intel didn’t have years of optimizations to make sure they run well especially with a tech change I can’t remember the details on. I believe they fixed it.,pcmasterrace,2025-11-15 01:47:25,2
Intel,np0qiu4,"Yeah, I would like to see a 16gb version or at least a better 12gb.",pcmasterrace,2025-11-15 18:50:22,1
Intel,nova84b,"Okay, that’s good to hear, though isn’t that more of a budget card? I’d *assume* the most common setup would be with an i3.  The main point of criticism I heard is that you can get a last-gen used AMD card for around the same price and get better performance overall. If that’s still true, then I’d withhold on saying it’s truly a good card. Just a step in the right direction.",pcmasterrace,2025-11-14 20:30:49,1
Intel,novgbnx,"Yeah, we even have our own subreddit, r/IntelArc",pcmasterrace,2025-11-14 21:03:10,5
Intel,noxtt61,Pay more than 1000 bucks for a card. You have to tweak the power consumption or it starts burning. Big clown face.,pcmasterrace,2025-11-15 06:18:58,15
Intel,now09b1,My 2080ti used to be too. Then I removed the dumbass fucking blower and mounted an aftermarket kit on it. Best money of my life. Was like 50 bucks. Temps dropped AND its quiet.,pcmasterrace,2025-11-14 22:49:42,8
Intel,now2exc,undervolt it,pcmasterrace,2025-11-14 23:02:09,3
Intel,noxqunr,My 2080 super is the same lol,pcmasterrace,2025-11-15 05:52:09,1
Intel,noxndez,GanG!  ![gif](giphy|ctkp9BZJ3zMkGhVZgC),pcmasterrace,2025-11-15 05:22:27,2
Intel,noutjh4,They've confirmed since this deal that Celestial will happen,pcmasterrace,2025-11-14 19:03:57,7
Intel,nowvh6c,I am interested in how it works. Care to explain?,pcmasterrace,2025-11-15 02:06:13,1
Intel,noy7r0k,Okay. How does it work then?,pcmasterrace,2025-11-15 08:36:39,1
Intel,noy7q57,"I just think people are sniffing copium. I want intel to do well in gpu department as much as the next guy, but reality is different. Often dissapointing.  The things that intel has to do to be on-par with current competitiors is astounding. However, I do hope I'm wrong and intel *can* find a way.",pcmasterrace,2025-11-15 08:36:25,1
Intel,novij2r,"Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",pcmasterrace,2025-11-14 21:14:43,9
Intel,nowhj3f,"Do you do a lot of video editing? I'm not familiar with this, and I'm wondering at what point I would need a separate GPU for video encoding.",pcmasterrace,2025-11-15 00:36:13,1
Intel,nox0wht,"Smart, are you using a display streaming like parsec?",pcmasterrace,2025-11-15 02:41:10,1
Intel,noy1fbf,"That's why I picked up single slot low profile version. Slightly more expensive and power limited, but it doesn't block that much.",pcmasterrace,2025-11-15 07:32:13,1
Intel,novy1xc,loosen up those pcie lanes step bro 😩,pcmasterrace,2025-11-14 22:37:20,3
Intel,noucvez,"I was agreeing, although it may not read like that...",pcmasterrace,2025-11-14 17:41:44,14
Intel,novdfy3,"Say this to yourself in the mirror 3 times, no more, no less. It MUST be 3 times for this to work  Come back to this thread  Then behold!  ![gif](giphy|12NUbkX6p4xOO4)",pcmasterrace,2025-11-14 20:47:54,4
Intel,noxf6q6,Might be! I haven't tested that myself.,pcmasterrace,2025-11-15 04:19:05,1
Intel,novlpmk,"its a productivity beast, but if youre frames are uncapped and your gpu isnt getting to 100 percent usage, youre cpu bottlenecked, as most games only use 1 or 2 cores usage percentage means very little",pcmasterrace,2025-11-14 21:31:06,1
Intel,nouo4a0,I'm assuming 8 pcie lanes,pcmasterrace,2025-11-14 18:37:06,20
Intel,novg6ny,"8 pcie lanes instead of the usual 16 so half the bandwidth for a given generation.  The PCIe bus is used to share information between the card and the rest of the system. It's usually not a bottleneck (the GPU can't process data as fast as it comes in through the bus), but it's a noticeable one when you're low on VRAM and the GPU needs to fetch data in the system RAM through the PCIe bus. Mostly an issue with 8 GB cards like the 5060.",pcmasterrace,2025-11-14 21:02:25,2
Intel,np1amfi,Ah. Shame that the Intel cards never got a market in my country. Wouldve definitely snapped one when I built my PC,pcmasterrace,2025-11-15 20:38:55,1
Intel,noupyn1,"If I were to get one for my kiddo, would I expect more troubleshooting for Steam games or ability to play less of them?",pcmasterrace,2025-11-14 18:46:12,1
Intel,noxve3u,Its only 1000 now?,pcmasterrace,2025-11-15 06:33:42,1
Intel,noy7bbq,$3000* 5090 is stupid expensive,pcmasterrace,2025-11-15 08:32:01,1
Intel,noxw47t,"If you are calling me a clown by not only getting the best card on the market for gaming, but also being able to save nearly half of the electrical cost AND getting a small performance boost on top of that, then you are the clown.",pcmasterrace,2025-11-15 06:40:35,-8
Intel,noxvvcx,Deshrouding rules,pcmasterrace,2025-11-15 06:38:14,1
Intel,noy13mv,"No, I don't do a lot. The main goal wa to use it as a display GPU, but the AV1 encoding is a nice bonus. I run OBS in replay mode (10 minutes) in the background and this intel GPU is used for encoding.  I want my Nvidia GPU to not have any monitors connected to it, so I can switch between using it in my host OS and in the VM, here's an example of how it works: [https://youtu.be/3fiXFv85iRU](https://youtu.be/3fiXFv85iRU)  This allows me to for example stream a game directly from a VM to my TV and still use my PC to do something else, like for example few weeks ago we played Trine 2 with my brother, when I was running the game in both the VM and host (one copy on Nvidia and one on Intel) and we played multiplayer together.  I used to use the VM to play everything like year or two ago, but the Nvidia drivers on linux improved significantly, so nowadays I don't use it nearly as much, but I still do from time to time. Plus it's always nice to have a backup plan for running some anticheats.",pcmasterrace,2025-11-15 07:29:00,2
Intel,noy04zy,"I'm using looking-glass. It's a bit different than parsec, it works by copying frames from the main GPU's framebuffer into the shared system memory segment and then to the 2nd GPU's memory. No compression and very low latency (I use virtual display in my Windows VM that I can set to 500Hz, which gives me around 2.5ms of added latency in total).  I also use Sunshine and Moonlight when straming to a TV or remotely to my laptop or phone. I have the cheapest server in OVH that I use as a VPN, so I can access my PC or share some services over internet (like my personal cloud for example: [https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM](https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM) ), so I can game from anywhere, just login over SSH to my home server, wake on lan PC, start a VM and connect using Moonlight).",pcmasterrace,2025-11-15 07:19:22,1
Intel,now85cz,"Good point, I guess it isn't necessarily the best gaming CPU, but I quite like its speed for compiling etc.  Getting some 5xxx CPU seems like just a small step up and wouldn't be cheap. And anything even higher would mean basically replacing everything: Mainboard, RAM and CPU. Going with a lower core count and higher boosts might be better for gaming, but would feel like too much of a downgrade.  Ironically most games, which I play besides Tiny Tina's Wonderlands, are such old titles, that both GPU and CPU are complete overkill. And lately I found just far too little time for gaming in general to make it worth it with the current prices to get some DDR5.  Edit: I just checked and at least according to Wikipedia the Ryzen 5000 CPUs aren't clocked all that differently. Its successor the 5900X has a boost of 4,8GHz, but same core count and same L3 cache and same TDP.",pcmasterrace,2025-11-14 23:36:44,1
Intel,noupm2u,I don't know what that means in layman's terms. Does it cap bandwidth like older hdmi resolution? Does it limit my kid's Minecraft terrain generation range? Does it mean I only run 69 mods on Skyrim instead of 420?,pcmasterrace,2025-11-14 18:44:29,2
Intel,nout6ep,"To be fair I don't play *that* many different games, but I haven't come across one that simply doesn't work. The biggest game compatibility issue I ever ran into was with StarCraft Remastered not rendering at full resolution. My theory is that they were specifically checking for Intel (what would've been only slow iGPUs at the time) and disabling high resolution. Blizzard fixed it though thankfully.   This video is over a year old, so things have only improved since then. I wouldn't be surprised if a lot of the issues shown have been addressed: https://youtu.be/Y09iNxx5nFE",pcmasterrace,2025-11-14 19:02:06,3
Intel,nowjqbz,For what I've played using an A750 card on steam the only game I've ever had issues with is Kenshi which would crash out on me for whatever reason but nothing else seems to have any noticeable issues.,pcmasterrace,2025-11-15 00:49:53,2
Intel,noz3h95,WTF. I corrected it a bit.,pcmasterrace,2025-11-15 13:28:20,1
Intel,noy63gl,The point is that premium product should be perfectly tuned out of the box.,pcmasterrace,2025-11-15 08:19:22,8
Intel,noyazgp,"There is always room to tweak. Fact is, the burning connector should not happen on a product that expensive.",pcmasterrace,2025-11-15 09:10:39,1
Intel,noyssdb,"It’s cool you’ve got the best GP that’s ever U-ed and it’s even cooler that you made it suck less (power), but the reason people are diasgreeing with you is because you’re defending an inarguably bad design with the argument of “but it works on my machine”.",pcmasterrace,2025-11-15 12:08:53,0
Intel,noz4pma,"Your not a clown, you’re the whole circus buddy",pcmasterrace,2025-11-15 13:36:16,0
Intel,noy4scy,"Wait, if you don't connect your Nvidia to monitors, does that mean the thing you see on monitors is only rendered by the Arc?    So, is it like how you stream from game pass? Where you let other PC run and calculate the game (your VM in this case) and then stream the game on your host OS?    Are there advantages in doing this? How are the graphics compared to directly using your Nvidia?  When you say that you rarely play on the VM nowadays, do you mean your Nvidia is connected to the monitors? Or is there a way to split the task without VM?  Also when you played Trine 2, does that mean the game looks better on TV than your PC? Since Ark is weaker.  Sorry for asking many questions lol",pcmasterrace,2025-11-15 08:05:59,1
Intel,nouwn20,"😂 Just has less bandwidth, but still really damn good.",pcmasterrace,2025-11-14 19:19:45,6
Intel,nowythh,"It caps the bandwidth. But that's fine because it's not strong enough to even utilize the full bandwidth that 16 lanes would provide. And those extra lanes would just increase cost.   This *may* be a problem on an old motherboard, because 8 lanes of PCIe *may* be too little (idk, didn't see any tests). But 8 lanes of PCIe4 are more than sufficient.",pcmasterrace,2025-11-15 02:27:44,3
Intel,noy9d8k,"You need to change your expectations if that is your point.  If you buy a stick of RAM today, it is going to run at half speed if you don't go into the bios and correct it. If you don't want to tune your PC, don't build one yourself.",pcmasterrace,2025-11-15 08:53:32,-7
Intel,np2cs9i,Looks like we found someone poor AND stupid over here.,pcmasterrace,2025-11-16 00:20:04,1
Intel,noyj6di,"https://preview.redd.it/5utu2xxvee1g1.png?width=5240&format=png&auto=webp&s=a281c9815d6408496deeaa752ed61862de335754  No, I can choose which GPU I use for rendering. That's basically what laptops are doing this with multiple GPUs. I just set these environmental variables in the steam's launch parameters: DXVK\_FILTER\_DEVICE\_NAME=""NVIDIA"" \_\_NV\_PRIME\_RENDER\_OFFLOAD=1 \_\_GLX\_VENDOR\_LIBRARY\_NAME=nvidia   and it will run on Nvidia. I can do the same with any other app, like browser, blender, etc. The screenshot shows an example of the game running bare-metal on nvidia-gpu and being displayed on intel GPU.  To not repeat the same parameters with every game, I've made a simple bash script that just exports these variables (and few more, like DLSS upgrade, etc), so I just type ""setenv %command%"" in steam lauinch parameters and that's it.  When running a VM, I use looking-glass, which is different than streaming the game to another device over LAN (like in your example with gamepass). Looking-glass uses shared memory to copy frames from one device to another, so it only works within one PC, but the video is uncompressed and with minimal latency (like 2-3ms of added latency). This is essentially the same path as offloading rendering but with one additional limitation - windows forces vsync, so I need to run as high refresh rate inside windows as possible to reduce latency. I use Virtual display adapter that allows me to set up to 500Hz and run desktop without any physical monitors attached.",pcmasterrace,2025-11-15 10:36:56,2
Intel,noykase,"Trine 2 is not that demanding, both GPUs can run it at max settings just fine. This is also true with a lot of older games. With new games it wouldn't work like this, but there is another way. Linux has a built-in multiseat, that allows logging in 2 different users at the same time, giving them access to different monitors, input devices, etc. I could use a 4k dummy plug to enable multiseat on that device and stream from there to the TV, but I don't have it configured right now, as I still need VM sometimes, so usually it's enough.",pcmasterrace,2025-11-15 10:48:26,1
Intel,novcfue,"So like a v6 Camaro? Not as fast at the suped up v8, but still better than a Corolla?",pcmasterrace,2025-11-14 20:42:36,3
Intel,noybaib,"I think it’s less about “not getting the full performance”, and more about it being a fire hazard.   Especially when you pay that kind of money for it.    In fact, one could argue that the consumer shouldn’t be able to make your product into a fire hazard, even if they aggressively fuck around with software parameters. As long as no modifications were done to the hardware - it should be perfectly safe. Sure it may not function properly, but it shouldn’t melt itself.",pcmasterrace,2025-11-15 09:13:51,2
Intel,noyhewb,"Your ram clocks actually depends on your other hardware, its not the case for gpu. Its fine that you like it, but other may not, and nothing on the box say i need to understand something about overcklocking\\downcklocking. Also i doubt that some random dude will spend time to play with your gpu settings if you buy prebuilt, you will just get it as it is.",pcmasterrace,2025-11-15 10:18:19,0
Intel,novljpy,"More like the transmission than the engine. PCIe lanes determine how much compute (horsepower) can get to the CPU (Road) so if your GPU (Engine) is performing really well but is bottlenecked by PCIe lanes, you aren't really getting that level of performance.   It's also why this objection to the Intel cards is a bit of a moot point. If you are running a v6 than you don't need the best transmission, the car isn't making the power to necessitate more HP to the wheels.",pcmasterrace,2025-11-14 21:30:15,9
Intel,noydha4,"I'm the guy that bypasses restrictive software and hardware to do what I want, so I won't advocate for any more dummy proofing. That is one of the reasons I hate Apple.",pcmasterrace,2025-11-15 09:37:18,-1
Intel,now1zef,"So sounds like an Average Joe won't notice, especially compared to other cards of the same price?",pcmasterrace,2025-11-14 22:59:36,1
Intel,noylp79,"I don’t mind tinkering(in fact I enjoy it a lot, both at my job as a software dev, and as a hobby).  However, hardware+firmware(when not modified) should not allow software to set unsafe parameters, period. And the fact that those unsafe parameters are set from the factory - is all the more insane.",pcmasterrace,2025-11-15 11:02:45,1
Intel,now4agd,Of all the differences between cards at that price range the number of PCIe lanes it has will be the last thing you'll notice.,pcmasterrace,2025-11-14 23:13:14,3
Intel,nnr5s66,"A couple things stand out to me here:  - CPU not going above 60°C is strange, unless you’re running a very beefy water cooler. Did you try running something like Cinebench and compare the results to other 5700X?  - Are those 16gb of RAM a single stick or two? If two, are the in the correct slots (one free slot between the two)?  Intels gpu drivers are known to be quite heavy on the CPU, but I don’t think a 5700X should struggle that much.",pcmasterrace,2025-11-08 12:09:10,2
Intel,nnr6ah2,So  the cooler is not anything special like an assasin spirit evo 120 but I got 2 new coolers for the case because i was sure heating was the problem.But under load It researched 75c yesterday. but it runs cool thats what I was trying to say.I will fix that.  Dual slot and they are in the first and third slot.  My motherboard doesnt support PCIE4 and people told me that could be weird but from everything that i saw on videos and forums it shouldnt affect that much the fps.,pcmasterrace,2025-11-08 12:13:31,1
Intel,nnr7cdj,"The slow RAM is definitely dragging your CPU down a bit, but it shouldn't be that much of an issue. PCIe 3.0 should also be fine for that combo.  Have you tried running a CPU benchmark?",pcmasterrace,2025-11-08 12:22:19,1
Intel,nnr7gsb,yes and  yes furmark I think? Seemed normal.,pcmasterrace,2025-11-08 12:23:20,1
Intel,nnr7j74,"but thats my problem everything should be normal and ok running ,not optimally ,but its much worse.",pcmasterrace,2025-11-08 12:23:53,1
Intel,nnrsmf2,What were the results? Especially Cinebench?,pcmasterrace,2025-11-08 14:41:00,1
Intel,nnrzmk3,Cinebench 2024 did it now and it is 757 multicore,pcmasterrace,2025-11-08 15:20:53,1
Intel,nmzuwyb,Can you up your budget to $350?  9060xt 16gb is more powerful than wither of those options and vram should keep it relevant a long time.,pcmasterrace,2025-11-04 02:55:03,4
Intel,nmzva9l,"I think the b580 is the better buy. The 5060 has worse fps/dollar, and less vram",pcmasterrace,2025-11-04 02:57:13,5
Intel,nmzw2cc,At a base it's around 20ish percent better but the extra vram in the b580 might help it last longer.  I'd go for the b580,pcmasterrace,2025-11-04 03:01:49,1
Intel,nmzxsn6,"Intel Arc is still today for a  Tech Savy Enthussiast with alot of patience,   There is alot of probleme to be fix still.  The 5060 is your best bet if you want to game without have to troubleshoot every time. and the software stack (feature) is superior, Even if stability-wise the last year of NvidiaApp driver as been a hit and miss.  Radeon would be a choice but checking the current pricing for 9060 xt 8go variant. the saving dont make up for the nvidia advantage.  Specialy that in a close futur Nvidia Neural Texture compression is going to leave Beta and turning the VRAM argument to old history.     [https://www.youtube.com/watch?v=fhECKZQI\_Y8](https://www.youtube.com/watch?v=fhECKZQI_Y8) Here you can see a recent revisit of the Intel Arc B580 driver situation.",pcmasterrace,2025-11-04 03:12:16,0
Intel,nmzwux3,Never looked into AMD cards (only reason I looked at Intel was because I was bored in my hardware class) but I will take a look,pcmasterrace,2025-11-04 03:06:37,1
Intel,nmzy4sx,"The b580 is a decent budget card, but the 16gb vram of the amd make it an attractive option for the price point.",pcmasterrace,2025-11-04 03:14:21,4
Intel,nmjxf4y,i4-14700 and Arc B580.. not saying any of those choices are wrong... but the i7 is a high end CPU paired with a budget GPU. i5-14600K and 9060XT could've been better but ok... and also the Arc B580 can't really play on 4K without HEAVY XeSS.,pcmasterrace,2025-11-01 16:14:26,4
Intel,nmkeyx2,"It is a pretty questionable HTPC build tbh. You definitely should’ve allocated more money to the GPU.   14700 is a pricey chip, I’m sure the 14600k is cheaper and you easily could’ve power limited it in bios. Alternatively a 14400 would’ve been fine too.  As well I went ddr4 with my build and it was quite a bit cheaper.   Sf750 for this build is super overkill. It’s a great PSU, I have 2 of them, but they are very pricey.   Just the cost of fans you probably could’ve gotten a 9060xt as well.",pcmasterrace,2025-11-01 17:44:02,2
Intel,nmk0lh8,"This is not a gaming PC, it’s a media centre PC. For gaming, I use an RTX 4070 Super. The 65 watt i7 was chosen because it’s a cool running CPU, averaging 30–32 degrees in this build. Also, to semi future proof it for if I ever do turn it into a gaming PC. The Arc B580 was chosen for it’s great through-hole design that helps with airflow, which is beneficial given this SFF PC’s restrictive layout. That said, I have another build with a ARC B580 that games at 1440p just fine, so this PC would still be perfectly capable for gaming.",pcmasterrace,2025-11-01 16:31:03,7
Intel,nmkm09v,"It’s pointless to invest more money into the GPU at this stage, given that this is a media centre PC and not a gaming rig. The 65 watt i7-14700 was chosen not only because it’s a cool-running CPU, averaging 30–32 degrees, but also to semi future-proof the system in case I decide to convert it into a gaming PC later on. If that happens, I’d be using a higher-end GPU and wouldn’t want to be limited by an i5. The Arc B580 was selected because it’s more than capable for my current needs and features a through-hole design that helps with airflow. As for the PSU, while a 750-watt PSU isn’t strictly necessary, it’s 2025 and with GPUs continuing to demand more power, I wouldn’t go any lower. With the way this build is set up, I’d only need to upgrade the GPU in a few years, and that’s only if I choose to turn it into a gaming PC. The fans, been using Noctua fans for many years and I wouldn’t touch anything else.",pcmasterrace,2025-11-01 18:19:59,1
Intel,nmk3piz,Lol sorry mb. But you should mention that somewhere,pcmasterrace,2025-11-01 16:46:59,1
Intel,nmkoln6,What does media center pc mean in your context? For me my HTPC is being used like a gaming pc for my TV.   Also Intel is very good with idle power draw but make sure you’ve limited the boost wattage. For me I’ve limited my 14500 to 95w on short bursts. The 14700 should have a higher base boost power.,pcmasterrace,2025-11-01 18:33:33,1
Intel,nmkq92m,"I mean just watching movies, listening to music, streaming and video encoding. No gaming.  I also have a 14500, a 14700K, and another 14700 in three other PCs. Those are gaming systems, and I’ve found that when set to Intel default settings, they average around 60–65 degrees during gaming without me needing to enforce power limits, since Intel default settings enforce them automatically.  Edit: 60–65 degrees for the non-K CPUs. My 14700K runs more like 70–75 degrees. Honestly, I much prefer the non-K 14700, since in my benchmarks it performed only 3–5% lower, yet ran cooler and used less power. Not like anyone overclocks these days anyway.",pcmasterrace,2025-11-01 18:42:19,1
Intel,nmhduxm,"Welcome to the PCMR, everyone from the frontpage! Please remember:  1 - You too can be part of the PCMR. It's not about the hardware in your rig, but the software in your heart! Age, nationality, race, gender, sexuality, religion, politics, income, and PC specs don't matter! If you love or want to learn about PCs, you're welcome!  2 - If you think owning a PC is too expensive, know that it is much cheaper than you may think. Check [http://www.pcmasterrace.org](http://www.pcmasterrace.org) for our famous builds and feel free to ask for tips and help here!  3 - Consider supporting the folding@home effort to fight Cancer, Alzheimer's, and more, with just your PC! [https://pcmasterrace.org/folding](https://pcmasterrace.org/folding)  4 - Do you need a new PC? We're giving away a high-end PC build in a WORLDWIDE constest: https://www.reddit.com/r/pcmasterrace/comments/1nnros5/worldwide_giveaway_comment_in_this_thread_with/  We have a Daily Simple Questions Megathread for any PC-related doubts. Feel free to ask there or create new posts in our subreddit!",pcmasterrace,2025-11-01 03:43:10,1
Intel,nmerzpv,Meanwhile AM4 still getting new CPU's,pcmasterrace,2025-10-31 18:04:33,2656
Intel,nmem9lw,"AMD have clarified their original stupid as fuck statement mate. Apparently what the utterly inept fuckwads in their marketing department were MEANT to say, according to the engineers, is that any NEW software tech that's developed from now on, will only be available for 7000 series and up GPU's. RDNA1 and RDNA2 GPU's will continue to receive the same game ready style drivers as they always have.   It just means that if AMD develop anything new, like FSR4, it won't be made backward compatible with older cards. A lot of us have been saying for years that AMD's entire marketing team should be fired, and replaced with people from the engineering department, because this level of fuck up is literally so common that their nickname these days is ""Advanced Marketing Department"", and not in a good way.",pcmasterrace,2025-10-31 17:36:00,3225
Intel,nmev636,"neither, whip out your voodoo and start blasting some quake.",pcmasterrace,2025-10-31 18:20:58,116
Intel,nmem1jh,"check toms hardware,they just cant communicate properly,your hardware is fine you will get updates and game support",pcmasterrace,2025-10-31 17:34:53,591
Intel,nmf9ssy,"My 6700xt hasnt actually gotten any of the new features in the last few years anyway. All this did was make it official. As long as my games play the way they always have though, who cares",pcmasterrace,2025-10-31 19:36:54,69
Intel,nmf0s2e,"It’s 2025, there are no ethical billion dollar companies.",pcmasterrace,2025-10-31 18:49:46,64
Intel,nmelym2,"Neither, backlog is the way.",pcmasterrace,2025-10-31 17:34:28,57
Intel,nmeljgi,"Turn back and chill with your older, but still perfectly adequate, hardware.  My 3050 is still going strong, I just can't play with top settings.",pcmasterrace,2025-10-31 17:32:18,108
Intel,nmerk8m,I don’t think the 5070 or 5070ti are melting. So that’s where I’m at.,pcmasterrace,2025-10-31 18:02:22,37
Intel,nmeoaf1,This is if you have fomo. I have a base model 4070 carrying me just fine after using a 1080ti sense 2019.,pcmasterrace,2025-10-31 17:46:04,6
Intel,nmevy57,"I dunno, I'll take old drivers over potential house fire.",pcmasterrace,2025-10-31 18:25:00,25
Intel,nmf8eac,Probably Lisa Su right now:  ![gif](giphy|W3a2OU6RGyv7OmJaw0),pcmasterrace,2025-10-31 19:29:24,6
Intel,nmfi83p,my 13 year old 7950 still works and runs games.  support doesnt mean shit,pcmasterrace,2025-10-31 20:21:48,5
Intel,nmfxny9,at least minimum support means shit still works.  It just means that it won't get better. The performance doesn't really degrade.  Melting Cables on the other hand means you're fucked.,pcmasterrace,2025-10-31 21:48:15,14
Intel,nmexs17,2070s.. *there is still fight left in me*  Gotta hold on for another year.. or until next gen of gpu’s arrive..,pcmasterrace,2025-10-31 18:34:25,8
Intel,nmgzn7o,I don’t understand what kind of support you’re looking for on a 5 year old card? Idk about windows but on Linux cards even older than that work fantastic.,pcmasterrace,2025-11-01 01:59:45,3
Intel,nmi5lex,"doesn't amd's case only apply to the windows drivers? on linux the drivers are open source, so realistically the support ends whenever the drivers stop being maintained by the community.",pcmasterrace,2025-11-01 08:25:28,3
Intel,nmia5lz,Thank God the 5070 TI is fine had it for about 6 months now and no issues at all The burning connector is she seems to only be with the 5080 and 5090s mostly,pcmasterrace,2025-11-01 09:16:28,3
Intel,nmiuc2f,"Are 5090 still melting? I have seen like 5 the entire year, reddit would make me think there 10 gpus a day burning",pcmasterrace,2025-11-01 12:25:18,3
Intel,nmlr1yz,https://preview.redd.it/v5r80zlrwpyf1.jpeg?width=1179&format=pjpg&auto=webp&s=12a01e28df4e2a710fb7d1d511a3b78987bd5bd7,pcmasterrace,2025-11-01 21:59:09,3
Intel,nmf55yh,"Drivers are still supported on 6000 and 5000 series cards, just no new game optimization for old cards.",pcmasterrace,2025-10-31 19:12:24,5
Intel,nmf5i11,I had such high hopes for celestial and battle mage,pcmasterrace,2025-10-31 19:14:10,2
Intel,nmfaqi9,The statement of AMD wasn't true though. Seems to be an old problem with their marketing people fucking up stuff.,pcmasterrace,2025-10-31 19:41:56,2
Intel,nmfbesp,Intel: Best I can do is layoff half the company,pcmasterrace,2025-10-31 19:45:33,2
Intel,nmfqqm7,Amd because I don't want to bother with proprietary drivers,pcmasterrace,2025-10-31 21:08:02,2
Intel,nmfxtaa,"AMD has good open source drivers, i don't see a problem.",pcmasterrace,2025-10-31 21:49:09,2
Intel,nmg033i,its simply not true.,pcmasterrace,2025-10-31 22:03:01,2
Intel,nmg0mdk,Hey! Thats not fair! Don't forget: games flagging 'updated/current' AMD drivers as a problem asking the user to rollback the driver until the problems are fixed!,pcmasterrace,2025-10-31 22:06:21,2
Intel,nmhdoui,*Red Sun in the Sky Plays*,pcmasterrace,2025-11-01 03:41:50,2
Intel,nmi7049,"Dont forget the w11 market manipulation to fuel the AI bubble, as well as DRM chips. lol... at this point its sad how latestage capitalism isn't for the market, and only for the profit.",pcmasterrace,2025-11-01 08:41:35,2
Intel,nmjsvtf,https://preview.redd.it/60sslsa23oyf1.jpeg?width=1080&format=pjpg&auto=webp&s=66010cbb44e829162bdbe445a7bdb2b9b850a1a6,pcmasterrace,2025-11-01 15:50:52,2
Intel,nmg6sps,NVIDIA by a mile and it's not close. DLSS alone should make the choice an easy one. If you are able to not fuck up plugging in the GPU power cable you've only got a 99.9999% chance of being just fine.,pcmasterrace,2025-10-31 22:46:19,4
Intel,nmgabl4,"I mean, my 5700 xt is older than covid and still getting updates.",pcmasterrace,2025-10-31 23:09:22,2
Intel,nmhs8dn,I will never regret 7900XTX. It's a beast.,pcmasterrace,2025-11-01 05:55:52,3
Intel,nmi3zn0,Lol I use AMD on Linux.  Long live my RX480.,pcmasterrace,2025-11-01 08:07:28,3
Intel,nmid8uq,"People like to shit on Nvidia but I've never had an issue with the hardware, and the software and support is buttery smooth.  Amd experience is like pulling teeth...",pcmasterrace,2025-11-01 09:50:07,4
Intel,nmix5jz,meanwhile AMD GPU users of 13 year old cards are still getting regular updates. [On](https://www.phoronix.com/news/AMDGPU-More-GCN-1.0-SI) [Linux](https://www.phoronix.com/news/Linux-6.19-AMDGPU-Analog).,pcmasterrace,2025-11-01 12:45:33,2
Intel,nmeuffv,my rx 5700 driver always crashing at radom at cyberpunk 2077 and Battlefield 6. it becames anoying at the point that my next gpu will be a Nvidia,pcmasterrace,2025-10-31 18:17:07,5
Intel,nmftmwf,"The lies against AMD spreading fast, wow.",pcmasterrace,2025-10-31 21:24:40,4
Intel,nmf11dl,NVIDIA. Much more stable drivers and the melting cables are almost always user error and less than 1% of cards shipped. Use the adapter. Don't buy into the freakout. 99% of people have zero issues. I had to switch off my 7900xtx to a 5080 and it's night vs day on stability and quality.,pcmasterrace,2025-10-31 18:51:08,5
Intel,nmgttxj,AMD already clarified they're NOT dropping support for RDNA 1 and 2.,pcmasterrace,2025-11-01 01:19:33,3
Intel,nmhfmfu,"Y’all are dumb. And it shows.  Tom’s Shitware misinterpreted the translation from German website and AMD then had to clarify that they aren’t dropping support. RDNA1 and 2 will still get updates, will still work with future games.   https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games   (Unironically had to be shared by tom’s shitware again)",pcmasterrace,2025-11-01 03:57:24,2
Intel,nmgkg59,"Welp, Arc is now out the window now that Nvidia is an Investor of Intel and encouraging Intel to use their low end Chips as iGPUs.  Your only real hope is AMD.",pcmasterrace,2025-11-01 00:16:08,4
Intel,nmeoa0h,Wait until he hears about zeus bolt,pcmasterrace,2025-10-31 17:46:01,2
Intel,nmesvgb,Arc,pcmasterrace,2025-10-31 18:09:06,2
Intel,nmg8gqo,I love how kids think this is a brand new phenomenon.  They've never heard of Terascale...  Plus AMD themselves have clarified their previous statement so there's that. You all can calm down now...,pcmasterrace,2025-10-31 22:57:12,2
Intel,nmja8de,"For those who use Linux, you'll never have driver problems with AMD, lmao.",pcmasterrace,2025-11-01 14:08:07,2
Intel,nmfdzy5,"When 9060xt showed up its ugly 8 GB head, Steve said that AMD should just shut the fuck up. In fact, most of the video (titled 9060xt) was a huge rant about their (AMD's) marketing department. They really need a Spanish Inquisition style cleansing.",pcmasterrace,2025-10-31 19:59:14,2
Intel,nmevqd4,I just wait awhile between GPU upgrades I used a 2080 up until this year it still worked perfectly fine but I had to start dropping seetings to medium or so to keep 120+ FPS so I just got a 5080 and will probably upgrade in another 2 or 3 generations.,pcmasterrace,2025-10-31 18:23:53,1
Intel,nmexts4,"Brother, you have a 7900gre",pcmasterrace,2025-10-31 18:34:40,1
Intel,nmf93ux,Why not both with AsRock and Sapphire?,pcmasterrace,2025-10-31 19:33:10,1
Intel,nmfa5iy,I'm praying my MSI RTX 3080 Suprim X card doesn't die until NVIDIA figures out how to make a proper card again. Still trucking along fine on medium/high settings.,pcmasterrace,2025-10-31 19:38:46,1
Intel,nmfcont,What's even happening with intel arc at this point ? I remember seeing some stuff about intel moving away from discrete gpus. Theres also been reports of them partnering with nvidia for something.,pcmasterrace,2025-10-31 19:52:19,1
Intel,nmfidxc,Why is AMD only supporting GPU for short time?,pcmasterrace,2025-10-31 20:22:39,1
Intel,nmfihwm,"Me, using an Intel Arc card I got on sale: ""Ha. I am superior to neither of these.""",pcmasterrace,2025-10-31 20:23:15,1
Intel,nmflh9k,"gamers are no longer my friend, there is only AI",pcmasterrace,2025-10-31 20:39:19,1
Intel,nmfls1v,buy a 5060 or 5070... they dont burn cables,pcmasterrace,2025-10-31 20:40:54,1
Intel,nmfmpd9,![gif](giphy|3oz8xLlw6GHVfokaNW)  Me with my 7900XTX,pcmasterrace,2025-10-31 20:45:50,1
Intel,nmfofqd,Never had a cable melt on any of my cards.,pcmasterrace,2025-10-31 20:55:10,1
Intel,nmfox6f,average closed source drivers L,pcmasterrace,2025-10-31 20:57:47,1
Intel,nmfpsyk,"I have a 4090 with an ATX 3.1 power supply, my cable has been good for two years and I game on it pretty much every day.",pcmasterrace,2025-10-31 21:02:41,1
Intel,nmfrzy5,https://preview.redd.it/4fh35v40kiyf1.jpeg?width=6240&format=pjpg&auto=webp&s=8d0b4fc823e9a4d83a926370927cbe6c14ad0b60  The secret ending,pcmasterrace,2025-10-31 21:15:14,1
Intel,nmftp6y,I’ve had no problems with my 7800xt,pcmasterrace,2025-10-31 21:25:02,1
Intel,nmfuwzz,I'd buy 5090 if not melting cables.,pcmasterrace,2025-10-31 21:32:07,1
Intel,nmfvd9v,Amd for the win,pcmasterrace,2025-10-31 21:34:46,1
Intel,nmfxhzq,Ironically the best new feature I ever got for my 3070 was FSR framegen. I don’t really understand the expectation of new features that were never promised.,pcmasterrace,2025-10-31 21:47:16,1
Intel,nmg20rn,All of this was reported inappropriatly by toms hardwareas well.  I absolutely loved in 2010s era but the fact that anybody gives it any attention to anything they publish in 2025 is beyond me.,pcmasterrace,2025-10-31 22:15:12,1
Intel,nmg25jn,"What percentage of cables do you really think has melted bro? 0.0001%? Meme aside, stupid shit like that is not worth even considering as a factor for a purchasing decision.",pcmasterrace,2025-10-31 22:16:03,1
Intel,nmg2e7u,Not just the support the performance and industry support for amd is abysmal to say the least,pcmasterrace,2025-10-31 22:17:35,1
Intel,nmg4dbv,Stable drivers > Day 1 Game Ready drivers with issues  I'd rather have a working outdated driver than troubleshooting and rolling back a new poorly coded driver,pcmasterrace,2025-10-31 22:30:21,1
Intel,nmg5pxe,Arc is effectively cancelled.,pcmasterrace,2025-10-31 22:39:14,1
Intel,nmg5zh7,My 4090 is fine,pcmasterrace,2025-10-31 22:40:58,1
Intel,nmg64rs,And then there is UE5.  Sad days. Really sad days...,pcmasterrace,2025-10-31 22:41:56,1
Intel,nmg8907,"don't forget Nvidia's shitty drivers. They somehow managed to fuck up my latest driver install on my 4070 mobile and now I can't play unreal engine games, OpenGL games, or Vulkan games without crashes/not working at all. I also can't easily install drivers because Nvidia driver installer has a problem with Intel Hyperthreading and throws a 7z.crc error every time you try installing drivers, you have to go to msconfig and limit number of processors to 2 when doing a driver update",pcmasterrace,2025-10-31 22:55:47,1
Intel,nmgd1xz,"Wait, there's people updating drivers? I've been stuck on a version from 2021, granted it's an RX 580",pcmasterrace,2025-10-31 23:27:02,1
Intel,nmggpu8,I have a 5070ti what's  with the melting port thing ? Just had mine for like a month,pcmasterrace,2025-10-31 23:50:59,1
Intel,nmgia14,"I wouldn't get your hopes up for Intel, [discrete GPUs may be on the chopping block](https://www.notebookcheck.net/Intel-Arc-Celestial-dGPU-seems-to-be-first-casualty-of-Nvidia-partnership-while-Intel-Arc-B770-is-allegedly-still-alive.1118962.0.html) following their NVIDIA partnership.  Not confirmed or anything, but it is also notable Tom Petersen was on a podcast a week ago and [refused to talk about Celestial](https://videocardz.com/newz/intels-tom-petersen-explains-xe-and-arc-gpu-naming-spaghetti-declines-to-discuss-b770-and-celestial-updates). And even if discrete GPUs still do happen with Celestial, will they keep going after that?",pcmasterrace,2025-11-01 00:01:27,1
Intel,nmgieb9,"For anyone tempted by big blue, my ARC B580 has been fantastic in both Windows and Linux.",pcmasterrace,2025-11-01 00:02:16,1
Intel,nmgpjhy,Intel Arc is fire and more people should give it a chance.   The B580 gave me an absolutely beautiful experience.,pcmasterrace,2025-11-01 00:50:37,1
Intel,nmgqc7l,"I have felt like such the fucking outsider when it comes to like 99% of hardware gripes...  Oh anything past a 30 series card is gunna melt!   - Really? Cause my 3060ti litterally never complains regardless of what Im doing  Old Ryzen chips and board are fucking trash and never last  - Really my First gen Ryzen 5 2600x has litterally never failed me after being ran nonstop for years, which is understandable horrible...  You wont be able to sustain decent thermals without XYZ AIO etc etc etc...  - ./looks at temp 35c with a stock cooler and only 2 intake fans. ' Uh ok? '",pcmasterrace,2025-11-01 00:55:55,1
Intel,nmgqdne,Arc = Nvidia now,pcmasterrace,2025-11-01 00:56:10,1
Intel,nmgt0mj,I mean there is a 3rd path with Intel arc.,pcmasterrace,2025-11-01 01:13:53,1
Intel,nmgt0ok,"If you had asked me when I bought the 3090 that it would be the last card I’d ever buy I would’ve laughed, but here we are. Not only would I have to upgrade my PSU, but I’d risk the melting cables",pcmasterrace,2025-11-01 01:13:54,1
Intel,nmgt96s,"Sitting here with my Intel GPU and AMD CPU, wondering how soon I'll get fucked over",pcmasterrace,2025-11-01 01:15:34,1
Intel,nmgtmh7,"Me, as a shareholder of both: 💀",pcmasterrace,2025-11-01 01:18:06,1
Intel,nmh25o8,"That's called the middle, walk the untreaded path or walk back",pcmasterrace,2025-11-01 02:17:23,1
Intel,nmh283o,"Intel ARC is dead. Termination of Intel ARC would have been part of the Nvidia deal. If it was not, Nvidia was freely  helping ntel to gain a solid market share of gpus by temporarily providing a good GPU until ARC becomes a solid product. Listen!! INTEL ARC IS DEAD!¡",pcmasterrace,2025-11-01 02:17:52,1
Intel,nmh2iv0,"Amd support is only relevant if you are in windows, come to the dark side, we have unlimited driver support.",pcmasterrace,2025-11-01 02:20:02,1
Intel,nmh2zbk,If have an AMD RX 480 and I still receive driver updates every few months. I bought my card on 2017 I think I'm alright.,pcmasterrace,2025-11-01 02:23:14,1
Intel,nmh39j4,High regards,pcmasterrace,2025-11-01 02:25:14,1
Intel,nmh76mu,*0-5 years old cards...   The Ryzen Z2A was just released with the white Xbox Ally. It uses RDNA 2... In late 2025... When they already released RDNA 4.,pcmasterrace,2025-11-01 02:53:14,1
Intel,nmh9z20,Someone explain plz,pcmasterrace,2025-11-01 03:13:38,1
Intel,nmhaq5i,"I know intel had problems with their cpus, and i stopped buying them becaue of it.  But has anyone heard of any problem with their GPU's ? I'm interested in them but don;t want to guy them and find out their GPU's rot too...",pcmasterrace,2025-11-01 03:18:54,1
Intel,nmhbqg2,"This is Intel's chance for a comeback, if they can secure it. There's a gap in supply to a growing demand.",pcmasterrace,2025-11-01 03:26:27,1
Intel,nmhck3l,Intel Arc is a shovel next to the rock telling you to dig your way out but the shovel is only 10 dollars.,pcmasterrace,2025-11-01 03:32:48,1
Intel,nmhcp93,"The alternative is, you don't really need the highest end model",pcmasterrace,2025-11-01 03:33:56,1
Intel,nmhfbtu,RX6000 can't do 60 real FPS 4K in 2025,pcmasterrace,2025-11-01 03:55:01,1
Intel,nmhl876,"melting connectors is an issue for people who can spend $1500-4000 on gpu. If you can pay $1500+ then  amd have no issues with support at this price range (but much cheaper actually). rx580 was supported 7 years, then moved to legacy (which is still decent). Pointless post in many ways",pcmasterrace,2025-11-01 04:46:14,1
Intel,nmhm5zu,My 4070 is sick and y'all are haters,pcmasterrace,2025-11-01 04:54:50,1
Intel,nmhmk78,Support for Radeons can be provided by the community since their drivers are open source. I'm playing SWTOR on high on a 12-year-old Radeon.,pcmasterrace,2025-11-01 04:58:29,1
Intel,nmhmlmw,It's that intel that added one more pin to its every new gen CPU to make it incompatible with previous?,pcmasterrace,2025-11-01 04:58:51,1
Intel,nmhqp1j,so full amd cpu and graphics set. Cause intel melting things too.,pcmasterrace,2025-11-01 05:39:41,1
Intel,nmhrn2k,Man how I wish they would release something like a B780 which competes with the 9070 XT and 5070 Ti because apparently the B770 will compete with the 9060 XT 16GB and 5060 Ti 16GB. Might be wrong though.,pcmasterrace,2025-11-01 05:49:27,1
Intel,nmhxcd2,My 5700xt is running fine…. 2019 card.,pcmasterrace,2025-11-01 06:52:32,1
Intel,nmi0g43,"AMD with ryzen - yes we have supported the am4 platform for 8 years and we're not done yet.  AMD with Radeon - noooo, just because it's still in production and is one of our best selling cards doesn't mean we can support a card from 2 generations ago, that's too much to ask.",pcmasterrace,2025-11-01 07:27:39,1
Intel,nmi4hwr,pc enthusiast in a nutshell.,pcmasterrace,2025-11-01 08:13:05,1
Intel,nmi4l2a,"Yay even more reason to move to Linux, you didn't have to do this AMD! My RX 6900XT will be glad to run on open source drivers",pcmasterrace,2025-11-01 08:14:03,1
Intel,nmi7en6,Im still using my rx580. It's 10 years old,pcmasterrace,2025-11-01 08:46:09,1
Intel,nmi7jcp,"I'm waiting for the B770, says it will be close to the 5070 but cheaper with 16GB VRAM...",pcmasterrace,2025-11-01 08:47:35,1
Intel,nmi9qea,"My 4670 still be getting updates. Not an AMD card though, but an ATi one.",pcmasterrace,2025-11-01 09:11:51,1
Intel,nmia3l4,Sure that intel will find its own arc to piss off consumers too.  At this point we can only hope the positives are more than the negatives.,pcmasterrace,2025-11-01 09:15:50,1
Intel,nmieuq4,"Don't buy the beta GPU (aka RTX 4090 / RTX 5090). People are paying for R&D reason to refine for future RTX 6080 reason, then RTX 6090 issue will be refined for RTX 7080",pcmasterrace,2025-11-01 10:06:39,1
Intel,nmiixfy,"Gamers? Employed adult gamers, maybe. Dude's back there gaming on a P1000 🤷💐",pcmasterrace,2025-11-01 10:47:50,1
Intel,nmire07,"""minimum support"" lol  How many new features are being adding to Nvidia's 2-3 generation old cards?",pcmasterrace,2025-11-01 12:02:41,1
Intel,nmjdady,"You could switch to linux, nyuknyuk.",pcmasterrace,2025-11-01 14:25:58,1
Intel,nmjiaxd,"It's kind of sad at this point that if I want to use nVidia, I literally can't because the cable's unsafe.",pcmasterrace,2025-11-01 14:54:20,1
Intel,nmjlt0b,I chose amd for cpus and nvidia for graphics,pcmasterrace,2025-11-01 15:13:28,1
Intel,nmjs380,Bring in the melting cables !,pcmasterrace,2025-11-01 15:46:47,1
Intel,nmjxqzg,How about monopoly of the brothers...   Amd blocking rocm on all but their top cards means you gotta buy an Nvidia card if you want your own LLM runner.   Hurray for unchecked capitalism...,pcmasterrace,2025-11-01 16:16:09,1
Intel,nmkhpua,There's AMD cards with 12VHPWR connector too. And let's not start on Nvidia's backwards compatibility...,pcmasterrace,2025-11-01 17:58:05,1
Intel,nmkjvbo,4090 performance,pcmasterrace,2025-11-01 18:09:04,1
Intel,nml61po,Very few people are dealing with melting cables.,pcmasterrace,2025-11-01 20:06:10,1
Intel,nmlev2o,It is MIND BOGGLING that a 5 trillion dollar company can’t develop a cable and insist on a new standards that accommodate the cable. Nobody buying a GPU is gonna balk at a new PS standard. You’re telling me an electrical engineer straight out of college couldn’t do the math on this one in under a year for under a million bucks?  5Trillion is 5 milllion million.,pcmasterrace,2025-11-01 20:52:13,1
Intel,nmllv22,"My rx5700xt is fine, problem is new games where devs don’t give a shit about you. Remember PS1-2 games where devs was literaly making magic?",pcmasterrace,2025-11-01 21:30:22,1
Intel,nmuhwsz,https://preview.redd.it/36mqxnyv10zf1.png?width=1920&format=png&auto=webp&s=d9b1e032c5fe4745a9758d21634f5591bd5d7d1d,pcmasterrace,2025-11-03 08:05:39,1
Intel,nmupnno,"I think the shit AMD has now done is either a mistake or they are going on the dark side.  After all, they allegedly ""leaked"" the fsr 4 that can work on GPUs such as 6000 series 7000 , but still all we can do is wait and see. That's our only power",pcmasterrace,2025-11-03 09:28:09,1
Intel,nmx2oyz,One thing in common you’ve across every post about melting cables is the guy was a complete moron and either used some cheap extension or didn’t plug it in properly. I’ve had no issues with my 5090 and know plenty of people who have the same build with 5090s who also have had no issues.,pcmasterrace,2025-11-03 18:06:02,1
Intel,nmygz7h,"Its doesn’t say “I don’t know” or “I’m wrong” because that isn’t in the training data. It doesn’t say this because there is no negative penalty for making a wrong guess when it doesn’t know, and a positive reward if its guess is right. Therefore it makes sense to always guess if it doesn’t know.   Source: open ai released a paper on it, https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf",pcmasterrace,2025-11-03 22:11:29,1
Intel,nn0jdh0,"i mean tbf nvidia doesnt provide any more support than amd does for their old cards. Also, AMD has community supported drivers which while not run by AMD, is a damn sight better than how locked up nvidia cards are. At least AMD doesn't force companies to use a melty cable for 2 whole generations.     Just to clarify, I'm not saying AMD are good towards consumers - but they're a damn sight better than nvidia.",pcmasterrace,2025-11-04 05:46:52,1
Intel,nn1ncvb,Intel drops support even earlier,pcmasterrace,2025-11-04 12:05:20,1
Intel,nn7nbuc,"Ehh, pretty sure melting cable only plague higher end Nvidia GPU  For most folks who went with entry level and midrange, we’re good",pcmasterrace,2025-11-05 09:48:49,1
Intel,nngwxx6,Idiots,pcmasterrace,2025-11-06 19:21:11,1
Intel,nnm8wme,My Vega Frontier is still going strong for 1080p gaming. Idk what everyone's fuckin' problem is.,pcmasterrace,2025-11-07 16:18:09,1
Intel,nmevi6x,Now I'm confused cuz on Linux those gpus will be supported and why not on windows?,pcmasterrace,2025-10-31 18:22:44,1
Intel,nmetpx7,"Man oh man, the amount of vitriol that was in this sub earlier was absolutely barbaric.   From the comments you would think amd broke into their house and shot their pets and loved ones it was insane",pcmasterrace,2025-10-31 18:13:28,0
Intel,nmex5qt,I hate that I read this with Leia's voice in mind. Angry upvote...  ![gif](giphy|Q9yOCHupg1mqrXhfAZ),pcmasterrace,2025-10-31 18:31:15,1
Intel,nmfi0ue,I will still choose amd. nvidia is the apple of gpus.,pcmasterrace,2025-10-31 20:20:42,1
Intel,nmiistn,"If you care about political stuff influencing your purchasing decisions, the US government bought a 10% stake in Intel with money that was supposed to help them set up domestic fabrication from the CHIPS Act. So 10% of every dollar you spend on them goes directly to what the government is doing right now. No that doesn't mean you get any profits from Intel's increasing share price on your tax returns.  I'll stick with AMD.",pcmasterrace,2025-11-01 10:46:35,1
Intel,nmixqej,I personally regretting switching to AMD... Not the perfs but drivers and soft,pcmasterrace,2025-11-01 12:49:29,1
Intel,nmlbcsv,https://preview.redd.it/ct42fn2lhpyf1.jpeg?width=843&format=pjpg&auto=webp&s=e6824c5de4fe7a49f4f722ef3058956086c40f1f,pcmasterrace,2025-11-01 20:34:00,1
Intel,nmml9na,Nvidia and Even Nexus said it's caused by people bending their cables.,pcmasterrace,2025-11-02 00:54:30,1
Intel,nmnjkqr,"the AMD hate is forced. just because they decided FSR 4 wasn't worth it on RDNA 1 and 2, doesn't mean the company is greedy and going to kill support for RDNA 4 in 2028 or whatever. Radeon is still far superior and option to Nvidia, period.",pcmasterrace,2025-11-02 04:27:29,1
Intel,nmep1fa,It’s really not that serious 😂,pcmasterrace,2025-10-31 17:49:44,-1
Intel,nmer1y4,Just buy a 5070ti.,pcmasterrace,2025-10-31 17:59:48,-1
Intel,nmevw6u,Amd cards are also melting...,pcmasterrace,2025-10-31 18:24:44,-3
Intel,nmfa319,"Here's me with my MSI 1070 just about holding it together worried as fuck about all the price scalping, melting cables and lack of choice that will be due in the next year or so, not to mention the crappy operating system choices now.",pcmasterrace,2025-10-31 19:38:24,0
Intel,nmgomrk,"This is miss information.  AMD isn't ending support for RNA 1 and 2, they just aren't going to bend over backwards to optimize the for that specific hardware, and honestly after how many years of optimization I don' think there anything left on the bone.  They didn't even move it to legacy support and you will still download the same launch day driver packages as people with RDNA3 and newer use to play games.   You want know how support AMD really is?  I have a RX590, it was originally released in 2018, which was a was process shrink of the 580, which was a tweaked/revised 480 originally released in 2016.  This architecture is getting close to 9 years old, and  AMD still released a new drivers for both windows 10 and 11 in August. (2025-08-04).     This whole thing is a nothing burger.",pcmasterrace,2025-11-01 00:44:33,0
Intel,nmepagg,Middle path that leads to Hell:  Intel Arc,pcmasterrace,2025-10-31 17:50:59,-2
Intel,nmfffcp,and if anything the melting were mostly the user's fault for not plugging it in correctly lol,pcmasterrace,2025-10-31 20:06:52,-2
Intel,nmeswj6,Should have been specific for the Melting Cables part which have been only on 5090's as far as I have seen.,pcmasterrace,2025-10-31 18:09:16,-2
Intel,nmeta70,The only cables I've seen melting are for the 90's.  Just get an 80 and you're fine.  Also use official cables for your PSU.,pcmasterrace,2025-10-31 18:11:12,-4
Intel,nmh6xsi,"Oh so now that AMD doelsit, we gotta downplay eh, where was everyone losing their minds like when Nvidia did it  Oh right, gotta dick ride AMD because dickriders only exist when it's Nvidia, AMD dickriders are just pro cosumer and therefore not dickriders  Still with the overused joke of melting cables which I suppose is as true as AMDs dogshit drivers",pcmasterrace,2025-11-01 02:51:30,-1
Intel,nmerqds,"Intel Arc whilst there's electric arcs all over, I see what you did there OP",pcmasterrace,2025-10-31 18:03:14,0
Intel,nmesu3n,"I'm not advocating for it, but my strategy was to buy a 5090 and get the 3 year Micro Center warranty in case the cable melts.",pcmasterrace,2025-10-31 18:08:55,0
Intel,nmf9mds,Used to be other way round,pcmasterrace,2025-10-31 19:35:55,0
Intel,nmfnq19,The 5080 does not melt cables. The minimum for 1440p is a 5070.,pcmasterrace,2025-10-31 20:51:19,0
Intel,nmfqmss,Only a couple super high end cards melt cables,pcmasterrace,2025-10-31 21:07:26,0
Intel,nmgb9w3,To be fair your gpu won’t be melting unless you own a 5090,pcmasterrace,2025-10-31 23:15:31,0
Intel,nmgtc3v,"AMD released their first AM4 CPU in September 2016. They have actively supported that socket for nearly a decade now, releasing yet another new AM4 CPU last year.  New versions of Radeon 6000 series GPUs were being released until October 2023 (6750GRE was the last). That's 2 years ago. Imagine buying a brand new product on its release date and within two years they're no longer receiving optimization updates.",pcmasterrace,2025-11-01 01:16:07,0
Intel,nmhvb5k,"Yeah those melting cables... such a worry. I'll never buy nvidia again, unless they are the best so I am getting another 5080 soon.  But those cables man, it's hard to sleep at night knowing that if someone breaks in, pulls out the cable a bit and runs games it might melt a bit.",pcmasterrace,2025-11-01 06:29:42,0
Intel,nmjd08j,"They barely support the latest games, especially on day 0 nothing but random driver timeouts on day 0 with Hell divers 2 The last of us part 1/2 and even past day 0 Cyberpunk having random driver timeouts for months, Dying light 2 had RGB laser show issue for over a year on RDNA3 and even random driver timeouts,  There always a day 0 game that has issues, and sometimes even older games.  It won't matter if they drop support or not they keep launching cards with broken drivers at launch, and they keep getting away with it because there is always that random guy defending AMD.  seriously tho fuck melting 12vhpwr connectors and cables, that issue is just as bad as AMD still struggling to have stable drivers for the latest games.",pcmasterrace,2025-11-01 14:24:20,0
Intel,nmlxey1,"Intel ARC can't save u mate. I heard the division is all but dead. And with nvidia working with intel, they said they would work on CPUs with integrated nvidia processors for GPUs and AI, but didn't say anything about graphics cards. I wish nvidia would throw intel some graphics bones so their GPU division would make gaming GPUs to compete with AMD since nvidia are so focused on AI now.",pcmasterrace,2025-11-01 22:36:05,0
Intel,nmvxeqt,The question is - will you guys buy Intel Arc at all?,pcmasterrace,2025-11-03 14:45:47,0
Intel,nmifkvx,You are literally contradicting yourself.   Nvidia cables only melt when drawing a lot of power which is only done by high end XX80 and XX90 GPUs. Amd does not have a GPU in their lineup to compete with XX80 and XX90s so you literally have no choice but to go nvidia **meaning that your meme makes no sense**.   And if melting cables are truly stopping you... Just undervolt wtf.,pcmasterrace,2025-11-01 10:14:01,-3
Intel,nmf52qu,"I will never buy an AMD card. Their software and drivers are just garbage and their cards are always behind on features. They burned me a few. Times already, never again.",pcmasterrace,2025-10-31 19:11:57,-5
Intel,nmewxcz,the melting cables can be fixed with undervolting your cards,pcmasterrace,2025-10-31 18:30:04,-3
Intel,nmh1n56,https://preview.redd.it/88dgfffa1kyf1.jpeg?width=1262&format=pjpg&auto=webp&s=6aad526f1e9dfad1d91654624387a67eb5a610fd  Every month I steal my grandparents’ credit card and buy new PC parts for free. I don’t care about Nvidia or AMD I’m evil. 😈,pcmasterrace,2025-11-01 02:13:47,-1
Intel,nmh3quz,"Yeah I’ll take a Jerry rigged or underclocked nvidia GPU over a normal AMD GPU. I have an AMD GPU right now in my tower computer and even though my nvidia GPU in my laptop literally died, I’m still 100% going back to nvidia for my next tower pc GPU.",pcmasterrace,2025-11-01 02:28:42,-1
Intel,nmeuh3a,"Intel: lets pretend dx11 never existed, lets just optimize our drivers for the latest and lamest of titles (cybershit 77 onwards).   Btw many ppl buy midrange cards to enjoy older, well made titles at silky smooth frametimes and sky high fps, everything on ultra of course. Thats where intel sucks big time",pcmasterrace,2025-10-31 18:17:22,-6
Intel,nmi39km,"I mean, i am rooting for more competition, but which nvidia card burns the cables? Is it the top of the range? That one does not have an AMD counterpart anyways..",pcmasterrace,2025-11-01 07:59:29,-2
Intel,nmewwyl,"AMD makes cpus as their primarly source of income and their discrete desktop gpus as a side thing. Nvidia was a gpu company, now its a self proclaimed AI company.",pcmasterrace,2025-10-31 18:30:00,1402
Intel,nmfwclc,"""AM4 is practically dead!"" they said for about 10 years now. Not complaining really with my 5800X here.",pcmasterrace,2025-10-31 21:40:30,39
Intel,nmfq7xl,"I hate how people keep saying this. Yeah, TECHNICALLY it's receiving ""new"" CPUs, but they're all just cut down manufacturing rejects of the mainline CPUs with very limited releases. It really doesn't even count.",pcmasterrace,2025-10-31 21:05:03,31
Intel,nmfoy38,https://preview.redd.it/trx8l9vxgiyf1.jpeg?width=1280&format=pjpg&auto=webp&s=65a2ad415073653986377a822c0a62af2315c8d9,pcmasterrace,2025-10-31 20:57:55,14
Intel,nmh5zt1,You mean rebranded cpus of a rebranded cpu?,pcmasterrace,2025-11-01 02:44:41,3
Intel,nmfpv71,Ayyo what,pcmasterrace,2025-10-31 21:03:02,3
Intel,nmfr1c3,Meanwhile in Linux Ati cards still get driver updates,pcmasterrace,2025-10-31 21:09:43,2
Intel,nmfy3zw,New CPU? BAH Rebranded 2020 Release CPU,pcmasterrace,2025-10-31 21:50:58,1
Intel,nmgd6zd,Is there a new X3D coming out? 👀,pcmasterrace,2025-10-31 23:27:56,1
Intel,nmhcpny,Wait…new-new? Like 2025 CPUs? 2026? I haven’t done much research since I already assumed I was locked down to am4 forever.,pcmasterrace,2025-11-01 03:34:02,1
Intel,nmhpa7d,"AMD truly hates its GPU division, lol.",pcmasterrace,2025-11-01 05:25:12,1
Intel,nmiih3j,CPUs.,pcmasterrace,2025-11-01 10:43:23,1
Intel,nmu8k7r,AM4 x3d cpus are no longer produced though,pcmasterrace,2025-11-03 06:31:47,1
Intel,nmentcw,https://preview.redd.it/hctyj3saihyf1.png?width=447&format=png&auto=webp&s=6f5a431d766a026fa82917884724fa4fe09eb795  Basiclly what you said,pcmasterrace,2025-10-31 17:43:46,1967
Intel,nmern3f,Their marketing team is certainly one that gives the vibes of 100% nepo appointments.,pcmasterrace,2025-10-31 18:02:46,456
Intel,nmepn0f,"The Advanced Marketing department thing comes from that deranged weirdo at UBM, if memory serves, and the irony is that he makes that joke to imply that they are only good at marketing, but bad at creating products, which is the literal inverse of reality. AMD's products are pretty good, but jesus christ on a cracker is their market utterly horrendous",pcmasterrace,2025-10-31 17:52:42,167
Intel,nmetlhq,This makes perfect sense with redstone supposedly coming this year of course they were going to split the driver stack. Terrible communication aside this seems pretty obvious,pcmasterrace,2025-10-31 18:12:49,36
Intel,nmesc8m,I hope thats what AMD will actually do and not just PR corporate bullshit meant to save face. Still kinda sucks that 6000 series will never get official FSR 4 support unlike the 2000 series getting DLSS 4 which is a massive improvement.,pcmasterrace,2025-10-31 18:06:21,79
Intel,nmf5qfe,So kind of like microsofts feature support vs extended support  You don't get the new shiny but we fix what you already have  I suppose you'd only see over time if that's a big deal or not. Personally I do like products having at least some on no new features extended support. That way if a new feature brakes things it isn't one of the final patches you see(How long that should be I'm not sure),pcmasterrace,2025-10-31 19:15:23,11
Intel,nmft2ni,">this level of fuck up is literally so common that their nickname these days is ""Advanced Marketing Department"", and not in a good way.  https://preview.redd.it/2rt6ywm3liyf1.jpeg?width=960&format=pjpg&auto=webp&s=51347cbe45ec4ef3faa4e1aa7bace3d26a1eb9ea",pcmasterrace,2025-10-31 21:21:24,11
Intel,nmeuoic,I’ve seen this explainer in like 5 posts over the last day and people are STILL posting misinformation 🙄,pcmasterrace,2025-10-31 18:18:26,40
Intel,nmfwb5f,"Except they say “New features, bug fixes and game optimizations will continue to be delivered as required by **market needs**…”  Basically, Game Optimizations will be released on an inconsistent basis with new drivers, which is still a baffling decision considering RDNA2 marketed optimizations as a selling point 3 years ago.  I expect BARE MINIMUM 6 1/2 years of consistent game optimizations, not 5.",pcmasterrace,2025-10-31 21:40:16,10
Intel,nmf0p1q,Why does AMD even have a marketing team when Reddit and Youtube does it for free and does it 500 times better?,pcmasterrace,2025-10-31 18:49:23,24
Intel,nmezy28,"Honestly their marketing pissed me off when FSR4 was released and they were like "" sooooo we arent bringing that to the 7000xtx after all"".",pcmasterrace,2025-10-31 18:45:32,6
Intel,nmf5wub,"It's still pretty dumb: if eg. FSR4 or any new feature set can improve the performance of an older card, without an inherently crapton of additional massaging, it should be made to do so. And reportedly, FSR4 has already been shown to work on and improve performance of these older generation RDNA cards in modded drivers, so it has been demonstrated that the only obstacle in implementing it is a profit-seeking one. Though saying that is there not profit in providing value for customers on older hardware, doesn't that build the brand, etc. or are they just interested in pushing the next generation of eWaste into the bin sooner.",pcmasterrace,2025-10-31 19:16:19,5
Intel,nmfbg3e,I love your flair,pcmasterrace,2025-10-31 19:45:44,1
Intel,nmffndo,Pro tip to let the marketing department only to deal with product banners and social media accounts and leave press statements to engineers in any company,pcmasterrace,2025-10-31 20:08:02,1
Intel,nmfh6ay,How do they manage to fuck up their only job this badly every single time?,pcmasterrace,2025-10-31 20:16:09,1
Intel,nmfo92j,No no man. I just learned from this meme that AMD is not support graphics cards anymore. Keep up!,pcmasterrace,2025-10-31 20:54:10,1
Intel,nmfutsx,It's amazing how bad their marketing department is.,pcmasterrace,2025-10-31 21:31:35,1
Intel,nmfxl8j,"Advanced marketing devices is exactly how userbenchmark calls amd, but in the opposite meaning  As in their marketing by sponsoring every single youtuber on earth accomplished their desktop cpus to be bought by gamers more than intel",pcmasterrace,2025-10-31 21:47:48,1
Intel,nmg6dil,Oh damn. Thats absolutely not what i thought that announcement was. Thats totally reasonable. Were the marketing department about to do share buybacks or something. Because that level of incompetence would kill people in any blue collar job.,pcmasterrace,2025-10-31 22:43:31,1
Intel,nmg6ykz,"Yeah I saw the right side and was like wtf, they keep supporting 10+ year old cards  lol",pcmasterrace,2025-10-31 22:47:22,1
Intel,nmg7umx,Advanced Marketing Disaster,pcmasterrace,2025-10-31 22:53:10,1
Intel,nmgc1vr,You mean following a panic meeting they decided to throw the marketing team under the bus and backtrack,pcmasterrace,2025-10-31 23:20:32,1
Intel,nmgfmfo,"The answer ive been looking for, thank you!!!",pcmasterrace,2025-10-31 23:43:48,1
Intel,nmghy5e,> the utterly inept fuckwads in their marketing department   https://www.youtube.com/watch?v=tHEOGrkhDp0,pcmasterrace,2025-10-31 23:59:12,1
Intel,nmglpxb,They tried to replace the communications team with AI didn’t they?,pcmasterrace,2025-11-01 00:24:43,1
Intel,nmh3frx,I didn’t read your entire comment but I’d like to say fuck AMD and double-fuck NVDIA,pcmasterrace,2025-11-01 02:26:29,1
Intel,nmhfdaq,AMD wasting money on these marketing people smh.,pcmasterrace,2025-11-01 03:55:20,1
Intel,nmoqx8t,Yeah userbenchmark has been giving that marketing department WAY too much credit lmfao,pcmasterrace,2025-11-02 11:24:31,1
Intel,nmexwzy,">is that any NEW software tech that's developed from now on, will only be available for 7000 series and up GPU's  Well that's disappointing. It's way too soon. I'm sure it's not just due to hardware limitations, since 6000 and 7000 share similar hardware necessary for upscaling. They can't throw that ""we drop support because compatibility blah blah"" card on consumers.  Well on a positive note, it's good that 6000 series will still get FSR4 support thru mods, although that might be limited, as some online games could trigger anticheats.",pcmasterrace,2025-10-31 18:35:07,1
Intel,nmepgcq,That just sounds like BS from AMD.   Why come out and say something you already said. What this sounds like is AMD trying to back peddle a decision that would cost them future sales. The best part is if they get this out there now. Then oops don't update for games moving forward. Then no one will say a word. It will be done and over with.   Everyone knew FSR4 and the like wasn't coming to RDNA1/2 gpu's they said this before. They don't back port new stuff anyway.   So just more corpo speak for hey we screwed up now forget bout it look a shiny thing consumer look at the shiny.,pcmasterrace,2025-10-31 17:51:48,-13
Intel,nmf5yzg,Which I feel like they didn’t need to say? Like obviously not every feature is going to be supported an older hardware already right now FSR 4 is not available on the 7000 series though the rumour is they will have it eventually. Like why is this something the end user needs to be informed about?   I almost wonder if they were planning on pulling Support and then just decided to say that it was misinterpreted because they saw the reaction,pcmasterrace,2025-10-31 19:16:38,0
Intel,nmesu8q,"> RDNA1 and RDNA2 GPU's will continue to receive the same game ready style drivers as they always have.  Where did they say that. All I've found is:  >""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch""  Which isn't the same thing at all.",pcmasterrace,2025-10-31 18:08:56,-10
Intel,nmeouda,"""communications has been bad"" kinda sounds like ""cables aren't melting due to design flaw, rather because user error""",pcmasterrace,2025-10-31 17:48:46,-30
Intel,nmh4s2d,Oh take us back!,pcmasterrace,2025-11-01 02:36:07,4
Intel,nmeqaps,AMD Marketing communicating like idiots!   ![gif](giphy|clotJg9IqBitMcRJ61),pcmasterrace,2025-10-31 17:55:59,224
Intel,nmga2p8,They should just fire their whole marketing department and build a new one i guess,pcmasterrace,2025-10-31 23:07:43,13
Intel,nmffzui,"It's ironic because it's a sub for PC master race, but as a PC gamer it's like the biggest sub I avoid lol. You'll see something posted all over the sub about how it's bad, and then next week its vice versa.",pcmasterrace,2025-10-31 20:09:52,5
Intel,nmh2fca,"I'm pretty sure the clarification had a big asterisk of something along the lines ""we will continue updating them according to market demand"", i think they're still moving them to maintenance mode the same they did for Polaris and Vega which means they will continue necessary bug and security fixes, unlikely to recieve optimizations (tho still possible) but more importantly no new features will be added, this kinda sucks when you consider RTX 20 series which is older than RDNA2 is still receiving partial support for new DLSS features.",pcmasterrace,2025-11-01 02:19:20,3
Intel,nmf4ji2,the original message was slightly ambiguous but then literally every news outlet choose to interpret it in the very worst way possible and then add clickbait headlines on top of that misinterpretation. The resulting mess is only marginally AMD's fault.  They aren't saying anything now that wasn't in what they said originally.,pcmasterrace,2025-10-31 19:09:08,24
Intel,nmicw02,"""New features, bug fixes and game optimizations will continue to be delivered as required by market needs in the maintenance mode branch.""   Isn't the marketing department the one responsible to judge ""market needs""? Not looking good.",pcmasterrace,2025-11-01 09:46:16,2
Intel,nmnjz05,"this is proof that all the AMD hate is extremely forced and happens no matter what they do, they could launch an 24GB GPU for 400 dollars and people would point at it as a reason no to buy Radeon.",pcmasterrace,2025-11-02 04:30:04,1
Intel,nmhovii,The updates AMD meant: Games will be playable.   The updates PCMR expect: Future AI upscaling support on old cards.,pcmasterrace,2025-11-01 05:21:01,0
Intel,nmh2wfn,"It did recieve AFMF and Anti lag 2, but now they pulled the plug and new features won't be added even if they can work on these cards. People should care, nvidia is still shipping new DLSS features to 2018 and 2019 GPUs, we can't be normalizing AMD pulling the plug on feature updates for 3-5 years old cards.",pcmasterrace,2025-11-01 02:22:40,27
Intel,nmgksob,"Same, I don't give a shit about shiny new things I just want it to be stable.",pcmasterrace,2025-11-01 00:18:27,7
Intel,nmnfz43,Help yourself by installing fsr4 int8 with optiscaler. Is game changer for my rx 6800,pcmasterrace,2025-11-02 04:04:07,1
Intel,nmnkoou,"the cards function and gets updates, which is more than people are giving them credit for, proof that the HAte train never stops because people love to do free marketing at any chance for Nvidia, even if it hurts them in the eyes of people with brains. which is unreasonably hard to come across these days.",pcmasterrace,2025-11-02 04:34:55,1
Intel,nmgb5cv,Accurate,pcmasterrace,2025-10-31 23:14:43,11
Intel,nmgrhfv,Nvidia is worth $4 trillion right now. We're way past them _just_ being a multi-billion dollar company.,pcmasterrace,2025-11-01 01:03:24,14
Intel,nmk88ma,"There are no ethical companies , full stop",pcmasterrace,2025-11-01 17:09:33,2
Intel,nmnayfw,did you think there ever were?  Companies worth any meaningful amount are by definition unethical to reach that.,pcmasterrace,2025-11-02 03:30:48,0
Intel,nmeo0b6,"3050 isn't a bad card. It was just too expensive at launch. I was gifted one, and it is now in my kid's PC.",pcmasterrace,2025-10-31 17:44:41,50
Intel,nmeovnm,1070ti here,pcmasterrace,2025-10-31 17:48:57,8
Intel,nmi79ob,">older,  >My 3050  Brother",pcmasterrace,2025-11-01 08:44:37,1
Intel,nml3fpi,My FE 2060 Super is running fine for all the weird indie games I like anyways,pcmasterrace,2025-11-01 19:52:02,1
Intel,nmevd5m,"My 3050 does the job for what I need to just fine. Don't get me wrong, there is a temptation to upgrade to a 3070 (Before anyone comments, it's cheaper than a RX 6700 10GB on the used market( to go alongside my new 180Hz screen, but I'm unsure whether i'll pull the trigger for it or not.",pcmasterrace,2025-10-31 18:22:00,0
Intel,nmeuelw,"At least one 5070Ti did, but it’s certainly less common than the 5080/5090",pcmasterrace,2025-10-31 18:17:00,21
Intel,nmgc1ym,My buddy's 9070 uses the 12 pin and it hasn't melted either. I think it's partially the wattage of the higher end cards running into the very tiny safety margins that's the issue. The cables a have a lot more breathing room in the mid range.,pcmasterrace,2025-10-31 23:20:33,3
Intel,nmnl9p9,you need to educate then. its EVERY card with a 12VHPWR that is melting. including the 9070XTs that come with it.,pcmasterrace,2025-11-02 04:39:07,1
Intel,nmlbk0f,This is exactly me lol. 1080ti was an extreme beast. I should frame that beauty!,pcmasterrace,2025-11-01 20:35:04,2
Intel,nmnlbj4,7970 runs fine here too.,pcmasterrace,2025-11-02 04:39:28,1
Intel,nmgb7o9,You have the oldest gen card that supports dlss,pcmasterrace,2025-10-31 23:15:08,1
Intel,nmf8q8s,It better be ready to do another decade when it becomes affordable enough for me. Too early to hit retirement,pcmasterrace,2025-10-31 19:31:10,1
Intel,nmnlfx8,my HD 7970 is throwing a fit on modern linux updates so i don't think thats necessarily true.,pcmasterrace,2025-11-02 04:40:21,1
Intel,nmfb0dw,Losing day 1 game optimization is basically losing driver support. Security patches won't optimize the new games for your GPU,pcmasterrace,2025-10-31 19:43:25,2
Intel,nmijxo5,People != you.,pcmasterrace,2025-11-01 10:57:09,0
Intel,nmnlv56,the few people who shit on nvidia do so because they deserve it. the people who shit on AMD are doing so for clout and almost always never have an actual reason for it. spread misinfo and paint them out to be inferior and disgusting when its literally the other way around.,pcmasterrace,2025-11-02 04:43:25,-1
Intel,nmfowq3,Did you do some OC/UV?,pcmasterrace,2025-10-31 20:57:43,1
Intel,nmf4ti9,Mine played cyberpunk just fine.,pcmasterrace,2025-10-31 19:10:37,-1
Intel,nmi2jr1,"The cheapest 5080 here is 1050€. Fuck. That. Cheapest used is like 900.  I wish I could say Im glad I dont need a gpu because i have a 3080, but I couldnt even get a stable 60 on 1440p WITH dlss on freaking keeper.  Unoptimized games are actively sponsored by nvidia in secret, change my mind.",pcmasterrace,2025-11-01 07:51:25,2
Intel,nmf55gw,Just a 1% chance it might catch your house on fire. Who wouldn't take those odds.  And all so nvidia could save a a few extra cents.,pcmasterrace,2025-10-31 19:12:20,-6
Intel,nmh7s10,"They said they will optimize for games ""as needed"".",pcmasterrace,2025-11-01 02:57:34,2
Intel,nmi6qeu,YESS! I love fake competition! Now I don't have to choose as much because the entire market is led by 2 companies that are too busy chasing the next wall street fad so they don't have the incentive to make competent products!,pcmasterrace,2025-11-01 08:38:29,1
Intel,nmnm2t6,"not true. older cards can have issues. like my HD 7970 having a massive shit itself session after i update for like the past month, this is a 13 year old card so its not like its actually gonna affect anyone nowadays cause GCN 1.0 is long dead,",pcmasterrace,2025-11-02 04:44:57,1
Intel,nmg0hf7,"A and B are the last desktop GPUs (most likely), and C will be 1 (maybe) dGPU with a few APUs. Idk about Druid if that will be delayed or canceled",pcmasterrace,2025-10-31 22:05:30,2
Intel,nmhxg32,B580 is really good from my friend's experience. Would support team blue/green now onwards at least they dont need to have to have marketing to explain their products and support is there.,pcmasterrace,2025-11-01 06:53:41,2
Intel,nmgqr7x,"No, Intel and Nvidia are just working together for CPUs with integrated graphics.   Arc is here to stay, thankfully!",pcmasterrace,2025-11-01 00:58:33,1
Intel,nmhx0zh,Yeah AMD is like another Nvidia now charging - 50 for subpar feature set. Really hoping for a quick intel win here along with the CPU as well.,pcmasterrace,2025-11-01 06:49:00,1
Intel,nmjio2h,I mean I’m still getting updates to the drivers in my laptop’s 3060.,pcmasterrace,2025-11-01 14:56:20,0
Intel,nmf6l8z,"Linux drivers OpenSource, Windows drivers not..",pcmasterrace,2025-10-31 19:19:52,4
Intel,nmfp7oh,Finally someone got the refrence,pcmasterrace,2025-10-31 20:59:23,1
Intel,nmlbep7,https://preview.redd.it/t52c9zumhpyf1.jpeg?width=1024&format=pjpg&auto=webp&s=ea3d1c720034b8cd8a02f52dfc22082650fb90fb,pcmasterrace,2025-11-01 20:34:17,1
Intel,nmeujlx,This is the sweet spot. I'm considering upgrading my 3060ti for this.,pcmasterrace,2025-10-31 18:17:43,0
Intel,nmffjfb,Which ones? I haven't seen em posted here yet?,pcmasterrace,2025-10-31 20:07:27,3
Intel,nmg19x6,Same. Seriously considering just grabbing a 3060Ti and calling it good.,pcmasterrace,2025-10-31 22:10:29,1
Intel,nmg5n7t,"My ARC a750 has been ace, I still get driver updates every other week or so, can play the games I wanna play and even more intense games like Space Marine 2",pcmasterrace,2025-10-31 22:38:44,1
Intel,nmezmf7,"There has been a case of 5070ti cable melting, though. They have the same bad power delivery setup from what i know, so they can still melt. 5080s can do that too",pcmasterrace,2025-10-31 18:43:54,-2
Intel,nmip1eh,So your solution to a bad product is that you fix it for them. Got it.,pcmasterrace,2025-11-01 11:43:24,1
Intel,nmgggri,I haven't heard of any driver issues in years,pcmasterrace,2025-10-31 23:49:19,2
Intel,nmhplwb,AMD has free and open source drivers on Linux.   Immediately makes Nvidia worse,pcmasterrace,2025-11-01 05:28:29,2
Intel,nmhx9dd,I dont know why people would honestly buy AMD in 2025 unless its either way cheaper or more available in their region.,pcmasterrace,2025-11-01 06:51:36,1
Intel,nmepra0,"Why are you pissed? Amd clarified that they messed up with communication. 60 series cards will still get game ready drivers, just not new tech that comes out. So if/when fsr4 happens, 60 series cards won’t get it. That’s it.",pcmasterrace,2025-10-31 17:53:17,5
Intel,nmepwc7,"Cable issue is due to a well known, incredibly well documented design flaw that has been detailed by people like DerBauer. As to the AMD thing, as I said in my comment in this thread, AMD have already allowed the engineers to clarify the fucktastically stupid statement made by the marketing department. Your friend will be just fine, his card will still get game updates. You're zero for two at the moment.",pcmasterrace,2025-10-31 17:54:00,2
Intel,nmer4cl,They have already reversed this decision. They are not in fact ending game ready driver support. Just new features (think FSR4 / MFG) won't come to older cards.,pcmasterrace,2025-10-31 18:00:08,1
Intel,nmfss5c,Looking forward the bubble bursting finaly. Overvalued fucks. Gonna hurt us all but its better then it growing more. Privatize gains and socialize the loses.,pcmasterrace,2025-10-31 21:19:44,616
Intel,nmg5jf7,"Nvidia were planning on being an AI company for two decades, that is why they’re at the top now",pcmasterrace,2025-10-31 22:38:02,13
Intel,nmh4o0t,I don't know if it is self proclaimed when it has already become their primary source of profits. nVidia is an AI company that also sells gaming/studio GPU's.,pcmasterrace,2025-11-01 02:35:19,4
Intel,nmii8jk,Self proclaimed? They are 100% the market leader in AI hardware.,pcmasterrace,2025-11-01 10:41:02,5
Intel,nmib5ym,"AMD probably has decided for gaming GPUs, it's going to rely more on SOCs. It already has big contract from Sony for Playstation and  Microsoft for the current and next gen series. PS5 Xbox X/S will likely sell a combined of 150 million units at minimum.",pcmasterrace,2025-11-01 09:27:39,1
Intel,nmkdtfr,"was talking with an AMD rep at Kaseya DattoCon this year, mentioned how my company’s media servers run RTX Pro cards because we need the raw power and he said “yeah we’re okay with being number two in the GPU market”",pcmasterrace,2025-11-01 17:38:05,1
Intel,nmkvgu3,Wtf is the opposite of a discrete desktop GPU,pcmasterrace,2025-11-01 19:09:25,1
Intel,nmgoipe,"It is dead, nothing topped the 5800X3D on the AM4 side",pcmasterrace,2025-11-01 00:43:47,14
Intel,nmhcfji,"My girlfriend's GPU died, and she has literally been playing Outer Worlds 2 on an integrated gpu on an AM4 processor. Lowest possible settings of course but it doesn't make it any less funny.",pcmasterrace,2025-11-01 03:31:48,1
Intel,nmkma21,"I mean it's ""dead"" in the sense that there won't be another generation of chips released for it, but AM4 chips and mobo's are still plenty good for gaming.",pcmasterrace,2025-11-01 18:21:21,1
Intel,nmfxvfj,i swear people will say amd is great because they still produce and sell 3000Gs (aka 50$ dual core CPUs on AM4 that are e-waste and on par with a 2015 i5 4460 but with -30W TDP),pcmasterrace,2025-10-31 21:49:31,4
Intel,nmfy92v,"but when Intel refresh their old CPU, people say otherwise  Yeah bullshit AMD fanboy propaganda about AM4",pcmasterrace,2025-10-31 21:51:46,-6
Intel,nmfwh1i,Re-releases of old CPUs with slightly different clock speeds (binning) and names.  The last truly new AM4 CPU was the 5800X3D,pcmasterrace,2025-10-31 21:41:13,19
Intel,nmftjb1,"They're meh. XT series, not really much special just stick to the regular ones.",pcmasterrace,2025-10-31 21:24:05,1
Intel,nmhgp8d,"No, not really new. Just different Zen3 variants. Last real new CPU was the 5800X3D.",pcmasterrace,2025-11-01 04:06:14,1
Intel,nmg384j,"well, on linux amd have 10y+ support with raytracing drivers as far as the rx480  sadly it is linux only. not windows.",pcmasterrace,2025-10-31 22:22:55,159
Intel,nmfwnqu,![gif](giphy|9mtE009hcWPOesk8C4),pcmasterrace,2025-10-31 21:42:18,63
Intel,nmfklzh,"In the like 4 jobs I had in “tech” yeah pretty much.  I’ve mainly seen Nepotism and cronyism affect 3 major areas.   Marketing, HR (the cancer that spreads its own nepotistic disease), and the “head” of an already established mostly autonomous department where their whole schtick is to read the notes about “efficiencies” that they contribute nothing too besides being the one on camera during inter-department company calls. Take a guess where I was lol.   Nepo/crony hires are the worst because they are like the only ppl in the company that can get away with being totally inept.   This does not excuse AMD though since they are way past the point. I wonder if AI is being used to “streamline messaging” as sometimes it just words things wrong and if you don’t catch incorrect nuance or the unintended implications, things like this can happen.   But what the fuck do I know I grew up poor lol.",pcmasterrace,2025-10-31 20:34:38,95
Intel,nmfu69e,It feels like nVidia did the hiring process for their marketing.,pcmasterrace,2025-10-31 21:27:46,5
Intel,nmfwdke,So every marketing team ever?,pcmasterrace,2025-10-31 21:40:39,2
Intel,nmexxic,It is getting support with Redstone release,pcmasterrace,2025-10-31 18:35:12,28
Intel,nmfxu2o,"They arent going to stop fixing glitches on a gpu still in production  Vega still gets updates, its not pr",pcmasterrace,2025-10-31 21:49:17,9
Intel,nmeu67x,"It can’t, any more than RTX2000 can get all the features of the RTX5000 cards. The hardware is just different.",pcmasterrace,2025-10-31 18:15:48,-12
Intel,nmi07da,"Kind of, yes. With the GPU's that means AMD owners still get regular driver updates for new games.",pcmasterrace,2025-11-01 07:24:52,1
Intel,nmf9l80,Because the outrage is worth more clicks and karma than the clarification. People will be using the original info as ammo for a loooong time.,pcmasterrace,2025-10-31 19:35:44,32
Intel,nmjz2a4,"Tbf, this is the first time I've seen this clarification. AMD PR fucked up.",pcmasterrace,2025-11-01 16:23:05,1
Intel,nmk13pm,"the post you're replying to is the one misinforming people.  https://preview.redd.it/l9t6q6imaoyf1.png?width=613&format=png&auto=webp&s=4be9523b7481b01ed8448a1b9336c5951ddcc391  this is the real statement. notice how its says nothing about ""same driver treatment"" and still leaves the cards in maintenance mode?",pcmasterrace,2025-11-01 16:33:39,0
Intel,nmglov4,"Ok but that misinformation is the official announcement, maybe they shouldn't lie in advertising if they want people to tell the truth",pcmasterrace,2025-11-01 00:24:31,-5
Intel,nmfxckp,Are you the user benchmarks guy?,pcmasterrace,2025-10-31 21:46:21,10
Intel,nmfvblw,"I imagine the share holders would freak out if you told them you were getting rid of the entire marketing team, marketing is tangible to the average person compared to say QA or a documentation team.",pcmasterrace,2025-10-31 21:34:30,4
Intel,nmfye40,Because billion dollar companies down give a shit about reddit drama comments. That's not what matters,pcmasterrace,2025-10-31 21:52:37,3
Intel,nmi0e9n,"Lol at the idea that Reddit makes any difference to anything or anyone, anywhere. We're a very insular group here, with far less reach than certain peoples egos might like to imagine.",pcmasterrace,2025-11-01 07:27:03,1
Intel,nmfym5x,"According to userbenchmark, the marketing from Advanced Marketing Devices is mainly buying youtubers reviews to test at unrealistic scenarios where their tech barely wins and publishing fake info online about their shitty cpus",pcmasterrace,2025-10-31 21:53:59,1
Intel,nmgm21s,Especially funny since it's the most powerful AMD card in raw raster,pcmasterrace,2025-11-01 00:27:03,1
Intel,nmi0c29,"No it's not. It's like expecting ray tracing to work on a 10 series card. FSR4 CAN sort of kind of work, but not really. Why expect them to fudge it, when all that'll happen is people will hate wank themselves into a coma over it not working 100% properly.",pcmasterrace,2025-11-01 07:26:20,1
Intel,nmfdcko,"Meh, I just use optiscaler and use xess 2 and will use 3 since fsr4 doesn't work too well on 6800",pcmasterrace,2025-10-31 19:55:50,2
Intel,nmerfwj,Because communication issues exist and sometimes one team thinks they’re better than the other and will say dumb shit that the other team disagrees with,pcmasterrace,2025-10-31 18:01:45,34
Intel,nmes5i1,that message was yesterday,pcmasterrace,2025-10-31 18:05:23,2
Intel,nmevxxb,Thats exactly what this is lol BS.  They have been so scummy with software since the 9000 series.  I'm done with AMD GPUs I think.,pcmasterrace,2025-10-31 18:24:59,0
Intel,nmfl74s,The only one who's shocked is Hasans dog.,pcmasterrace,2025-10-31 20:37:50,32
Intel,nmfg7lm,At least they corrected it before Gamers Nexus had the chance to post yet another ragebait video,pcmasterrace,2025-10-31 20:11:00,15
Intel,nmi7yq5,"The problem with their marketing is that it's been this terrible and unreliable for the company for at least a decade... I mean, remember ""poor Volta""?",pcmasterrace,2025-11-01 08:52:18,1
Intel,nmhgjls,5 trillion*,pcmasterrace,2025-11-01 04:04:57,7
Intel,nmk6ztj,"All the same, pretending Intel’s not gonna be screwing folks just as hard as Nvidia or AMD is pure pipe dreams. It’s all dystopian these days.",pcmasterrace,2025-11-01 17:03:19,1
Intel,nmneqo2,"I’d argue if I could name some, I actually can’t off the top of my head. Just feels like there should be some I could name…",pcmasterrace,2025-11-02 03:55:43,1
Intel,nmndzvk,"No, I agree with you. My only stipulation is that in 2025 we should ALL know better.",pcmasterrace,2025-11-02 03:50:42,1
Intel,nmeovsg,there is no bad there is only bad pricing,pcmasterrace,2025-10-31 17:48:58,43
Intel,nmexcwz,"The 6GB one is what I wish the 8GB was. If the 8GB version was slot-powered and priced like the 6GB one, it may have been more attractive.",pcmasterrace,2025-10-31 18:32:16,6
Intel,nmhi1x5,That’s where my 3060ti is now.   Will run Minecraft and Goat simulator to his hearts content.,pcmasterrace,2025-11-01 04:17:45,2
Intel,nmiar4a,Just because it's not a 1080 doesn't mean that 3050 isn't an older card. It's two generations old.  It's objectively older than the current generation.,pcmasterrace,2025-11-01 09:23:04,1
Intel,nmftydf,"It is also not that common with 5080 it is just the overvolted garbage like Asus and aorus high-end shit with ""liquid metal"" and ""super vapour champer"" and ""ultimate performance"" I mean they do this for GPUs and motherboards you pay more for ""premium versions"" to only get instability    But 5090 tho this shit is flaming",pcmasterrace,2025-10-31 21:26:30,17
Intel,nmi7eph,One melting could be user error/a single faulty product (shit happens)  Multiple is likley a problem that could affect most/all cards,pcmasterrace,2025-11-01 08:46:10,2
Intel,nmgvd9k,"🤓Emm ""actually""  one did burn‚ but just one.",pcmasterrace,2025-11-01 01:30:17,-2
Intel,nmfblan,Do most new games even run on 5000 series cards?,pcmasterrace,2025-10-31 19:46:31,3
Intel,nmhxqrc,"The cards aren't losing game optimizations, only new features.",pcmasterrace,2025-11-01 06:56:59,1
Intel,nmfniyf,>Security patches won't optimize the new games for your GPU  Since when is it AMD's job to make sure a game runs and not the game developers?,pcmasterrace,2025-10-31 20:50:16,1
Intel,nmj3vvg,"nah, its weird because is each one a diferent problem, in black ops 6 i'm forced to play at minimum because memory leak, in battlefield 6 the driver just die at random, and cyberpunk just crash because the gpu",pcmasterrace,2025-11-01 13:29:27,1
Intel,nmgbd9v,None of them have started fires that spread beyond the connector melting,pcmasterrace,2025-10-31 23:16:08,8
Intel,nmf6bgf,">Just a 1% chance it might catch your house on fire.  Less than 1% globally and almost all user error and that isn't how statistics work.  If you are that concerned then you should also unplug every electronic in your home if you aren't using them. Turn off all the lights. Might as well flip the breakers. Also, cooking? You might as well forget about that. That's the leading cause of fires in the US.  Much higher chance you catch your house on fire by cooking bacon then with a properly installed and seated NVIDIA GPU.",pcmasterrace,2025-10-31 19:18:27,13
Intel,nmoofxd,Which is what I would expect.,pcmasterrace,2025-11-02 11:01:28,1
Intel,nmol3g8,"Wow, I've never heard of that GPU. But anyway, people are more concerned about the RX 6600 or RX 580/570, and they're safe for now.",pcmasterrace,2025-11-02 10:29:28,1
Intel,nmg2z0b,Interesting. Thanks for the info.,pcmasterrace,2025-10-31 22:21:17,1
Intel,nmjob0a,"1. that's only one generation old, and also not getting any new features (AFAIK).  2. AMD's GPUs are also still getting driver updates.",pcmasterrace,2025-11-01 15:26:51,1
Intel,nmf789t,Yeah I know that but umm you're right.,pcmasterrace,2025-10-31 19:23:14,0
Intel,nmic76w,"O we got the reference, just most people are too angry about something to do anything about it",pcmasterrace,2025-11-01 09:38:54,1
Intel,nmfftj0,1. 9070xt 2. If you didn't see then you are blind,pcmasterrace,2025-10-31 20:08:56,-5
Intel,nmgz3yr,"I had the bifrost model. Had several games that had weird graphical issues as they weren't optimized properly for yhr GPU. Just wasn't ideal, and while it was fairly priced, I just want a non jank experience.",pcmasterrace,2025-11-01 01:55:56,1
Intel,nmitz14,5070 5060 and 5050 do not melt,pcmasterrace,2025-11-01 12:22:36,0
Intel,nmj9vyv,"No one gives a shit about Linux. And to that point, the open source drivers are the reason why AMD performs better on Linux, because their own Windows drivers are just trash.",pcmasterrace,2025-11-01 14:06:06,-1
Intel,nmfupdc,They will forever focus on AI now. I think that cat is out of the bag forever. Gaming GPUs is a nice side hustle for them. The 1080TI will forever be the king to us boomer,pcmasterrace,2025-10-31 21:30:52,333
Intel,nmfvk4b,"You vastly underestimate how long rich people can keep a bubble going by just pumping more money into it.  Jeff Bezos, Mark Zuckerberg, Elon Musk, Bill Gates... they are all keeping the AI bubble alive.  And you can bet your ass that they won't run out of money any time soon.",pcmasterrace,2025-10-31 21:35:52,13
Intel,nmn684d,"Overvalued, horrible for creatives in all areas, and a serious burden on our energy security. Between crypto and AI, these industries can't end soon enough.",pcmasterrace,2025-11-02 02:59:23,2
Intel,nmfy373,And AMD don't make money from the AI race? Explain to me why their stock jumped after a deal made for that specific purpose then,pcmasterrace,2025-10-31 21:50:49,1
Intel,nmgjdfi,"AI gained them around ten years of profit in a single year, can't blame them for changing their game. Even though it sucks for all of us.  I doubt ai will go away. If the bubble pops, at most it'll become a niche thing that people do, like NFT junk. The bubble burst years ago but people are still clinging on to NFT going into new NFT scams and what not.",pcmasterrace,2025-11-01 00:08:50,1
Intel,nmhbx9h,"It is, but the AI drive will never stop. Remember, while LLMs are the current hype, there has never been a cessation to the ever growing demand for true general AI, or in other words, sentient machine intelligence.  Whether that is a good idea or not, is another matter altogether. But make no mistake - we will get there, and soon.",pcmasterrace,2025-11-01 03:27:55,1
Intel,nmkgnm3,Keep looking,pcmasterrace,2025-11-01 17:52:39,1
Intel,nmowdnm,">Looking forward the bubble bursting finaly.  The thing is, it won't. Datacenter GPUs are always more needed than gamer GPUs...",pcmasterrace,2025-11-02 12:10:36,1
Intel,nmgodkk,people been wanting the bubble to burst for 4 years now.  It ain't gonna burst,pcmasterrace,2025-11-01 00:42:51,-5
Intel,nmjlswl,lol,pcmasterrace,2025-11-01 15:13:27,0
Intel,nmhhrhn,They were planning that back in 2005? Was generative AI even a thing then?,pcmasterrace,2025-11-01 04:15:16,1
Intel,nmhwd3v,"i mean not everyone going to buy best one, as long as it good enough with reasonable price, then most people will buy that instead",pcmasterrace,2025-11-01 06:41:32,3
Intel,nmg8p1p,"You could argue about the 5600X3D as it offered a new value proposition, but yeah.",pcmasterrace,2025-10-31 22:58:42,4
Intel,nmh1dk5,"I still think it counts as a new release and a good thing overall, the last few SKUs were very clearly defective chips that failed to reach target performance of existing SKUs so they would've never made it to market had they not rebranded them so it's extra supply that lengthened AM4 lifespan.     However the XT refresh before that was a scam as they took perfectly functional CPUs, applied a useless overclock and sold them as a new model for a higher price when they could've just released them without the useless OC at regular street prices and everyone would be happy.",pcmasterrace,2025-11-01 02:11:54,0
Intel,nmhn9en,Ok I’m not missing much.,pcmasterrace,2025-11-01 05:05:07,1
Intel,nmg8vxu,"R300 series gained new drivers 2 months ago on Linux. 20+ year old cards, even the youngest and last of the series, Radeon 9550's are literally ancient yet there they are.",pcmasterrace,2025-10-31 22:59:58,50
Intel,nmhhioy,And SteamOS is... Linux. Really rooting for that to become the de facto gaming OS for the masses so the communities can stop bickering. I've been enjoying it.,pcmasterrace,2025-11-01 04:13:09,26
Intel,nmiufgt,And another W for Linux,pcmasterrace,2025-11-01 12:26:01,5
Intel,nmm24vr,Another reason to leave MS behind.,pcmasterrace,2025-11-01 23:03:39,1
Intel,nmfm5hc,"There's such a huge difference between marketers and """"""marketers"""""", the former are mostly numbers people, the latter are almost all ""idea people"" that contribute nothing positive",pcmasterrace,2025-10-31 20:42:54,33
Intel,nmi5fxm,Actually maybe they should just let AI do their marketing. At this point it probably would be an improvement.,pcmasterrace,2025-11-01 08:23:45,2
Intel,nmgkqyu,Source?,pcmasterrace,2025-11-01 00:18:09,2
Intel,nmk0u17,"youre just making stuff up now. they didnt even specify anything about ""same driver treatment"" let alone new features.",pcmasterrace,2025-11-01 16:32:16,1
Intel,nmevdtr,"I disagree, there was a world of difference when i tried the dlss 4 transformer model compared to dlss 3 when i was playing oblivion remastered on my 2060",pcmasterrace,2025-10-31 18:22:06,18
Intel,nmevmi3,"Except people have been getting it working, quit dick riding the billion dollar corp.",pcmasterrace,2025-10-31 18:23:21,6
Intel,nmfy7qs,We get dlss upscaler but not fg because thats a different thing  Fsr 4 UPSCALER could work on rx6000 without much issue (it alredy does with unofficial patches),pcmasterrace,2025-10-31 21:51:33,1
Intel,nmfg5vk,I mean it would have been great if they officially supported 6000. Technically there's no reason to exclude 6000 from the FSR4 backporting aside from them wanting to force consumers to buy their recent GPUs.  I say this because quality wise FSR4 is better than XeSS2 or FSR3 for that argument.,pcmasterrace,2025-10-31 20:10:45,1
Intel,nmex0wp,"that is true but when every statement that comes out of your communication team mouth is back tracked. One has to stand back and ask... is it a mistake or whats going on.   AMD gets enough free passes as is, if this was Nvidia reddit be torching them quicker then you could blink.",pcmasterrace,2025-10-31 18:30:35,-10
Intel,nmhmtby,https://preview.redd.it/xmt1f2k3vkyf1.jpeg?width=686&format=pjpg&auto=webp&s=877685b58fac8711ab505deba340b7d5a50a2307,pcmasterrace,2025-11-01 05:00:52,15
Intel,nmtfxme,steam?,pcmasterrace,2025-11-03 02:50:00,1
Intel,nmesuve,Till your house burns down cause you bought a 5090,pcmasterrace,2025-10-31 18:09:01,35
Intel,nmgaznm,"Bad products exist but they tend to be an active kind of bad like flashdrives claiming to be 1tb, but really being 1gb and if you write to them they keep deleting the oldest data   Or psu which light on fire and burn your house down",pcmasterrace,2025-10-31 23:13:41,1
Intel,nmi853x,"The 6500xt was straight up bad. It just did not work as it should have, and no pricing would have saved it. Or a connector that burns down your house. This saying makes no sense, like if a car was cheap enough you would still buy it even though it had no brakes???",pcmasterrace,2025-11-01 08:54:17,0
Intel,nmkan6l,"The 3050 I have is a compact ITX card, so when my kid inherits the 6700XT, I've got plans for that 3050.",pcmasterrace,2025-11-01 17:21:47,1
Intel,nmfwrz3,"It shouldn’t happen at all, I think in all my years in overclocking communities since before Reddit was even a thing, I can count on one hand the number of times I saw a 6 or 8 pin connector melt. There’s zero reason to even use the 12 pin except to save card makers a few pennies.",pcmasterrace,2025-10-31 21:42:59,12
Intel,nmhmx8c,Wow lol. A sapphire 9070 xt melted. Amazing.,pcmasterrace,2025-11-01 05:01:53,2
Intel,nmffyfa,"Wasn't the entire point of the thread we are on that game optimization _would actually_ continue as normal, and old cards just wouldnt be getting new features?   And yea. I play pretty much any game I want on decade old cards. When you arent trying to push frames at 4k ultra settings you'd be surprised what old cards can run lol.",pcmasterrace,2025-10-31 20:09:39,2
Intel,nmfi1fk,"Well maybe not 5000 series, but 6000 series are really powerful",pcmasterrace,2025-10-31 20:20:47,1
Intel,nmfs402,"There are certain optimisations that are entirely driver controlled especially within shaders. AMD writes driver level optimisations for shader compilation and usage, that increase performance.",pcmasterrace,2025-10-31 21:15:53,1
Intel,nmfodv7,Day 1 game drivers help devs to optimize their games for their gpus though... Why else would you think they'd exist?,pcmasterrace,2025-10-31 20:54:54,-1
Intel,nmg80bl,Have there actually been any fires at all? All I have ever heard of is a few melted cables.,pcmasterrace,2025-10-31 22:54:12,9
Intel,nmfrmqj,"connector has at most a 10% margin of safety, fuck off with that user error bullshit gamers nexus spewed out of his oportunistic mouth when literal EE's looked at connector and found some serious problems with design even after multiple revisions  fucking sapphire rx9070xt's with 12V high failure rate connector have melting problems and that card is nowhere near the 660w limit connector has even with transients getting into 500w range",pcmasterrace,2025-10-31 21:13:07,-1
Intel,nmumrr9,was rebranded with a slight chip refresh into the R9 280 and shows up as such in the drivers under its various names,pcmasterrace,2025-11-03 08:56:52,2
Intel,nmjp89q,"1. They are on the 50 series GPUs now, so wrong. Also they are getting better support for more games and a few more stuff with ray tracing. Small stuff but still nice to have.  2. Unrelated because I was only referring to whether Nvidia GPUs were getting updated. I did not say anything about AMD, nor did I intend to.",pcmasterrace,2025-11-01 15:31:50,0
Intel,nmfh38j,1. Thank you  2. Pcmr remains the most randomly hostile sub I follow,pcmasterrace,2025-10-31 20:15:41,3
Intel,nmg0kg9,"Just as with games consoles and the internet before it, the AI bubble will burst. But just as with those other two markets, AI will persist regardless. Just with less enthusiasm in the  support it retains.",pcmasterrace,2025-10-31 22:06:01,122
Intel,nmfv02w,Until the bubble burts lol. And when people realize how overplayed and overhyped AI is. Peole like them less and less luckily.   Ofc companies jump on it as they think they can save some money. Plot twist to unfuck whatever the hell ai is gonna do for them is gonna cost way more than they saved lol.,pcmasterrace,2025-10-31 21:32:37,70
Intel,nmguv3d,I hope they make AI actually AI then. Cause that shot barely works for me,pcmasterrace,2025-11-01 01:26:42,1
Intel,nmlh6zk,"OK, 1080ti you aren't wrong. I still have one alive and well in a rig with an accellero 3 cooler conversion.  But...  1080ti only happened because NVIDIA got spooked that AMD were about to drop some absolute fire. They believed it enough that they actually let some performance out of the bag and into the market... otherwise they would have released a crappier card on purpose, because that is legitimately what NVIDIA do without any real competition: hold back technology and dribble it out in ever crappier improvements year on year.  AI is cool, but apart from GPT doing my spreadsheets for me (which costs OpenAI far more than my subscription) , it's made every single product it's integrated in worse.  It's already at the point where a customer faced with the choice between AI assistant integrated VS vocally proclaiming no AI...they will pick the one without AI because it's just data harvesting guff.  It is a bubble. There will be no AGI. Customers are already tired of the lies, and companies aren't going to keep losing money to do my spreadsheets for me.",pcmasterrace,2025-11-01 21:04:40,1
Intel,nmglphc,Not gonna lie. AI is extremely useful. But the energy consumption is what is going to be the limiting factor. They need to focus on power efficiency if they want to prevent that burst but I bet they won’t.,pcmasterrace,2025-11-01 00:24:38,-4
Intel,nmijdio,But they won't burn all their money for a bubble though,pcmasterrace,2025-11-01 10:51:55,1
Intel,nmghgab,"C'est peut être la traduction Reddit qui est inexacte mais je ne comprends pas ce que vous voulez dire par ""bulle IA"". Je veux dire, c'est clair maintenant que l'IA va perdurer et que les besoins ne vont faire que croître d'années en années.   Aujourd'hui c'est tellement rentable que le marché PC Gamer représente une niche pour les entreprises qui fabriquent des GPU, et ce n'est pas prêt de changer.",pcmasterrace,2025-10-31 23:55:54,-6
Intel,nmixvtl,"Will we though? I can mostly understand how LLMs etc. came to be but true AI that will think for itself, upgrade itself and have its own opinions without pre-set conditions is still pure fantasy at least to me",pcmasterrace,2025-11-01 12:50:29,2
Intel,nmijjxt,There was no AI hype 4 years ago? What are you on lol? Bubble doesn't mean it's not useful. It means it's highly overvalued as compared to profits generated. It should burst within a year now.,pcmasterrace,2025-11-01 10:53:36,1
Intel,nmhwl4k,"Machine learning was a thing way before that, and CUDA came out around 2005.",pcmasterrace,2025-11-01 06:44:01,14
Intel,nmiani8,">The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.  https://en.wikipedia.org/wiki/Machine_learning",pcmasterrace,2025-11-01 09:21:58,5
Intel,nmgjyih,"It wasn't even widely released, so, meh  5700x3d was widely released and a slight down bin of 5800x3d. Tbh I think 5700x3d existed more as an official price cut (which is fine!)  5600x3d did come in at a lower price but is ultimately a cut down single CCD x3d chip. Not exactly new new, but interesting for those near a microcenter.",pcmasterrace,2025-11-01 00:12:46,6
Intel,nmidad4,In general Linux has awesome compatibility with old and exotic hardware.,pcmasterrace,2025-11-01 09:50:34,8
Intel,nmidah0,"I pulled out my radeon HD 7870 out of my first gaming computer, good to know my workstation made into a server will work with it lol",pcmasterrace,2025-11-01 09:50:36,2
Intel,nmijep0,"Gaming OSs won't catch on. Dual booting is just too unreliable for average people to be fucking with it. Hell I'm a very advanced user and I won't install 2 oses on the same drive. I have just had too many problems including one OS killing the other several times in just the last couple of years. I'm genuinely bothered that it seems you can't just disable NVME drives in the bios anymore. What I actually want is one or more distros to have official Radeon and nVidia support. I know the drivers basically work on all distros, but I want to know if they break they actually have a commitment to solve it.",pcmasterrace,2025-11-01 10:52:14,2
Intel,nmevh1b,How’s the 4x frame gen working for you?,pcmasterrace,2025-10-31 18:22:34,-12
Intel,nmfqib1,if people want to accept the performance decrease to get FSR 4 INT8 why don't they contact AMD to include the support?,pcmasterrace,2025-10-31 21:06:43,3
Intel,nmg3xhq,"Happens to tech sales, a lot more often than it should and other faucets of tech industry (and other industries for that matter but I’m not as familiar in those industries)  Part of the issue is marketing team that doesn’t really understand what’s being offered or told of them too",pcmasterrace,2025-10-31 22:27:30,1
Intel,nmtzcnu,Better than most certainly. They haven’t released Half Life 3 though… so…,pcmasterrace,2025-11-03 05:08:07,1
Intel,nmfgpa2,So it still just a pricing problem. They just need to lower the price by about $200000,pcmasterrace,2025-10-31 20:13:38,17
Intel,nmgb3t3,Nobody's house has burned down from a 5090   Some people have had some plastic smoke damage from the connector failing and melting,pcmasterrace,2025-10-31 23:14:26,2
Intel,nmi8b5s,Well hiring SB to make brakes sounds easy enough if the 500k car Cost me 5k.,pcmasterrace,2025-11-01 08:56:09,2
Intel,nmgbh1j,None at all   Its just melted cables   People imagine their entire house burning down because some plastic got squishy,pcmasterrace,2025-10-31 23:16:48,15
Intel,nmjuaoe,"1. That's my oversight. Honestly, I straight up forgot Nvidia even had a 40 series. Probably because it was so forgettable. lol  2. Unrelated how? My original comment was about both AMD *and* Nvidia not giving old cards new features, and your rebuttal was ""my 3060 gets driver updates"".  Driver updates, patches, and bug fixes are not new features, though. Adding official DLSS3 support to 30x0 GPUs would be a new feature.",pcmasterrace,2025-11-01 15:58:07,1
Intel,nmi3qk6,"It might burst but I very much doubt nvidia will come out a loser. They have their hands on almost everything related to AI, I'm not an expert but talked a bit with some peolle doing research on machine learning and there's almost no alternatives for nvidia in some cases.",pcmasterrace,2025-11-01 08:04:45,24
Intel,nmg4zdz,"Sure, it's a bubble, but it's a bubble like the Dot Com bubble was in the late 90s, early 2000s.  A lot of investors are going to lose their shirts, a lot of companies are going to go bankrupt and people will realize that not everything needs to be (or should be) AI, just as not everything needed to be a website or an online retailer.  But just as some websites and online retailers and the internet persisted and grew, so will AI grow and persist where it makes sense. So, bubble or not, it's here to stay.",pcmasterrace,2025-10-31 22:34:23,80
Intel,nmgyzkw,"Android/iOS apps are the new best thing  Cloud is the new best thing  BigData is the new best thing  Alexa and the other voice assistants are the new best thing  VR is the new best thing  NFTs are the new best thing  Chatbots are the new best thing  AIs are the best best thing  <- You are here    I forgot serverless, but too lazy to find out where it goes.",pcmasterrace,2025-11-01 01:55:06,8
Intel,nmhavan,"Nvidia is in a position that sure, when the bubble bursts they'll lose evaluation as AI demand decreases but they won't burst themselves. The bubble bursting will destroy any company that just slapped AI on their app description and hoped to get bought by VC/investors before they had to show how they made money. No different than the Dot Com bubble or Mobile App bubble or social media bubble. Some companies like OpenAI or Google or Anthropic will ""win"", they'll buy up any startup/smaller company that actually shows promise (or potentially has promise and would deny competitors the engineers working there) then the market will calcify. Nvidia, however, supplies whoever wins regardless so while there won't be bidding wars, demand isn't likely to decrease for a long while.",pcmasterrace,2025-11-01 03:19:56,1
Intel,nnta7ah,Ai can correct sentences.,pcmasterrace,2025-11-08 19:25:57,1
Intel,nmg2enw,Its not overplayed and overhyped. Just because reddit hates it doesnt mean its going away anytime soon.,pcmasterrace,2025-10-31 22:17:39,-8
Intel,nmg6z0x,Yeah not happening,pcmasterrace,2025-10-31 22:47:27,0
Intel,nmhdv8s,"Data Centers use a lot of power, and there's more demand for data centers than before, but this isn't a specific to AI problem. I've ran local LMs (Llama primarily) on my own server, they're not significantly harder or demanding to run than a typical server. Per the UPS I have attached to it, that single server uses around 200-400W the average is around 1700kW/h per year, and this is a fairly inefficient homelab.   Image/Video generation on the other hand, requires a lot more processing power, as anyone who's worked VFX will tell you, but this isn't the bulk of work that LLMs are doing. It's also why AI was able to ""pop up"" on every site so quickly, as it's really not significantly more intensive for Cloud providers than most K8S systems. This isn't a _pro_ AI comment btw, just noting that the power consumption aspect is a bit of misinformation that companies like OpenAI are _themselves_ pushing as if they make it seem too intensive to ""get in"" at this point, they solidify themselves as the defacto winner of the AI bubble. It's also why they recently have started bringing up ""ethical concerns"" about data theft and wanting regulation on data scraping...because they've already scraped everything. They want regulations now because it helps solidify their position.",pcmasterrace,2025-11-01 03:43:14,3
Intel,nmjp5ql,Unless they are conviced it'll be worth it in the end.,pcmasterrace,2025-11-01 15:31:27,1
Intel,nmjp30e,"His words, not mine.",pcmasterrace,2025-11-01 15:31:02,1
Intel,nmig7if,That thing is going to be supported as late as 2040 by the Linux community. If not even longer.,pcmasterrace,2025-11-01 10:20:25,3
Intel,nmiupha,"yeah but you don't need a gaming specific OS, you just need your main OS to support your gaming needs. linux is getting there, fast. i recently switched and haven't looked back. all my games worked basically out of the box and i'm even getting more fps than on windows.",pcmasterrace,2025-11-01 12:28:07,2
Intel,nmirqbq,"on the same drive indeed.  most PC support multiple drives luckily.  just selecting the os from the bios or boot menu is so much easier, just need a shortcut for that.",pcmasterrace,2025-11-01 12:05:23,1
Intel,nmewhx5,"Its been nice actually ever since i upgraded to a 5060ti recently, such a game changer, although the tolerance of people for latency might vary from person to person. I havent felt the need for 4x though, just up to 3x for stalker 2 with ultra plus mod graphics.",pcmasterrace,2025-10-31 18:27:52,6
Intel,nmnkzmp,"they'd need to pay me to get a 5090, and even then i'd just turn around and sell it so i can get a 9070XT and the system it will go in. if its 6000 dollars new here in australia, i'll sell for 4500.",pcmasterrace,2025-11-02 04:37:06,1
Intel,nmnl678,"still more than it ever should have been, the fact there isn't and never was a lawsuit about this is depressing, Nvidia should have been broken up already",pcmasterrace,2025-11-02 04:38:24,1
Intel,nmjvaio,1. Fair.  2. That’s what is happening. [This driver posted on 9/10](https://www.nvidia.com/en-us/geforce/drivers/results/254394/) adds DLSS support for some games.,pcmasterrace,2025-11-01 16:03:20,1
Intel,nmin5i6,Their value is set to plummet. It's massively overinflated. They'll survive but it's perhaps not such as good idea to have shares as it was 12 months ago.,pcmasterrace,2025-11-01 11:26:48,13
Intel,nmibjd4,"Their stock value will probably crater though, since it's based on the fantasy that AI will just keep growing exponentially forever.",pcmasterrace,2025-11-01 09:31:44,1
Intel,nmg85ci,"Fun fact, the stock of amazon dropped from over 100$ to about 6 during the crash while all business metrics of amazon went up and up and up... For the company, it wasn't a huge issue because they didn't need to gather more capital on the markets urgently, but it just goes to show that even in a bubble crash, it doesn't mean that all the companies that drop weren't actually a fair value even before the drop",pcmasterrace,2025-10-31 22:55:08,19
Intel,nmijmge,"What's the .com bubble?   Sorry if it's a stupid question, I was 10 in the 2000 and not fully aware of how the internet worked (also, not from the US, we mostly had .it here)",pcmasterrace,2025-11-01 10:54:15,1
Intel,nnt9rry,"For Nvidia a lot of the money is real. Their chips are still superior and demand for data center gpus exceeds availability.   Not only Nvidia but also AMD might focus more on data centers in the near future to drive revenue with higher margins and profits through business oriented cards, inflate the stock price and keep shareholders happy.   If any other company will be able to produce a capable enough card and software to compete with Nvidia, I bet they would love to become business focused as well.",pcmasterrace,2025-11-08 19:23:41,1
Intel,nmg56s3,Yeah and. I never said ai is going completely away lol.,pcmasterrace,2025-10-31 22:35:43,-3
Intel,nmg33qz,"It absolutely is. It's not without uses, it's a useful tool in narrower applications.  It's not the sci-fi level AGI that techbros and wallstreet are creaming themselves fantasizing about. The insane investments and valuations aren't from what the technology is or has a plan to develop into... it's solely from investors watching cyberpunk sci-fi movies and having wet dreams at the idea of replacing everyone.",pcmasterrace,2025-10-31 22:22:08,11
Intel,nmg2ko9,It 100% is overhyped lol. Studdy it more. Gother some information. Its half useless and gimmicky at best. Added value is diminishingly low.,pcmasterrace,2025-10-31 22:18:42,5
Intel,nmg3zt6,"Look at the amount of money going into AI, and then the amount of revenue being generated by AI. The difference is insane, and I see no realistic path to profitability.   It's all driven by speculation and a downright messianic belief that if they just crack ""AGI"" first they're going to be the rulers of a new world where everyone needs AI for everything.  At some point the investors are going to want to get paid and then  the whole house of cards collapses.",pcmasterrace,2025-10-31 22:27:55,1
Intel,nmg6hdz,"It's absolutely overhyped.  AI will be a useful too, but it's being treated as if it's gonna solve all the world's problems.",pcmasterrace,2025-10-31 22:44:13,1
Intel,nmi120o,"That's only half of the truth isn't it? Running them locally is only possible due to the fact that it was trained and made ready for deployment. Running them isn't the part that needs a lot of resources, it's the training.   And the bigger the model the more hardware you need to throw at it and therefore more power is consumed.   I agree with the rest you've said but this part is important too.",pcmasterrace,2025-11-01 07:34:34,1
Intel,nmjsh0t,It's been almost 10 years since we heard that the bubble was going to burst. The reality is that NVIDIA boxes no longer give a damn about gamers.,pcmasterrace,2025-11-01 15:48:46,1
Intel,nmkmwqn,"Agree with the idea that you just need a replacement for Windows, not 2 OSes. The elephant in the room is still the anticheat problem. Huge swathes of people play multiplayer games, and an OS that can't run Fortnite is still massively handicapped.",pcmasterrace,2025-11-01 18:24:35,3
Intel,nmisa4y,"I use clover on mine. Default boot is Linux, then Windows is there to play the incompatible games which for me is just GTA5.",pcmasterrace,2025-11-01 12:09:40,1
Intel,nmewmeg,So you weren’t using 4x FG on your 2060? Why not?,pcmasterrace,2025-10-31 18:28:30,-9
Intel,nmnml7a,"The reason nobody can effectively sue over it is that manufacturers have been replacing what people lose from the failures   To sue, you need to prove damages and that the company did not sufficiently cover them   Its like nintendo joycon drift   They just keep replacing peoples joycons for free... so there is nothing to sue over",pcmasterrace,2025-11-02 04:48:44,1
Intel,nmkeaj2,"That's updating an existing feature, though, as the 3060 already supports DLSS4.  An actual *new* feature would be something like Nvidia enabling DLSS frame generation on the 30 series; something it doesn't currently officially support.  It seems like it's kinda splitting hairs, but that's what the whole AMD thing is/was about: People thinking that ""no new features"" means the same thing as ""no updates"".  Like, the RX 6600 supports FSR3, so if FSR3 gets an update, that update will apply to the RX6600. Just like the DLSS update applied to your 3060.",pcmasterrace,2025-11-01 17:40:32,1
Intel,nmjlx9b,RemindMe! 5 years.,pcmasterrace,2025-11-01 15:14:06,1
Intel,nmj8u42,"It's was a tech bubble in the US in the late 90s. Basically startups were popping up and getting investors just to start online businesses or websites, so you'd have huge purchases or investments in new companies that were basically just ""noun"" + "".com"" (the ending to our website addresses in the US at the time).  You had ridiculous stuff pop up nearly everyday and get investment, and then in 2-3 years the investors (and the companies) realized they had no way to actually make money, so the bubble burst.  You can look up articles and videos on it by searching for ""Dot Com Bubble"", it's often discussed.  Ironically, many of the failed businesses' ideas are a profitable thing now, and similar websites and companies exist - but the fact was the infrastructure and demand just wasn't there 25 years ago.",pcmasterrace,2025-11-01 13:59:55,1
Intel,nmg6aem,"Yeah, but we're talking about Nvidia. So they'll still be in the AI game for as long as AI exists. So the bubble bursting will never bankrupt Nvidia, only companies buying from Nvidia. The point is that Nvidia is never leaving the AI game now, because they are THE source of the hardware for AI.",pcmasterrace,2025-10-31 22:42:57,26
Intel,nmg4b2z,"The overreaction are indeed crazy. But that being said, Nvidias stock is so absurdly high due to their profit margins and sales volume.  Companies arent going to stop buying Nvidia gpus anytime soon (unless AMD, Intel, or Cerebras potentially compete).  Look at their profit year by year! Companies arent going to stop buying gpus anytime soon. Their stock wont crash unless serious completion comes in  https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue",pcmasterrace,2025-10-31 22:29:57,1
Intel,nmg2zaz,"""Study it more"". No, I'm not in college doing a dissertation lol. Maybe re-educate yourself and you will realize why its not going anywhere.",pcmasterrace,2025-10-31 22:21:20,-5
Intel,nmjvfwa,Are you even listening to me?,pcmasterrace,2025-11-01 16:04:07,1
Intel,nmogi4x,"yeah, that's true, 100% a big downside, if you happen to play games where that matters. i didn't even know fortnite had kernel level anticheat, because i'm lucky enough that all the games i play don't require that.  i think it's a market saturation thing. if a significant enough part of the user base switched (steam deck is helping a lot there), the developers would have to adjust their anticheat solutions eventually. since kernel level anticheat still doesn't effectively prevent cheating anyway, i think it's just a current trend that will eventually go away again.",pcmasterrace,2025-11-02 09:43:54,1
Intel,nmex05q,"uh because its not supported?, or you thinking you have a gotcha moment because fg is obviously not supported on 3000 series and below.  I might have not been clear but the 3000 series and below can actually benefit from the upscaling of the transformer model.",pcmasterrace,2025-10-31 18:30:28,8
Intel,nmnq5fe,this shouldn't legally be enough to cover their asses. consumer protection needs to be better than that.,pcmasterrace,2025-11-02 05:17:00,1
Intel,nmjm385,I will be messaging you in 5 years on [**2030-11-01 15:14:06 UTC**](http://www.wolframalpha.com/input/?i=2030-11-01%2015:14:06%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/pcmasterrace/comments/1ol0uuv/help_me_intel_arc_youre_my_only_hope/nmjlx9b/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fpcmasterrace%2Fcomments%2F1ol0uuv%2Fhelp_me_intel_arc_youre_my_only_hope%2Fnmjlx9b%2F%5D%0A%0ARemindMe%21%202030-11-01%2015%3A14%3A06%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201ol0uuv)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,pcmasterrace,2025-11-01 15:14:59,1
Intel,nmg6mcw,Yeah and again. Never stated any of those things. I kinda actualy said what you said.,pcmasterrace,2025-10-31 22:45:08,3
Intel,nmg6qbc,"There's a lot of ""market human centipede"" going on though too. With these companies investing in the companies buying their products. It's not exactly a straightforward or healthy thing. Especially when the bulk of ""AI"" has no path to profitability without some ""magic"" tech breakthrough.  You ever hear that joke about two economists walking in the woods?  >The first economist says to the other “I’ll pay you $100 to eat that pile of shit.” The second economist takes the $100 and eats the pile of shit. > >They continue walking until they come across a second pile of shit. The second economist turns to the first and says “I’ll pay you $100 to eat that pile of shit.” The first economist takes the $100 and eats a pile of shit. > >Walking a little more, the first economist looks at the second and says, ""You know, I gave you $100 to eat shit, then you gave me back the same $100 to eat shit. I can't help but feel like we both just ate shit for nothing."" > >""That's not true"", responded the second economist. ""We increased the GDP by $200!""  It very much feels relevant lately.",pcmasterrace,2025-10-31 22:45:52,1
Intel,nmg32im,Never said it is going lol. I said i am looking forward it bursting. And it is overhyped.....,pcmasterrace,2025-10-31 22:21:54,4
Intel,nmexa0h,"So you originally disagreed with me, but now you agree that in fact the RTX2000 series can’t do everything the RTX5000 can do. Thank you.",pcmasterrace,2025-10-31 18:31:52,-6
Intel,nmnx6nk,"Less than 1% fail, and those which do, they replace   What else do you want from them",pcmasterrace,2025-11-02 06:22:31,0
Intel,nmjxmuq,Seeing that 2030 is in 5 years physically hurt me,pcmasterrace,2025-11-01 16:15:33,5
Intel,nmg7trr,"Riddle me this. Why does Reddit think it is so much smarter than those who literally work on Wall Street?  Reddit always thinks they are right when they are usually in the minority.  Remember the election last year and how it was literally impossible for Kamala to lose? Or that the switch 2 and MK8 would be a failure because it was a $80 game and a $500 console? Or even just look at this sub a few years back. People have been waiting for the Nvidia stock to ""burst"" for years now.  Get my point?",pcmasterrace,2025-10-31 22:53:00,1
Intel,nmey9hy,"When did I even say that, not my fault you automatically assumed that I was implying that the 2060 can do nvidia fg just because i said that DLSS 4 Upscaling worked with it.",pcmasterrace,2025-10-31 18:36:55,12
Intel,nmod2xu,a real product that doesn't melt and force you to get new units.    some of yall need brains. fuck me,pcmasterrace,2025-11-02 09:08:39,1
Intel,nnx7r4d,"Don’t worry, the RAM price isn’t going to go down anytime soon.",pcmasterrace,2025-11-09 12:45:02,36
Intel,nnxbbyp,the 7600x will sacrifice performance but will be cheaper in the long run when you decide to upgrade,pcmasterrace,2025-11-09 13:11:18,8
Intel,nnx94a4,"gamer's nexus tested 245k in adobe premiere last year https://gamersnexus.net/cpus/intel-core-ultra-5-245k-cpu-review-benchmarks-vs-5700x3d-13700k-more, it scores 9824 to the 7600x's 8159 or 20% faster. It's also 18% faster in Baldur's Gate but only 6% in ~~Stellaris~~ Starfield. Techpowerup finds the 245k within 1.5% of the 7600x on average over their gaming suite, here's the 1080p page https://www.techpowerup.com/review/intel-core-ultra-5-245k/19.html",pcmasterrace,2025-11-09 12:55:21,15
Intel,nnx77vm,245k or 14600k competes with 9700x far ahead of 7600x.,pcmasterrace,2025-11-09 12:40:56,11
Intel,nnxlcqh,"Neither cpu needs an aio, get a cheaper $20ish air cooler and invest the difference in ram",pcmasterrace,2025-11-09 14:15:20,5
Intel,nnx7rqe,"245k is WAY better for editing, about the same for gaming iirc",pcmasterrace,2025-11-09 12:45:09,7
Intel,nnxcs38,editing/gaming: Intel  pure gaming: AMD,pcmasterrace,2025-11-09 13:21:11,5
Intel,nnxdbh0,245K is better but it seems that LGA 1851 socket is dead. They will switch to a new socket (LGA 1954) with the next gen. That means no upgrades without a new mobo.,pcmasterrace,2025-11-09 13:24:44,2
Intel,nnxihkc,"245K fights with the 9700X on productivity. A 265K fights with a 9900X on productivity. Is a far better value than AMD at the price. Gaming oerofmemace they are mostly fine (they are only bad compared to the X3D).  I would suggest get more money for a 265K, but either way a 245K is pretty good. Of course, you could say there's no upgrade path... But just think about it: I how much time are you going to upgrade?  And even if you plan to, you could get a 285K in some years and that thing is one of the fastest CPU productivity wise rn.",pcmasterrace,2025-11-09 13:57:41,2
Intel,nnxgwkl,"Considering the limited upgradability of that Intel gen, you might as well just get a 265k. And yes, Intel is better for your purposes.",pcmasterrace,2025-11-09 13:47:47,1
Intel,nnxbhe0,"Worth noting that LGA 1851 won’t be getting anymore CPUs, so if you plan on upgrading your CPU later, go for AM5. Also I know RAM prices are crazy right now, but please don’t get a single 8gb stick of 5600 CL40 RAM. 2x 16gb that’s has a speed of 6000 and CL30 or CL32 is much better.",pcmasterrace,2025-11-09 13:12:20,1
Intel,nnxxjbb,"Intel takes the lead in video editing, in my opinion.",pcmasterrace,2025-11-09 15:24:44,1
Intel,nnxih7v,the 7600x and the 9600x are budget CPUs while the 245k is mid-range (that it's so down on its MSRP to compete with the 7600x and 9600x is another matter). i have no idea about video editing workloads but going by [pugetsystems.com/solutions/video-editing-workstations/adobe-premiere-pro/hardware-recommendations/](http://pugetsystems.com/solutions/video-editing-workstations/adobe-premiere-pro/hardware-recommendations/) i would go with AMD for an hybrid gaming & productivity workload since core ultra is horrendous for gaming but ryzen matches core ultra in the productivity benchmarks.,pcmasterrace,2025-11-09 13:57:38,-1
Intel,nnx96pk,That's terrible,pcmasterrace,2025-11-09 12:55:51,2
Intel,nnxf18p,"Yea that makes sense, I'd have to make a whole new build instead of just upgrading one part",pcmasterrace,2025-11-09 13:35:59,2
Intel,nnx9dmt,"Thanks, appreciate it, this is exactly what I'm looking for.",pcmasterrace,2025-11-09 12:57:16,4
Intel,nnx9sq0,Oh wow ok yea that's significantly ahead. What do you think between the 245k and 14600k?,pcmasterrace,2025-11-09 13:00:18,1
Intel,nnxmofd,"If I got a $40 aircooler, I'd also get a 3 pack of exhaust fans for $40. Which is $80 so I thought, might as well just get the $80 AIO and call it a day.",pcmasterrace,2025-11-09 14:23:23,2
Intel,nnx9nv2,"WAY better ok, that's saying something. I was thinking I could go with a full AMD gaming build and get a Mac for editing, but that's all too expensive.",pcmasterrace,2025-11-09 12:59:19,2
Intel,nnxf36l,Yea that makes sense,pcmasterrace,2025-11-09 13:36:19,0
Intel,nnxe4j4,"Yea I know, but if I'm mainly using this for editing then I don't have to upgrade every generation. I'll just make an AM5 build in a few years.",pcmasterrace,2025-11-09 13:30:03,2
Intel,nnxnnm7,"Yea, that seems to be where I'm stuck at. Intel is better for editing but they change sockets every other chip. They're expensive. They lack the same gaming capability. I kinda wanna go full amd am5 build for gaming and get a Mac for video editing.",pcmasterrace,2025-11-09 14:29:14,1
Intel,nnxmwqr,"Yea that's where I'm stuck at, I cant upgrade but intel is better for editing than amd. At the end of the day I want a computer to game and a computer to edit on separately. I'm staring lean towards just getting a Mac and calling it a day",pcmasterrace,2025-11-09 14:24:45,1
Intel,nnxev9c,"True, which is annoying, ram is also annoyingly expensive, $110 premium for something that was $85 two weeks ago is crazy",pcmasterrace,2025-11-09 13:34:55,1
Intel,nny76nh,Yea that seems to be the idea,pcmasterrace,2025-11-09 16:13:35,1
Intel,nnxnvnm,Ok that's kind of what I was thinking. Would you go 7600x or 9600x? Is the 9600x performance worth the extra 40+ dollars?,pcmasterrace,2025-11-09 14:30:35,1
Intel,nnxwlsd,Yeah you’re looking at another year minimum,pcmasterrace,2025-11-09 15:19:50,3
Intel,nnyxg32,"i did my own test for 14700k vs 9800x3d and this is how it was  [https://youtu.be/sZIlzI\_F2XM?si=ZZghGYJ0FM92jH3l](https://youtu.be/sZIlzI_F2XM?si=ZZghGYJ0FM92jH3l)  14700k vs 9700x  [https://youtu.be/X2zpplzKfuc?si=qjcnSUdY1xHRlWmc](https://youtu.be/X2zpplzKfuc?si=qjcnSUdY1xHRlWmc)  ive tested 14700k on 6c12t configuration and it managed to beat 9700x in every game  6c12t of 14700k vs 9700x  Spider man2 170fps vs 145fps  Re4 140fps vs 130fps  Horizon zero dawn 150fps vs 136fps  Cp 2077 150fps vs 136fps  Though that was with DDR5-7200; on DDR5-6000, they perform on par, since Intel scales with memory speed, while AMD doesn’t because the IMC will run at a 2:1 mode on ddr5 6400+, so practically on ddr5 7200 14600k should be faster than 9700x  in my opinion, the choice should be either the 7000/9000 X3D series or Intel’s 14th gen. I’ve never tested the Core Ultra chips before, but I bet they aren’t as bad as some reviewers make them out to be.",pcmasterrace,2025-11-09 18:19:45,2
Intel,nnxyyx1,Get a case with preinstalled fans like montech king 65 pro,pcmasterrace,2025-11-09 15:32:09,3
Intel,nnxwvzj,"You might as well just go for a 14700K or 14900K. Or if you’re planning on hanging on to this thing for a while, just go ahead and spend the little extra to run a 265K or 285K. They overclock very well and do get meaningful bumps in performance from faster RAM.  The extra cost compared to a 14600K will be meaningless after a year or two but the performance difference will be very noticeable.   Having spent most of my PC building/Gaming Life as an Intel fanboy, personally I think the x700K (what is now 265K for the Core Ultra) model of Intel CPUs always seemed to be the best bang for the buck performance vs cost vs longevity.",pcmasterrace,2025-11-09 15:21:20,1
Intel,nnxu4ah,"If you want to go Intel, get the 265K. Its such a good upgrade from the 245K for under 100 bucks. Its around 280 dollars, competes with the 9700X in gaming and with the 9900X (400 dollar CPU) in productivity. Also supports higher RAM speeds if that is important for you.",pcmasterrace,2025-11-09 15:06:17,2
Intel,nny0dut,"Hop on AM5 and dont look back, you have a far wider range of upgrade options than anything intel could offer right now. the 7600X is a good entry point for AM5 but there are far better offerings in the ryzen 7/9 range for your workload. once you get into ryzen 7 and beyond you dont need anything else for any workload.",pcmasterrace,2025-11-09 15:39:25,2
Intel,nny73au,That's so sad 😭,pcmasterrace,2025-11-09 16:13:06,1
Intel,nnxziba,Good idea. I was gonna go with the montech air 903,pcmasterrace,2025-11-09 15:34:57,3
Intel,nnyvt5r,"Doesn't really make sense to get 14th gen. A new 265k is cheaper than a new 14700k or 14900k and getting them used seems ill-advised to me (it also isn't even much cheaper than a new 265k anyways, so kind of a pointless risk).",pcmasterrace,2025-11-09 18:12:13,0
Intel,nny7x1y,"Ram is important, none of these seem to be bad choices, theres at least 2 intel CPUs I could pick and 2 AMD so it really comes down to if I want to go editing or gaming focused.",pcmasterrace,2025-11-09 16:17:19,2
Intel,nny3au2,Understood 🫡 7700X it is.,pcmasterrace,2025-11-09 15:54:08,1
Intel,nny62s8,"Its subjective but I find the air 903 looks quite different irl from the advertised images make sure to check th builds in reviews to see if it fits your taste, the king 65 pro on the other hand has tinted glass   This build is with 245kf, no integrated graphics since you have a gpu https://pcpartpicker.com/list/sKts9C  This one with a 9600x https://pcpartpicker.com/list/4hz9b2  245kf is better for editing, 9600x is slighly better for gaming and like 12% slower for editing but the platform is upgradable",pcmasterrace,2025-11-09 16:07:55,2
Intel,nnz7xql,"I guess you will have to find an equilibrium that provides the most performance between the 2. AMD officially supports 5600MHz RAM but it can be overclocked in the BIOS with EXPO and Intel supports 6400MHz and with 200S Boost it supports up to 8000MHz but the specific RAM kit matters.   If you go with AMD I do believe 6400MHz would be stable on their CPU's. Just research based on people who actually have systems with 6400MHz RAM kit with Ryzen 9000 CPU's. good luck also with the RAM prices, hopefully your budget is flexible with whats going on.",pcmasterrace,2025-11-09 19:08:11,2
Intel,nnyjya6,"From what I've read, you still want the intel iGPU and its quick sync feature for video editing.",pcmasterrace,2025-11-09 17:16:09,1
Intel,nnyoaoo,"Same, did a quick search and apparently quicksync can help accelerate some tasks in some editing suits, granted its data from 3 years ago but should still be relevant, you also get something to fallback on if your gpu fails, seems worth for $30 extra if you can squeeze it",pcmasterrace,2025-11-09 17:36:46,1
Intel,np2sikf,It's the PCI slots that do it for me.,pcmasterrace,2025-11-16 01:54:00,2
Intel,np37q1q,pretty sweet! do you plan on doing a platform update later on,pcmasterrace,2025-11-16 03:29:18,1
Intel,np39mph,Serving us proudly!,pcmasterrace,2025-11-16 03:41:56,1
Intel,np3sba2,Love it. Enjoy!  I briefly panicked when I first saw the PCI slots and an Intel ARC thinking it was an ancient motherboard. That motherboard sure did throw me.,pcmasterrace,2025-11-16 06:02:03,1
Intel,np2ywbs,I believe that's called vintage thank you,pcmasterrace,2025-11-16 02:32:59,1
Intel,np3h8rv,"Most likely, but nothing concrete planned out yet. Hoping to string some upgrades together over a couple years so I can build something out of the leftovers for my young kids. Stuff might just be too old by that time. I guess paint is pretty easy on the specs though lmao.  Could be looking for something sooner if I actually hit this fabled arc bottleneck.",pcmasterrace,2025-11-16 04:35:18,1
Intel,np4rw4y,Yeah it is pretty old for PC parts tbf. Definitely a gamble to try the arc with 8th gen too. Kind of why I ended up just building something new for my wife instead of going down this path first.,pcmasterrace,2025-11-16 11:50:38,1
Intel,np2z6ie,I never thought I'd see a depreciated interface on a modern board it's mind blowing.,pcmasterrace,2025-11-16 02:34:47,1
Intel,nntvl54,No B580?,pcmasterrace,2025-11-08 21:24:14,2
Intel,nntqqms,"9060XT. Newer, better performance",pcmasterrace,2025-11-08 20:57:16,3
Intel,nnugqji,8gb is ass. 16gb is great.  Just get a B580 if the 16gb isn't in your budget.,pcmasterrace,2025-11-08 23:28:12,1
Intel,nnwb9su,"Digital art as in 2d Photoshop, Clip Studio Paint, Rebelle, Corel Painter, etc are almost completely CPU and memory bound. Even the 3d model poser in CSP is completely CPU. So I would recommend you max out CPU and invest in 64gb (or more) in ram before worrying about the GPU - it'll give her a much better experience in these programs.  But to answer your GPU question directly, I think investing in an Nvidia card would be good since a lot of 2d artists will eventually move into dabbling 3d work (Blender, Cubism, etc) and those thrive on Nvidia. I worked professionally doing 2d work with a 1660Super 6gb for years. So you could get a 5060 8gb, or a used 3060 12gb, and be completely fine (especially since you mentioned she doesn't game).",pcmasterrace,2025-11-09 07:29:15,1
Intel,nntr0wa,That’s what I was thinking however she doesn’t really game as much as she does art and is coming from a old thinkpad laptop so I’m just trying to figure out one what’s worth it to her and myself money wise for me and enjoyment for her,pcmasterrace,2025-11-08 20:58:50,1
Intel,nnugslx,That’s what I’ve decided,pcmasterrace,2025-11-08 23:28:34,1
Intel,nntto7u,I mean if the difference is more than 50$ and she doesn't need the 9060XT then get the A770,pcmasterrace,2025-11-08 21:13:36,2
Intel,nnugtmd,Getting the B580,pcmasterrace,2025-11-08 23:28:45,1
Intel,nnxq4km,It makes sense. A770 is technically a little faster I think but it was the first generation product and... well the focus is definitely on the B580 (and possible B770 yet to release) as far as driver development etc given it's the current gen product.  It's also a great price for a good general purpose GPU.,pcmasterrace,2025-11-09 14:43:48,1
Intel,npewdz8,TrueNAS.  You probably wont need a CPU that powerful. Depends on the game servers though.,pcmasterrace,2025-11-18 00:48:31,2
Intel,npe1fpw,11 LTSC,pcmasterrace,2025-11-17 21:54:25,-1
Intel,npbm7lq,"I started playing Arc Raiders a few days ago with my mate, and I have to say I’m really enjoying it. It’s also refreshing that I can max out all the settings and actually see great performance, unlike most of the standard UE5 slop out there. The devs have really done a great job with the optimisation.",pcmasterrace,2025-11-17 14:39:27,176
Intel,npcqdsi,I’m impressed. The game is good that’s for sure.,pcmasterrace,2025-11-17 18:00:41,26
Intel,npcqnhe,"Currently considering buying it.   Tried tarkov, but that's a too steep learning curve for me. (Had to watch a 4min video, just to figure out how to mount a scope lol)   Is it more ""casual"" than tarkov?",pcmasterrace,2025-11-17 18:02:01,19
Intel,npcqfsr,"Isn't Helldivers II on a downward trend atm due o the abhorrent (or somehow, more so as of late) performance and crashing issues folks have been having?  I've got a pretty beefy rig and not only is HD2 the only game I've had whole system lock ups on, but it's also the only game I've had in the last decade that's done it with such regularity and it's not my own dumbass fault.",pcmasterrace,2025-11-17 18:00:57,37
Intel,npcwf5i,Why has extraction games suddenly become popular?   Hunt showdown has been doing it for years and no one seems to care 😂,pcmasterrace,2025-11-17 18:29:27,8
Intel,npcckld,But it’s comparable to hell divers? I really like the game play of hell divers,pcmasterrace,2025-11-17 16:52:54,9
Intel,npcnpn7,"I maxed out the stash size and it's nearly full. Now I have nothing to sell my goodies for. I don't buy gear. So I guess it's just about how high I can make the numbers go up.  So I stopped playing. I think the addiction broke for me. I get my life back though, so that's a plus.  If there was a system to convert 1000 credits to 1 gold, that would incentivize further play.",pcmasterrace,2025-11-17 17:47:59,6
Intel,npdohg2,Not for me. Not a PvP person,pcmasterrace,2025-11-17 20:49:15,2
Intel,npckpzy,Hard pass on extraction shooters.,pcmasterrace,2025-11-17 17:33:12,3
Intel,npcu76p,I'd play it but I don't care for getting killed constantly by griefers,pcmasterrace,2025-11-17 18:18:51,1
Intel,npdz1gb,"Rightly so too I’d say, I’m getting like 300 frames on high settings, talk about well optimised. Gdamn.",pcmasterrace,2025-11-17 21:42:29,1
Intel,npc5inv,That’s a shame. I was hoping the extraction shooter fad would end.,pcmasterrace,2025-11-17 16:18:26,-16
Intel,npd2yzo,Helldivers is dogshit,pcmasterrace,2025-11-17 19:01:06,-5
Intel,npbrrfy,"Easy anti-cheat: well, forget it.  Just had to deal with uninstalling that hot mess, as it was causing windows update failures, which caused all kinds of issues with other game updates.",pcmasterrace,2025-11-17 15:09:24,-25
Intel,npcavkc,"Umm, it never “crossed”. It had been 10x of helldivers since launch",pcmasterrace,2025-11-17 16:44:38,-16
Intel,npcfbm5,I have a great time playing it on my steam deck. I'm so grateful that it plays on it.,pcmasterrace,2025-11-17 17:06:28,36
Intel,npcmcdg,Seriously. It's so sad that so many games release with absolutely no intention of giving people the performance they paid for,pcmasterrace,2025-11-17 17:41:18,8
Intel,npfkuea,They just stripped features out and used old settings. New features always pushes new tech holy why is everyone like this now. Can PC gaming not push further than what we have? Games scaled even worse back then when tech was doubling every year. If anything we are in a drought as tech is no longer having big gains as the best nvidia and amd can do is make bigger chips and pushing more wattage.   Its not just devs or engines its hardware manufacturers no longer able to keep up. Since every time pc gaming looks to stabilize we get a big boom and something spikes and those prices stay. As of now everything past 2020 is just price gouging that you paid for. Your money to performance is worse than ever before and you paid for it. You should be mad at nvidia with messing with their stacks. Then increasing the prices. Next gen is only to get worse.   The way I see it top end gpus have a harder time keeping up with what unreal 5 wants to do and since nvidia cuts the stack by 40% for the next tier down their is no gains on moving up unless you move up a tier. At the same time devs just dont have the time to optimize cause they dont have infinite money or time. But the better unreal 5 games dont run that bad.   We are so screwed.,pcmasterrace,2025-11-18 03:14:15,2
Intel,npdbhfn,"Embark's other game, The Finals was also a lot of fun, until I stopped playing a while back. It's still pretty good, based on what I hear from friends.",pcmasterrace,2025-11-17 19:43:35,3
Intel,npfal7j,Turns out UE5 games can run great if you basically turn UE5 into UE4,pcmasterrace,2025-11-18 02:13:57,1
Intel,npe9xk7,"I have been having a blast with it and I never play PvP games!  I play solo and everyone has been so friendly. Died a few times to people that shoot at first sight, but that's just part of play a PvPvE game, so no hate to them at all. Really recommend it even for people who would normally avoid this genre.",pcmasterrace,2025-11-17 22:39:15,1
Intel,npdn4m3,I have been watching a bunch of streams for it lately. Taking up more time than I'd like tbh. Really hope my wife gets it for me for Christmas lol.,pcmasterrace,2025-11-17 20:42:21,2
Intel,npcrn9i,"Much more casual: no out of round health systems to manage, gear is much more plentiful and there’s less punishment for dying (you still lose your gear but it’s easier to craft) Free loadouts that aren’t on a timer like Tarkov And putting attachments on guns is idiot proof unlike tarkov",pcmasterrace,2025-11-17 18:06:44,34
Intel,npcsqx7,Definitely feels like a much more casual Tarkov. It scratches a similar itch and I have been able to groom my more casual friends into playing this game after Tarkov being too much.,pcmasterrace,2025-11-17 18:11:58,3
Intel,npdhqv9,"I really wasn't a fan of Tarkov. Arc Raiders, on the other hand, might be my GOTY. Way more accessible, looks better, performs better, and the community for the most part, is great. Solos is very chill, I reckon 95% of players are just doing quests or looting.",pcmasterrace,2025-11-17 20:15:03,2
Intel,npdf3cu,"It’s like tarkov for fortnite players.   Theres only a handful of guns, losing your gear means nothing because you can get free loadouts at any time, most loot is just for crafting or tasks etc.   I like it a lot but it felt a little shallow having come from tarkov, but it’s definitely way more palatable to the average gamer.   Tarkov requires constant focus whereas i can just chill out and goof off in arc",pcmasterrace,2025-11-17 20:01:33,2
Intel,npcrc80,Much more which is why it has been appealing to a wider audience,pcmasterrace,2025-11-17 18:05:18,1
Intel,npfn0up,"Way more casual than Tarkov (or anything other extraction ahooter on the market, really). This is why it's so successful. People have been throwing around the term ""extraction shooter for the employed,"" lol.  The game is good fun and if you play solo too. Probably 70% of the people I run into are friendly. I will say that having a mic in this game improves your chances of survival massively.",pcmasterrace,2025-11-18 03:27:51,1
Intel,npdldbs,"This article would be comparing the all time peak concurrent figure for the two games, a little under 500,000. But yes Helldivers is in a sorry state right now.",pcmasterrace,2025-11-17 20:33:25,3
Intel,npcvfns,"Somewhat, but the community has also been fairly willing to hear them out as they've made some recent commitments to fix things up and reducing file size for PC users. The game has been rock solid stable for me after a weird fix that support recommended though, so I'll pass it along:  Browse the local files on steam. Find the HD2 exe. Select its properties (not the usual properties that W11 wants, show more -> properties) and run it in Windows 8 compatibility mode. Also select the checkbox that runs it as administrator.  This does mean there's one popup thing that happens every time you launch it, but it solved all my crashing issues. The game might run really bad for a few minutes the first time you launch it after changing this, but it clears itself up.",pcmasterrace,2025-11-17 18:24:43,11
Intel,npfzouq,imo the game loop got boring  I’m glad Helldivers 2 exists though. I hope they make a Helldivers 3 that’s even better. I’ll play it for sure.,pcmasterrace,2025-11-18 04:56:05,1
Intel,npdeg4m,"Performance issues are resolved now. Problem is because the devs decided to focus on performance and bug fixes, it's been under a content drought for about 3 months now so playercount is falling.  It was expected from the beginning that if they were to stop making content and focus on bug fixes, playercount would fall which was one of the reasons why AH held it off for so long.  They had to make a choice between maintaining the health of the game at the cost of low numbers for a while versus keep pumping content every month.",pcmasterrace,2025-11-17 19:58:20,0
Intel,npcxgjh,"HUNT has a very brutal competitive streak, ARC banks on less PvP and encourages a more cooperative gameplay loop.",pcmasterrace,2025-11-17 18:34:28,11
Intel,npcxfeb,"Hunt are Tarkov are the only ones that have done in right in that time. It’s not really a new thing, a ton of them have released and failed.  Difference is that Arc is actually doing it well, in a way that’s approachable to more casual players. So it’s blowing up.",pcmasterrace,2025-11-17 18:34:18,4
Intel,npedaor,I have about 20h in hunt but I wasn't a fan of being 1 tapped. At least in Arc you have a chance to fight back. There is no weapon that can 1 tap you.,pcmasterrace,2025-11-17 22:57:49,1
Intel,npcdsgf,"In a way, sure.  It’s also not Helldivers at all",pcmasterrace,2025-11-17 16:58:53,50
Intel,npcngut,"They are both shooters, it stops about there",pcmasterrace,2025-11-17 17:46:48,8
Intel,npcxomw,"My only hesitation is the PVP, getting smoked at extraction because somebody wants your loot would get tiring fast.",pcmasterrace,2025-11-17 18:35:31,4
Intel,npdly2b,"In terms of third person, the way PvE works, and the fact that helldivers is also extraction-based, yes. The gameplay and movement is really similar. The addition of PvP makes it quite different however, as well as the absence of airstrikes lol",pcmasterrace,2025-11-17 20:36:21,2
Intel,npcvjbf,"Third person, have to extract to be successful, you shoot big ass baddies... could be comparable, yes. LOL",pcmasterrace,2025-11-17 18:25:12,1
Intel,npcrc48,Did you finish your expedition project?,pcmasterrace,2025-11-17 18:05:17,3
Intel,npcvu05,"I was thinking the same, but since the price was accessible, feels like Arc Raiders is a evolution of the formula.  People said the same for turn based games, and Expedition 33 happened.   Never keep your mind close to these things, something always comes around to surprise us. Arc Raider is a damn good surprise so far.",pcmasterrace,2025-11-17 18:26:38,4
Intel,npdmaxv,I felt the same way but it’s been super fun,pcmasterrace,2025-11-17 20:38:09,0
Intel,npd338j,PVP isn't griefing. It's a part of the game.,pcmasterrace,2025-11-17 19:01:40,11
Intel,npc8cz6,Plenty of PvE single player games out there for you bud.,pcmasterrace,2025-11-17 16:32:21,14
Intel,npcdq59,See ya never,pcmasterrace,2025-11-17 16:58:34,7
Intel,npcdsl8,You make a fine point. The “ummm” gave me the ick lol,pcmasterrace,2025-11-17 16:58:54,1
Intel,npcmied,Does it support only deck or works on all distros?,pcmasterrace,2025-11-17 17:42:07,9
Intel,npezswq,Yeah it’s still a really solid game. They also do a great job with cosmetics and treating the community well,pcmasterrace,2025-11-18 01:09:02,1
Intel,npff5vh,what?,pcmasterrace,2025-11-18 02:40:37,1
Intel,npd7b3n,"Fuck it, bought it",pcmasterrace,2025-11-17 19:22:42,22
Intel,npejoom,"I was looting with 4 other randoms in the same area, like next to each other, one other random decided to open fire in one of us, we killed him and shared the loot, one of the best experiences I had on a extraction shooter.",pcmasterrace,2025-11-17 23:34:55,2
Intel,npdf4jd,"I'll try that compatibility fix next time I play. Beyond the lockups, my game was running quite well. Not sure why folks are downvoting you, but Reddit do be that way sometimes.",pcmasterrace,2025-11-17 20:01:43,3
Intel,npf56ul,"Its been a bit more than 3 months. We've basically had illuminate (4 enemies), cities, bug reskins and a few bots with reused weapons. In 2 years.   They've never exactly been pumping out anything but warbonds and store items, which are also mostly reskins of things we already have and effectively a subscription more expensive than paying for an MMO subscription if you want literally any new equipment, with something like 2 exceptions this year.  Performance is very much not fixed either, its better, but still awful on many tiles and hardlocks are still occurring for many among other long term bugs which remain untouched a year later.  Oh and they joked about lying to the community, then lied to the community, multiple times. I don't understand having faith in devs that literally try to deceive their customers, repeatedly.",pcmasterrace,2025-11-18 01:41:51,3
Intel,npd27rk,Have probably done 50 runs and only had 1 where it was cooperative. That was because my buddy and I came up on them and could have killed them but chose not too. If I say friendly I’m getting shot every single time,pcmasterrace,2025-11-17 18:57:25,5
Intel,npce9ge,"Lmao so do I buy or not smh  Edit - everyone says to go on the Internet and look for myself, have you ever heard of asking somebody for their opinion?  I know a lot of you may not know this, but the information on the Internet even from some of your favorite influencers is very curated or paid for.  The interaction between me and the individual above is a real interaction in his real opinion  Always remember this, when somebody asks you for their opinion.  It’s not because they can’t do the search, it’s because they want an authentic opinion knowing that the Internet is littered with bullshit marketing.",pcmasterrace,2025-11-17 17:01:10,-43
Intel,npd3hcs,"It really doesn't. They both have a very similar gameplay loop, but Helldivers 2 is PvE exclusively with *far* lower stakes. And also HD2 doesn't use AI voices, for all its other technical flaws.",pcmasterrace,2025-11-17 19:03:37,5
Intel,npcydrb,"While I won’t say it doesn’t happen, PvP is a super inefficient way to progress in Arc Raiders. The gameplay far more heavily encourages cooperation between players as opposed to competition, especially when it comes to downing the big Arc",pcmasterrace,2025-11-17 18:38:52,10
Intel,npd7zt8,I hit 100 hours last night and this has only happened a dozen or so times. Just go next and forget about it. There are ways to mitigate it too. Griefers have not slowed me down in any meaningful capacity.  Don't trust anyone with the stupid orange and black skin,pcmasterrace,2025-11-17 19:26:07,4
Intel,npcy92t,Are there pve only servers? That’s how I did wow and I loved it sm,pcmasterrace,2025-11-17 18:38:14,3
Intel,npedvx9,on the PC version I have noticed more players helping each other versus killing each other. like 75/25 helpful/want to kill me.,pcmasterrace,2025-11-17 23:01:07,1
Intel,npcysrg,"Not to mention the entire North Line project, which is loaded with goodies, and just launched last week",pcmasterrace,2025-11-17 18:40:51,3
Intel,npd3a41,"No, that's the one thing I can do. If I go back, that's probably what I'll focus on, that and blueprints, and parts for those high end recipes.  It's just a bit sad when money means nothing anymore when it was a big draw.  I think the game progression is balanced around PvP, and as fun as it is to find a ton of high end loot, the new map has too much of it and not enough PvP. It's too easy to get 40-50k of loot and dip playing solo. No one shoots, or at least it's easy enough to spot and avoid players.",pcmasterrace,2025-11-17 19:02:38,1
Intel,npdcufd,"""I'd play soccer, but I don't care for getting scored on by griefers""",pcmasterrace,2025-11-17 19:50:20,1
Intel,npdgaak,Something a griefer would say,pcmasterrace,2025-11-17 20:07:41,-5
Intel,npc938e,"You misunderstand.  Similar to BR, this trend has caused a lot of devs to waste time on making extraction games or extraction modes for non-extraction games that just end up being a waste of time and money. Look at Battlefield 2042’s attempt at extraction, it ended up being a complete waste of resources.  Plus the complete lack of PvE extraction games is odd. I want to like the genre, but it ***ALWAYS*** just ends up being unemployed players who no-life the game and have best in slot gear stomping new players. I just wanna play the PvE aspect of it. People act like PvE can’t be challenging which is just ignorant.",pcmasterrace,2025-11-17 16:35:56,-11
Intel,npdfjwk,It should work on any distro. It works on my vanilla arch installation (btw).,pcmasterrace,2025-11-17 20:03:53,9
Intel,npdefj5,Also you aren't supporting Russia by playing tarkov. So there is that going for you too,pcmasterrace,2025-11-17 19:58:14,18
Intel,npdn45i,"If that doesn't work, it can also be the anticheat. The tools folder includes an uninstaller and reinstaller for it, which should reset that back to defaults as well.",pcmasterrace,2025-11-17 20:42:17,3
Intel,npdl483,Duos and Trios are much more hostile. Solos are chill,pcmasterrace,2025-11-17 20:32:07,6
Intel,npcff3p,Do some research man. It’s not like Helldivers at all really. They are 3rd person shooters in a futuristic setting. That’s about where the similarities stop.,pcmasterrace,2025-11-17 17:06:57,34
Intel,npd4zrz,Lazy as fuck generation,pcmasterrace,2025-11-17 19:11:06,10
Intel,npdgahy,Imagine making a decision for yourself lmao,pcmasterrace,2025-11-17 20:07:43,4
Intel,npd1gli,Idk bro go look at gameplay and decide for yourself,pcmasterrace,2025-11-17 18:53:48,3
Intel,npcg5qn,I would be surprised if you regretted your purchase.,pcmasterrace,2025-11-17 17:10:38,2
Intel,npdbu3g,"That sounds pretty refreshing, I've put a ton of hours into Tarkov and have been big into Hunt lately, I'd love to be able to properly work with other players towards a common goal like that rather than always being on guard. Is there proximity chat in Arc?",pcmasterrace,2025-11-17 19:45:21,1
Intel,npewmt6,There is no pve only servers,pcmasterrace,2025-11-18 00:49:57,1
Intel,npd346b,I'm near the end of that actually. It moves really quick.,pcmasterrace,2025-11-17 19:01:48,1
Intel,npds8cf,You're supposed to use the lvl 3 refiner and condense a stash full of tier 4 weapons and other top gear. Crafting 1 maxed out weapon by itself uses a lot of stash slots worth of materials just by itself.,pcmasterrace,2025-11-17 21:08:20,2
Intel,npeappv,Tell me you don't understand what griefing is without telling me,pcmasterrace,2025-11-17 22:43:29,4
Intel,npcdcvq,The devs said they tried the pve only and said themselves that it was boring,pcmasterrace,2025-11-17 16:56:45,3
Intel,npchvxe,why dont you play other games then?,pcmasterrace,2025-11-17 17:19:10,3
Intel,npcj2q0,"If you think the extraction shooter genre is oversaturated, name at least 5 notable ones besides ARC and Tarkov.  Battle royale on the other hand IS (or was) oversaturated, as even AAA developers tried their hand at it.",pcmasterrace,2025-11-17 17:25:01,0
Intel,npcbk8h,"I didn’t misunderstand. The issue you have with extraction shooters is the PvP. That’s why I said there’s plenty of games with just PvE out there for you. I understand there are some shitty extraction shooters out there and a lot of them have been a flash in the pan but to say it’s a “shame” that ARC Raiders is successful is just a dog shit opinion. So many people are having fun and enjoying the game. Even if it isn’t around in a year, people still had fun with a $40 game. Is that really so shameful? You mention BF Firestorm but who cares if AAA studios try and fail. They need a wake up call anyway.   To your final point about about no lifers getting the best gear and stomping all the players with less playtime, I agree this is an issue in some extraction shooters and no developer has cracked the code to remedy that issue. However, if devs stop making these games like you suggest, the code will never be cracked. So again, saying ARC Raiders’ success is a shame is just ridiculous. Also, if you did an iota of research on the game, you’d know it’s easily the most casual friendly extraction shooter out there.",pcmasterrace,2025-11-17 16:47:59,-1
Intel,npee2ct,"Do you know about escape from duckov, Helldivers or Incursion red river? That's probably more your style",pcmasterrace,2025-11-17 23:02:10,-1
Intel,npdtm41,"Yeah problem is I don’t like the game enough to play solo. So now I just run free kits only. This way when they lose a shield, ammo and grenades it’s a net negative for them if I die. Even better if I kill them.   Prefer ABI and tarkov.",pcmasterrace,2025-11-17 21:15:22,1
Intel,npdl7v7,"Did You know that marketing always puts their best product forward, that’s why you ask people for their opinions on it because they purchased a product and they’ve experienced it.  If I go online what do I see, a bunch of pre-created marketing material trying to sell me a product. The person above is giving me his honest opinion, something you can’t just find unless you ask for it on the Internet.  So while you claim I’m lazy as fuck, I see you lack critical thinking in perspective and trying to understand why people ask questions and other people for their opinion opinions.  I can tell you’re very easy to sell too, and you’ll probably eat up all the marketing material. So I have at fam spend your money away but don’t criticize people that are looking for opinions from not marketing sources.  Let me guess you think politicians care about you too?",pcmasterrace,2025-11-17 20:32:38,-6
Intel,npdccfn,"There is prox chat and it gets pretty well used in my experience. On solos, with cross play off, I’ve found that things go really smooth around other players the vast majority of the time. Very rarely have I had to PvP.",pcmasterrace,2025-11-17 19:47:52,5
Intel,npdse10,Ah ok. Thank you. I'll work on that then. Cash value was fun but it's not everything,pcmasterrace,2025-11-17 21:09:09,2
Intel,npfxr9i,Can’t craft tier 4 weapons if you find no fucking weapon blueprints,pcmasterrace,2025-11-18 04:41:31,1
Intel,npcohvx,That’s a failure on the devs’ part. Tons of compelling PvE games exist and have existed even before multiplayer games existed.,pcmasterrace,2025-11-17 17:51:45,-6
Intel,npcc3j2,"No, you still definitely misunderstood. That’s not the only issue I have and I specifically said that.",pcmasterrace,2025-11-17 16:50:35,-4
Intel,npee7gx,Helldivers is not an extraction shooter.,pcmasterrace,2025-11-17 23:02:59,1
Intel,npfnpmy,Dunno why you were downvoted - it’s ok to have opinions.,pcmasterrace,2025-11-18 03:32:14,1
Intel,npf2wsb,"Damn bro, you really wrote all that without realizing you're a clown. Maybe you'll figure out halfway through your next reply, I'm rooting for you.",pcmasterrace,2025-11-18 01:27:57,2
Intel,npcsnjv,It’d be boring as pve and would not be popular at all,pcmasterrace,2025-11-17 18:11:32,4
Intel,npci7mt,"you dont like the genre, so dont play it? i dont like puzzle games, i dont play puzzle games. pretty simple.",pcmasterrace,2025-11-17 17:20:46,-2
Intel,npd0lmo,You say that’s not the only issue you have but I addressed every point you made except for your last line about people claiming PvE can’t be challenging. I’ve literally never seen anyone say that as it makes no sense. Every game was PvE before PvP was even a thing. So you can’t say I am still misunderstanding.   Funny that you chose that response and then didn’t address any of the points I made.,pcmasterrace,2025-11-17 18:49:39,-5
Intel,npeel21,How exactly do you leave the map? Next you're gonna tell me you don't shoot.,pcmasterrace,2025-11-17 23:05:11,-1
Intel,npctdha,"Again, only because the devs failed at their jobs. Tons of great PvE games exist. And since it’s PvEvP, they really have no excuse for the PvE parts to not be good.",pcmasterrace,2025-11-17 18:14:56,-2
Intel,npd16ll,If that’s what you wanna tell yourself. You’re not worth arguing with.,pcmasterrace,2025-11-17 18:52:27,2
Intel,npef2nk,"That’s not what makes an extraction shooter an extraction shooter. By that logic Titanfall, Warframe, and Deep Rock Galactic are also extraction shooters.  Extraction is all about dropping in, gathering loot, and attempting to escape with that loot. If you fail, you lose that loot and all loot you brought in with you. None of that is true in Helldivers. Sure there’s samples, but you’re not collecting weapons or armor and you don’t lose any weapons or armor you bring in.  Helldivers is a mission-based co-op horde shooter. It’s not an extraction game.",pcmasterrace,2025-11-17 23:08:00,1
Intel,npf0mqn,Helldivers seems like the only PvE game that had success lately. What other games are there?,pcmasterrace,2025-11-18 01:14:05,1
Intel,npejc27,Well then yes. To stay logical consistent I'm fine with calling those extraction shooters.  Now what about the other 2 games I mentioned?,pcmasterrace,2025-11-17 23:32:51,0
Intel,nos1l2t,Congrats! Love the look!   Whats the CPU fan btw?,pcmasterrace,2025-11-14 08:34:57,4
Intel,not4fal,Very cool,pcmasterrace,2025-11-14 13:53:42,1
Intel,nos1t22,Thanks! It’s a themalright peerless assassin 120 se v2 I got from Amazon,pcmasterrace,2025-11-14 08:37:10,2
Intel,nme4zm5,"Great timing, if the Intel driver works and supports the older GPUs   :)",pcmasterrace,2025-10-31 16:10:46,2
Intel,nmdyzvq,Is that the channel ran by children?,pcmasterrace,2025-10-31 15:41:56,2
Intel,nme0r1j,I thought that was LTT?  (only half joking),pcmasterrace,2025-10-31 15:50:19,2
Intel,non0jqt,whats your case,pcmasterrace,2025-11-13 14:46:08,2
Intel,nonqe80,Not my particular style but I love looking at it.,pcmasterrace,2025-11-13 16:54:38,2
Intel,non1cex,Segotep Nexus 1,pcmasterrace,2025-11-13 14:50:22,2
Intel,non1igt,thanks,pcmasterrace,2025-11-13 14:51:14,2
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,75
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,74
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,14
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,18
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,8
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,7
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,28
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,5
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,9
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,7
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,3
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,4
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,17
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,4
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,7
Intel,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norpqss,USB-C port still no power? :/,AMD,2025-11-14 06:40:49,1
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,Did AI create these new drivers?,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,notyosp,"I don't see any information about the freezes, lockups that started with 9.1.   No a fix, but the problem isn't acknowledged either.",AMD,2025-11-14 16:30:15,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,2
Intel,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,2
Intel,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,nopiwaq,Any fixing incoming for the eeudumps logs being written every second on ssds? and thus damaging ssds?,AMD,2025-11-13 22:14:03,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,21
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,6
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,9
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,26
Intel,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,3
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,3
Intel,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,2
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,97
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,2
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,5
Intel,nonhggg,"Same here, but oddly enough I use modded 25.9.1 drivers",AMD,2025-11-13 16:10:52,1
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,9
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,5
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,12
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,6
Intel,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,1
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,6
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,0
Intel,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,18
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,3
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,1
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,0
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,5
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,7
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,3
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,10
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,3
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,5
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,3
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,2
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,1
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,1
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,6
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,3
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,1
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,9
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,11
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,15
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,10
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,16
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,15
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,20
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,13
Intel,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,4
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,5
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,4
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,11
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,4
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,5
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,3
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,2
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,6
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,7
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,4
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,0
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,2
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,-1
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noumgxv,"Possible I had not any freezing till 9.1 and rollng back to 8.1 solved the problem.  Yet, I don't use any rare hardware. The rig is formed of most common brands and models. So there is something wrong with the driver.",AMD,2025-11-14 18:28:59,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,9
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,0
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,43
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,2
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,13
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,5
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,24
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,1
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,3
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,4
Intel,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,4
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,3
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,1
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,1
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-2
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,5
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,22
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,7
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,7
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,8
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,6
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,2
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,5
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,1
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,0
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,-1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,1
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,0
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,1
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,159
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,81
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,48
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,15
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,5
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,3
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,45
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,20
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,16
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,15
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,12
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,3
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,2
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,7
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,3
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,19
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,14
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,6
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,32
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-6
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,78
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,11
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,22
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,15
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-15
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,10
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,54
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,20
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-13
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,46
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,3
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-9
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,11
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,6
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-2
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,4
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,12
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,28
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-3
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,4
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,24
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,11
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,5
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,11
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,3
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,25
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,113
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,18
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,122
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,70
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,8
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,38
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,39
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,57
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,21
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,5
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,27
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,6
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,5
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,14
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,3
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,6
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,19
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,4
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,5
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,2
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,3
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,4
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-7
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,44
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,28
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,14
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,6
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,34
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,14
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,18
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,9
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,3
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,8
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,5
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,29
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,9
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,7
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-5
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,13
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,4
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-5
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,1
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,3
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,3
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,1
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-14
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,10
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,15
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,2
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-2
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,29
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,5
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,45
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,27
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,3
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,15
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,3
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,8
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,9
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,5
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,2
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,0
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,14
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,10
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,21
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,21
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,5
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-12
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,-1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,28
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,29
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,4
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-4
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,8
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,11
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,0
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,14
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,11
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,4
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,8
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,10
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,4
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,6
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-2
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-1
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,1
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,1
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-5
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,-2
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,11
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-10
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-12
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,6
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,1
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,6
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,13
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,4
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,26
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,4
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,12
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,21
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,16
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,16
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,13
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,15
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,4
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,5
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,15
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,2
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,5
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-7
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-5
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-1
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-3
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,6
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,7
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,18
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,7
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,6
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,8
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-2
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,3
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-2
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,4
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,12
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,3
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,11
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,15
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,15
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,6
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,8
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,1
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,4
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,kb07k5w,"As you notice the photoshop version differs, so you can't compare them really",AMD,2023-11-27 18:28:41,5
Intel,kao517l,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),AMD,2023-11-25 07:14:52,2
Intel,k74d0ev,So basically PC games are never going to tell us what the specs are to run the game native ever again.,AMD,2023-10-30 18:21:26,541
Intel,k751wfu,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",AMD,2023-10-30 20:52:50,52
Intel,k748hf1,The true crime here is needing FSR to reach these requirements.,AMD,2023-10-30 17:53:41,194
Intel,k74fig7,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",AMD,2023-10-30 18:36:48,36
Intel,k74fyp7,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),AMD,2023-10-30 18:39:33,16
Intel,k74c3qr,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,AMD,2023-10-30 18:15:55,65
Intel,k7407mp,"well, at least the chart is easy to read, not a complete mess",AMD,2023-10-30 17:03:14,17
Intel,k7466bs,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,AMD,2023-10-30 17:39:45,12
Intel,k741zrd,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,AMD,2023-10-30 17:14:11,59
Intel,k748t1r,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,AMD,2023-10-30 17:55:39,52
Intel,k76jevo,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",AMD,2023-10-31 02:51:26,5
Intel,k76we4z,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",AMD,2023-10-31 04:46:34,7
Intel,k75qe4v,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,AMD,2023-10-30 23:33:24,3
Intel,k74d5ad,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",AMD,2023-10-30 18:22:16,27
Intel,k748nsg,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",AMD,2023-10-30 17:54:45,11
Intel,k74kjmk,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",AMD,2023-10-30 19:07:25,17
Intel,k7533wp,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",AMD,2023-10-30 21:00:16,3
Intel,k77mpgx,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,AMD,2023-10-31 10:35:56,3
Intel,k74hphk,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,AMD,2023-10-30 18:50:08,12
Intel,k773lbl,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",AMD,2023-10-31 06:11:46,7
Intel,k74eavi,I hope they will bundle this game with CPUs/GPUs,AMD,2023-10-30 18:29:20,2
Intel,k74fxtg,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,AMD,2023-10-30 18:39:23,2
Intel,k77022h,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,AMD,2023-10-31 05:27:42,2
Intel,k78gl5o,This game better look like real life with those specs. I does looks beautiful!,AMD,2023-10-31 14:43:10,2
Intel,k75as90,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",AMD,2023-10-30 21:48:53,5
Intel,k748ctr,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,AMD,2023-10-30 17:52:56,2
Intel,k74mjvt,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,AMD,2023-10-30 19:19:43,2
Intel,k74oqng,I love it how absurd these things are these days.,AMD,2023-10-30 19:33:13,2
Intel,k74awed,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,AMD,2023-10-30 18:08:29,2
Intel,l06s6dc,Does anyone know if it supports SLI or crossfire?,AMD,2024-04-18 19:16:23,1
Intel,k73yt0i,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,AMD,2023-10-30 16:54:36,1
Intel,k74kcre,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",AMD,2023-10-30 19:06:17,1
Intel,k74mb9y,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,AMD,2023-10-30 19:18:13,1
Intel,k74oabx,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,AMD,2023-10-30 19:30:24,1
Intel,k74sd5w,Why in the f*ck is upscaling included on a specs page?,AMD,2023-10-30 19:55:26,1
Intel,k77tobm,no more software optimization and full upscaling  bleah,AMD,2023-10-31 11:50:04,1
Intel,k749m34,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,AMD,2023-10-30 18:00:34,-2
Intel,k74d7ll,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",AMD,2023-10-30 18:22:41,1
Intel,k74phj0,"I've never even heard of this game, nor care about it, but these system requirements offend me.",AMD,2023-10-30 19:37:49,0
Intel,k79vqgg,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,AMD,2023-10-31 20:00:03,0
Intel,k744khv,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",AMD,2023-10-30 17:29:53,-2
Intel,k79zrsa,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",AMD,2023-10-31 20:25:07,0
Intel,k7c9frd,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,AMD,2023-11-01 08:20:56,0
Intel,k75roib,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,AMD,2023-10-30 23:42:07,-2
Intel,k741y61,Nice,AMD,2023-10-30 17:13:54,-1
Intel,k765iq2,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",AMD,2023-10-31 01:14:49,0
Intel,k77fyxm,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",AMD,2023-10-31 09:08:01,0
Intel,k77zvvn,Native gaming died or what ? Wtf they turning pc gaming into console gaming,AMD,2023-10-31 12:44:58,0
Intel,k76r1ve,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",AMD,2023-10-31 03:53:47,-1
Intel,k74pme9,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,AMD,2023-10-30 19:38:39,1
Intel,k74qrim,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,AMD,2023-10-30 19:45:38,1
Intel,k74t7ty,"Omg, it would be a graphic master piece or  bad optimized thing.",AMD,2023-10-30 20:00:39,1
Intel,k74ybxr,First time I see matches recommendations for nv and amd GPUs...,AMD,2023-10-30 20:31:22,1
Intel,k756ha4,Rip laptop rtx 3060 6gb,AMD,2023-10-30 21:21:24,1
Intel,k759rkx,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",AMD,2023-10-30 21:42:14,1
Intel,k75e3m3,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,AMD,2023-10-30 22:10:21,1
Intel,k75ej2l,*NATIVE* resolution gang ftw!,AMD,2023-10-30 22:13:11,1
Intel,k75k1fl,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",AMD,2023-10-30 22:50:05,1
Intel,k75n4rl,Looks capped at 60fps?,AMD,2023-10-30 23:11:06,1
Intel,k75ops0,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",AMD,2023-10-30 23:21:57,1
Intel,k75qeie,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,AMD,2023-10-30 23:33:28,1
Intel,k760or5,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",AMD,2023-10-31 00:42:35,1
Intel,k77465m,Is it using UE5?,AMD,2023-10-31 06:19:31,1
Intel,k77bz45,"Ubisoft, rip on launch.",AMD,2023-10-31 08:09:13,1
Intel,k77htgp,guessing no DLSS3 then?,AMD,2023-10-31 09:33:36,1
Intel,k77l01d,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,AMD,2023-10-31 10:15:17,1
Intel,k77twmu,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",AMD,2023-10-31 11:52:12,1
Intel,k77x5ay,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",AMD,2023-10-31 12:21:45,1
Intel,k77z2e6,Farewell 1660ti… looks like it’s time for an upgrade,AMD,2023-10-31 12:38:12,1
Intel,k78p32u,well my 3300x is now obsolete for these new AAA games...,AMD,2023-10-31 15:37:57,1
Intel,k798boq,4k ultra right up my alley 😏,AMD,2023-10-31 17:36:35,1
Intel,k799x2x,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,AMD,2023-10-31 17:46:17,1
Intel,k7ec5fz,7900xtx will do 4k 120fps with FSR 3 then I guess?,AMD,2023-11-01 18:22:39,1
Intel,k7fkb1o,What must one do to achieve a higher rank than an enthusiast? A demi-god?,AMD,2023-11-01 22:56:10,1
Intel,k7gz4pf,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",AMD,2023-11-02 05:23:56,1
Intel,k7k2rqe,Is vrr fixed with frame gen then?,AMD,2023-11-02 20:32:54,1
Intel,k859q58,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",AMD,2023-11-07 00:16:09,1
Intel,k8kv1d8,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",AMD,2023-11-10 00:27:13,1
Intel,kaeixym,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,AMD,2023-11-23 05:18:15,1
Intel,k74h55l,They recommend upscaling even at 1080p.. disgusting ew,AMD,2023-10-30 18:46:41,220
Intel,k75hnu4,pretty much this we all knew they would start using upscaling as a crutch.,AMD,2023-10-30 22:34:04,14
Intel,k76ze7a,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,AMD,2023-10-31 05:19:49,4
Intel,k74f7tr,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",AMD,2023-10-30 18:35:00,26
Intel,k74e6u4,My thoughts too…,AMD,2023-10-30 18:28:39,5
Intel,k778snb,Thanks to all of you that were screaming dlss looks better than native lmao.,AMD,2023-10-31 07:23:19,5
Intel,k79niu3,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",AMD,2023-10-31 19:09:38,1
Intel,k77adqn,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,AMD,2023-10-31 07:46:18,1
Intel,k74x8ab,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",AMD,2023-10-30 20:24:48,-1
Intel,k74zbiw,Nope we as a community abused a nice thing,AMD,2023-10-30 20:37:19,1
Intel,k755j7t,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",AMD,2023-10-30 21:15:25,-6
Intel,k75o59y,Didn't take them long to make upscaling worthless.,AMD,2023-10-30 23:18:03,-1
Intel,k78qsdu,i smell a burgeoning cottage industry of game spec reviewers!,AMD,2023-10-31 15:48:28,0
Intel,k7hhgqi,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",AMD,2023-11-02 09:36:54,0
Intel,k7k4fzn,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",AMD,2023-11-02 20:43:01,0
Intel,k74js59,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",AMD,2023-10-30 19:02:44,-2
Intel,k78m5tc,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,AMD,2023-10-31 15:19:10,-1
Intel,k74pa1g,yeah this is the new standard,AMD,2023-10-30 19:36:32,1
Intel,k77dlbv,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",AMD,2023-10-31 08:33:14,24
Intel,k78n190,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,AMD,2023-10-31 15:24:48,1
Intel,k77nqtn,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",AMD,2023-10-31 10:47:59,-5
Intel,k74z5qq,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",AMD,2023-10-30 20:36:21,13
Intel,k76jxv9,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",AMD,2023-10-31 02:55:18,2
Intel,k76rqab,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,AMD,2023-10-31 04:00:06,2
Intel,k75qnpb,The actual true crime here is even having fsr to begin with. It should just have dlss,AMD,2023-10-30 23:35:12,-4
Intel,k76ju0a,Seems like they tried to cover every basis with these.,AMD,2023-10-31 02:54:33,7
Intel,k75ao0c,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",AMD,2023-10-30 21:48:08,9
Intel,k74htwr,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",AMD,2023-10-30 18:50:52,15
Intel,k74jsww,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,AMD,2023-10-30 19:02:51,3
Intel,k760d0a,"GCN support is over, RDNA1 is the lowest currently supported arch.",AMD,2023-10-31 00:40:29,6
Intel,k74k4hi,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",AMD,2023-10-30 19:04:51,2
Intel,k77v39i,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",AMD,2023-10-31 12:03:13,0
Intel,k74efc2,What do you mean forced raytracing?,AMD,2023-10-30 18:30:05,11
Intel,k77dids,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,AMD,2023-10-31 08:31:59,6
Intel,k74fapr,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",AMD,2023-10-30 18:35:29,15
Intel,k7gd959,It's actually 960p :(,AMD,2023-11-02 02:12:51,2
Intel,k78nhhp,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",AMD,2023-10-31 15:27:45,0
Intel,k74go33,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,AMD,2023-10-30 18:43:49,6
Intel,k78s16k,We'll see in a year.,AMD,2023-10-31 15:56:02,-1
Intel,k744q6u,AFAIK  &#x200B;  It is with RT,AMD,2023-10-30 17:30:51,20
Intel,k745n5v,Seems pretty good to me given it is at 4K with RT,AMD,2023-10-30 17:36:31,17
Intel,k743fzm,"Likely includes the RT features , its also 4k",AMD,2023-10-30 17:23:02,6
Intel,k743sfv,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",AMD,2023-10-30 17:25:06,-13
Intel,k74aacw,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",AMD,2023-10-30 18:04:43,30
Intel,k74c9q7,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",AMD,2023-10-30 18:16:57,23
Intel,k78pyma,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,AMD,2023-10-31 15:43:22,1
Intel,k771jw5,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",AMD,2023-10-31 05:46:01,1
Intel,k7aj8ac,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",AMD,2023-10-31 22:34:23,2
Intel,k77bs4b,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,AMD,2023-10-31 08:06:28,1
Intel,k75qm0a,Exactly! It even got xess so Intel users also can use xess,AMD,2023-10-30 23:34:53,2
Intel,k75k4vk,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",AMD,2023-10-30 22:50:44,3
Intel,k74wzhj,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",AMD,2023-10-30 20:23:22,5
Intel,k7708p1,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,AMD,2023-10-31 05:29:56,6
Intel,k77n6u7,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,AMD,2023-10-31 10:41:33,3
Intel,k75ya7o,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",AMD,2023-10-31 00:26:50,3
Intel,k7a5u7g,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",AMD,2023-10-31 21:03:27,0
Intel,k77o6pg,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",AMD,2023-10-31 10:53:04,1
Intel,k755rf9,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",AMD,2023-10-30 21:16:52,5
Intel,k75jput,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",AMD,2023-10-30 22:47:54,3
Intel,k761p0l,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",AMD,2023-10-31 00:49:12,1
Intel,k75i4pq,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,AMD,2023-10-30 22:37:13,2
Intel,k77cvge,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",AMD,2023-10-31 08:22:22,-1
Intel,k755zaf,"Mirage is PS4 game, Avatar is PS5",AMD,2023-10-30 21:18:15,5
Intel,k74a84y,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",AMD,2023-10-30 18:04:20,4
Intel,k75qyxk,Timed epic exclusivity? Aww man.,AMD,2023-10-30 23:37:17,3
Intel,k74vid8,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,AMD,2023-10-30 20:14:32,0
Intel,k74dkaf,You... are.. joking... right..?,AMD,2023-10-30 18:24:49,4
Intel,k770p8h,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,AMD,2023-10-31 05:35:28,1
Intel,k7ftdl4,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,AMD,2023-11-01 23:57:41,3
Intel,k74azia,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,AMD,2023-10-30 18:09:00,0
Intel,k749l36,Try ubisoft achievements :),AMD,2023-10-30 18:00:24,-1
Intel,k7a8c78,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",AMD,2023-10-31 21:19:36,1
Intel,k767klo,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",AMD,2023-10-31 01:28:47,1
Intel,k783twl,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,AMD,2023-10-31 13:15:34,0
Intel,k76sj05,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),AMD,2023-10-31 04:07:38,3
Intel,k76mhsf,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",AMD,2023-10-31 03:14:45,3
Intel,k747o6w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-30 17:48:49,1
Intel,k74w3s3,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",AMD,2023-10-30 20:18:07,0
Intel,k75if4b,yup that is why I will play 1440 UW native with a 7900XTX.,AMD,2023-10-30 22:39:08,1
Intel,k74vwla,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,AMD,2023-10-30 20:16:56,1
Intel,k75qeu1,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",AMD,2023-10-30 23:33:32,2
Intel,k75p425,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",AMD,2023-10-30 23:24:38,1
Intel,k75qigc,Very similar performance I guess,AMD,2023-10-30 23:34:13,1
Intel,k77bjls,Ye,AMD,2023-10-31 08:03:05,0
Intel,k7813wv,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,AMD,2023-10-31 12:54:43,0
Intel,k77obao,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",AMD,2023-10-31 10:54:29,1
Intel,k77o9a4,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",AMD,2023-10-31 10:53:51,1
Intel,k77udwp,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-31 11:56:37,1
Intel,k78piiq,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,AMD,2023-10-31 15:40:37,1
Intel,k7e7bz2,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,AMD,2023-11-01 17:53:25,1
Intel,k7eem4z,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,AMD,2023-11-01 18:37:35,0
Intel,k7fs4ss,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",AMD,2023-11-01 23:49:18,0
Intel,k7k3mjx,"with AFMF it works , didnt test with FSR3 now.",AMD,2023-11-02 20:38:03,1
Intel,k8l2h6h,Reading before raging :),AMD,2023-11-10 01:18:48,1
Intel,k74j51i,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",AMD,2023-10-30 18:58:47,64
Intel,k774q01,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,AMD,2023-10-31 06:26:48,3
Intel,k74rg7l,For 30fps even lol,AMD,2023-10-30 19:49:50,2
Intel,k74tk9h,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",AMD,2023-10-30 20:02:46,1
Intel,k779kpw,Or be happy your shitty video card is still supported.,AMD,2023-10-31 07:34:39,1
Intel,k78mpu0,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,AMD,2023-10-31 15:22:45,0
Intel,k77deta,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,AMD,2023-10-31 08:30:28,7
Intel,k77kqzi,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,AMD,2023-10-31 10:12:09,1
Intel,k74w3jk,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,AMD,2023-10-30 20:18:05,14
Intel,k74frkt,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,AMD,2023-10-30 18:38:21,11
Intel,k74k70d,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",AMD,2023-10-30 19:05:17,2
Intel,k7a00mh,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",AMD,2023-10-31 20:26:38,1
Intel,k74w35a,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",AMD,2023-10-30 20:18:01,0
Intel,k77ddfz,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,AMD,2023-10-31 08:29:53,6
Intel,k79hiyj,But it does in many ways.,AMD,2023-10-31 18:33:00,6
Intel,k77ktee,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",AMD,2023-10-31 10:13:00,1
Intel,k78mx3z,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",AMD,2023-10-31 15:24:03,-2
Intel,k792qlt,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,AMD,2023-10-31 17:02:03,6
Intel,k77jb2i,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",AMD,2023-10-31 09:53:38,4
Intel,k776y63,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",AMD,2023-10-31 06:57:06,11
Intel,k75f8fi,speaking facts my guy,AMD,2023-10-30 22:17:50,1
Intel,k77ezjy,You shouldn't have downvote dood wtf redditors ?,AMD,2023-10-31 08:54:03,0
Intel,k79hyhx,People with AMD cards dislike upscaling more because FSR sucks ass lol.,AMD,2023-10-31 18:35:41,4
Intel,k77ebzi,"Uhm, me btw...",AMD,2023-10-31 08:44:25,3
Intel,k77kv35,"> Who actually plays games at native these days, if it has upscaling?  I do.",AMD,2023-10-31 10:13:35,3
Intel,k75rfzg,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",AMD,2023-10-30 23:40:30,2
Intel,k7kxjkt,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",AMD,2023-11-02 23:49:47,3
Intel,k7xpols,onto absolutely nothing.,AMD,2023-11-05 15:20:55,1
Intel,k785u5t,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",AMD,2023-10-31 13:30:13,11
Intel,k75w2l1,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,AMD,2023-10-31 00:12:04,7
Intel,k76yitl,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,AMD,2023-10-31 05:09:41,1
Intel,k77agw9,The game is raytracing only with a ridiculous ammount of foooliage.,AMD,2023-10-31 07:47:35,7
Intel,k77h1xr,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",AMD,2023-10-31 09:23:11,7
Intel,k78n7c0,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,AMD,2023-10-31 15:25:54,3
Intel,k74jbjo,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,AMD,2023-10-30 18:59:54,3
Intel,k77uxf6,False.  guy blocked me lmao,AMD,2023-10-31 12:01:42,0
Intel,k770n5h,"I thought that was on Linux, though I might be wrong",AMD,2023-10-31 05:34:47,0
Intel,k75wi30,1070 doesn't have hardware RT though.,AMD,2023-10-31 00:14:59,4
Intel,k74g227,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),AMD,2023-10-30 18:40:06,23
Intel,k78amg9,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,AMD,2023-10-31 14:03:39,0
Intel,k74qlaz,Completely agree.,AMD,2023-10-30 19:44:35,3
Intel,k7hr3n6,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,AMD,2023-11-02 11:28:02,1
Intel,k7476pu,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),AMD,2023-10-30 17:45:53,3
Intel,k75aybf,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",AMD,2023-10-30 21:50:00,8
Intel,k74gjdn,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",AMD,2023-10-30 18:43:00,9
Intel,k74gjiu,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,AMD,2023-10-30 18:43:01,1
Intel,k74d1es,"Derp, derp.",AMD,2023-10-30 18:21:37,0
Intel,k7gqdq3,Sounds like John on Direct Foundry Direct every week.,AMD,2023-11-02 03:55:28,1
Intel,k7721zf,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",AMD,2023-10-31 05:52:19,3
Intel,k77id8y,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",AMD,2023-10-31 09:41:01,4
Intel,k75quo4,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,AMD,2023-10-30 23:36:30,3
Intel,k75wl3m,Avatar is RT only. There is no non RT mode.,AMD,2023-10-31 00:15:31,8
Intel,k77ar4b,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,AMD,2023-10-31 07:51:38,5
Intel,k76ly3i,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",AMD,2023-10-31 03:10:28,2
Intel,k78qssi,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",AMD,2023-10-31 15:48:32,3
Intel,k75m1nd,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,AMD,2023-10-30 23:03:44,7
Intel,k75t1dz,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,AMD,2023-10-30 23:51:23,1
Intel,k74c5po,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",AMD,2023-10-30 18:16:15,12
Intel,k74ejc7,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",AMD,2023-10-30 18:30:46,4
Intel,k762gt6,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,AMD,2023-10-31 00:54:19,0
Intel,k7795r7,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,AMD,2023-10-31 07:28:35,6
Intel,k8xdnpw,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,AMD,2023-11-12 13:53:39,1
Intel,k77azgf,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",AMD,2023-10-31 07:54:57,0
Intel,k7ad42q,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",AMD,2023-10-31 21:51:09,-1
Intel,k771qsz,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",AMD,2023-10-31 05:48:28,1
Intel,k7baowx,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",AMD,2023-11-01 01:58:28,1
Intel,k76iowi,This is what I am thinking too.,AMD,2023-10-31 02:46:08,1
Intel,k7al3b9,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",AMD,2023-10-31 22:48:01,1
Intel,k78r7ik,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",AMD,2023-10-31 15:51:02,0
Intel,k7eeuc1,Let's hope so. 30fps is far from recommended for FSR 3,AMD,2023-11-01 18:38:59,1
Intel,k7kbe2u,Yeah I heard it works with that hoping it works with fsr3 now,AMD,2023-11-02 21:25:15,1
Intel,k74jv5m,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",AMD,2023-10-30 19:03:14,39
Intel,k770h31,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",AMD,2023-10-31 05:32:44,0
Intel,k74va9v,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",AMD,2023-10-30 20:13:10,-8
Intel,k78mlfi,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,AMD,2023-10-31 15:21:57,5
Intel,k776zix,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,AMD,2023-10-31 06:57:37,2
Intel,k77ebsi,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",AMD,2023-10-31 08:44:20,-1
Intel,k82kpsv,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",AMD,2023-11-06 14:19:02,1
Intel,k792d5j,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",AMD,2023-10-31 16:59:46,2
Intel,k77dtn2,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",AMD,2023-10-31 08:36:45,3
Intel,k77togb,Assassins creed origins and odyssey side quests/collectibles oh my god,AMD,2023-10-31 11:50:06,2
Intel,k74jzj8,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,AMD,2023-10-30 19:04:00,0
Intel,k74m5ig,I appreciate your insights and opinions. Thank you.,AMD,2023-10-30 19:17:14,-1
Intel,k775lx7,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",AMD,2023-10-31 06:38:51,1
Intel,k77sn6f,console games started upscaling way before PCs .,AMD,2023-10-31 11:40:07,3
Intel,k79hu3v,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,AMD,2023-10-31 18:34:55,3
Intel,k79vmwl,you forgot who owns one of the most popular engines out there?,AMD,2023-10-31 19:59:27,-1
Intel,k7abo7k,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",AMD,2023-10-31 21:41:33,-5
Intel,k77b8ol,downvoted by devs lol,AMD,2023-10-31 07:58:40,5
Intel,k79ib8i,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,AMD,2023-10-31 18:37:52,0
Intel,k78h0ux,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",AMD,2023-10-31 14:46:05,-1
Intel,k792yre,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,AMD,2023-10-31 17:03:29,0
Intel,k79ll9w,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,AMD,2023-10-31 18:57:46,2
Intel,k78xgtm,Why is there a dialog message about unsupported hardware when you try and run a 390X?,AMD,2023-10-31 16:29:37,3
Intel,k77blcl,It can do software based RT just like every other modern GPU out there.,AMD,2023-10-31 08:03:49,2
Intel,k74gdb3,Well then no wonder rx 5700 can't manage 30 fps lol,AMD,2023-10-30 18:41:59,27
Intel,k74m0rw,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",AMD,2023-10-30 19:16:25,9
Intel,k74ihd8,Avatars uses a Lumen like software RT solution.,AMD,2023-10-30 18:54:49,19
Intel,k78qd8n,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",AMD,2023-10-31 15:45:52,2
Intel,k74lfw8,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",AMD,2023-10-30 19:12:53,13
Intel,k75797r,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),AMD,2023-10-30 21:26:16,3
Intel,k74khy7,"Yeah, thatsl happened.",AMD,2023-10-30 19:07:09,0
Intel,k7a6im2,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",AMD,2023-10-31 21:07:50,1
Intel,k75wfsk,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",AMD,2023-10-31 00:14:32,1
Intel,k77chzy,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",AMD,2023-10-31 08:16:52,2
Intel,k77pinh,You didn't understand. It's another Swiss knife engine,AMD,2023-10-31 11:07:56,0
Intel,k76pny2,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",AMD,2023-10-31 03:41:22,1
Intel,k75nvkv,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,AMD,2023-10-30 23:16:13,-1
Intel,k77ap82,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,AMD,2023-10-31 07:50:56,-2
Intel,k78n2dv,The PS5 has a 6700.,AMD,2023-10-31 15:25:00,1
Intel,k7i9v2e,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",AMD,2023-11-02 13:58:02,1
Intel,k74jbph,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",AMD,2023-10-30 18:59:55,1
Intel,k8xdwme,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,AMD,2023-11-12 13:55:46,1
Intel,k77fln1,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,AMD,2023-10-31 09:02:53,0
Intel,k7ahuzb,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,AMD,2023-10-31 22:24:31,1
Intel,k78sexp,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,AMD,2023-10-31 15:58:21,1
Intel,k74o0rt,TAA,AMD,2023-10-30 19:28:44,53
Intel,k74rhkm,Temporal anti aliasing.,AMD,2023-10-30 19:50:04,7
Intel,k78mepd,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",AMD,2023-10-31 15:20:46,3
Intel,k78zoea,"Ahh, welcome to r/FuckTAA",AMD,2023-10-31 16:43:15,1
Intel,k7fo2qt,Even 4K looks blurry with some implementations of TAA,AMD,2023-11-01 23:21:50,1
Intel,k775ulz,they're giving you the bare minimum until your upgrade!,AMD,2023-10-31 06:42:13,0
Intel,k74lgqm,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,AMD,2023-10-30 19:13:01,-6
Intel,k7551yk,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",AMD,2023-10-30 21:12:24,11
Intel,k74xjh2,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",AMD,2023-10-30 20:26:40,12
Intel,k7549zz,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",AMD,2023-10-30 21:07:34,3
Intel,k78jvij,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",AMD,2023-10-31 15:04:19,5
Intel,k79h6tb,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,AMD,2023-10-31 18:30:53,2
Intel,k77gqlw,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",AMD,2023-10-31 09:18:51,1
Intel,k78mn3v,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",AMD,2023-10-31 15:22:16,5
Intel,k7kwbcl,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,AMD,2023-11-02 23:41:35,1
Intel,k783kme,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",AMD,2023-10-31 13:13:39,-1
Intel,k7adm22,No I havent. AMD is bigger than Epic.,AMD,2023-10-31 21:54:30,2
Intel,k7aqfmz,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",AMD,2023-10-31 23:27:21,5
Intel,k77exua,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",AMD,2023-10-31 08:53:22,0
Intel,k79j5la,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",AMD,2023-10-31 18:43:01,6
Intel,k78wlj1,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,AMD,2023-10-31 16:24:13,7
Intel,k79ipwc,It's software ray tracing which isn't accelerated by hardware.,AMD,2023-10-31 18:40:20,2
Intel,k79sb21,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,AMD,2023-10-31 19:39:06,1
Intel,k79a73q,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,AMD,2023-10-31 17:47:59,2
Intel,k74o26m,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,AMD,2023-10-30 19:28:58,20
Intel,k778s32,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",AMD,2023-10-31 07:23:06,1
Intel,k75bea7,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,AMD,2023-10-30 21:52:53,5
Intel,k75j5le,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,AMD,2023-10-30 22:44:05,7
Intel,k74nnyo,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",AMD,2023-10-30 19:26:34,4
Intel,k7a88v5,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",AMD,2023-10-31 21:19:00,1
Intel,k77pvj3,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,AMD,2023-10-31 11:11:45,5
Intel,k78lq5t,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",AMD,2023-10-31 15:16:17,3
Intel,k76zwtm,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,AMD,2023-10-31 05:25:55,2
Intel,k77ddap,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",AMD,2023-10-31 08:29:49,5
Intel,k77dhd7,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",AMD,2023-10-31 08:31:33,7
Intel,k77h10q,Both excellent 👌 Down the Rabbit Hole was another solid one.,AMD,2023-10-31 09:22:50,1
Intel,k7b776f,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",AMD,2023-11-01 01:31:29,1
Intel,k78ut1x,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",AMD,2023-10-31 16:13:03,0
Intel,k74xlfo,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,AMD,2023-10-30 20:27:00,28
Intel,k74uq1m,I have to turn it off in borderlands 3. Ugh.,AMD,2023-10-30 20:09:47,2
Intel,k78zmsb,r/FuckTAA,AMD,2023-10-31 16:42:58,0
Intel,k7n69qz,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",AMD,2023-11-03 12:46:56,1
Intel,k7gyunv,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",AMD,2023-11-02 05:20:45,1
Intel,k7bw0wl,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,AMD,2023-11-01 05:13:29,-1
Intel,k78p5rj,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",AMD,2023-10-31 15:38:25,2
Intel,k7kx549,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,AMD,2023-11-02 23:47:05,1
Intel,k7c5sa3,epic is tencent...,AMD,2023-11-01 07:26:14,-1
Intel,k79evb5,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",AMD,2023-10-31 18:16:40,0
Intel,k79vqds,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",AMD,2023-10-31 20:00:02,1
Intel,k79dbl9,You don't need RT hardware to do software RT. That's what I'm saying.,AMD,2023-10-31 18:07:06,3
Intel,k74wirt,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",AMD,2023-10-30 20:20:36,19
Intel,k755cwh,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",AMD,2023-10-30 21:14:19,20
Intel,k74ogkv,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),AMD,2023-10-30 19:31:29,15
Intel,k751p3z,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,AMD,2023-10-30 20:51:38,9
Intel,k75za9c,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",AMD,2023-10-31 00:33:28,10
Intel,k76k5hm,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,AMD,2023-10-31 02:56:53,3
Intel,k77u7zx,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,AMD,2023-10-31 11:55:06,2
Intel,k77anl2,Yes hah,AMD,2023-10-31 07:50:17,1
Intel,k78kxsq,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",AMD,2023-10-31 15:11:02,1
Intel,k75bm3k,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",AMD,2023-10-30 21:54:17,1
Intel,k75jy4k,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,AMD,2023-10-30 22:49:28,8
Intel,k75cf9l,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",AMD,2023-10-30 21:59:30,11
Intel,k74s4b6,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",AMD,2023-10-30 19:53:57,4
Intel,k78qh5r,Oh man the hair... its so crazy how much upscaling kills the hair..,AMD,2023-10-31 15:46:32,0
Intel,k74vpzd,i mean they already arent the smartest bulbs considering they went team green.,AMD,2023-10-30 20:15:50,-9
Intel,k7bwwni,Reading comrehension dude.,AMD,2023-11-01 05:24:02,1
Intel,k77rq8u,"I do, but thanks for your interest.",AMD,2023-10-31 11:31:01,0
Intel,k781oqh,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,AMD,2023-10-31 12:59:12,1
Intel,k77gylo,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",AMD,2023-10-31 09:21:55,0
Intel,k77prws,Nice. It’s on the list. Thanks man,AMD,2023-10-31 11:10:40,0
Intel,k79111l,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,AMD,2023-10-31 16:51:38,1
Intel,k74y62b,Probably because TAA is (unfortunately) more prevalent than it ever was.,AMD,2023-10-30 20:30:23,12
Intel,k77crxd,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",AMD,2023-10-31 08:20:57,4
Intel,k7ky7yi,Oh my bad if I read that wrong,AMD,2023-11-02 23:54:13,1
Intel,k79nw3z,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,AMD,2023-10-31 19:11:53,1
Intel,k74xr8d,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,AMD,2023-10-30 20:27:57,-10
Intel,k78nlj0,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",AMD,2023-10-31 15:28:29,1
Intel,k78nx4p,imagine being mad that 4 year old cards arent high end,AMD,2023-10-31 15:30:32,2
Intel,k78qsw0,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,AMD,2023-10-31 15:48:33,0
Intel,k75i9u4,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",AMD,2023-10-30 22:38:10,1
Intel,k77gx0m,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",AMD,2023-10-31 09:21:18,1
Intel,k74smha,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",AMD,2023-10-30 19:57:03,2
Intel,k78qwba,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",AMD,2023-10-31 15:49:08,0
Intel,k74wqog,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",AMD,2023-10-30 20:21:55,2
Intel,k7bxbe9,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",AMD,2023-11-01 05:29:04,1
Intel,k77y4x2,"It's for textures, not object edges from FSR use, lol. Two completely different things",AMD,2023-10-31 12:30:16,5
Intel,k793lf6,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",AMD,2023-10-31 17:07:23,-1
Intel,k77g0i6,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",AMD,2023-10-31 09:08:37,12
Intel,k77ubrs,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",AMD,2023-10-31 11:56:04,-2
Intel,k8f5yw4,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",AMD,2023-11-08 22:27:09,1
Intel,k79oe6d,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,AMD,2023-10-31 19:14:56,1
Intel,k753gel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",AMD,2023-10-30 21:02:23,13
Intel,k78lli0,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,AMD,2023-10-31 15:15:24,4
Intel,k7au2y4,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",AMD,2023-10-31 23:54:23,1
Intel,k75wc3v,5700 was probably the lowest AMD card they had to test with.,AMD,2023-10-31 00:13:51,1
Intel,k74tcrl,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",AMD,2023-10-30 20:01:29,4
Intel,k7byrit,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,AMD,2023-11-01 05:47:35,1
Intel,k7a8jw6,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,AMD,2023-10-31 21:20:59,1
Intel,k7kn3dl,AMD CAS is aimed to restore detail,AMD,2023-11-02 22:40:21,1
Intel,k7k5dha,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",AMD,2023-11-02 20:48:40,1
Intel,k7cl7iw,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,AMD,2023-11-01 10:59:12,2
Intel,k74ugbm,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",AMD,2023-10-30 20:08:12,0
Intel,k7bzcr6,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",AMD,2023-11-01 05:55:17,1
Intel,k7cntlm,Yeah but a 3060ti didn't,AMD,2023-11-01 11:27:22,1
Intel,k78mcrt,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",AMD,2023-10-31 15:20:25,2
Intel,k74zgnn,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",AMD,2023-10-30 20:38:09,3
Intel,k1rzmke,Confirmed Can it run CP2077 4k is the new Can it run Crysis,AMD,2023-09-22 22:22:20,590
Intel,k1s9f6d,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,AMD,2023-09-22 23:31:10,579
Intel,k1rtgmz,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,AMD,2023-09-22 21:41:53,308
Intel,k1tezss,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",AMD,2023-09-23 05:11:42,24
Intel,k1spmgk,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,AMD,2023-09-23 01:30:07,18
Intel,k1ryede,3080 falling behind a 3060? what is this data?,AMD,2023-09-22 22:14:11,123
Intel,k1say8p,wake me up when we have a card that can run this at 40 without needing its own psu.,AMD,2023-09-22 23:42:17,43
Intel,k1seqym,5090 here I come!,AMD,2023-09-23 00:09:51,28
Intel,k1sf8id,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",AMD,2023-09-23 00:13:23,36
Intel,k1samn8,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",AMD,2023-09-22 23:39:57,29
Intel,k1se7w7,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,AMD,2023-09-23 00:06:00,29
Intel,k1tcwd4,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",AMD,2023-09-23 04:49:38,6
Intel,k1swxw6,It's a good thing nobody has to actually play it native.,AMD,2023-09-23 02:26:41,19
Intel,k1sd0w5,Well good thing literally nobody is doing that…,AMD,2023-09-22 23:57:14,10
Intel,k1tv5m1,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,AMD,2023-09-23 08:24:44,10
Intel,k1svrss,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,AMD,2023-09-23 02:17:32,16
Intel,k1u5n8e,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,AMD,2023-09-23 10:40:09,4
Intel,k1t06iw,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",AMD,2023-09-23 02:52:43,12
Intel,k1sta67,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,AMD,2023-09-23 01:58:23,15
Intel,k1ru0dy,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",AMD,2023-09-22 21:45:23,28
Intel,k1sh29m,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",AMD,2023-09-23 00:26:40,17
Intel,k1rwvmw,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,AMD,2023-09-22 22:04:05,21
Intel,k1s5ads,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,AMD,2023-09-22 23:01:20,8
Intel,k1t5pl6,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",AMD,2023-09-23 03:40:25,7
Intel,k1sde74,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",AMD,2023-09-22 23:59:57,17
Intel,k1sbtem,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",AMD,2023-09-22 23:48:27,4
Intel,k1sr9s2,This doesn’t seem right…. 3080 performing worse than a 2080TI?,AMD,2023-09-23 01:42:55,2
Intel,k1t2kii,How tf is a 2080 ti getting more fps than a 3080?!?,AMD,2023-09-23 03:12:42,2
Intel,k1t66da,I'm not seeing the RTX A6000 on here...,AMD,2023-09-23 03:44:41,2
Intel,k1tglq7,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,AMD,2023-09-23 05:29:29,2
Intel,k1u9v0r,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",AMD,2023-09-23 11:28:08,2
Intel,k1ufe40,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",AMD,2023-09-23 12:22:22,2
Intel,k1witvs,Does anyone actually play this game? Its more of a meme game imho,AMD,2023-09-23 20:46:05,2
Intel,k1xd78i,4.3 fps 😂😂😂,AMD,2023-09-24 00:13:50,2
Intel,k1sczh9,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",AMD,2023-09-22 23:56:57,10
Intel,k1rumnu,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",AMD,2023-09-22 21:49:26,10
Intel,k1shlno,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,AMD,2023-09-23 00:30:36,4
Intel,k1t2byj,Lol the 3060ti is better than a 7900xtx...crazy,AMD,2023-09-23 03:10:40,3
Intel,k1vuadd,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,AMD,2023-09-23 18:12:06,3
Intel,k1s8b2u,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,AMD,2023-09-22 23:23:07,11
Intel,k1t61f2,ThE gAMe iS PoOrLY OpTImiSed!!!,AMD,2023-09-23 03:43:26,4
Intel,k1ttlda,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,AMD,2023-09-23 08:04:31,4
Intel,k1sgjyj,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",AMD,2023-09-23 00:22:53,3
Intel,k1sxb06,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",AMD,2023-09-23 02:29:34,2
Intel,k1sydqu,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",AMD,2023-09-23 02:38:06,2
Intel,k1tbdbm,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,AMD,2023-09-23 04:34:09,2
Intel,k1twz2u,"""4090 is 4 times faster than 7900XTX""",AMD,2023-09-23 08:48:39,2
Intel,k1uouoc,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",AMD,2023-09-23 13:40:46,2
Intel,k1wadup,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,AMD,2023-09-23 19:52:54,2
Intel,k1tl3qo,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),AMD,2023-09-23 06:20:29,3
Intel,k1s8g0f,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",AMD,2023-09-22 23:24:08,3
Intel,k1s8o3g,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",AMD,2023-09-22 23:25:45,3
Intel,k1sg5kw,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,AMD,2023-09-23 00:20:00,3
Intel,k1skakr,Native resolution is a thing of the past. DLSS looks better than native anyway.,AMD,2023-09-23 00:50:07,2
Intel,k1txff8,Is this with DLSS + FG?,AMD,2023-09-23 08:54:33,1
Intel,k1smbmr,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",AMD,2023-09-23 01:05:06,-1
Intel,k1s3u4i,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",AMD,2023-09-22 22:51:01,1
Intel,k1t2qkw,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,AMD,2023-09-23 03:14:12,1
Intel,k1sgzlr,No one is using these settings.,AMD,2023-09-23 00:26:07,1
Intel,k1spqtc,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",AMD,2023-09-23 01:31:01,1
Intel,k1shcs4,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",AMD,2023-09-23 00:28:49,1
Intel,k1sj5mo,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",AMD,2023-09-23 00:41:56,1
Intel,k1ulwx6,cyberpunk sucks don't worry about it,AMD,2023-09-23 13:17:47,1
Intel,k1rvukw,3080ti?,AMD,2023-09-22 21:57:22,1
Intel,k1sljwz,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",AMD,2023-09-23 00:59:20,1
Intel,k1sqr9g,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,AMD,2023-09-23 01:38:55,1
Intel,k1tt1b6,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",AMD,2023-09-23 07:57:19,1
Intel,k1tuc3v,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",AMD,2023-09-23 08:14:11,1
Intel,k1uq4ce,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,AMD,2023-09-23 13:50:25,1
Intel,k1s9ert,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),AMD,2023-09-22 23:31:05,-1
Intel,k1rzvbb,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",AMD,2023-09-22 22:23:58,-16
Intel,k1sfn3w,What body part do you think will get me a 5090?,AMD,2023-09-23 00:16:18,0
Intel,k1txo0m,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",AMD,2023-09-23 08:57:42,0
Intel,k1u7ngf,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",AMD,2023-09-23 11:04:00,0
Intel,k1uxont,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",AMD,2023-09-23 14:44:13,0
Intel,k1s5mej,That not even proper path tracing.,AMD,2023-09-22 23:03:45,-1
Intel,k1s6qt1,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,AMD,2023-09-22 23:11:53,-2
Intel,k1sb5wk,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",AMD,2023-09-22 23:43:51,-9
Intel,k1svwfa,Native is a thing of the past when dlss produces better looking image,AMD,2023-09-23 02:18:33,-2
Intel,k1sdfih,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,AMD,2023-09-23 00:00:13,-3
Intel,k1s7xzc,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,AMD,2023-09-22 23:20:30,-9
Intel,k1slr2s,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,AMD,2023-09-23 01:00:47,-1
Intel,k1sfvki,"Rather than having 2-3fps, I would go and see a video of path tracing.",AMD,2023-09-23 00:17:59,0
Intel,k1sihxn,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",AMD,2023-09-23 00:37:09,0
Intel,k1su9sh,cool idc i wont play that shit at such shit settings,AMD,2023-09-23 02:05:55,0
Intel,k1syk9g,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,AMD,2023-09-23 02:39:33,0
Intel,k1tpxos,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,AMD,2023-09-23 07:18:17,0
Intel,k1tvoqm,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",AMD,2023-09-23 08:31:41,0
Intel,k1tytwk,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,AMD,2023-09-23 09:12:53,0
Intel,k1tzugl,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,AMD,2023-09-23 09:26:15,0
Intel,k1uhusa,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",AMD,2023-09-23 12:44:21,0
Intel,k1v2ghf,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,AMD,2023-09-23 15:16:08,0
Intel,k1v8o7m,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",AMD,2023-09-23 15:55:59,0
Intel,k1vfxae,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,AMD,2023-09-23 16:41:57,0
Intel,k1vjs6i,"Looks like 4K is mainly just for videos, not gaming for the time being.",AMD,2023-09-23 17:06:09,0
Intel,k1vnxlu,How can you bring out such technology and no hardware can process it properly.  an impudence,AMD,2023-09-23 17:32:09,0
Intel,k1wm3gk,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,AMD,2023-09-23 21:06:13,0
Intel,k1ysyyz,You could just you know... disable path tracing and get 70 fps or go below 4k,AMD,2023-09-24 08:12:13,0
Intel,k1z173o,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",AMD,2023-09-24 09:58:57,0
Intel,k25idkz,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",AMD,2023-09-25 16:06:18,0
Intel,k1rv0mr,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,AMD,2023-09-22 21:51:56,-33
Intel,k1sbmt5,"so it is settled, Devs have zero optimization in games.",AMD,2023-09-22 23:47:08,-13
Intel,k1t5ilx,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",AMD,2023-09-23 03:38:40,-2
Intel,k1t8qzc,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",AMD,2023-09-23 04:08:39,-3
Intel,k1syqs1,Once again proving RT is useless,AMD,2023-09-23 02:41:00,-6
Intel,k1sfuie,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,AMD,2023-09-23 00:17:45,322
Intel,k1sw4a8,"""only gamers get this joke""",AMD,2023-09-23 02:20:15,1
Intel,k1tqj3o,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",AMD,2023-09-23 07:25:29,-3
Intel,k1uey5o,Starfield is peaking around the corner.,AMD,2023-09-23 12:18:19,0
Intel,k1ume3r,starfield as well,AMD,2023-09-23 13:21:33,0
Intel,k1smpf3,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,AMD,2023-09-23 01:07:57,412
Intel,k1sm1vi,The new Crysis,AMD,2023-09-23 01:03:02,18
Intel,k1t68ad,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",AMD,2023-09-23 03:45:09,38
Intel,k1tduwg,How long until a $200 card can do that?,AMD,2023-09-23 04:59:33,10
Intel,k1tl6bz,Most improvement is probably going to AI software more than hardware in the next few years.,AMD,2023-09-23 06:21:18,12
Intel,k1sui5w,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",AMD,2023-09-23 02:07:40,33
Intel,k1tggmb,GPU need to have its own garage by then,AMD,2023-09-23 05:27:53,7
Intel,k1sjd2a,Yep! Insane how fast things change.,AMD,2023-09-23 00:43:26,14
Intel,k1th7kz,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",AMD,2023-09-23 05:36:13,25
Intel,k1tksc7,Yeah rtx 12060 with 9.5 gb Vram will be a monster,AMD,2023-09-23 06:16:50,13
Intel,k1toc97,I'm willing to bet hardware improvement will come to a halt before that.,AMD,2023-09-23 06:59:10,4
Intel,k1tp7w0,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",AMD,2023-09-23 07:09:42,7
Intel,k1u5t5c,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,AMD,2023-09-23 10:42:10,2
Intel,k1u9trt,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,AMD,2023-09-23 11:27:46,2
Intel,k1umihs,But then the current gen games of that era will run like this. The cycle continues,AMD,2023-09-23 13:22:31,2
Intel,k1vcazz,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,AMD,2023-09-23 16:19:04,2
Intel,k1t8b6k,Who knows what new tech will be out in even 4 years lol,AMD,2023-09-23 04:04:25,2
Intel,k1tl2z8,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",AMD,2023-09-23 06:20:14,2
Intel,k1u0yj3,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",AMD,2023-09-23 09:40:52,1
Intel,k1u6uez,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",AMD,2023-09-23 10:54:31,1
Intel,k1sby3m,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,AMD,2023-09-22 23:49:24,312
Intel,k1rw4fg,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",AMD,2023-09-22 21:59:09,103
Intel,k1sbwfh,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",AMD,2023-09-22 23:49:03,11
Intel,k1t5qx0,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",AMD,2023-09-23 03:40:46,-4
Intel,k1tf1z9,I think that most of the progress will go together with software tricks and upscalers.,AMD,2023-09-23 05:12:21,3
Intel,k1s07rb,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",AMD,2023-09-22 22:26:17,131
Intel,k1rz345,That's VRAM for you,AMD,2023-09-22 22:18:45,134
Intel,k1tc7vb,we are counting decimals of fps its just all margin of error.,AMD,2023-09-23 04:42:40,11
Intel,k1s03of,If you buy a card with low ram that card is for right now only lol,AMD,2023-09-22 22:25:32,4
Intel,k1v5pv8,Wake me up when a $250 GPU can run this at 1080p.,AMD,2023-09-23 15:37:12,8
Intel,k1uiun2,Well DLSS isn't best. DLAA is,AMD,2023-09-23 12:52:43,7
Intel,k1svcks,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,AMD,2023-09-23 02:14:15,25
Intel,k1si3ng,Can’t you just disable TAA? Or do you just have to live with the ghosting?,AMD,2023-09-23 00:34:15,4
Intel,k1v2dce,TAA is garbage in everything. TAA and FSR can both get fucked,AMD,2023-09-23 15:15:33,2
Intel,k1svesl,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",AMD,2023-09-23 02:14:43,7
Intel,k1v301u,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",AMD,2023-09-23 15:19:41,2
Intel,k1v2lmw,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",AMD,2023-09-23 15:17:04,2
Intel,k1sv2p6,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",AMD,2023-09-23 02:12:07,7
Intel,k1vdbm6,Imagine buying a 4090 and then using upscaling.,AMD,2023-09-23 16:25:29,-5
Intel,k1scblp,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,AMD,2023-09-22 23:52:08,18
Intel,k1sfzkt,PC gaming is in Crysis.,AMD,2023-09-23 00:18:47,5
Intel,k1y7rss,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,AMD,2023-09-24 04:15:05,-1
Intel,k1t8lmy,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,AMD,2023-09-23 04:07:13,3
Intel,k1rxtnn,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",AMD,2023-09-22 22:10:20,9
Intel,k1u60lu,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",AMD,2023-09-23 10:44:39,14
Intel,k1sizpa,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",AMD,2023-09-23 00:40:43,-5
Intel,k1u5vqx,Yeh because native 4k looks worse than dlss + RR 4k,AMD,2023-09-23 10:43:02,9
Intel,k1xi8dd,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",AMD,2023-09-24 00:50:15,2
Intel,k1u5mhq,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,AMD,2023-09-23 10:39:55,12
Intel,k1sshhs,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",AMD,2023-09-23 01:52:13,8
Intel,k1sr5lk,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",AMD,2023-09-23 01:42:00,0
Intel,k1t41gi,VRAM,AMD,2023-09-23 03:25:30,3
Intel,k1uiuuv,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",AMD,2023-09-23 12:52:46,1
Intel,k1zcvhv,still better than 90% of games in 2023,AMD,2023-09-24 12:08:19,0
Intel,k1rvtrp,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,AMD,2023-09-22 21:57:13,20
Intel,k1s0kpr,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",AMD,2023-09-22 22:28:43,-2
Intel,k1sbnjt,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",AMD,2023-09-22 23:47:17,-7
Intel,k1u6o55,All graphics you see on your computer screen is fake,AMD,2023-09-23 10:52:26,1
Intel,k1t62rv,Path tracing is more demanding than Ray tracing,AMD,2023-09-23 03:43:46,6
Intel,k1u17zi,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",AMD,2023-09-23 09:44:22,3
Intel,k1s9p4d,I guess between the 3090 and the 4070,AMD,2023-09-22 23:33:11,0
Intel,k1sjydp,Yep hahaha,AMD,2023-09-23 00:47:40,2
Intel,k1v42p3,RemindMe! 7 years,AMD,2023-09-23 15:26:41,2
Intel,k1s3vi6,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",AMD,2023-09-22 22:51:16,19
Intel,k1s45n5,"This is an absurd, fanboyish thought lmao",AMD,2023-09-22 22:53:15,15
Intel,k1u3kdb,dont let novidia marketing see this youll get down voted into oblivion,AMD,2023-09-23 10:14:19,2
Intel,k1s2le1,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,AMD,2023-09-22 22:42:21,-7
Intel,k1sylo6,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,AMD,2023-09-23 02:39:51,3
Intel,k1sltal,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",AMD,2023-09-23 01:01:15,2
Intel,k1u6rsu,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,AMD,2023-09-23 10:53:38,2
Intel,k1t37ky,Fanboyism of both kinds is bad,AMD,2023-09-23 03:18:18,0
Intel,k1sbir3,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",AMD,2023-09-22 23:46:20,-1
Intel,k1u85m7,Since it needs more than 10gb vram,AMD,2023-09-23 11:09:41,3
Intel,k1tq0gr,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,AMD,2023-09-23 07:19:12,4
Intel,k1rw9dl,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,AMD,2023-09-22 22:00:02,28
Intel,k1sl1fh,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",AMD,2023-09-23 00:55:34,7
Intel,k1rx0ka,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",AMD,2023-09-22 22:04:58,-15
Intel,k1u2f5k,It's settled that you have no idea what you talking about,AMD,2023-09-23 09:59:44,4
Intel,k1sczqa,Its full path tracing u cannot really optimize this much.,AMD,2023-09-22 23:57:00,11
Intel,k1tpj68,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",AMD,2023-09-23 07:13:34,3
Intel,k1s9h3d,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",AMD,2023-09-22 23:31:33,2
Intel,k1sprwd,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,AMD,2023-09-23 01:31:15,141
Intel,k1tds1v,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",AMD,2023-09-23 04:58:42,28
Intel,k1u7ebf,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,AMD,2023-09-23 11:01:03,18
Intel,k1vo73s,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",AMD,2023-09-23 17:33:46,5
Intel,k1u7ms4,Amd had a tessellation unit. It went unused but it was present,AMD,2023-09-23 11:03:47,2
Intel,k1srvji,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",AMD,2023-09-23 01:47:34,162
Intel,k1sqn2i,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",AMD,2023-09-23 01:38:00,34
Intel,k1tzlza,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",AMD,2023-09-23 09:23:04,5
Intel,k1uatm9,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:38:16,6
Intel,k1u8wkg,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",AMD,2023-09-23 11:17:47,16
Intel,k1tdixd,"Oh damn, I forgot about those cards.  I wanted one so badly.",AMD,2023-09-23 04:56:03,3
Intel,k1u3mme,Remember when companies tried to sell physics cards lol,AMD,2023-09-23 10:15:05,3
Intel,k1thwg4,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,AMD,2023-09-23 05:43:55,13
Intel,k1tjl5e,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",AMD,2023-09-23 06:03:08,2
Intel,k1umriw,I remember when people had a dedicated PhysX card.,AMD,2023-09-23 13:24:29,2
Intel,k1un2sg,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,AMD,2023-09-23 13:26:56,16
Intel,k1tob83,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",AMD,2023-09-23 06:58:48,4
Intel,k1txgpk,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,AMD,2023-09-23 08:55:01,5
Intel,k1ugyqs,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",AMD,2023-09-23 12:36:34,4
Intel,k1ubuqx,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",AMD,2023-09-23 11:48:42,2
Intel,k1uc6ox,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",AMD,2023-09-23 11:51:55,9
Intel,k1uln5g,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",AMD,2023-09-23 13:15:37,4
Intel,k1ull2c,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,AMD,2023-09-23 13:15:10,2
Intel,k1tocdn,25 years,AMD,2023-09-23 06:59:12,6
Intel,k1u1mrk,"The software needs to run on hardware, right now it eats through GPU compute and memory.",AMD,2023-09-23 09:49:38,4
Intel,k1tqm89,And sadly ended up with 450 Watt TDP to achieve that performance.,AMD,2023-09-23 07:26:36,10
Intel,k1tvo5h,This. We'd be lucky to see more than 3 generations in the upcoming decade.,AMD,2023-09-23 08:31:28,4
Intel,k1u1jnl,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",AMD,2023-09-23 09:48:32,3
Intel,k1ucdje,Not with that attitude,AMD,2023-09-23 11:53:45,2
Intel,k1ugj3a,Love how it's still gimped on memory size 😂,AMD,2023-09-23 12:32:43,6
Intel,k1u3901,"They just need to render the minimum information needed , and let the ai do the rest",AMD,2023-09-23 10:10:17,2
Intel,k1u9l5i,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",AMD,2023-09-23 11:25:11,-4
Intel,k1u27rs,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",AMD,2023-09-23 09:57:10,3
Intel,k1ufc12,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",AMD,2023-09-23 12:21:51,2
Intel,k1t3hpt,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",AMD,2023-09-23 03:20:42,157
Intel,k1tek93,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",AMD,2023-09-23 05:07:01,11
Intel,k1uoawi,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",AMD,2023-09-23 13:36:30,3
Intel,k1su9mw,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,AMD,2023-09-23 02:05:53,-20
Intel,k1sbp7z,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",AMD,2023-09-22 23:47:36,81
Intel,k1sxtj8,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",AMD,2023-09-23 02:33:36,6
Intel,k1svx2t,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,AMD,2023-09-23 02:18:41,4
Intel,k1sbvoe,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",AMD,2023-09-22 23:48:54,16
Intel,k1slgjk,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,AMD,2023-09-23 00:58:39,9
Intel,k1tf7yk,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",AMD,2023-09-23 05:14:13,4
Intel,k1sad2c,So rendering at 960p? Oof...,AMD,2023-09-22 23:38:00,6
Intel,k1sd3lh,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",AMD,2023-09-22 23:57:47,-7
Intel,k1rxn27,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,AMD,2023-09-22 22:09:06,-18
Intel,k1s6fmd,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,AMD,2023-09-22 23:09:38,-23
Intel,k1tzl0f,Dont use useless raytracing and you wont have any problems lol,AMD,2023-09-23 09:22:43,0
Intel,k1vpidg,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",AMD,2023-09-23 17:42:04,8
Intel,k23ed9f,AW2 looks insane. Can't wait to play soon,AMD,2023-09-25 04:14:36,2
Intel,k1si0t0,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,AMD,2023-09-23 00:33:40,22
Intel,k1s5qaj,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",AMD,2023-09-22 23:04:33,33
Intel,k1sor0g,My 3080 in shambles,AMD,2023-09-23 01:23:27,4
Intel,k1u5gbh,"It looks better than native even with fsr quality, the Taa in this game is shit",AMD,2023-09-23 10:37:48,-1
Intel,k1t69ca,You dont need to increase raw performance. You need to increase RT performance.,AMD,2023-09-23 03:45:25,32
Intel,k1vko89,Imagine!,AMD,2023-09-23 17:11:45,6
Intel,k1vz5q2,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,AMD,2023-09-23 18:42:28,6
Intel,k22igme,Only people who don't give a shit about high quality graphics don't care about ray tracing.,AMD,2023-09-25 00:09:28,3
Intel,k1tamw3,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,AMD,2023-09-23 04:26:51,3
Intel,k1s8m60,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",AMD,2023-09-22 23:25:22,3
Intel,k1v3dh2,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",AMD,2023-09-23 15:22:07,7
Intel,k1u1pck,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,AMD,2023-09-23 09:50:33,8
Intel,k1sl137,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",AMD,2023-09-23 00:55:30,2
Intel,k1u5xqe,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,AMD,2023-09-23 10:43:42,-5
Intel,k22yxwq,The Evangelical Church of Native,AMD,2023-09-25 02:08:44,1
Intel,k1vk8iw,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",AMD,2023-09-23 17:09:00,2
Intel,k22yk42,Nice but ive been playing video games since the 70s and its Shit end off..,AMD,2023-09-25 02:05:53,2
Intel,k1s3cm8,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,AMD,2023-09-22 22:47:37,-12
Intel,k1ssze4,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",AMD,2023-09-23 01:56:05,-4
Intel,k1tpcbe,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,AMD,2023-09-23 07:11:12,10
Intel,k1srkr7,"Yeah, sure, now go back to play starfield.",AMD,2023-09-23 01:45:16,8
Intel,k1siya9,"There literally is a /s, what else do you need to detect sarcasm?",AMD,2023-09-23 00:40:26,3
Intel,k1ycket,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",AMD,2023-09-24 05:03:07,1
Intel,k1wclz4,"I know, which makes path tracing even worse off imo.",AMD,2023-09-23 20:06:58,1
Intel,k1v47c6,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2023-09-23 15:27:32,2
Intel,k1vh7xa,haha this is gold 🥇,AMD,2023-09-23 16:50:10,2
Intel,k1sakbd,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",AMD,2023-09-22 23:39:29,-8
Intel,k1s5318,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",AMD,2023-09-22 22:59:52,6
Intel,k1t4yk0,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",AMD,2023-09-23 03:33:41,-3
Intel,k1sakwq,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",AMD,2023-09-22 23:39:36,-10
Intel,k1ryjta,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,AMD,2023-09-22 22:15:11,2
Intel,k1stpfv,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,AMD,2023-09-23 02:01:36,41
Intel,k1w9mwj,Funnily Crysis still have better destructible environments than Cyberpunk has tho,AMD,2023-09-23 19:48:19,1
Intel,k1vonge,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",AMD,2023-09-23 17:36:37,9
Intel,k1txp0j,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,AMD,2023-09-23 08:58:05,17
Intel,k1tg0xs,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",AMD,2023-09-23 05:23:00,6
Intel,k1syqzz,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",AMD,2023-09-23 02:41:04,4
Intel,k1u23r0,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,AMD,2023-09-23 09:55:42,3
Intel,k1u67ug,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,AMD,2023-09-23 10:47:01,14
Intel,k1t2x0x,LOL I remember this exact scene also.,AMD,2023-09-23 03:15:45,38
Intel,k1ua0tu,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",AMD,2023-09-23 11:29:52,8
Intel,k1t6cm4,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",AMD,2023-09-23 03:46:15,20
Intel,k1tlbna,"I'd like to see ray tracing addon cards, seems logical to me.",AMD,2023-09-23 06:23:01,9
Intel,k1tyf7o,"TBH, I could probably run some of the old games I have on CPU without the GPU.",AMD,2023-09-23 09:07:31,3
Intel,k1ub8fu,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:42:32,3
Intel,k1uhy6a,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,AMD,2023-09-23 12:45:10,3
Intel,k1tcex2,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,AMD,2023-09-23 04:44:40,4
Intel,k1tsmml,What gpu you have,AMD,2023-09-23 07:51:58,2
Intel,k1tyt5q,Now it even runs fine on a Ryzen 2400G.,AMD,2023-09-23 09:12:37,2
Intel,k1t9hds,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",AMD,2023-09-23 04:15:43,39
Intel,k1vaqdn,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",AMD,2023-09-23 16:09:02,10
Intel,k1tf4z4,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,AMD,2023-09-23 05:13:17,3
Intel,k1tt5y9,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",AMD,2023-09-23 07:59:01,19
Intel,k1w7xof,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",AMD,2023-09-23 19:37:48,4
Intel,k1tyzar,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,AMD,2023-09-23 09:14:51,2
Intel,k1v3dv1,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",AMD,2023-09-23 15:22:12,2
Intel,k1uaqi2,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,AMD,2023-09-23 11:37:22,5
Intel,k1uke2m,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,AMD,2023-09-23 13:05:30,2
Intel,k1udkaf,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:05:15,5
Intel,k1tl1au,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",AMD,2023-09-23 06:19:42,26
Intel,k1t9pys,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",AMD,2023-09-23 04:17:58,15
Intel,k1u3wuq,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",AMD,2023-09-23 10:18:38,1
Intel,k1sx1p7,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",AMD,2023-09-23 02:27:32,43
Intel,k1t81d2,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,AMD,2023-09-23 04:01:49,5
Intel,k1szqei,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",AMD,2023-09-23 02:49:05,12
Intel,k1syew7,It's significantly better than raster. It kills fps but the quality is great,AMD,2023-09-23 02:38:21,9
Intel,k1ug9ty,Better graphics needing more expensive hardware is hardly a hot take.,AMD,2023-09-23 12:30:24,17
Intel,k1u52xg,"Yes, better graphics costs performance. SHOCKING",AMD,2023-09-23 10:33:12,13
Intel,k1t1ej8,People do it!,AMD,2023-09-23 03:02:50,8
Intel,k1tqio6,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,AMD,2023-09-23 07:25:19,11
Intel,k1t7tj4,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",AMD,2023-09-23 03:59:48,2
Intel,k1sx4o4,Sounds like most nvidia fanboys,AMD,2023-09-23 02:28:11,14
Intel,k1u2qx4,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",AMD,2023-09-23 10:03:52,1
Intel,k1u3no0,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,AMD,2023-09-23 10:15:26,-1
Intel,k1y7ifv,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,AMD,2023-09-24 04:12:33,0
Intel,k1ss6lq,What? I thought amd was always more tuned to raster as opposed to reflections,AMD,2023-09-23 01:49:57,2
Intel,k1staby,Which is why nvidia is rabidly chasing AI hacks,AMD,2023-09-23 01:58:25,17
Intel,k1sv3ph,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,AMD,2023-09-23 02:12:20,4
Intel,k1sm72w,Say that to the people playing upscaled games at 4k (540p) on PS5.,AMD,2023-09-23 01:04:07,-2
Intel,k1s6hg3,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",AMD,2023-09-22 23:10:00,31
Intel,k1s7ntd,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,AMD,2023-09-22 23:18:28,11
Intel,k1slxqu,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",AMD,2023-09-23 01:02:10,11
Intel,k1s73qo,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",AMD,2023-09-22 23:14:27,-14
Intel,k1ttfgf,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,AMD,2023-09-23 08:02:21,11
Intel,k1tby96,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,AMD,2023-09-23 04:39:58,2
Intel,k1sb10i,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",AMD,2023-09-22 23:42:52,14
Intel,k1sf5sr,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",AMD,2023-09-23 00:12:51,2
Intel,k1u66ws,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),AMD,2023-09-23 10:46:42,6
Intel,k1t46jg,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",AMD,2023-09-23 03:26:44,0
Intel,k1u6zio,You can already have 120fps on $1000 PC,AMD,2023-09-23 10:56:12,8
Intel,k1vnjmf,It improves both looks and fps so it is a win win,AMD,2023-09-23 17:29:44,0
Intel,k1scklw,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",AMD,2023-09-22 23:53:55,7
Intel,k1s9sl2,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,AMD,2023-09-22 23:33:53,6
Intel,k1s6x1t,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,AMD,2023-09-22 23:13:07,5
Intel,k1s97ev,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",AMD,2023-09-22 23:29:38,1
Intel,k1s30i9,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,AMD,2023-09-22 22:45:15,-8
Intel,k1s8eym,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,AMD,2023-09-22 23:23:56,-1
Intel,k1wd23h,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",AMD,2023-09-23 20:09:52,1
Intel,k1save0,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",AMD,2023-09-22 23:41:42,4
Intel,k1sama1,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,AMD,2023-09-22 23:39:52,-1
Intel,k1tastr,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",AMD,2023-09-23 04:28:29,20
Intel,k1tf175,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",AMD,2023-09-23 05:12:08,10
Intel,k1v57gq,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,AMD,2023-09-23 15:33:56,4
Intel,k1uehqr,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,AMD,2023-09-23 12:14:02,3
Intel,k1uvdfc,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,AMD,2023-09-23 14:28:22,3
Intel,k1uvhzl,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",AMD,2023-09-23 14:29:15,2
Intel,k1vxcz9,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",AMD,2023-09-23 18:31:17,2
Intel,k1u94ng,moving data between the two is the issue,AMD,2023-09-23 11:20:15,11
Intel,k1ud2dh,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:00:31,9
Intel,k1tzket,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",AMD,2023-09-23 09:22:31,2
Intel,k1tcnbk,Game physics doesn't seem to be a focus anymore though,AMD,2023-09-23 04:47:04,16
Intel,k1vx974,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,AMD,2023-09-23 18:30:38,1
Intel,k1y74dq,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,AMD,2023-09-24 04:08:48,0
Intel,k1zf2ze,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",AMD,2023-09-24 12:27:53,0
Intel,k1u0kty,"Yes, but not after Nvidia bought Ageia.",AMD,2023-09-23 09:35:56,4
Intel,k1y5xsz,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",AMD,2023-09-24 03:57:42,1
Intel,k1usgsa,I like how you said: static image  because in motion upscaling is crappier,AMD,2023-09-23 14:07:47,-1
Intel,k1t97dw,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",AMD,2023-09-23 04:13:02,-3
Intel,k1t7en2,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",AMD,2023-09-23 03:55:55,4
Intel,k1t22gi,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",AMD,2023-09-23 03:08:23,1
Intel,k1u3zfu,"It *is* way slower, you're just compensating it by reducing render resolution a ton",AMD,2023-09-23 10:19:33,-9
Intel,k1ubnzq,If it was bloodborne i am guilty of that myself,AMD,2023-09-23 11:46:49,6
Intel,k1uerlc,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:16:37,5
Intel,k1y7wcn,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,AMD,2023-09-24 04:16:20,1
Intel,k1tarlv,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",AMD,2023-09-23 04:28:09,5
Intel,k1tfawo,Rasterisation is a “hack” too,AMD,2023-09-23 05:15:07,37
Intel,k1sxetf,If it works it works....computer graphics has always been about approximation,AMD,2023-09-23 02:30:22,34
Intel,k1t8lwj,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",AMD,2023-09-23 04:07:17,21
Intel,k1tqzxo,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",AMD,2023-09-23 07:31:22,4
Intel,k1vlzcm,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",AMD,2023-09-23 17:20:00,0
Intel,k1s7up5,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,AMD,2023-09-22 23:19:50,-11
Intel,k1sony7,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",AMD,2023-09-23 01:22:48,-6
Intel,k1sot43,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,AMD,2023-09-23 01:23:54,-4
Intel,k1s8jx2,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,AMD,2023-09-22 23:24:55,4
Intel,k1ubk56,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",AMD,2023-09-23 11:45:47,2
Intel,k1snovf,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",AMD,2023-09-23 01:15:25,3
Intel,k1sd914,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,AMD,2023-09-22 23:58:55,8
Intel,k1ubcdm,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",AMD,2023-09-23 11:43:38,-2
Intel,k1tno35,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",AMD,2023-09-23 06:50:58,10
Intel,k1ucpbc,Get that fps with a 5120*1440 240Hz monitor,AMD,2023-09-23 11:56:59,1
Intel,k1u928y,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",AMD,2023-09-23 11:19:32,-3
Intel,k1vo11q,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",AMD,2023-09-23 17:32:45,1
Intel,k1s3igv,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,AMD,2023-09-22 22:48:46,5
Intel,k1ugb6v,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",AMD,2023-09-23 12:30:45,1
Intel,k1wecws,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",AMD,2023-09-23 20:18:14,1
Intel,k1sbp25,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,AMD,2023-09-22 23:47:34,-6
Intel,k1sj47l,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",AMD,2023-09-23 00:41:39,-7
Intel,k1tbdg8,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,AMD,2023-09-23 04:34:12,8
Intel,k1tgkh1,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",AMD,2023-09-23 05:29:06,3
Intel,k1ueofm,8600gt in SLI man you are Savage!🔥,AMD,2023-09-23 12:15:48,2
Intel,k1v8q15,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",AMD,2023-09-23 15:56:19,4
Intel,k20zqc9,there there’s no way there will ever be another 8800 GT. you got so much for your money.,AMD,2023-09-24 18:32:27,3
Intel,k1v3f8y,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",AMD,2023-09-23 15:22:27,2
Intel,k1vqgag,Seemed like a perfect use case for the sli bridge they got rid of.,AMD,2023-09-23 17:48:01,3
Intel,k1y719d,Why would it need to send the data to the other card? They both feed into the same game.,AMD,2023-09-24 04:07:58,2
Intel,k1uqeyb,Big up the Vega gang,AMD,2023-09-23 13:52:34,3
Intel,k1tgpce,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",AMD,2023-09-23 05:30:35,40
Intel,k1u8rwh,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",AMD,2023-09-23 11:16:24,9
Intel,k1tffr8,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,AMD,2023-09-23 05:16:37,17
Intel,k1toby8,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,AMD,2023-09-23 06:59:04,3
Intel,k1w7uac,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",AMD,2023-09-23 19:37:13,1
Intel,k1thrmi,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",AMD,2023-09-23 05:42:24,3
Intel,k1t66vz,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",AMD,2023-09-23 03:44:48,3
Intel,k1ueiu5,We are guilty of the exact same sin.,AMD,2023-09-23 12:14:20,2
Intel,k1tzi21,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-09-23 09:21:41,0
Intel,k1u5aaz,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",AMD,2023-09-23 10:35:43,2
Intel,k1u0whe,would you care to explain ? Kinda interested to here this,AMD,2023-09-23 09:40:08,1
Intel,k1scoi2,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,AMD,2023-09-22 23:54:43,6
Intel,k1tcqbd,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",AMD,2023-09-23 04:47:55,6
Intel,k256x6l,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,AMD,2023-09-25 14:57:47,2
Intel,k1s8ne8,Turn on path tracing. Embrace the PowerPoint.,AMD,2023-09-22 23:25:36,3
Intel,k1ssjdy,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,AMD,2023-09-23 01:52:37,10
Intel,k1vpcr3,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",AMD,2023-09-23 17:41:03,0
Intel,k1s8f3n,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",AMD,2023-09-22 23:23:57,-4
Intel,k1wloml,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",AMD,2023-09-23 21:03:39,1
Intel,k1tpscr,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",AMD,2023-09-23 07:16:33,6
Intel,k1sen3q,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",AMD,2023-09-23 00:09:03,6
Intel,k1sdc8m,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",AMD,2023-09-22 23:59:34,0
Intel,k1tcqfl,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",AMD,2023-09-23 04:47:57,2
Intel,k1ttjgv,Shame you don't. They did it for a reason.,AMD,2023-09-23 08:03:50,2
Intel,k1v42qp,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,AMD,2023-09-23 15:26:42,3
Intel,k1wmal2,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",AMD,2023-09-23 21:07:26,2
Intel,k1w2bdg,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,AMD,2023-09-23 19:02:32,4
Intel,k1wmeoj,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",AMD,2023-09-23 21:08:09,0
Intel,k1zjknf,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",AMD,2023-09-24 13:05:06,0
Intel,k1ue2iz,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,AMD,2023-09-23 12:10:02,3
Intel,k1tkit9,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",AMD,2023-09-23 06:13:48,4
Intel,k1t75sa,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",AMD,2023-09-23 03:53:39,4
Intel,k1v732r,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",AMD,2023-09-23 15:45:58,6
Intel,k1ub5y8,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,AMD,2023-09-23 11:41:49,12
Intel,k1t4gtg,Or a bag of fake tricks.,AMD,2023-09-23 03:29:16,12
Intel,k1u716i,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",AMD,2023-09-23 10:56:45,0
Intel,k1tnk4a,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",AMD,2023-09-23 06:49:39,0
Intel,k1szwwn,Have you tried Metro Exodus Enhanced?,AMD,2023-09-23 02:50:34,5
Intel,k1ttno1,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,AMD,2023-09-23 08:05:18,3
Intel,k1wlywq,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",AMD,2023-09-23 21:05:25,1
Intel,k1u602a,What is this? A reasonable comment in this dumpster fire of a sub?,AMD,2023-09-23 10:44:29,3
Intel,k1sg0o3,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,AMD,2023-09-23 00:19:01,2
Intel,k1tgd2j,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",AMD,2023-09-23 05:26:46,2
Intel,k1v2w6l,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",AMD,2023-09-23 15:18:58,3
Intel,k1wpc9k,Except it was just a rebadged 8800GTX/Ultra,AMD,2023-09-23 21:27:07,2
Intel,k1wso05,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",AMD,2023-09-23 21:49:06,1
Intel,k1tlekn,Pixel art go brr,AMD,2023-09-23 06:23:58,2
Intel,k1uefox,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,AMD,2023-09-23 12:13:29,3
Intel,k1tws4k,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",AMD,2023-09-23 08:46:03,0
Intel,k1shejh,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",AMD,2023-09-23 00:29:11,4
Intel,k1tykau,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",AMD,2023-09-23 09:09:21,2
Intel,jws0ze9,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",AMD,2023-08-18 21:33:20,70
Intel,jwrrxaq,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),AMD,2023-08-18 20:34:56,54
Intel,jwtujwf,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,AMD,2023-08-19 06:46:11,10
Intel,jws0ogv,I wonder if this will get added to Mangohud and Gamescope.,AMD,2023-08-18 21:31:17,10
Intel,jws656f,This is quality. Great work.,AMD,2023-08-18 22:08:17,6
Intel,jwtl7yn,Doesn’t capframeX uses presentmon as its monitoring tool?,AMD,2023-08-19 04:58:08,3
Intel,jwse5d4,I can finally see if it really is the ENB taking down my Skyrim gamesaves,AMD,2023-08-18 23:05:28,3
Intel,jwt3rjk,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",AMD,2023-08-19 02:18:52,9
Intel,jwv7vh1,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),AMD,2023-08-19 15:06:20,2
Intel,jwx068e,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",AMD,2023-08-19 21:46:34,2
Intel,jx6nzqo,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,AMD,2023-08-21 21:00:38,2
Intel,jwt26ko,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",AMD,2023-08-19 02:06:15,8
Intel,jwvd88w,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,AMD,2023-08-19 15:41:34,2
Intel,jwsaw5e,Thanks Intel! I will try this out at least since I hate MSI afterburner.,AMD,2023-08-18 22:42:00,3
Intel,jwss1oh,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,AMD,2023-08-19 00:48:31,3
Intel,jwsaaac,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",AMD,2023-08-18 22:37:37,-2
Intel,jwteu8t,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,AMD,2023-08-19 03:54:41,0
Intel,jwscc3e,And AmD gives far more than Nvidia.,AMD,2023-08-18 22:52:16,-10
Intel,jwwf6wi,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,AMD,2023-08-19 19:37:09,1
Intel,k25hh4a,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,AMD,2023-09-25 16:00:53,1
Intel,jwsaffm,People have reported that cpu usage in Radeon software is not very accurate.,AMD,2023-08-18 22:38:38,30
Intel,jwsejpm,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,AMD,2023-08-18 23:08:23,6
Intel,jwsu2it,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",AMD,2023-08-19 01:03:33,3
Intel,jwubtsm,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",AMD,2023-08-19 10:34:05,2
Intel,jws5mkd,Just use afterburner as OSD.,AMD,2023-08-18 22:04:42,2
Intel,jwtvx7w,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,AMD,2023-08-19 07:03:38,1
Intel,k3rohbn,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",AMD,2023-10-06 20:51:17,1
Intel,jwrzqfw,You beat me to it :),AMD,2023-08-18 21:25:03,2
Intel,jx08btc,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",AMD,2023-08-20 15:17:16,9
Intel,jwu03xe,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,AMD,2023-08-19 07:58:51,6
Intel,jx8l1n5,Technically it should be possible to add in MSI afterburner because it's open source,AMD,2023-08-22 06:01:19,1
Intel,jwtnlcl,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",AMD,2023-08-19 05:23:42,1
Intel,jwyec42,It was a pet project of one of the Intel engineers.   6/10 is not bad!,AMD,2023-08-20 03:57:36,5
Intel,jx6uru1,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,AMD,2023-08-21 21:43:19,1
Intel,jwtopeu,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,AMD,2023-08-19 05:36:25,6
Intel,jwus2bx,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",AMD,2023-08-19 13:12:58,2
Intel,jwwr3eh,I hate afterburner and RTSS. This is way better,AMD,2023-08-19 20:49:19,1
Intel,jwst65i,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",AMD,2023-08-19 00:56:51,8
Intel,jwtohd3,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",AMD,2023-08-19 05:33:51,-10
Intel,jwt37au,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",AMD,2023-08-19 02:14:23,1
Intel,jwsd0ai,"Well, everyone uses RTSS anyway and it gives you basically everything.",AMD,2023-08-18 22:57:07,0
Intel,jx7hc39,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,AMD,2023-08-22 00:20:25,1
Intel,jwscka1,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",AMD,2023-08-18 22:53:52,8
Intel,jwxcu6w,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,AMD,2023-08-19 23:14:33,1
Intel,jwsf4i1,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",AMD,2023-08-18 23:12:39,2
Intel,jwsnfu5,Afterburner fucks with my settings in adrenaline,AMD,2023-08-19 00:13:59,5
Intel,jwu3toe,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",AMD,2023-08-19 08:48:58,5
Intel,jwv9n3n,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",AMD,2023-08-19 15:18:05,3
Intel,jxi33zm,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,AMD,2023-08-24 02:22:09,1
Intel,jwsus0t,I get downvoted for asking a valid question?,AMD,2023-08-19 01:08:51,2
Intel,jwtt73f,Thank you for continuing to contribute Nothing to this conversation.,AMD,2023-08-19 06:29:18,9
Intel,jwslkce,Clearly not more than this beta of presentmon,AMD,2023-08-18 23:59:56,1
Intel,jwsd4sf,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,AMD,2023-08-18 22:58:01,-2
Intel,jwsfkyz,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",AMD,2023-08-18 23:16:00,9
Intel,jwxd4ky,Where are you seeing this?,AMD,2023-08-19 23:16:32,1
Intel,jwsfr15,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",AMD,2023-08-18 23:17:13,1
Intel,jwsno08,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",AMD,2023-08-19 00:15:40,14
Intel,jwuero7,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,AMD,2023-08-19 11:08:44,4
Intel,jwu8j4q,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,AMD,2023-08-19 09:51:11,3
Intel,jwv8abu,Great news to gamers though,AMD,2023-08-19 15:09:05,2
Intel,jwvbwid,ah sorry I meant NVK,AMD,2023-08-19 15:32:34,1
Intel,jwsuuv1,"I don't know, I didn't downvote you.",AMD,2023-08-19 01:09:27,6
Intel,jwsec7c,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,AMD,2023-08-18 23:06:52,4
Intel,jwt2g8b,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",AMD,2023-08-19 02:08:23,3
Intel,jwxvkn7,It’s where you oc in adrenaline,AMD,2023-08-20 01:27:06,1
Intel,jwuly2q,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",AMD,2023-08-19 12:21:17,5
Intel,jwv200x,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",AMD,2023-08-19 14:26:34,3
Intel,jwtzdkr,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",AMD,2023-08-19 07:49:04,5
Intel,jwuvpd6,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",AMD,2023-08-19 13:41:23,2
Intel,jwv1po6,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",AMD,2023-08-19 14:24:33,6
Intel,jwuapdo,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",AMD,2023-08-19 10:19:39,2
Intel,jx3nm3i,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",AMD,2023-08-21 06:56:43,1
Intel,jwv7qel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",AMD,2023-08-19 15:05:22,2
Intel,jwucdfz,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",AMD,2023-08-19 10:40:52,4
Intel,jwuwjmm,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",AMD,2023-08-19 13:47:45,1
Intel,jx4di9f,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",AMD,2023-08-21 12:07:55,1
Intel,jwuithf,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",AMD,2023-08-19 11:51:44,4
Intel,jwuq603,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",AMD,2023-08-19 12:57:40,3
Intel,jwuozk8,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",AMD,2023-08-19 12:47:47,2
Intel,nnbivzp,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Intel,2025-11-05 22:28:54,34
Intel,nnbkl4o,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Intel,2025-11-05 22:37:53,15
Intel,nnbr1vs,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Intel,2025-11-05 23:13:11,6
Intel,nncz7bz,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Intel,2025-11-06 03:36:19,4
Intel,nnftti4,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Intel,2025-11-06 16:14:24,2
Intel,nndm2ga,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Intel,2025-11-06 06:31:30,2
Intel,nnc0ph2,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Intel,2025-11-06 00:08:27,-7
Intel,nnjsaw2,Will the 10 core Xe be better than radeon 890m or worse?,Intel,2025-11-07 05:14:22,0
Intel,nnbhb9s,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Intel,2025-11-05 22:20:34,-6
Intel,nnbj579,Still weaker than x3d,Intel,2025-11-05 22:30:15,-15
Intel,nnm29u0,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Intel,2025-11-07 15:46:06,3
Intel,nnbmjlf,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Intel,2025-11-05 22:48:16,13
Intel,nnd5hpv,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Intel,2025-11-06 04:19:26,6
Intel,nnclctl,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Intel,2025-11-06 02:10:09,4
Intel,nnde8rz,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Intel,2025-11-06 05:25:14,8
Intel,nngpesf,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Intel,2025-11-06 18:45:02,0
Intel,nnklixi,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Intel,2025-11-07 09:57:31,1
Intel,nnlg7cm,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Intel,2025-11-07 13:50:18,1
Intel,nnbigvq,What're you doing on a laptop?,Intel,2025-11-05 22:26:37,19
Intel,nnbiwg3,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Intel,2025-11-05 22:28:59,8
Intel,nnbowvu,It's for handhelds and office laptops not hyper enthusiast shit.,Intel,2025-11-05 23:01:09,2
Intel,nncdk44,Nobody buys AMD laptops,Intel,2025-11-06 01:23:02,10
Intel,nnbk5wm,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Intel,2025-11-05 22:35:40,7
Intel,nnco8fw,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Intel,2025-11-06 02:27:36,1
Intel,nnz3axo,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Intel,2025-11-09 18:46:41,1
Intel,nnbn2as,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Intel,2025-11-05 22:51:02,9
Intel,nndnorc,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Intel,2025-11-06 06:46:30,6
Intel,nndrn8v,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Intel,2025-11-06 07:23:49,2
Intel,nnbivk1,Highly immersive porn on the go,Intel,2025-11-05 22:28:50,13
Intel,nnblwqh,"Idk, FEM sim of a pressure boiler?",Intel,2025-11-05 22:44:56,0
Intel,nnc0bgd,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Intel,2025-11-06 00:06:14,5
Intel,nnbkp8h,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Intel,2025-11-05 22:38:29,6
Intel,nnbkxvp,PTL extends up to the -H series too.,Intel,2025-11-05 22:39:46,2
Intel,nnbkts2,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Intel,2025-11-05 22:39:10,4
Intel,nnbl91n,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Intel,2025-11-05 22:41:25,3
Intel,nnc0woc,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Intel,2025-11-06 00:09:35,3
Intel,nnc2335,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Intel,2025-11-06 00:16:15,9
Intel,nnblgop,Just get a Vision Pro?,Intel,2025-11-05 22:42:32,5
Intel,nnbyre8,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Intel,2025-11-05 23:57:18,1
Intel,nnbmp20,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Intel,2025-11-05 22:49:05,3
Intel,nnbrc9f,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Intel,2025-11-05 23:14:48,0
Intel,nnc5j69,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Intel,2025-11-06 00:35:47,4
Intel,nnd2wbt,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Intel,2025-11-06 04:01:09,3
Intel,nnc1auf,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Intel,2025-11-06 00:11:48,1
Intel,nncm4mg,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Intel,2025-11-06 02:14:47,8
Intel,nnj3sul,"Two options seems right, either you care about it or you don't.",Intel,2025-11-07 02:28:49,1
Intel,nnh5cgt,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Intel,2025-11-06 20:02:16,1
Intel,nncnlt8,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Intel,2025-11-06 02:23:44,2
Intel,nncrc61,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Intel,2025-11-06 02:46:29,0
Intel,nnd4ak4,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Intel,2025-11-06 04:10:51,2
Intel,nndr8zp,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Intel,2025-11-06 07:19:59,3
Intel,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,12
Intel,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,5
Intel,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
Intel,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
Intel,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
Intel,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,1
Intel,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
Intel,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
Intel,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
Intel,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
Intel,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
Intel,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,1
Intel,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
Intel,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
Intel,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
Intel,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
Intel,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
Intel,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
Intel,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
Intel,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
Intel,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
Intel,nmoztyk,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Intel,2025-11-02 12:36:20,6
Intel,nmzufnv,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Intel,2025-11-04 02:52:11,2
Intel,nmquye0,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Intel,2025-11-02 18:34:43,1
Intel,noa52e1,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Intel,2025-11-11 13:57:33,1
Intel,nlbgoss,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Intel,2025-10-25 14:51:22,11
Intel,nlawueh,That naming scheme really is complete and utter dogshit,Intel,2025-10-25 12:59:00,7
Intel,nlp3wrh,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Intel,2025-10-27 19:03:55,1
Intel,nlrilyu,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Intel,2025-10-28 02:55:34,1
Intel,nlz6ywu,I'm looking forward to check how those series will perform!!,Intel,2025-10-29 09:04:56,1
Intel,nlbj26i,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Intel,2025-10-25 15:03:33,18
Intel,nlbihha,look forward to new APUs,Intel,2025-10-25 15:00:35,2
Intel,nlhde6w,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Intel,2025-10-26 14:36:10,2
Intel,nlpqfuu,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Intel,2025-10-27 20:58:10,1
Intel,nmbs4xl,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Intel,2025-10-31 06:11:07,1
Intel,nldt8zb,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Intel,2025-10-25 22:15:21,0
Intel,nlclyxd,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Intel,2025-10-25 18:23:40,-9
Intel,nlguu28,You’ll be waiting till 2027 on the amd side.,Intel,2025-10-26 12:44:06,3
Intel,nmbs0u8,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Intel,2025-10-31 06:09:58,1
Intel,nlq5vco,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Intel,2025-10-27 22:20:54,1
Intel,nlg1l4u,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Intel,2025-10-26 08:16:14,8
Intel,nlp4s9i,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Intel,2025-10-27 19:08:23,2
Intel,nlpqrfz,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Intel,2025-10-27 20:59:49,1
Intel,nlcna8j,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Intel,2025-10-25 18:30:27,9
Intel,nlp4f4m,Really? This should be good for Intel in 2026.,Intel,2025-10-27 19:06:31,1
Intel,nmbt7jh,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Intel,2025-10-31 06:22:00,1
Intel,nlqexwa,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Intel,2025-10-27 23:11:50,1
Intel,nmbsirj,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Intel,2025-10-31 06:15:03,1
Intel,nld2j3h,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Intel,2025-10-25 19:51:15,-4
Intel,nlqjpxg,👍,Intel,2025-10-27 23:38:19,1
Intel,nld2x4w,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Intel,2025-10-25 19:53:17,8
Intel,nld43st,You are right. I meant 'Gorgon Point'.,Intel,2025-10-25 19:59:34,1
Intel,nln841i,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Intel,2025-10-27 13:21:07,1
Intel,nlp48zs,My brain hurts and I’m still confused,Intel,2025-10-27 19:05:38,1
Intel,nmbrwqs,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Intel,2025-10-31 06:08:49,1
Intel,nkxsilf,"I think him saying ""unreleased products"" could mean it's still coming.",Intel,2025-10-23 11:21:44,25
Intel,nkz30mp,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Intel,2025-10-23 15:39:54,9
Intel,nkzp226,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Intel,2025-10-23 17:25:52,3
Intel,nl8a9mb,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Intel,2025-10-25 00:22:50,1
Intel,nkxmfto,Intel is not serious with dGPUs,Intel,2025-10-23 10:33:35,-6
Intel,nl0j2ic,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Intel,2025-10-23 19:51:32,8
Intel,nlec4sq,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Intel,2025-10-26 00:09:57,2
Intel,nm6dcp4,B50 is interesting once the software is there (planned Q4).,Intel,2025-10-30 11:58:35,1
Intel,nkxz8vy,"Yes, they are literally just playing. It's all a game lol",Intel,2025-10-23 12:08:42,11
Intel,nkz3604,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Intel,2025-10-23 15:40:37,2
Intel,nlbmybg,They are as serious as AMD.,Intel,2025-10-25 15:23:34,1
Intel,nl1d1w8,This interview happened during the quiet period so I don't think he could talk about the future,Intel,2025-10-23 22:26:07,7
Intel,nlbmm1g,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Intel,2025-10-25 15:21:47,2
Intel,nlc5u9q,Where did you hear that it was cancelled?,Intel,2025-10-25 17:01:38,1
Intel,nkzrbdu,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Intel,2025-10-23 17:36:29,3
Intel,nkzb6g5,There has been no update regarding celestial dGPUs internally.,Intel,2025-10-23 16:19:27,5
Intel,nl9n96c,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Intel,2025-10-25 06:15:43,1
Intel,nlc8fdp,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Intel,2025-10-25 17:14:34,1
Intel,nlcandy,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Intel,2025-10-25 17:25:50,1
Intel,nlbpug0,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Intel,2025-10-25 15:38:24,1
Intel,nlc7sgw,Ex-Intel coworkers/acquaintances.,Intel,2025-10-25 17:11:23,2
Intel,nl34or5,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Intel,2025-10-24 05:10:43,2
Intel,nl3q53x,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Intel,2025-10-24 08:35:10,2
Intel,nlafjvl,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Intel,2025-10-25 10:48:26,1
Intel,nlcioic,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Intel,2025-10-25 18:06:26,1
Intel,nlbtqtn,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Intel,2025-10-25 15:58:32,3
Intel,nlcbiv4,"So, no news story has come out stating that?",Intel,2025-10-25 17:30:18,3
Intel,noidhfz,Xe3P-HPM suggests otherwise,Intel,2025-11-12 19:47:04,1
Intel,nlc8k5a,"Yes, through my ex colleagues",Intel,2025-10-25 17:15:14,2
Intel,nleb7hi,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Intel,2025-10-26 00:04:10,1
Intel,nlbuyry,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Intel,2025-10-25 16:04:56,2
Intel,nmbst69,Why would Intel tell you? It would just stall selling all current ARC cards.,Intel,2025-10-31 06:17:59,3
Intel,nlchyo2,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Intel,2025-10-25 18:02:48,2
Intel,noifll4,What about it? That some reference exists in drivers?,Intel,2025-11-12 19:57:24,1
Intel,nlc2d7k,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Intel,2025-10-25 16:44:12,3
Intel,nlc7zs4,BMG was 0 margin product,Intel,2025-10-25 17:12:23,3
Intel,noiihla,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Intel,2025-11-12 20:11:56,1
Intel,nlc3nqx,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Intel,2025-10-25 16:50:50,2
Intel,noijing,They're also using Xe3p for NVL-P and that Island AI product.,Intel,2025-11-12 20:17:09,3
Intel,nlci8uq,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Intel,2025-10-25 18:04:14,3
Intel,noimd5x,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Intel,2025-11-12 20:31:37,0
Intel,nlck9s5,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Intel,2025-10-25 18:14:45,1
Intel,nled55i,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Intel,2025-10-26 00:16:21,1
Intel,nmbspxb,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Intel,2025-10-31 06:17:05,1
Intel,noinhcz,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Intel,2025-11-12 20:37:24,2
Intel,nkl7ghk,Think pretty easy naming. Any X infront just automatically means better iGPU,Intel,2025-10-21 12:17:01,38
Intel,nklikpx,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Intel,2025-10-21 13:28:12,19
Intel,nkkzyp1,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Intel,2025-10-21 11:20:48,14
Intel,nkodq7t,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Intel,2025-10-21 21:59:26,7
Intel,nkkoc9f,Naming for these chips are terrible,Intel,2025-10-21 09:31:57,11
Intel,nkmeuem,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Intel,2025-10-21 16:22:11,2
Intel,nkzbmky,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Intel,2025-10-23 16:21:35,1
Intel,nkld363,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Intel,2025-10-21 12:54:21,15
Intel,nkuhiob,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Intel,2025-10-22 20:56:33,2
Intel,nl9xb0y,"If the game could be 50–60% stronger, that would be That would be a killer",Intel,2025-10-25 07:53:56,1
Intel,nkljsi3,"That’s been true for the past generations, but it looks like it will change this generation",Intel,2025-10-21 13:35:30,6
Intel,nl9xpor,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Intel,2025-10-25 07:57:55,1
Intel,nkkrh6o,Still better than Ryzen 365 AI pro MAX+,Intel,2025-10-21 10:04:31,54
Intel,nkm3cl3,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Intel,2025-10-21 15:22:49,3
Intel,nkv8b22,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Intel,2025-10-22 23:24:43,1
Intel,nkm4j1i,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Intel,2025-10-21 15:28:57,3
Intel,nkksh88,Yea putting ai the model name is disgusting 😂,Intel,2025-10-21 10:14:17,23
Intel,nkl1j6x,I can't wait for the Ryzen 688S AI Pro MAX+++,Intel,2025-10-21 11:33:19,13
Intel,nkmi2u0,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Intel,2025-10-21 16:38:27,3
Intel,nkmgpw7,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Intel,2025-10-21 16:31:44,3
Intel,nkpnp7f,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Intel,2025-10-22 02:32:12,3
Intel,nklj3is,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Intel,2025-10-21 13:31:20,-3
Intel,nknc85p,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Intel,2025-10-21 18:59:33,2
Intel,nklps98,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Intel,2025-10-21 14:09:54,0
Intel,nko4b5d,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Intel,2025-10-21 21:10:14,4
Intel,nklngwf,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Intel,2025-10-21 13:56:45,13
Intel,nkns6e4,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Intel,2025-10-21 20:13:23,3
Intel,nkoestf,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Intel,2025-10-21 22:05:28,4
Intel,nkmk3eq,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Intel,2025-10-21 16:48:08,2
Intel,nkpsi6y,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Intel,2025-10-22 03:02:49,3
Intel,nkof5d7,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Intel,2025-10-21 22:07:26,1
Intel,nkq1gt0,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Intel,2025-10-22 04:06:17,2
Intel,nm4gnwa,Same core counts too,Intel,2025-10-30 02:20:15,1
Intel,njdkfg9,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-10-14 01:35:52,1
Intel,njo3me8,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Intel,2025-10-15 18:57:43,51
Intel,njp2gv2,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Intel,2025-10-15 21:57:08,22
Intel,njq65e2,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Intel,2025-10-16 01:51:35,8
Intel,njoj14y,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Intel,2025-10-15 20:14:46,7
Intel,njpl7pm,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Intel,2025-10-15 23:47:19,3
Intel,nkgif8b,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Intel,2025-10-20 11:51:27,1
Intel,nkzvqp0,"It's going to cost like $10,000, right?",Intel,2025-10-23 17:57:21,1
Intel,njo7pwi,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Intel,2025-10-15 19:18:08,25
Intel,njoyfid,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Intel,2025-10-15 21:34:53,5
Intel,njpivm1,"It really is a mess, someone should've been fired long ago.",Intel,2025-10-15 23:34:18,7
Intel,nmbtn83,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Intel,2025-10-31 06:26:27,1
Intel,njrrggy,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Intel,2025-10-16 10:08:28,1
Intel,njolo2l,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Intel,2025-10-15 20:28:01,16
Intel,njo9lo1,because 50% more cores need 50% more power for 50% more performance.,Intel,2025-10-15 19:27:37,33
Intel,njopbmv,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Intel,2025-10-15 20:46:36,15
Intel,njpsirt,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Intel,2025-10-16 00:28:36,0
Intel,njrpj24,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Intel,2025-10-16 09:49:54,-3
Intel,njsegc9,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Intel,2025-10-16 12:58:39,1
Intel,njopfv9,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Intel,2025-10-15 20:47:12,-4
Intel,njoa1f3,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Intel,2025-10-15 19:29:49,12
Intel,njqexqp,But LNL's TDP is too low compared to H45 cpu,Intel,2025-10-16 02:47:44,4
Intel,njrcz7w,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Intel,2025-10-16 07:38:36,2
Intel,nmbtgoa,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Intel,2025-10-31 06:24:35,1
Intel,njqijcd,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Intel,2025-10-16 03:12:16,1
Intel,njrgeuz,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Intel,2025-10-16 08:14:34,1
Intel,njote4v,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Intel,2025-10-15 21:07:31,20
Intel,njow153,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Intel,2025-10-15 21:21:46,17
Intel,njoudob,Intel is gonna have better integrated graphics than AMD,Intel,2025-10-15 21:12:50,6
Intel,njoar3w,I have no idea.,Intel,2025-10-15 19:33:29,11
Intel,njqtu4m,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Intel,2025-10-16 04:38:47,5
Intel,njqtb7t,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Intel,2025-10-16 04:34:25,4
Intel,njrhrow,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Intel,2025-10-16 08:28:47,2
Intel,njoxfxz,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Intel,2025-10-15 21:29:28,7
Intel,njp7qsx,Yeah people act like strix point is in that segment..... It's not.,Intel,2025-10-15 22:28:04,8
Intel,njp58ob,🫨,Intel,2025-10-15 22:13:13,3
Intel,njs0rkq,Wouldn't be the first time.,Intel,2025-10-16 11:27:26,1
Intel,njqutc3,"Oh, I guess it be like that.",Intel,2025-10-16 04:46:59,0
Intel,nis4cle,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Intel,2025-10-10 14:44:00,16
Intel,nirfu8b,"""we are tending to prefer e cores now when gaming""   That's very surprising",Intel,2025-10-10 12:29:22,23
Intel,nisac9a,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Intel,2025-10-10 15:13:09,5
Intel,niv55kq,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Intel,2025-10-11 00:24:27,5
Intel,niryzd1,I’m guessing Xe3P will be on Intel 3-PT.,Intel,2025-10-10 14:17:01,4
Intel,nitrjvx,"Tom is a funny guy, love it when he gets camera time",Intel,2025-10-10 19:37:13,4
Intel,nisfksg,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Intel,2025-10-10 15:38:45,2
Intel,niwlua9,"When is the panther lake reveal going to be, CES?",Intel,2025-10-11 07:13:32,1
Intel,nirs4bu,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Intel,2025-10-10 13:41:08,33
Intel,nirh07n,Wonder if there's some energy star requirements.,Intel,2025-10-10 12:36:46,4
Intel,nirq8f2,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Intel,2025-10-10 13:30:47,2
Intel,nitssdt,It's either N3P or 18AP.,Intel,2025-10-10 19:43:43,4
Intel,nirzgoz,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Intel,2025-10-10 14:19:27,19
Intel,niuj3v7,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Intel,2025-10-10 22:04:33,3
Intel,nirymua,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Intel,2025-10-10 14:15:14,3
Intel,nirya2r,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Intel,2025-10-10 14:13:27,2
Intel,nispl6v,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Intel,2025-10-10 16:27:41,7
Intel,niwprxq,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Intel,2025-10-11 07:54:44,3
Intel,niwoax2,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Intel,2025-10-11 07:39:15,1
Intel,nisife2,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Intel,2025-10-10 15:52:33,3
Intel,niwk1j6,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Intel,2025-10-11 06:55:10,3
Intel,nitfd2e,I'm tired of AMD slop consoles and handhelds,Intel,2025-10-10 18:34:17,-9
Intel,nixk3v6,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Intel,2025-10-11 12:43:47,2
Intel,nj1kiyw,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Intel,2025-10-12 02:44:36,1
Intel,niy0x5h,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Intel,2025-10-11 14:29:59,2
Intel,nix9wlx,Huh????,Intel,2025-10-11 11:23:59,2
Intel,nixt1zv,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Intel,2025-10-11 13:42:49,2
Intel,nj4298q,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Intel,2025-10-12 15:05:26,3
Intel,niy0ck1,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Intel,2025-10-11 14:26:44,1
Intel,niylmew,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Intel,2025-10-11 16:21:28,1
Intel,nizip6t,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Intel,2025-10-11 19:15:30,1
Intel,nj1tulq,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Intel,2025-10-12 03:51:16,2
Intel,nizcl4d,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Intel,2025-10-11 18:42:08,4
Intel,nj2hurj,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Intel,2025-10-12 07:32:20,2
Intel,nih90cf,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Intel,2025-10-08 19:56:24,19
Intel,nihman0,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Intel,2025-10-08 21:00:02,7
Intel,nigsicb,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Intel,2025-10-08 18:33:15,16
Intel,nij93k3,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Intel,2025-10-09 02:46:46,5
Intel,nii800b,Says nothing about discrete graphics. Think that's clearly dead at this point.,Intel,2025-10-08 23:00:42,-8
Intel,nihwdsn,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Intel,2025-10-08 21:53:14,2
Intel,niq882u,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Intel,2025-10-10 05:55:28,1
Intel,niljjpz,Sounds like you're clearly wrong.,Intel,2025-10-09 14:02:51,2
Intel,niicq4o,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Intel,2025-10-08 23:28:43,6
Intel,niihlwk,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Intel,2025-10-08 23:58:03,6
Intel,nilq0j4,"Lmao, sure. Any day now...",Intel,2025-10-09 14:36:08,0
Intel,nij8lxc,I don't think so considering how expensive LNL was,Intel,2025-10-09 02:43:39,-1
Intel,niv5lac,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Intel,2025-10-11 00:27:22,6
Intel,niqq5bp,What should be the desktop gpu equivalent?,Intel,2025-10-10 08:54:26,1
Intel,nkwiahy,So around a 3050 laptop performance?,Intel,2025-10-23 04:15:22,1
Intel,nobxw7e,"Xe2 wasn't a benchmark btw. Arc 140T is hot, can""t manage benchmarks staing under 100w, just to pair with an 980M.",Intel,2025-11-11 19:25:24,1
Intel,nipaq6n,The problem is how much is the price?,Intel,2025-10-10 01:57:39,0
Intel,nirogih,they should really do better on GPU,Intel,2025-10-10 13:20:46,0
Intel,nip7gnd,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Intel,2025-10-10 01:39:13,-6
Intel,njovxfo,"Just because you have 50% more physical units, doesn't mean you'll automatically get 50% more performance. Remember when the Z1E came out and it was supposed to be 50-100% more power than the steam deck?",Intel,2025-10-15 21:21:11,1
Intel,nouqj4l,Because Xe3 is like Xe2+,Intel,2025-11-14 18:49:00,1
Intel,niqqyxy,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Intel,2025-10-10 09:02:58,5
Intel,nlbl2xp,Yes,Intel,2025-10-25 15:13:57,1
Intel,nis9kyv,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Intel,2025-10-10 15:09:34,10
Intel,niq7bnn,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Intel,2025-10-10 05:47:17,13
Intel,nje1ogx,Maybe I'm confused about what they're advertising but weren't they just going head to head with 4060s with the b580? Wouldn't 3050 range be a huge step backwards?,Intel,2025-10-14 03:21:13,1
Intel,niqr96c,"Damn, still behind my old 1060 pc",Intel,2025-10-10 09:05:57,-2
Intel,nitxpqo,And some of the tiles are internal not on TSMC.,Intel,2025-10-10 20:09:28,3
Intel,nix2sd3,"On-package memory doesn't raise the device price from and end user perspective. It's a margin challenge for Intel because OEMs want the margins from the memory, so Intel needs to pass it along at cost.   Technically, on-package memory can even be cheaper because it can let you simplify the rest of the PCB. Iso-speed, that is.",Intel,2025-10-11 10:15:42,2
Intel,nje21sq,Are you saying you expect people to not want discrete gpu's in the future?  What about the direction of gaming would make you think that?,Intel,2025-10-14 03:23:34,1
Intel,nix2vkc,"> Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.  The 4 Xe tile is on Intel 3, so it'll have a significant clock speed deficit vs the 12Xe tile.",Intel,2025-10-11 10:16:39,0
Intel,niq9fth,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Intel,2025-10-10 06:06:48,-3
Intel,njxpw7k,The b580 is a full power desktop card. The one they're advertising here is a tiny little integrated gpu on a laptop processor,Intel,2025-10-17 07:48:12,1
Intel,niqri91,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Intel,2025-10-10 09:08:33,3
Intel,niydl4w,"We already knew they cancelled desktop variants though, Panther lake was going to be around Arrow Lake laptop performance no matter what.",Intel,2025-10-11 15:38:29,-1
Intel,njoe8zc,"If people can get a XX60ti level iGPU in a laptop, which means you don't have to worry about MUX switches (hardware or software), possibly have a cheaper platform (if scaled) since you don't need a separate GPU board, have access to a ton of RAM (many mid-range GPUs are RAM limited), and due to reduced complexity and the ability to better dynamically manage power can get a more effective slim chassis with less throttling... then yeah, it'd be pretty good.  The next big jump is LPDDR6 which looks to potentially hit a 50% bandwidth increase and marginally lower latency.  Combine that with a wide bus and you're looking at enough to power a XX70 series mobile chip.  The framework is immature but it could be competitive in the future.  We'll see!",Intel,2025-10-15 19:51:03,1
Intel,niqbe7s,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Intel,2025-10-10 06:25:06,9
Intel,niqal4v,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Intel,2025-10-10 06:17:27,8
Intel,niqaj3v,Isn't that lunar lake before and after?,Intel,2025-10-10 06:16:56,5
Intel,nir70pe,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Intel,2025-10-10 11:28:51,2
Intel,nje1eeu,"Dota2 mentioned, I like Intel.",Intel,2025-10-14 03:19:26,1
Intel,nj0ndfp,"this has been leaked a lot, but not confirmed",Intel,2025-10-11 23:11:37,2
Intel,njoo0kf,"Oh laptop gaming sure, but that's pretty low level gaming. Don't most of the people at that level just put up with whatever laptop/prebuilt stuff is available anyway? I really don't think it's going to move the discrete market that much?",Intel,2025-10-15 20:39:56,1
Intel,niqhv6g,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Intel,2025-10-10 07:28:09,3
Intel,niqhrao,Oh really? Wow thanks for the heads up. I really missed the context.,Intel,2025-10-10 07:27:02,1
Intel,niqhxw8,Apparently so! I made a mistake.,Intel,2025-10-10 07:28:56,1
Intel,niv48z3,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Intel,2025-10-11 00:18:22,1
Intel,nh7ov8y,\>HUB  That's shovelware and I'm not clicking that shit.,Intel,2025-10-01 16:46:06,5
Intel,nh35vfm,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Intel,2025-09-30 22:21:40,10
Intel,nh4qct7,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Intel,2025-10-01 04:07:28,4
Intel,nh7wmoo,The problem is still here. 9060 xt with 5600 is faster than B580 with 9800x3d and B580 still loses 8% with 5600 instead of 9800x3d. Which means that cpu bottleneck starts much sooner for ARC gpu.,Intel,2025-10-01 17:23:05,1
Intel,nh1pojo,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Intel,2025-09-30 18:04:03,-1
Intel,nh5d94z,"the B580 is a modern gpu, it should be paired with a modern cpu, preferably an Intel one, going with old school Ryzen cpus is a bad path to take.",Intel,2025-10-01 07:33:46,-3
Intel,nh2ket3,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Intel,2025-09-30 20:32:44,27
Intel,nh2q72d,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Intel,2025-09-30 21:00:02,17
Intel,nh5salf,"The exact CPU model is hardly relevant. The point was to show CPUs of a certain power level. Not sure how this makes it less ""real life"" as if all three CPUs shown weren't extremely popular for their time. Also not sure what one would achieve when using Intel CPUs specifically.",Intel,2025-10-01 10:08:26,6
Intel,nha4yh9,"These guys absolutely *refuse* to test with an Intel CPU, it's very weird.",Intel,2025-10-02 00:11:44,4
Intel,nh4qb2n,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Intel,2025-10-01 04:07:07,2
Intel,nh5omi0,Exactly. Its nice that some games were fixed but between that and calling the issue (entirely) fixed there is a big gap.,Intel,2025-10-01 09:32:35,4
Intel,nh5zjzc,"Indeed and it is something very interesting, because the reason the driver overhead is an issue on 6 core cpu's is because modern games use all 6 cores, so if the driver has a big overhead the CPU has nu free cores available for the driver.  Especially Intel CPU's with more (e-)cores in their budget CPU's should probably suffer a lot less.   I would expect an i5-12400 to suffer the same as a Ryzen 5 5600,  But an i5-13400/14400 has an additional 4-e cores to deal with the overhead. The 13500/14600 even has 8 e cores.      I would very much like to see the comparison   i5-12400, i5-13400, i5-13500, Ryzen 5 5600(X)",Intel,2025-10-01 11:10:33,6
Intel,nh2x17x,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Intel,2025-09-30 21:34:07,9
Intel,nh2vrkw,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Intel,2025-09-30 21:27:35,3
Intel,nh293fj,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Intel,2025-09-30 19:38:19,10
Intel,nh2wf2h,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Intel,2025-09-30 21:30:55,10
Intel,nh63wlq,"The driver compiler is for GPU, not CPUs.",Intel,2025-10-01 11:42:38,2
Intel,nh63zie,9800X3D is the fastest CPU for gaming on the market. The usage of Intel or AMD CPUs doesn't inherently matter.,Intel,2025-10-01 11:43:11,3
Intel,nh2w6ck,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Intel,2025-09-30 21:29:41,14
Intel,nh2wkyc,Do you have peer-review data that disputes the data they presented?  Their video covers the scope of what got fixed.,Intel,2025-09-30 21:31:46,2
Intel,nhcifz7,Welcome to the age of influencers...,Intel,2025-10-02 11:35:19,1
Intel,nlycxb6,"Nvidia 5000 series has overheads issues even with 5800x3d... like 20% performance hit, you can fix it by capping fps or using Intel cpus. AMD CPUs always act weird when reaching 100% utilization.",Intel,2025-10-29 04:12:18,1
Intel,nh63ttf,That's not how CPUs work in games.,Intel,2025-10-01 11:42:06,3
Intel,nh2wpqg,Then they optimized drivers.,Intel,2025-09-30 21:32:27,-1
Intel,nha4uln,Do you have peer-review data that confirms it?,Intel,2025-10-02 00:11:05,0
Intel,nh5t225,The video shows the B580 not having driver overhead issues with the 5600X but we don't know if it did previously on anything other than Spider-Man. His tone suggests the 5600X performance used to be worse but he provides no data other than Spider-Man to suggest it was for other games.,Intel,2025-10-01 10:15:35,1
Intel,nh6os05,The fact that a switch from a 5600 to a 5700 with hardly a frequency gain eliminates the overhead issue suggest that is kind of  how games/drivers work.,Intel,2025-10-01 13:48:12,2
Intel,nh328pe,Which also fixed performance on 5600X,Intel,2025-09-30 22:01:23,8
Intel,nh8rcea,"But that's now how games utilize CPU's.  What you get from games is usually 1 main thread that takes up one big core, then many less intense threads, that may or *may not* utilize the other cores, and to varying degrees. AKA you can have mainthread use 100% of core 0, but cores 1,2,3,4,5 are all 50% used, leaving 50% for other tasks to run freely.",Intel,2025-10-01 19:50:32,2
Intel,nfolcd9,"Eventually you get the drivers where they need to be. People love to whine about Arc drivers but on the iGPU side people never complained. The community is never, ever happy with anything. Focusing their eggs on the Higher performance parts and then a slower LTS cadence keeping the old stuff alive is the right move.",Intel,2025-09-22 23:20:53,22
Intel,nfvr586,"They will update, just not day 1 for certain games. Who's playing with integrated graphics?",Intel,2025-09-24 02:32:50,3
Intel,nfw1d1a,"cant say i blame them for scaling down driver updates for pre-arc graphics hardware, ever since they announced intel was getting into gpus proper ive been wondering how long it would take before this happened.  lets be honest no one was needing monthly igpu updates for 2021 hardware that was so underpowered it couldnt even tie with a GT1030 ^(\[why does everything come back to 14nm\].)",Intel,2025-09-24 03:41:26,3
Intel,nge544o,Running an N97 Mini PC with integrated and noticed the recent driver branching. I'm still impressed that Star Trek Online is playable on it.,Intel,2025-09-26 23:18:44,1
Intel,nfxw4kc,Developers have been complaining about iGPU drivers for years. Matrix multiplications in OpenGL driver are still broken on a lot of integrated chips. It's driving me nuts every time I get a user report. 😭,Intel,2025-09-24 13:04:37,3
Intel,nlw94jc,"me man omg, im on intel iris xe and wanna try out battlefield 6 so bad. Arc Graphic users are having fun with day 1 driver updates.",Intel,2025-10-28 20:57:48,1
Intel,ng1xrn2,Get some ARL CPU’s which have matrix math in hardware.,Intel,2025-09-25 01:38:55,1
AMD,nnfo7h1,"Two ""leaked"" Strix Halo products, both with 8060S iGPU but 12C24T (392) or 8C16T (388).",hardware,2025-11-06 15:48:08,76
AMD,nng1lrn,"Finally, locking the best igpu to monster chips always felt like a waste, most who needs a 16 core cpu is going to want bigger than a 8060s anyway",hardware,2025-11-06 16:51:37,21
AMD,nnfyerl,Now put it in a actual laptop,hardware,2025-11-06 16:36:19,25
AMD,nnfsjte,"388 sounds really nice but kinda too late. Feels like Medusa Halo with RDNA5 will be the ""real"" one to go with.",hardware,2025-11-06 16:08:27,36
AMD,nngxb2l,"When AMD makes a next gen version of this with FSR4 or higher, then we'll have something special.",hardware,2025-11-06 19:22:59,10
AMD,nng2n1a,AMD Ryzen AI Max+ 391 and a half.,hardware,2025-11-06 16:56:33,5
AMD,nngsi1v,"Excited about the 388 i would love an 8/16 cpu for lighter things like gaming and general computing with the 8060, right now you basically have to pay for 64-128gb ai machines and not everyone wants that",hardware,2025-11-06 18:59:30,4
AMD,nniqcmg,I'm just hoping this leads to a mini PC with this chip in it so I can replace my ancient gaming desktop with an all in one machine. The current Strix Halo line is everything I've ever wanted in an all-in-one machine but is too overkill and expensive for what it's worth. A cut down variant with the same iGPU would be sweet.,hardware,2025-11-07 01:07:27,2
AMD,nnq9kar,"ALRIGHT, will more laptop manufacturers finally start using Strix Halo?  really really really want more than 8GB of vRAM for video editing and game dev, but I cant afford RTX 5070Ti laptop prices....",hardware,2025-11-08 06:46:14,2
AMD,nnfs3wu,How many days of driver support can we expect?,hardware,2025-11-06 16:06:22,9
AMD,nng7z7l,Hopefully these would be affordable and priced in accordance to competition (4060/5060 laptops). But knowing how the world is these will be sold at exorbitant prices to generate AI slop.,hardware,2025-11-06 17:22:23,2
AMD,nnfo0a9,"Hello work-school-account! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 15:47:12,1
AMD,nngfc5e,When do you think consumer RDNA 5 gaming cards will come out? Maybe around June?,hardware,2025-11-06 17:57:41,1
AMD,nnfonzs,That's what we need!!!!  Perfect for handhelds,hardware,2025-11-06 15:50:18,53
AMD,nnkun5o,I'm praying that Minisforum somehow shoves these on an ITX motherboard.,hardware,2025-11-07 11:23:54,2
AMD,nng47qh,"This will still be a big, expensive processor.  8C CPU CCD is pretty small.",hardware,2025-11-06 17:04:07,17
AMD,nnjgzvw,"they should really do a APU silicon with only quad core + GPU equivalent of RX580 performance. (not a cut down chip but actually small chip). That handheld will be a monster.  \*note we had i7-7700K able to push GTX1080/GTX1080Ti, so quad core is actually more than enough.",hardware,2025-11-07 03:51:41,7
AMD,nnfz2ma,That's an OEM's job.,hardware,2025-11-06 16:39:29,13
AMD,nnfwu99,"Think that's a year out, at least",hardware,2025-11-06 16:28:44,22
AMD,nnfyyh8,Medusa Halo won't see the light of day before H1 2027.,hardware,2025-11-06 16:38:56,9
AMD,nnfvgho,Is rdna 5 confirmed?,hardware,2025-11-06 16:22:09,3
AMD,nnqpufs,"Keep an eye out at CES next year, if there are any more strix halo laptops imminent then they will be announced there or at the very least AMD will talk about strix halo. If it doesn't get mentioned then the means most OEMs are holding out for rDNA5, RTX 6000 or medusa halo in 2027.",hardware,2025-11-08 09:31:31,1
AMD,nngcvle,That requires AMD’s marketing to take their ADHD meds on a regular basis.,hardware,2025-11-06 17:46:10,6
AMD,nngun7g,2 year cadence. 2027,hardware,2025-11-06 19:09:54,5
AMD,nnfrbk2,Assuming that they're actually available and affordable.,hardware,2025-11-06 16:02:41,39
AMD,nnko7ge,"not really. it's still gonna drain the battery way too fast. slower than the 395, but not by a lot.",hardware,2025-11-07 10:24:19,5
AMD,nni7e5v,"These will be great (but expensive) for the GPD, AYANEO, or OneXPlayer devices. I doubt that we'll see these in a mass market ASUS, Lenovo, or MSI handheld unless we get a ""Z"" series version of Strix Halo. I am anticipating something like a ""Z2 Ultimate"" if AMD stockpiles enough yields with defective+disabled NPUs or something. Maybe there will be yields that are only stable up to 45w with reduced clocks that can find a home in more conventional handheld designs. The Phawx has proven that any TDP over 25W will offer massive gains over the existing Z2E.",hardware,2025-11-06 23:16:45,4
AMD,nngjm9g,"Not sure about handhelds. I think these processors are intended for NUCs, so maybe they don't work very well with a battery. Perhaps for a ""Steam Console""? I'm not sure either. I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV, see if it's any good.",hardware,2025-11-06 18:17:59,3
AMD,nnv8p4o,I'm still happy with the performance I get out of HX370 @ 5w TDP,hardware,2025-11-09 02:24:33,1
AMD,nolpn70,"Right, because you totally want a 60W APU in a handheld.",hardware,2025-11-13 08:44:20,1
AMD,nnfsovn,"Why? There's no need for a 12-core CPU in a ROG Ally, even 8 is overkill.",hardware,2025-11-06 16:09:06,-3
AMD,nng7s73,The onus is still on AMD to sufficiently supply them. That’s been a consistent issue for them versus intel.,hardware,2025-11-06 17:21:26,27
AMD,no3eqlv,OEM cannot do it if AMD does not ship any units.,hardware,2025-11-10 12:15:14,1
AMD,nngelza,"And if typical AMD release patterns hold true, only a few, very expensive,  SKUs will be available in 2027. I'm not banking on buying a Medusa Halo product that fits my budget until late 2028 at the earliest. Don't sleep on this expanded Strix Halo availability. These should be priced to compete with Panther Lake in 2026. I'm personally hoping to see gaming notebooks like the Asus TUF A14 updated to use the 388 chip",hardware,2025-11-06 17:54:17,17
AMD,nnkwofi,"In time for the market crash and prices returning somewhat normal, hopefully",hardware,2025-11-07 11:41:18,2
AMD,nnfyf51,I thought it would be released in the first half of 2026?  Edit: Oh lol seems like i was wrong lmao,hardware,2025-11-06 16:36:22,2
AMD,nnqxwar,"well, my current 7 year old laptop's hinge ([MSI GF63 8RC](https://www.notebookcheck.net/MSI-GF63-8RC-i5-8300H-GTX-1050-Laptop-Review.340606.0.html)) is torn in half, and its battery is completely dead  so I am absolutely in need of a new laptop in short notice, the latest new tech I can wait for is prolly Intel Panther Lake >!as the laptop itself has to release before [Arknights Endfield](https://www.youtube.com/watch?v=1XpQz8k1NwE) comes out in Q1 2026!<",hardware,2025-11-08 10:54:50,1
AMD,nngjqs0,ADHD meds with or without ADHD?,hardware,2025-11-06 18:18:34,4
AMD,nng47rh,"If we're being honest, they'll still be up-priced to sell to AI-bros instead of being tiny gaming boxes like we (here) want.",hardware,2025-11-06 17:04:07,21
AMD,nnv9ykh,GPD WIN MAX 5 does better with both 395 and 385  https://gpdstore.net/product/gpd-win-5/,hardware,2025-11-09 02:32:57,1
AMD,nnvaazw,">  I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV  If anyone has done that for reviews, it would be https://www.youtube.com/@ThePhawx/videos",hardware,2025-11-09 02:35:12,2
AMD,nnhi426,>  so maybe they don't work very well with a battery  I don't see why not though? Don't they have awesome performance at very low power?,hardware,2025-11-06 21:05:54,3
AMD,nnhsreb,"Well, that’s exactly my use case with beelink gtr9. I’m using it as a tv gaming console running bazzite (and also as an llm server).   Gaming performance for 8060s - it’s much better than I expected. About every modern fighting game runs in 4k at stable 60, though upscaling is usually required for that. It also megabonks perfectly.   So I have high hopes for these new chips, given that they use the exact gpus. It now all boils to price and form factors.   My gt9 is undervolted and slightly overclocked, and runs at about sustained 140 watts.   Given that I live in Ukraine and lately experience blackouts, so during those I put it on balanced profile (sustained 80watts) and run off ecoflow together with monitor (instead of tv when life is gucci). This balanced setup all together is consuming around 137 watts when megabonking at 4k.",hardware,2025-11-06 21:57:40,1
AMD,nnfw596,They are talking about the 8 core version,hardware,2025-11-06 16:25:26,36
AMD,nngcj44,"That, and a complete unwillingness to invest in engineering support for laptop OEMs.",hardware,2025-11-06 17:44:32,10
AMD,nngda20,"Since there's a bunch of noname companies releasing products with Strix Halo,  what make you think there's not sufficient supply?  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",hardware,2025-11-06 17:47:59,2
AMD,no4cib3,"Videocardz covered 6-10 devices that use Strix Halo, that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship sufficient units. Both ASUS and HP's laptops are in stock all around the world.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",hardware,2025-11-10 15:35:11,1
AMD,no4bk1f,"Videocardz covered 6-10 devices that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship any units.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",hardware,2025-11-10 15:30:25,1
AMD,nnfzbti,RDNA5 won't till 2027,hardware,2025-11-06 16:40:41,12
AMD,nng22ps,"Nah zen6 launches in H1 26 in servers, year end/early '27 for desktops/laptops. Rdna5 is probably early '27. From the most recent rumors I've seen atleast",hardware,2025-11-06 16:53:52,8
AMD,nnr0mk0,"Probably keep a lookout during black Friday then as there might be a sale on a 5070 ti or 5080 laptop that brings it down to your budget. Even if CES 2026 brings the perfect laptop, there's no guarantee it will be buyable before May 2026 based on previous years.  Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.",hardware,2025-11-08 11:22:07,1
AMD,nnh7qgd,Take a guess.,hardware,2025-11-06 20:14:13,2
AMD,nnj1xx4,"It's just a big, expensive architecture.",hardware,2025-11-07 02:17:39,3
AMD,nngodkr,No CUDA and RAM getting more and more expensive won't make them super desirable for AI.,hardware,2025-11-06 18:40:14,7
AMD,no3ehqd,They cost too much to make to be tiny gaming boxes.,hardware,2025-11-10 12:13:18,1
AMD,nniq5gn,Who tf uses a handheld for AI lmfao,hardware,2025-11-07 01:06:15,-1
AMD,nnfxgwk,Even 8 cores is excessive in a handheld. You'd get more with a stronger GPU,hardware,2025-11-06 16:31:45,-7
AMD,nng22hs,8 cores x86 cores takes up too much power in a handheld unless it’s some 4 big core 4 small core thing,hardware,2025-11-06 16:53:51,-5
AMD,nng3wxq,8 cores is still too many for a handled. 6 is ideal,hardware,2025-11-06 17:02:40,-8
AMD,nng8444,Surely it would still be a 100W+ chip when pushing gaming loads though?,hardware,2025-11-06 17:23:03,-4
AMD,nnger22,"One of those “no name” companies (GPD, the oldest and most established) has literally said AMD doesn’t give enough supply, lmao.  https://videocardz.com/newz/gpd-accuses-amd-of-breaching-contract-by-not-supplying-enough-ryzen-7-7840u-apus-on-time  Secondly, AMD being able to supply handhelds that sell several thousand units is a drop in the bucket compared to the millions major OEM laptops need to use. The fact that AMD supplies them in very small numbers, and not OEMS at capacity is proof of my point, not a refutation of it, lol. ￼",hardware,2025-11-06 17:54:57,19
AMD,nngukrm,"These SoCs are very interesting not just for AI stuff. The NPUs have a lot of potential to dramatically improve energy efficiency and perf for a lot of other tasks as well, its just that the tools are still in their infancy so we haven't managed to develop many applications on them yet.",hardware,2025-11-06 19:09:34,3
AMD,nnh4bg5,Would love to see more handhelds use them for the performance per watt at lower tdp.,hardware,2025-11-06 19:57:12,1
AMD,nnqbc5e,">The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.  for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB",hardware,2025-11-08 07:03:19,1
AMD,no4g99f,So a total of 4 laptops? Two of which are from OEMs i never even heard of?,hardware,2025-11-10 15:53:37,1
AMD,nng3o0d,That would be very early for Zen 6 based on AMD's typical timelines.  Nobody is really moving that fast anymore outside the smartphone chip space.,hardware,2025-11-06 17:01:28,2
AMD,nnrbfzj,">Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.  but since the [5050](https://www.youtube.com/watch?v=ihJRJryQXTE) & [5060](https://www.youtube.com/watch?v=-VM_H7soWBQ) doesnt seem to have much of a performance boost compared to RTX 4060 & 4070, are older laptops with RTX 4060 & 4070 still viable picks if they're cheaper?  The main thing is that I want a sub 2kg laptop with dedicated GPU and 32GB RAM, so the main laptops I'm eyeing rn are either:  * 2023 ROG Zephyrus G14 * 2024 HP Omen Transcend 14  but their new RTX 4060 versions are still hovering around 1300 USD in my country, way above my budget.  So ofc I'm hoping that their price substantially goes down either during Black Friday or 12/12 sale, but do you think that they're even still worth pursuing or should I just focus on 2025 & upcoming laptops? (My third pick that fits my requirements is the new Legion 5, but that one's even more expensive atm)  Also technically there are three used laptop stores trying to sell a used RTX 4070 Omens to me at 1300 USD, but no way used stores are going to give any substantial Black Friday discounts",hardware,2025-11-08 12:54:16,2
AMD,nnis0md,#1 usecase for these in AI is running medium sized MoE models for local LLMs.  And for that they offer a pretty compelling solution. You basically have to spend twice as much to do it with Apple. At which point you're not using CUDA either.  ROCm works and inference engines like llama.cpp also support Vulkan compute AI.  It's basically the most power efficient and most cost efficient solution on the market for local LLMs.,hardware,2025-11-07 01:17:39,6
AMD,nniqrl3,"Well, mobile ai, these things don’t use sodim ram (or ram sticks in general) and the speeds of ddr5x aren’t used for most CPU’s. This very well could be also targeting mobile ai ( specifically more ai/gaming where the 395+ was targeting development and simulations as well). Also ai doesn’t need cuda, rocm exists and it’s quite good, any downfalls of amd is on a architectural level, but if you’re doing local ai on the go then you don’t really need cuda if that makes sense.",hardware,2025-11-07 01:10:00,2
AMD,nnfyjb8,This is the strongest gpu they have available,hardware,2025-11-06 16:36:56,35
AMD,nng2x5i,"Yes, but this is the version with the best GPU with the least cores, so it becomes the best for handhelds",hardware,2025-11-06 16:57:53,10
AMD,nngwxna,"From a power point of view disabled-by-binning cores would be exactly the same as if you disabled them in the OS, or just didn't have enough work for the OS to schedule threads on them.  Any real savings would require a new die (and there may well be possible savings there, less complex busses, easier routing, fewer ports on sram blocks etc - but I suspect they might be relatively small), but this isn't that.  The *only* advantage this would have over the ""full fat"" die is cost. And that still depends on yields and how much supply of these binned dies they actually get.",hardware,2025-11-06 19:21:09,5
AMD,nng4794,"Yes, but this is version with the best GPU with the lower amount of cores. So the best possible in terms of performance for a handheld",hardware,2025-11-06 17:04:03,9
AMD,nng95e9,You can set the TDP to 15W actually,hardware,2025-11-06 17:28:04,12
AMD,nngm2zc,Its a new product. Seems they made it for AI but found other very interested into it including nvidia partnering with Intel to counter these products. So I imagine 5heyre taking it more serious now that they're releasing these,hardware,2025-11-06 18:29:30,5
AMD,nngfqas,"My friend, we were talking about Strix Halo supply, not 7840U from 2 years ago.",hardware,2025-11-06 17:59:32,-4
AMD,nnqcf1u,Haha. You fell for the integrated NPU trap.,hardware,2025-11-08 07:14:01,1
AMD,nnqby1j,Give me examples where for the same price you can get a Strix Halo laptop vs   5060/5070 ones.,hardware,2025-11-08 07:09:20,1
AMD,no4hq94,"Copy paste of one of my comments why OEMs are not in a hurry to use strix halo in laptops.  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",hardware,2025-11-10 16:00:44,1
AMD,nng4b4n,"Mi450x is shipping in Q3 with zen6, and zen6 server chips are already in partner hands from latest earnings call",hardware,2025-11-06 17:04:34,7
AMD,nnre7nr,"Unless the extra efficiency of the new CPUs are a factor for you then you aren't missing much getting a 2023 or 2024 laptop and yes the 4060 and 4070 are still plenty viable, the 5060 usually almost matches the 4070 but you're going to have a similar experience most of the time between the 4060, 4070, 5060 and 5070 most of the time. So yes, if you can get a 4060 cheaper then go for it.  It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.",hardware,2025-11-08 13:13:25,1
AMD,nnfznhx,"Even then I'd just rather not have the cores. You're GOU limited anyways, so all they're doing is eating a bit of power and hurting your battery life. I've got a Z1X Ally and I'm generally hapoy with it but the usability on battery is easily its biggest weakness",hardware,2025-11-06 16:42:14,-11
AMD,nnh8398,Well i was saying this wouldn’t be enough anyway. I agree some handheld specific die would need to be made.,hardware,2025-11-06 20:15:59,-1
AMD,nngp9q1,Sure but the performance will be dogshit.  It's a nominal 55W chip and the 395 boosts to 120W to deliver the 4060-ish performance it is capable of.,hardware,2025-11-06 18:44:23,-2
AMD,nngkupf,You invoked “no-name” OEM’s being supplied at all by AMD as proof of them not having supply problems. I linked a very infamous instance of an OEM publicly speaking about AMD’s issues in keeping with supplying even these small boutique OEMS.,hardware,2025-11-06 18:23:45,12
AMD,nngrst9,"Strix Halo sounds significantly more niche than 7840U ever was, tbh.",hardware,2025-11-06 18:56:12,4
AMD,nnsxlvm,"Except I know that those NPUs are actually AIEv2 tiles, making this effectively an integrated CGRA :)",hardware,2025-11-08 18:20:47,1
AMD,nnqeynv,"in my country (Indonesia), if you wanted a sub 2kg laptop with more than 8GB VRAM, [your only option is either paying 52 million Rupiah for the RTX 5070Ti Zephyrus G14, or ""just"" 32.5 million Rupiah for the Flow Z13](https://cdn.discordapp.com/attachments/1315401041369366590/1436621115454652557/Zephyrus_G14_vs_Flow_Z13.png?ex=6910452d&is=690ef3ad&hm=d1c48964f898f82129b6acc9496b6f6db52c764812d0cfad8b9f2411a2c74826).",hardware,2025-11-08 07:39:45,1
AMD,nngen4p,"Yeah, but since Zen 6 is coming early in Epyc that doesn't mean Ryzen will come just as fast.",hardware,2025-11-06 17:54:26,2
AMD,nnw4sif,"> It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.     I'm from Indonesia!     The deal with our laptop market here is that we basically get the US's MSRP, but not their insane discounts (we ain't never getting a Legion 5 for $1000 unlike the US)     and the used market here is just extremely sparse for anything from 2023 forwards",hardware,2025-11-09 06:26:41,2
AMD,nng2wmz,Yes but dropping 2 cores is not going to cause a massive price cut or battery life improvement,hardware,2025-11-06 16:57:49,22
AMD,nnh5ap6,"The 8060S is about three times as fast as the 780M in the Z1E. The only IGPUs you can get that are faster than the 780M are the Intel Arc 140T, which comes in CPUs ranging between 14 and 16 cores, and the 8060S, which launched in 16-core parts and is now being made available in 12- and 8-core versions. If you want a good IGPU, you're getting at least 8 cores.  You can always disable them in the OS or UEFI if you're really dead-set on it. At that point, all you're losing relative to AMD engineering an entire new chip is some space on your motherboard.",hardware,2025-11-06 20:02:01,4
AMD,nngrdht,"[It actually doesn't scale nearly that well with power](https://youtu.be/vMGX35mzsWg?si=NIM7ZYvjsFLFElL9), there's simply not enough memory bandwidth to feed the GPU effectively.",hardware,2025-11-06 18:54:12,7
AMD,nnvw8zt,Those NPUs are actually integrated time machines.,hardware,2025-11-09 05:13:12,1
AMD,nnqktla,"I find that impossible to believe that's the norm. Your proof must be a great case of cherry-picking because I've been following Strix Halo and the competition very closely.  Here's examples where Strix Halo is more expensive, and much slower to a 5070 Ti:  [https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ](https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ)  [https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS](https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS)  In EU, the Stix Halo 395 cheapest laptops is 500-700 euro more expensive than 5060 laptops, 300-500 euro more than 5070, and 600-700 more that 5070 Ti. And that's for Z13 **TABLET**, not a laptop.  So my case stands. Strix Halo is a great niche product, but it doesn't make sense gaming laptops because the tech it uses is expensive and has a big die, thus making it uncompetitive. It's amazing for AI and gaming handhelds, thus we don't see many laptop options.",hardware,2025-11-08 08:39:20,1
AMD,nnqx5ue,"I mean I did say ""**sub 2kg**"", because thats what I actually want  I know stuff like the MSI Vector 16 or ROG Strix G16 are available with RTX 5070Ti for cheaper than the Flow, but those laptops are heavy bricks",hardware,2025-11-08 10:47:26,1
AMD,nnr4l64,"You actually said ""**for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB**"" and the discussion was about **laptops,** not tablets. Stop moving the goalpost and learn how to have a logical conversation.",hardware,2025-11-08 11:58:55,2
AMD,nnqte90,"TLDW:    - 12 game average at 1080P medium:    9500F was 5% slower than 9600X     - 12 game average at 1080P ultra:    9500F was 3% slower than 9600X      - 9500F is on average 5% faster than the 7500F    - At current Aliexpress pricing,  9500F is only 45 AU$ cheaper than 9600X, which is not worth it",hardware,2025-11-08 10:08:30,34
AMD,nnsc2mr,"Ryzen 5 9500f is available from Proshop (Danish retailer that sells in Nordics and Germany) for 250€. Meanwhile Ryzen 5 7500f from same online store is mere 140€. Both prices are without cooler and with the Finnish 25,5% VAT.   But what really kills the 9500f is the price of the Ryzen 5 9600x, which is currently hovering around 219€ at Proshop and other retailers using the same 25,5% VAT. So yeah, even when available from local stores, not worth it at current prices. The 7500f/7600x or even the 9600x are better options.",hardware,2025-11-08 16:27:10,15
AMD,nnqvvko,"The 7500f has remained an absolutely awesome buy for me with a 1440p system but it all comes down to pricing.  In the summer of 2024 I paid just above 100€ for it here in my local market. While that's not a price that was available often, 125-135€ was a pretty normal price for the CPU.  With a CPU as cheap as this, my PC remains almost exclusively GPU limited at 1440p alongside a RTX 5070. So it's understandable why so many of Steve's viewers were interested in the part. The 7500f was awesome for the money.  That being said, AMD has so many processors with video output available above the 7500f's price point that the 9500f becomes a tougher proposition considering the miniscule performance uplift.",hardware,2025-11-08 10:34:19,25
AMD,nnsg7a8,"I said it before, So I say it again. There will never be a chip like newly launched 7500f at 115 pounds at aliexpress ever again this platform. It was to buy into the am5 family and make the transition less painful for low budget builds. Now the deal market has matured. You can find ryzen 7700s cheaper than this new 9500f  When 7500f was avaible through aliexpress or in some eastern retailers shipping worldwide, people were in denial the deal they were getting.  ""Surely 130 dollars for a chip performing the same as ryzen 7600 is too good to be true with some asterisks, right?""",hardware,2025-11-08 16:48:36,10
AMD,nnr43st,"Zen 5 architecture except for the 3D Cache version overall is just very underwhelming performance jump over the Zen 4, I hope that isn't the case with Zen 6 in the future at the least.",hardware,2025-11-08 11:54:43,22
AMD,nnswnb5,I'm really hoping for either the 9600x3d or 9700x3d to be true,hardware,2025-11-08 18:15:51,3
AMD,nnrtcrf,Hardware releases these days are getting boring. We're in that period where nothing exciting is coming out for a long time still.,hardware,2025-11-08 14:45:20,5
AMD,nnr5tnf,"Just confirms we need the 7500f worldwide, and don't bring up AliExpress whoever is typing that response right now",hardware,2025-11-08 12:09:31,2
AMD,nnr16wk,"The 8400G (6 core zen4)can be bought for $80 from aliexpress. I think its a tiny bit slower than 7500f. sounds like the perfect cheap cpu. you can even get it paired with obscure chinese $65 b650 boards that are probably ""good enough"".",hardware,2025-11-08 11:27:37,-1
AMD,no5xzfi,The return of Zen5%?,hardware,2025-11-10 20:16:56,9
AMD,nnu5emf,"I wouldn't say that that is a ""real"" price it will be, more so it hasn't really launched yet.  Like it's one retailer listing it with no images, very basic description and not in stock(though available to order 6-9 days apparently) with and the mentions of being ""oem cpu"". So I'd guess in X months if/when it has actual availability, it'll be the ""correct"" price of slightly cheaper than a 9600x is.",hardware,2025-11-08 22:19:35,7
AMD,nnuhlh3,"Or, ya know, older gen CPU's simply get discounted to sell quickly.    I've also seen the 9600 go for like £130-140 plenty on AliExpress.    There are still loads of crazy deals for CPU's on AliExpress if you're paying attention.",hardware,2025-11-08 23:33:36,3
AMD,nnssp50,The Zen 5 core architecture itself doesn't seem to measure up well with AMD's previous improvements tbh.,hardware,2025-11-08 17:55:33,6
AMD,nnt6aqs,Remember when leakers were claiming Zen 5 is a “Zen 3 moment”? Now they’re claiming Zen 6 is a “Zen 3 moment”,hardware,2025-11-08 19:05:39,4
AMD,nnt6eho,Remember when leakers were claiming Zen 5 is a “Zen 3 moment”? Now they’re claiming Zen 6 is a “Zen 3 moment”,hardware,2025-11-08 19:06:12,-1
AMD,nnwrn1m,"Or could get a 7500F/9500F now, and upgrade to Zen6 x3d down the road.",hardware,2025-11-09 10:16:13,2
AMD,nns7k1r,That's bound to happen until the next breakthrough is found,hardware,2025-11-08 16:03:22,8
AMD,nnuicp2,"New CPU and GPU releases coalesced in 2024, and with two year generations being the norm for both nowadays, 2025 was always going to be a complete dud of a year for new processor releases.",hardware,2025-11-08 23:38:20,5
AMD,no7qvty,Zen 6 will be exciting next year. Nova Lake is also looking more interesting than arrow lake and raptor lake were.,hardware,2025-11-11 02:21:04,1
AMD,nns229r,What about AliExpress,hardware,2025-11-08 15:34:09,10
AMD,nnrr0aw,"You're thinking of the 8400f and no it's not a good CPU. It's also not $80 anymore if you live in the USA.  The big problem with the 8400f is it lacks Pci-E lanes, it only has 4 Pci-E 4.0 lanes for the GPU. Basically will bottleneck any modern GPU.   Then, it only has half the cache which is not ideal for gaming performance. Really not worth buying the 8400f for gaming at any price.",hardware,2025-11-08 14:31:43,14
AMD,nnu7u7n,"Fair enough point, the price will likely come down if it properly launches. Right now though, no point in buying it.",hardware,2025-11-08 22:33:57,3
AMD,nnui2cd,"I originally thought so as well.   But 9800X3D really proves that wrong.  It seems to demonstrate that the new super wide Zen 5 cores really need to be fed better or else most of their advantage will go to waste.  A huge L3 cache seems to be a good answer for this, but I'm hoping this means AMD has some good low hanging fruit to take better advantage of these super wide cores with Zen 6 that doesn't simply require the brute forcing of a massive L3.",hardware,2025-11-08 23:36:33,6
AMD,no4gomp,"There are leaks of ""next gen be great"" for AMD for every single generation out there. Someones really sniffing on hopium at AMD.",hardware,2025-11-10 15:55:42,3
AMD,no7qrba,Sure but Zen 6 is getting a die big ass shrink going from N5 family to N2. Zen 6 is also increasing core count by 50%.   No matter what Zen 6 will be a bigger deal than even Zen 4 let alone 5 just due to core count increase alone.,hardware,2025-11-11 02:20:20,2
AMD,no6pk3p,Why does this matter so much to you?,hardware,2025-11-10 22:39:36,1
AMD,nnwsmg2,"yeah I just want to get on the AM5 platform asap, I made the mistake of investing into Alderlake and now I'm back on my old i7 3770.  Mind you, Linux makes this old guy still pack a punch, so I'm not complaining.  I think realiistically I willl go for either the 7500F or the 7500x3d if the latter is true.",hardware,2025-11-09 10:25:57,3
AMD,no4h40t,"I mean, all the GPUs released in 2025. They just did it at the start of the year.",hardware,2025-11-10 15:57:45,1
AMD,nnsrka6,Expressing my Ali right now,hardware,2025-11-08 17:49:33,6
AMD,nnry510,8400f is celeron of AM5 series.,hardware,2025-11-08 15:12:38,6
AMD,nnupt7g,"I think the 9800x3d just appears that way because it has the advantage of having a higher boost clock vs Zen 4X3D, due to the changes to how they structured the 3D V-cache, rather than the Zen 5 arch scaling better.   Techspot has an 11% perf gap between the 9800x3d vs the 7800x3d, while esentially no perf gap between the 9700x vs the 7700x, but his 9800x3d had an \~8% faster all core frequency vs the 7800x3d, while the 9700x had more than a 5% *lower* boost clock than the 7700x.",hardware,2025-11-09 00:24:05,15
AMD,no4lykt,"Yea yea, you know what I mean, though.",hardware,2025-11-10 16:21:15,1
AMD,no4gvon,"I have a 7800x3D. I am constantly bottlenecked both at work and at leisure by CPU. 9800x3D still not worth it the upgrade here, ill wait for larger gap.",hardware,2025-11-10 15:56:38,2
AMD,no8ry72,"I know what you mean, and that will get a lot worse in 2026 when we will see nothing but a few refreshes out.",hardware,2025-11-11 06:46:11,1
AMD,nofwtij,"**Tomsharware title is wrong about zen7 node, it's zen6 that is on 2nm, zen7 is on ""future node""**  1. 4% projected annual growth for dc cpu general compute 2. 18% projected annual growth for ai dc cpu 3. Ai dc cpu sales to more than double total dc cpu market by 2030 from 23b to 60b, ai is boosting dc cpu demand instead of cannibalizing it 4. Amd also ""hiring like crazy"" on software side, total number of hardware and software engineers exceeded 25k, minority on the payroll are managers  They are sayin that future epyc growth will be primarily driven by host processors of ai gpu clusters and hinted that uarch emphasis in the next few generations will be on per core perf (ipc + clocks + on cpu accelerators) instead of core spam because ai workloads benefit from st perf  This kinda aligns with speculations that zen7 ain't gonna bring major core count increase on the dc side. Client still has room for growth from 16 to 24 or 32, but the primary dc market ain't going for that anymore. Good thing for gamers due to st focus probably  tldr, Dc requirements for core count slowing down, amd moving on from mt perf and thread dense market to high per core perf market due to ai and enterprise",hardware,2025-11-12 11:52:33,148
AMD,noge1ii,As long as we see good performance increases I don't care if it has better AI features.,hardware,2025-11-12 13:48:56,34
AMD,nogr415,I guess this is pointing towards Zen7 for the introduction of DDR6 and AM6 then.,hardware,2025-11-12 15:01:59,19
AMD,noghqy0,Zen 7 likely wont come out until 2028.,hardware,2025-11-12 14:10:33,11
AMD,nogj6kx,i love ai in my cpu lmao,hardware,2025-11-12 14:18:45,21
AMD,nohj8r0,"It will be interesting to see when they jump socket due to DDR6.  Perhaps they will release a Zen 7 CPU on AM5/DDR5 using Zen 6 IOD (as they did for Zen 5 to Zen 4) and also introduce a Zen 7 with DDR6/PCIe6 support on a new socket.  While it creates a more complicated product stack, they can benefit from the existing AM5/DDR5 install base and manufacturing while offering a premium solution for workloads that need the faster memory/PCIe standard.",hardware,2025-11-12 17:20:35,4
AMD,nofwoge,"More wasted silicon and work on ""AI features"".",hardware,2025-11-12 11:51:26,74
AMD,nofz0df,">Zen 7 as the true ""next-generation"" leap with 2nm   It has to be, considering the leap from N4 to N2.",hardware,2025-11-12 12:09:43,14
AMD,noh2tp3,"And there will still be versions compatible with the AM4 socket!  I'm kidding, but that would be great; the longevity of that socket is legendary by now. :)",hardware,2025-11-12 16:00:23,2
AMD,noh53h0,Is Zen6 going to be an actual leap or is it just going to just be Zen5+.,hardware,2025-11-12 16:11:31,2
AMD,noil4y8,will we finally have SteamDeck 2?,hardware,2025-11-12 20:25:21,1
AMD,nojtkma,How many PCIE lanes and how much RAM ?,hardware,2025-11-13 00:24:52,1
AMD,notkk6h,"Maybe it's because I'm too broke to have a CPU with ""AI features"", but what does that even entail?",hardware,2025-11-14 15:20:43,1
AMD,noma9f4,Can we stop with the AI shit already?  Thanks.,hardware,2025-11-13 12:01:45,1
AMD,noj0r13,imagine if this whole AI chips fall apart and intel somehow comes out the biggest winner,hardware,2025-11-12 21:44:16,-1
AMD,nofv8fq,"Hello SirActionhaHAA! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-12 11:39:39,0
AMD,nog2aqb,"> Dc requirements for core count slowing down  That's a way to put it, another would be that node advances are slowing down so much that after zen6 on N2 it will become really hard to further increase core counts.  I don't think anyone really expects to get many more cores with zen7 vs zen6.",hardware,2025-11-12 12:34:12,44
AMD,nog116e,can we ban Tomshardware already,hardware,2025-11-12 12:25:00,85
AMD,nog1aqh,"Zen 6 on 2nm that's cool, i thought they would stick to 3nm to reduce costs",hardware,2025-11-12 12:26:56,9
AMD,nogefsk,That Toms would even put in 2027 as a potential Zen 7 date is ludicrous.,hardware,2025-11-12 13:51:15,12
AMD,nohui1f,>minority on the payroll are managers  Kinda funny this is something to brag about. Although probably a dig at Intel when they had more managers than engineers.,hardware,2025-11-12 18:14:53,2
AMD,nolaldq,"> This kinda aligns with speculations that zen7 ain't gonna bring major core count increase on the dc side.  I would be shocked if it did, why would it? From Zen 2 to Zen5 (two node shrinks) they had 8 Cores per-CCD.   If Zen6 brings 12 Cores per-CCD, why would anyone expect Zen7 to bring more again so quickly?   I'm speaking of full-phat Zen cores, not the Zen-c cores.",hardware,2025-11-13 06:17:25,2
AMD,nowsn5n,Feels like the computer hardware industry is putting a lot of eggs on the AI basket.,hardware,2025-11-15 01:47:48,1
AMD,nohisul,"For sure, but we need to see that actually happen first.  Adding more AI power is  gonna eat up a chunk of the transistor budget, so we have to hope this doesn't compromise general performance gains.  That said, 5nm to 2nm for Zen 6 is pretty damn significant, so there should be plenty of opportunity to do both.",hardware,2025-11-12 17:18:26,11
AMD,noh2yme,DDR6? We taking a loan for those i guess.,hardware,2025-11-12 16:01:03,32
AMD,noh7tq4,"There are rumors about zen7 being on AM5 as well. I think it will depend a lot on how things work out on the memory side - I assume they will want to launch AM6 with DDR6, but the memory market is very volatile now.",hardware,2025-11-12 16:24:50,14
AMD,nos6fqo,Well if the NPU can generate more fps or offload CPU workload then I'm all for it. Currently that's not done so yes it's 100% useless,hardware,2025-11-14 09:24:21,3
AMD,nojexu1,I suspect Zen 7 on DDR6 will be a server only thing if that happens. Zen 8 in 2030 with AM6 will probably be the case for Desktop.,hardware,2025-11-12 22:59:29,6
AMD,nogen3r,"AI is matrix multiplication, last time I checked MatMul is useful in a load of places.",hardware,2025-11-12 13:52:26,55
AMD,nogercc,"If you just keep to more SIMD extensions then it shouldn't be too much bloat, and can be used for non AI purposes.   But yeah if they start putting discreet NPU cores in the chiplet I'd be disappointed.",hardware,2025-11-12 13:53:06,8
AMD,nofxese,If it brings them money somehow and affects their software department in a good way (FSR4 and so on) - why not?   It's not like there's a deficit in CPUs right now.,hardware,2025-11-12 11:57:16,36
AMD,nogeorf,"Not a fan of it either. I don't know what had to be cut, or what costs needed to be increased to add the AI portion, but I would be thrilled if they continue to release versions without it.",hardware,2025-11-12 13:52:41,6
AMD,nofzioj,"Not really. Assuming zen6 goes to 24 cores and zen7 32 cores on client there's just gonna be an excess in mt capabilities. The focus should be on per core st perf and special workload acceleration that enable new features which they are pointing to  Not many people have a need for endlessly growing multithread perf, majority of client market benefit from st perf whether it's web browsing or gaming",hardware,2025-11-12 12:13:37,14
AMD,noh7pml,I disagree (likely against popular opinion).  If AI tasks are going to be performed on local systems I'd rather it happen on dedicated hardware that is more energy efficient and doesn't compete for CPU/iGPU resources with other system tasks.,hardware,2025-11-12 16:24:16,2
AMD,nog4jox,Consider it clock gated dark silicon to help thermals?,hardware,2025-11-12 12:49:56,4
AMD,nohhfqt,How dare AMD balance their designs towards the use cases that are driving growth!,hardware,2025-11-12 17:11:39,2
AMD,np8vk8f,I'm sure you know better than the engineers at AMD,hardware,2025-11-17 01:42:46,1
AMD,nofzxkb,Zen 6 is the one that's on N2.,hardware,2025-11-12 12:16:47,50
AMD,nog1ncd,That's zen 6 though. The title is fairly confusing,hardware,2025-11-12 12:29:27,20
AMD,nohdve1,"As others have said, the title is very wrong.  Zen 6 will already be 2nm.   And 'Next Generation' on this slide doesn't indicate it's gonna be some bigger leap than any other, just that it's the 'next generation' of Ryzen to be announced.    Frankly, a move from 5nm chiplets to 2nm chiplets for consumer Zen 6 products will itself be a pretty massive thing.  Or at least, gives AMD a whole lot of possibilities for improvements.",hardware,2025-11-12 16:54:11,9
AMD,nog0isd,"Zen7's speculated to be on a14 or ""future node"" according to the presentation, the tom's hardware title is wrong",hardware,2025-11-12 12:21:12,15
AMD,noh5yf4,"I mean, Zen7 is likely a 2028 product. Nova lake is rumored to use TSMC 2nm next year.",hardware,2025-11-12 16:15:41,1
AMD,noh8fvk,Everything below an average 15% uplift would be a disappointment considering Zen 6 is going to be produced on a smaller TSMC manufacturing process.,hardware,2025-11-12 16:27:49,11
AMD,nohpo1d,I think Zen 5X3D shows us there's quite a lot of potential with Zen 5.  It was a pretty huge architectural overhaul.  So my hope is that Zen 6 helps optimize the wider architecture and brings out that potential without necessarily needing to just add a shit ton of L3 cache.  Going from a 5nm family process to 2nm is also a really big jump.,hardware,2025-11-12 17:51:50,7
AMD,noje5ez,"Zen 6 has a load of packaging changes, new IOD, Infinity Fabric, new process node, more cores. It'll be a very big leap.",hardware,2025-11-12 22:55:09,5
AMD,nowzto7,New IO for and 50% more cores so that automatically means it'll big a significant generation compared to Zen 4 and 5.,hardware,2025-11-15 02:34:12,1
AMD,nofw9mv,"Hey rng847472495, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-12 11:48:08,1
AMD,noknxk9,Hopefully a good memory controller 😂 oh and fix that terrible ihs,hardware,2025-11-13 03:27:09,2
AMD,nojo1cq,"Intel is doing all of this stuff, too.",hardware,2025-11-12 23:52:30,5
AMD,nokntw1,Sadly the whole industry loves ai slop,hardware,2025-11-13 03:26:30,3
AMD,nog86f8,"I agree with this but I'd phrase it a little differently - we should expect increases in core counts every few nodes, and it currently takes \~6-7 years to jump several nodes - and may take slightly longer than that in the future.",hardware,2025-11-12 13:13:43,18
AMD,nogs4jz,"The current rumor is Zen 6 (consumer) topping out at 24 cores, and Zen 7 at 32 cores. All those being regular cores, not “c” (compact) cores like Epyc has. Those pack more cores in, the tradeoffs being less cache and a lower clock.",hardware,2025-11-12 15:07:20,10
AMD,noj0lpl,"Pc hardware is going to become the fridge model. Tiny incremental improvements and you use it till it dies   Im getting a 12 core zen6 x3d if it fits on one ccd , 64 GB of good ram and the top end rtx6000 gpu and that ll be my pc for ten years.  Progress is almost at a full halt now. And the above gen will probably be the last time a high end pc costs less than a small car",hardware,2025-11-12 21:43:32,11
AMD,nowsrph,I mean yea but that's already the case. When's the last time we got a chiplet core count increase? We've been on 8 cores for ages.,hardware,2025-11-15 01:48:38,1
AMD,nog2eb6,Been saying this for a long time,hardware,2025-11-12 12:34:55,18
AMD,nogqzt4,Can you tell me why? Apparently out of the loop on that one.,hardware,2025-11-12 15:01:21,5
AMD,nog7xka,No.,hardware,2025-11-12 13:12:12,-13
AMD,nog2ze9,"Then it would be nicknamed ""zen not even 6%"".",hardware,2025-11-12 12:39:04,32
AMD,noh3au4,"I was under the impression that some people are speculating that there could be both 2nm and 3nm chiplets. Depending on the pricing structure and availability of 2nm, this could make economics sense.",hardware,2025-11-12 16:02:43,4
AMD,nohjjox,"Yea I really thought they'd only do 2nm for Zen 6C chiplets.  If they really do 2nm for all Zen 6 chiplets, that's pretty huge.  Essentially going to be a two node generation leap, along with also going from FinFET to GAA.",hardware,2025-11-12 17:22:04,3
AMD,nowxna6,"Not sure they can fit 12 cores in a chiplet using N3, 50% more cores is one of Zen 6s biggest strengths. Zen 6 will have 12 cores per chiplet on N2 so consumer Zen 6 will top out at 24 cores and 48 threads and Nova Lake will have 8+16+2 per compute tile on N2 with consumer Nova Lake topping out at 52 cores 52 threads.",hardware,2025-11-15 02:20:11,1
AMD,noh8l3k,"Mobile will be 3nm, Desktop in late 2026/early 2027 will be 2nm, DC in early mid 2026 will be 2nm aswell.",hardware,2025-11-12 16:28:32,1
AMD,noh56py,We're talking about a CPU that is 3 years away. And it's not even confirmed if it will be on all their products. With Zen5 they reserved 3nm for the Zen5c cores.,hardware,2025-11-12 16:11:57,-3
AMD,noh8h12,"Why? AMDs roadmap puts it there aswell. There is a lot of inaccuracies with toms, but 2027 doesn't seem unreasonable for Datacenter (desktop will likely be a lot later than DC, same goes for Zen 6)",hardware,2025-11-12 16:27:58,10
AMD,nooo07y,">From Zen 2 to Zen5 two node shrinks  That was a single node shrink, Zen 5 is only on N3 for DC turin dense",hardware,2025-11-13 19:38:23,1
AMD,nohjxwp,It could just mean stronger iGPU. Which would be nice for compact builds and laptops.,hardware,2025-11-12 17:24:01,1
AMD,nohkc41,I assume the AI hardware on the chip is also being used for other stuff and not exclusively AI stuff?  Considering chips these days are made up of smaller chips it might not be a terrible idea to just make two versions of the same CPU and just not put the AI chip on the chip? If that makes sense.,hardware,2025-11-12 17:25:57,-1
AMD,nohqhoy,"Not likely due to the boom and bust production cycles, except for early pricing.  I'm more interested in what will the mere mortals get after all the DDR5 issues, especially the intentional physical incompatibility of UDIMM and RDIMM.  At this point I don't want to see any more non-ECC UDIMMs, the density is just too high for either part to be sensible.",hardware,2025-11-12 17:55:41,10
AMD,nowz2rj,Technically it's done with auto SR on windows but that's a lower quality solution than DLSS. Apparently autoSR2.0 for Xbox Ally will bring serious improvements.,hardware,2025-11-15 02:29:24,1
AMD,nogl5yu,"Sure, but not the low precision formats.",hardware,2025-11-12 14:29:52,29
AMD,nohoecl,"Like what, specifically?",hardware,2025-11-12 17:45:44,-2
AMD,noh5sxv,"The article says that they are accepting that workloads are shifting to accelerators, so they are going to focus on pushing single threaded performance to be faster at moving data around rather than pushing  256+ cores.  That is good for us normal people.",hardware,2025-11-12 16:14:57,4
AMD,nofzda6,"I have a laptop with an Intel 235U with an NPU and I had task manager open for a day monitoring whats using it, nothing used it in my general work day, not even teams or zoom. Except for that auto camera panning which I dont use because I dont use the laptop camera but a different one mounted to my monitor.",hardware,2025-11-12 12:12:28,8
AMD,nog43o2,"In theory yes, in practice that’s how you become a second rate citizen.",hardware,2025-11-12 12:46:53,8
AMD,nog7a4x,"ST improvements are the hardest to achieve. Slow improvements to ST aren't intentional. The core spam, and now focus on special workload accelerators are both just different ways to try and improve performance when improvements to IPC/PPC are coming harder and harder.",hardware,2025-11-12 13:08:07,25
AMD,nogei5r,"(Peak) Multi-thread performance is not necessarily the right way to look at the addition of more cores. Software support is just not mature enough with most approach still just trying to parallelize one task even well beyond reasonable scaling, making large core counts seem useless.  Once core counts where naive scaling approaches break too often become common, we'll start to see a shift revealing more benefits of having a lot of cores. Task preemption which was the foundation of modern complex systems may be eventually demoted to a last resort solution for keeping misbehaving processes in line.  Spreading processes to different cores generally reduces the peak performance requirement, so cores can be operated at a lower frequency, with lower voltage greatly increasing efficiency.  Tasks sticking to specific cores without getting preempted and moved around also increases efficiency, and reduces the occurrence and impact of worst case latency penalties.  Software is just way too behind hardware, but throwing faster hardware at the problem still works, so problems are not getting fixed. For example it's amusing to see that single CCD AMD CPUs are the most common for gaming, not because they are good enough, but because dual CCD CPUs just perform worse for the average gamer.  A Linux gaming setup with a container and/or Wine limited to a single CCD with the other CCD being used for the rest of the system is the superior approach to Windows bloatware services and background processes competing with the game for the resources of the same CCD, but the later is what's the easiest to setup/get for most users, so that's what they will plan builds for.  A lot of games also just spawn one software thread per hardware thread per thread manager, which stops reasonably scaling after a point. It may be worthy to even attempt to lie about having only 8 hardware threads, then spread around the mess on 8 cores with SMT more efficiently handling the overcommitment.  Browser loads may not be even worthy of a lot of analysis. Website design regressed to the point where latency hiding is intentionally prevented, and an incredible amount of bloatware logic is expected to be processed serially.",hardware,2025-11-12 13:51:38,5
AMD,nogpedt,"> there's just gonna be an excess in mt capabilities  I'm sorry, what?",hardware,2025-11-12 14:52:59,1
AMD,nog6dsr,"> Not many people have a need for endlessly growing multithread perf, majority of client market benefit from st perf whether it's web browsing or gaming  Yup, for a lot of consumers it would have been better if Intel had settled for 10P cores rather than the hybrid approach.",hardware,2025-11-12 13:02:18,-3
AMD,nojnwk9,"That's fine, but GPU's are much more capable in that regard.    Haven't really seen any evidence that lower power AI acceleration in a CPU is gonna be all that useful for PC's.",hardware,2025-11-12 23:51:43,2
AMD,noiiljn,people on reddit and their hate-boner for AI. name a more iconic duo,hardware,2025-11-12 20:12:30,1
AMD,nog0mdk,"Zen 6 seems to be a good update to the architecture and the process technology, but I feel it's the test bench for the new packaging in a mass produced client product, Strix Halo was the limited test.",hardware,2025-11-12 12:21:57,14
AMD,nolnqsb,Zen 6% jokes are writting themselves already.,hardware,2025-11-13 08:24:20,3
AMD,nolntlb,"Yep, instead of 5% its a 8% uplift for the x3D parts. Unless you are using a specific niche instruction sets like AVX512, then yeah, good uplift.",hardware,2025-11-13 08:25:08,2
AMD,nok0r5p,Next crash is going to be spectacular,hardware,2025-11-13 01:06:51,1
AMD,noh0g68,The -C cores in DC for Zen 6 aren't rumored to have less cache anymore either.,hardware,2025-11-12 15:48:56,10
AMD,nowt14u,How are they increasing chiplet core count from 12 on N2 to 16 on whatever node Zen 7 uses? Considering these are regular Zen cores and not C cores.,hardware,2025-11-15 01:50:20,1
AMD,nojsvde,"> and you use it till it dies  And that's a huge issue for the manufacturers as silicon tends to be pretty robust and not really die, they have to innovate in order to have any sales.",hardware,2025-11-13 00:20:49,11
AMD,noqw546,I create a lot of physics sims and tech vfx and other intensive content creation… I haven’t upgraded anything on my pc other than the GPU since 2019 (2070s>4090).  Still rocking a 3950x with 64gb ram and every time I’m tempted to upgrade I take a look at benchmarks and it’s just not worth the hassle.  I game on the side and I’m still very happy.,hardware,2025-11-14 03:00:24,1
AMD,nowwx4w,Depends on the application but you're right specially for single threaded performance.,hardware,2025-11-15 02:15:32,1
AMD,noh2yjt,"A lot of seemingly rushed misreporting, sometimes by people who don’t completely understand what they’re writing about",hardware,2025-11-12 16:01:02,31
AMD,nogi8oz,it'd be an improvement over Zen ±5%,hardware,2025-11-12 14:13:22,20
AMD,noj4x8g,"I wouldn't be so sure. The big thing with Zen 6 is the new IO die, which severely constrained desktop Zen 5. This is why you saw ""zen 5%"" to begin with, really. It's part of the reason X3D was so good Zen 5 too: it can bypass the memory constraints for longer due to bigger cache. In heavy RT loads tho even the 9800x3D gets pummeled due to the infinity fabric constraints (see: Outer Worlds 2) once the cache fills your memory gets thrashed and FPS goes to toilet.   If they can get the new sweet spot with infinity fabric to, say, 8000 MT/s then even with Zen 5 cores in the thing you'd see pretty big perf gains just by that alone. It's why Zen 5 was huge for data center who can run up the score with shit like 12 channel RAM to get around bandwidth constraints a lot easier.",hardware,2025-11-12 22:05:21,6
AMD,nogau6w,"Well, don't count your chickens yet, AMD is using the extra die for more cores, not necessarily IPC, so IPC gains might be below 10%.",hardware,2025-11-12 13:30:07,9
AMD,nog8wjv,That went away real fast when Intel was -20%. Funny how quiet that got and how fast.,hardware,2025-11-12 13:18:16,2
AMD,nohmo4s,"They usually indicate dual process node usage on their roadmap, though.  They're not doing that here.  Example:  [https://www.techpowerup.com/img/XcSGJNezBLjShxJm.jpg](https://www.techpowerup.com/img/XcSGJNezBLjShxJm.jpg)  It's not full confirmation, but seems pretty tantalizing evidence.  I would have guessed 3nm for Zen 6 and 2nm for Zen 6C initially as well.",hardware,2025-11-12 17:37:23,6
AMD,noib6kf,">I was under the impression that some people are speculating that there could be both 2nm and 3nm chiplets  Rumor was that you would see 3nm monolithic dies for mobile with Zen 6 variants, or an IOD on this node with Zen 6 as the lower power cores.",hardware,2025-11-12 19:35:47,5
AMD,noh5fqm,They could just use 2nm on laptops.,hardware,2025-11-12 16:13:10,1
AMD,noh5jno,Probably a year and a half from now for Zen 6,hardware,2025-11-12 16:13:42,6
AMD,nohnz3g,"AMD's roadmap indicates we may get another DC CPU architecture in 27', not that we will get Zen 7 in 27'. Who knows what Verano is, I don't think AMD officially said anything about it yet.",hardware,2025-11-12 17:43:44,8
AMD,noiaf06,"AMD and Intel do this all the time, the graphs aren't representative, they're just graphics that don't convey the meanings people read into them. Past AMD charts have done the same, and if you ignore Zen+ then every generation has been a solid two year cadence. Lastly, new nodes  don't happen every two years anymore, and since AMD is going to be the first customer on TSMC's N2 with Zen 6 then TSMC is still going to be on N2 a year later. It doesn't make any sense in any which way to launch Zen 6 if Zen 7 is launching the following year on the same nodes.",hardware,2025-11-12 19:31:57,2
AMD,nooxu5i,"Please correct me if I'm wrong here, but from my understanding there's 3 distinct nodes used between those products.  Zen2: N7 7FF  Zen3: N7 7FF  Zen4: N5 5FF  Zen5: N4X 4FF  I count two node shrinks. Granted the 7 -> 5 was a more significant shrink.  Disclaimer: Only talking about desktop, chiplet based Zen parts. As your point about Zen-C parts (and mobile parts for that matter) is indeed outside of my comment's scope.",hardware,2025-11-13 20:27:53,1
AMD,noho2fj,I don't think they would include it in the slide that is about core IP then if it was just a stronger iGPU.,hardware,2025-11-12 17:44:10,10
AMD,nohnxed,"Specialized AI hardware and low precision data formats usually aren't that useful for *that* much outside AI.  As for making two different chiplets, it kind of goes against AMD's strategy of making one chiplet and have that be scalable and usable for both consumer and datacenter products.  I know Zen C/dense chiplets were kind of exception to this, but that's solely for increasing datacenter core scaling, so I can see why they did that.  I think we're just gonna have to get used to CPU's and GPU's all having AI hardware on them going forward.",hardware,2025-11-12 17:43:30,9
AMD,noyfygy,"That's only for ARM CPUs, not yet available for x86",hardware,2025-11-15 10:03:16,1
AMD,noh7waz,Most dedicated NPUs support up to 32bit precision natively and 64bit through software,hardware,2025-11-12 16:25:11,11
AMD,nohr30n,Most scientific processing or engineering modeling work.,hardware,2025-11-12 17:58:29,8
AMD,nogm2ky,"With time it should be used for more stuff.  Now the amount of users with NPU is low, so it doesnt make sense to implement features for it  In the future, with a lot of people using it, it should increase the usage",hardware,2025-11-12 14:34:55,6
AMD,nogzj86,"Sorry, but consumers are a second class citizen already compared to data center. They make far more on Epyc than consumer.",hardware,2025-11-12 15:44:32,17
AMD,nohewv9,"It's harder, but ST is still the most important factor for the vast majority of consumer users(and of course is still relevant to MT performance itself, too).  Zen 4 also showed a very big improvement in ST, and ARM/Apple continues to make pretty decent gains, at least when projected over a similar two year period.  We definitely should not be happy if they think just pushing more and more MT performance will be compelling for consumer processors.",hardware,2025-11-12 16:59:11,3
AMD,nohgxun,"Single CCD CPU's are most popular for gaming because they are more affordable than higher core count options that wouldn't even provide them much in the way of meaningful benefits even if they were still on a single CCD.  You can say this is on software, but devs still have a hard time even getting good scaling going from 6 to 8 cores, and it's not simply them being lazy about it.  It's just gaming workloads are very hard to parallelize on the CPU end.  Moving to 12+ core CCD's is not going to change much here.",hardware,2025-11-12 17:09:11,1
AMD,nohfc7o,"They're saying that for consumers, more and more MT performance from where we're already at is a lot less useful than more ST performance.  So they're hoping that they aren't going to keep pushing more MT performance that is easier to get, while deprioritizing ST performance gains.  And mostly, I'm with them on this.",hardware,2025-11-12 17:01:17,6
AMD,noglsoz,"But why? What task can benefit from scaling up from 8 cores to 10, but doesn't benefit from scaling with even more cores?",hardware,2025-11-12 14:33:22,5
AMD,nolngd5,"The vast majority of consumer devices do not have a dedicated GPU. We had this issue before, where AMD had to start including iGPUs with every CPU because they lost tons of clients otherwise since there are a lot of systems without dGPUs.",hardware,2025-11-13 08:21:21,1
AMD,nonvjvu,"> That's fine, but GPU's are much more capable in that regard.   This very much depends on the GPU.  But either way it is the same argument.  Why not offload AI tasks onto and NPU and save your GPU for gaming performance?  Not to mention, some NPUs are competitive with GPUs for certain tasks, such as the NPU on the M5 chip.",hardware,2025-11-13 17:20:01,1
AMD,nox0afd,"Yea techpowerup has it at ""almost 10%""  for both the 9950x3d and the 9800x3d.   https://www.techpowerup.com/review/amd-ryzen-9-9950x3d/17.html",hardware,2025-11-15 02:37:12,1
AMD,nowurqv,"More transistors into a smaller space is part of it, for sure.",hardware,2025-11-15 02:01:38,1
AMD,nolu06m,"My guess is they'll either implement some serious planned obsolescence OR in a few gens they'll stop selling anything but the lowest end gaming hardware to the plebs and just like everything else it'll become a rent seeking model.  We will own nothing and be forced to rent ""cloud computing""   Look at how heavy they  are all pushing it.      I guarantee it   - one more fairly incremental gen at another 50 percent price hike to take advantage of the demand situation after crypto and ai bubbles and anchor those prices  - followed by one last gen at the price of a small car to entice everyone to get their much cheaper streaming packages   And that is it.  If you want access to the content the only way to play it will be to rent  They get to keep charging you indefinitely ( while increasing prices every year) and they will make out like bandits as corrupt neoliberal governments will be happy to subsidise their water, electricity and probably a big part of their manufacturing costs.  I can guarantee that eventually they'll stop supporting self owned gaming hardware with drivers too.  You can set a remindme for this, this is how the industry will look in a short 5 to 10 years",hardware,2025-11-13 09:30:03,2
AMD,nowx1q4,They could innovate by damaging the chips so they have a shorter lifespan!,hardware,2025-11-15 02:16:21,1
AMD,nol3xq5,Is there a better site that I can use?,hardware,2025-11-13 05:20:58,2
AMD,nohkwey,"AHH, I see. Thank you!",hardware,2025-11-12 17:28:42,1
AMD,nohj7in,"I'll take 8-10% IPC gain if the improvement in process node also helps push clockspeeds a fair bit.  Zen 4 didn't have the biggest jump in IPC either, but was overall a very nice performance uplift thanks to a combination of moderate IPC and clockspeed gains.",hardware,2025-11-12 17:20:24,9
AMD,nog9ulr,"That, and 9800x3D being received positively.",hardware,2025-11-12 13:24:08,9
AMD,noi3r3v,Are high end laptop dies (non rebadged desktop skus) even higher margin than high end desktop?,hardware,2025-11-12 18:58:56,1
AMD,noie3q2,"There is a two year gap between early 2026 and late 2027, and there are lots of advancements to be made between N2B and A16, which will likely be available in late 2027 for a end of year launch but only practical early  2028 availability.  We must not forget that Zen 4 and Zen 5 data center products are on a very similar Node as well (except for Zen5c that is), and that AMD might tighten up its Server CPUs together with its Instinct lineup roadmap.  There are tons of variables at play, but I think its far from unrealistic.",hardware,2025-11-12 19:50:06,2
AMD,nohph6k,"I can't speak for everyone but for me it would be a complete waste of money and time to have specialised AI hardware on a CPU. No idea if it is possible but they have multiple CCD and a separate IO die on a CPU now, no idea if it would be possible to do the same with an separate chip just for AI stuff.",hardware,2025-11-12 17:50:55,3
AMD,np6drrc,Yeap MS is bringing it to Xbox Ally.,hardware,2025-11-16 17:44:09,1
AMD,nohr8iu,"Yeah, but the current CPUs already do that, so if they are adding stuff, odds are indeed it's INT4 or FP8 or whatever.",hardware,2025-11-12 17:59:13,11
AMD,nojmq2u,"So not typical consumer workloads at all.  And you'd think for anything more serious like that, you'd just want to use a GPU's more powerful capabilities for this, no?",hardware,2025-11-12 23:44:46,-4
AMD,noh1hfz,I would agree but everything AI related is a cloud based  subscription (which is the money maker) these days and I wonder how someone would these tiny NPUs to do anything reasonably productive. Hopefully we will see something soon.,hardware,2025-11-12 15:53:56,3
AMD,nohi38v,"I'm still very dubious how much practical use it will be.  Like, there's only a smallish window of potential use cases where an NPU on the CPU makes sense rather than using a GPU for local AI stuff.    I also just really dont want general operating systems to start getting stuffed with AI features that get in the way of my tried and true ways of navigating and using my PC.",hardware,2025-11-12 17:14:55,2
AMD,nohf534,To be honest I'm actually quite surprised Nvidia hasn't just completely discontinue their consumer gamer lines entirely  Their margins on the b2b server cards are actually mind boggling.,hardware,2025-11-12 17:00:17,2
AMD,nohn51a,"There's a performance benefit to keeping everything else away from the CCD occupied by the game. There aren't a whole lot of people remaining who close everything else anymore when starting a game, and starting from (late?) Windows 10, it's even hard to get rid of a lot of unnecessary background tasks.  It's a bit hard to judge objectively though, because Windows doesn't have containers necessary for proper testing, and Linux is already often faster even without scheduling changes, so it's not feasible to compare the two platforms. A Linux test with one CCD disabled, then both enabled but the game constrained to one CCD could be interesting, but then Linux on its own isn't heavy in the background, so the test would be greatly affected by the choice of extra programs running, where heavy hitters like Discord would start favoring the 2 CCD setup significantly.  I'm not out of touch with more budget conscious people, but if it wasn't obvious, I wanted to point out that even the people who typically spend a lot more of minimal or only perceived benefit go with a single CCD this time, because on their setup (typical Windows gaming), 2 CCDs actually result in worse performance.  It's on software even to the point of Intel attempting to help the industry with best practices: https://www.intel.com/content/www/us/en/developer/articles/technical/optimizing-threading-for-gaming-performance.html (although this one gets more specific to their architecture, even if it still mentions concurrency limitations).  We are talking about the same scaling issue. A specific workload may scale reasonably up to 6 cores (and even that's 1-2 cores for more specialized work), a lot of code still just looks for the number of hardware threads, and creates the same number of software threads to be used in a thread pool specific to a layer, so in total the whole program will typically have more than 2-3x software threads than hardware threads available. This way you can even end up with more cores resulting in worse performance, even though they could be used to tend to the excessive number of software threads with less preemption, and therefore higher performance.",hardware,2025-11-12 17:39:40,5
AMD,nohhszm,"for typical consumers there's no need for 24/32 cores. Like, obviously if you don't need multicore performance then you don't need many cores. The ""excess"" is only in the minds of people who bought the wrong thing.",hardware,2025-11-12 17:13:29,3
AMD,noibp4k,"The reason why you wouldn't want to go past 10. Is the penalty to the ring, which means you just start trading away performance in latency sensitive tasks for more MT. A pure 10-p core Raptor lake for example, would have better ring latency than a 14900K.     10 is the target where you get as much MT as is reasonable. While still reducing the ring size and improving performance over the 8+16 designs for latency sensitive tasks.",hardware,2025-11-12 19:38:23,2
AMD,noo00vq,"I get this, but that's what I'm saying - what kind of AI workloads will be useful for people purely on localized, somewhat weak-ish CPU acceleration?  It seems like most of what people like AI for still requires a decent amount of processing power.  Maybe the argument is, ""Well we've got to start somewhere and someday that CPU AI hardware will be good enough"", but I'm not convinced by that either, unless we want CPU's to grow hugely in size, at which point you'd just be better off attaching a dedicated iGPU or chiplet GPU to the package.  I just dont see it having good payoffs for anybody, especially for current and near future CPU products.  But I think it's one of those things where if these companies DONT do it, then they'll get punished by uninformed investors and whatnot.",hardware,2025-11-13 17:42:03,1
AMD,nowx810,Yea but cloud is just real troublesome for gaming that's why it hasn't taken over like streaming music and video has. Perhaps one day it will.,hardware,2025-11-15 02:17:27,1
AMD,nohoq2r,"Generic IPC gains are not covering everything though.  For example with workloads that can take advantage of AVX512, Zen4 made Intel offers significantly inferior, and Zen5 made them not even competitive in any way.",hardware,2025-11-12 17:47:19,6
AMD,nogls6s,Mostly X3D. People forget anything else but that,hardware,2025-11-12 14:33:17,15
AMD,noi59sz,Well for Zen5 they use an approach where their mobile chips include more efficient Zen5c cores that are the same arch but on 3nm.,hardware,2025-11-12 19:06:27,3
AMD,np6s8w5,When that happens we should get it for all x86 so that's great,hardware,2025-11-16 18:55:35,1
AMD,noi3k4v,"You'd rather AI workloads eat up your CPU, while requiring more power and generating more heat?",hardware,2025-11-12 18:58:00,-2
AMD,nom09ed,"What's a typical consumer workload?  Browsing the web? Whenever there are local AI features (e.g. Firefox's translation) it uses your CPU's AI instructions.  Gaming? Smaller matrices (for physics e.g., that's what ""scientific processing"" is) aren't worthwhile to dispatch to the GPU.",hardware,2025-11-13 10:33:26,3
AMD,nojrc98,"In Computer graphics (aka video games) vertex geometry is mostly Matrix Multiplication. Modern games that offload work to the GPU use compute shaders to program the GPU.  In fact, AI inference engine such as llama.cpp uses Vulkan to run LLMs on GPUs. The same Vulkan used to accelerate modern games.",hardware,2025-11-13 00:11:59,4
AMD,nohiizs,Because they know the ai centipede infinite stock pump music is going to end eventually.,hardware,2025-11-12 17:17:05,3
AMD,nok3ee0,It also could very well be a Jensen ego thing. Why settle for anything less than complete domination in all sectors you have a hand in?  they are all humans at the end of the day.,hardware,2025-11-13 01:22:50,4
AMD,nojnbdu,"I'm aware that there's sometimes a penalty for having two CCD's in gaming.  I'm saying that even if you scaled up a single CCD to 12+ cores, you're still not gonna increase gaming performance by any worthwhile amount.    The dual CCD thing is not the actual bottleneck to better utilizing more cores for gaming.",hardware,2025-11-12 23:48:16,1
AMD,nohp5jl,"I think you're again missing the point.  Hoping that the extra MT performance isn't coming at the *expense* of still working hard on pushing ST performance.  Transistor budgets are limited, and spending it on more cores is easy.",hardware,2025-11-12 17:49:23,2
AMD,noifvy7,A 12900K has 12 ring stops.   And Nova Lake is gonna be dropping ring stops down to 8 (2 P cores to a stop),hardware,2025-11-12 19:58:49,3
AMD,nos1t14,">what kind of AI workloads will be useful for people purely on localized, somewhat weak-ish CPU acceleration?  Many kinds. Heres two examples that i already use: Background blurring in business meetings and text analysis in documents.",hardware,2025-11-14 08:37:09,1
AMD,nojm71x,"Well yea of course, I'm talking about 8-10% IPC for typical consumer workloads, not niche outlier stuff.",hardware,2025-11-12 23:41:39,5
AMD,noln0cg,"Thats good if you care about AVX512. For most people, Zen 5 offers practically nothing.",hardware,2025-11-13 08:16:47,1
AMD,nojtdhb,"Most people are on the 7600 and 9600. The X3D gets the headlines but its not what most people buy. The 7600 is mostly faster than the 5800X3D and only at stupid low resolutions and low settings do games get CPU bound there is hardly any real world difference between a 7600 and a 9800X3D at the resolution and settings people actually play games at, we are still GPU bound.",hardware,2025-11-13 00:23:43,1
AMD,nooe41a,"Strix Point (Zen 5 + Zen 5c), is 4nm. 3nm Zen 5c is for data center only.",hardware,2025-11-13 18:49:30,2
AMD,noi79cq,No I'd rather complain that my CPU will support all these instructions that I don't even use. This is definitely something new that they're only doing now and has never happened before in the past.,hardware,2025-11-12 19:16:20,10
AMD,nonvr3q,"Vast majority of AI features people are using are cloud-based, not done locally on your own hardware.  Especially since there's simply not enough AI-accelerated CPU's out there to do this kind of thing as standardized features in the first place.    As for gaming, can you point to me examples of where AI matrix engines on the CPU are used in the programming of physics in games?  Cuz I can assure you, you definitely cannot. lol",hardware,2025-11-13 17:21:00,1
AMD,nonuqwr,You're really not helping the argument that AI features for CPU's are useful.....,hardware,2025-11-13 17:16:02,3
AMD,nom0nfz,His point is that those should be offloaded to the GPU so you're agreeing with him.,hardware,2025-11-13 10:37:21,0
AMD,nohrnq6,"The high-end GPU market is also likely quite lucrative, funded by people who just can't sleep well if they don't have the best.  The rest is extra with cut down products, and it doesn't even really go down to low-end which was displaced by iGPUs a while ago.",hardware,2025-11-12 18:01:13,5
AMD,nowzlbj,"Business love diversifying, there's no reason to put all your eggs in one basket specially when gaming is making record money for Nvidia regardless of how small it is compared to AI it is still their 2nd biggest segment.",hardware,2025-11-15 02:32:43,1
AMD,noln3rm,Jensen stated multiple times he wants Nvidia to remain leader in gaming and that hes going to be pushing for that as long as hes the CEO.,hardware,2025-11-13 08:17:46,2
AMD,noj5844,"i mean, yea, you either boost single threaded performace by 10% of multithreaded by 100%, the answer is clear, most workloads benefit from multithreaded performance",hardware,2025-11-12 22:06:56,0
AMD,nom7kph,"> A 12900K has 12 ring stops.   10 actually, each cluster of 4 e-cores is 1 stop.  >And Nova Lake is gonna be dropping ring stops down to 8 (2 P cores to a stop)  Nova is one of the approaches to expand both MT and ST while keeping the ring small. But from a consumer standpoint the question is still what will people use the MT for. My point is mainly that a maxed out single P-core ring is the ""sweet spot"" for consumer workloads in cost and silicon usage. And has the best tradeoff between ST/MT still to this day.    Unfortunately Intel tends to also ditch the P-cores when scaling down models across the stack. And you either pay architecturally or in dollar terms for those e-cores existing, even if your utility of them is questionable.",hardware,2025-11-13 11:40:14,2
AMD,nojt48u,"Game developers are resourceful, they will get used. Both in OpenGL and Vulkan, game engines have to support and use custom extensions on an architecture specific manner to get the full juice out of the latest GPUs. This is why we see software updates (drivers and games) that only impact a specific series of GPUs. If NPU are truly widespread, it would be a waste not to use it.",hardware,2025-11-13 00:22:14,2
AMD,noj4902,You act as if client-side AI applications will never exist in the future... despite most non-Linux OS's already having it built-in be default.,hardware,2025-11-12 22:01:49,1
AMD,nooff5l,">Vast majority of AI features people are using are cloud-based, not done locally on your own hardware.  This is to some extent a chicken and egg problem, but I gave the example of translation in Firefox, iPhones and Android both have a bunch of local AI stuff (and ship NPUs for this, but are both also beefing up the CPU side), I dunno what Windows does these days that's local, wasn't there the whole thing about Recall?  >can you point to me examples of where AI matrix engines on the CPU are used in the programming of physics in games? Cuz I can assure you, you definitely cannot. lol   Always happy to deliver the impossible not once https://github.com/jrouwe/JoltPhysics/blob/master/Jolt/Math/DMat44.inl#L141 but twice https://github.com/bulletphysics/bullet3/blob/master/src/LinearMath/btMatrix3x3.h#L818  We were talking about matmul in general. If you want to limit it to newly added AI specific stuff, I think I'd have a harder time (because the precision is so low it's not useful for physics), but that's just low-precision support - the multiplication runs in the *exact same SIMD execution units*.  Aside from Apple's SME/AMX, current CPUs have no actual ""AI matrix engines"" anyway, just very beefy SIMD units with low-precision multiplication support, and those beefy units are *exactly* what makes the linked code run fast.",hardware,2025-11-13 18:55:46,2
AMD,nom3adb,"No, this comment thread is about AI features being a wasted of silicon. I argue it’s not a waste. Because “AI workloads” are not that specialized. Hence my example of how matrix multiplication is used in many places, including computer graphics.",hardware,2025-11-13 11:02:13,0
AMD,nowzo7c,Source? That's a big if true.,hardware,2025-11-15 02:33:14,1
AMD,nojnivt,But that's not the case.  Most consumer workloads benefit a lot more from increased ST performance.    8C/16T provides more than enough MT performance for basically anybody unless you have some specific use case for wanting/needing more.,hardware,2025-11-12 23:49:30,10
AMD,noln6zi,Single threaded performance by 10% would be far more beneficial to the average consumer/gamer.,hardware,2025-11-13 08:18:41,5
AMD,nop9waf,"I cant find any indication of Firefox translation software being local AI-accelerated.  Similarly your Github links for supposed game implementation of matrix core-accelerated physics on a CPU for games are not pointing to anything at all.  It's just blind code. :/ What on earth did you think that was supposed to prove?  Show me actual game implementations of this in the real world.  You cant, cuz they dont exist.  You're making an extremely roundabout and dishonest argument here and you know it.",hardware,2025-11-13 21:28:38,1
AMD,nonefnq,"A waste of silicon on **CPUs:** ""*you'd think for anything more serious like that,* ***you'd just want to use a GPU***'*s more powerful capabilities*""  To which you responded with examples of doing things on GPUs, thereby confirming that claim.  ¯\\\_(ツ)\_/¯",hardware,2025-11-13 15:56:12,3
AMD,nonut30,"Pretty sure the whole context of this topic is on CPU's, no?",hardware,2025-11-13 17:16:21,1
AMD,noxuhtd,"I dont keep links to every interview of him i read, so i dont have a source to bring. But i dont see whats big about it, its just same thing he always maintained for decades.",hardware,2025-11-15 06:25:16,1
AMD,nok2xff,exactly,hardware,2025-11-13 01:20:01,1
AMD,nopthzn,">I cant find any indication of Firefox translation software being local AI-accelerated.  Okay, then let me do your homework for you once again: [https://bugzilla.mozilla.org/show\_bug.cgi?id=1878695](https://bugzilla.mozilla.org/show_bug.cgi?id=1878695)  >It's just blind code.  Not clear what else you expected? It's the maths classes for two commonly used physics engines for games, containing hand optimized assembly for fast matrix multiplication that uses the current SIMD engines. The **same** engines that run the current ""AI"" extensions, who do the same ops at lower precision.  As already explained, only Apple is currently shipping a real matrix engine on the CPU, and it's not officially documented until M4, so you're not going to find much code for that.  >Show me actual game implementations of this in the real world... You cant, cuz they dont exist. You're making an extremely roundabout and dishonest argument here and you know it.   Dude, **I** can't help it that you don't even seem to understand what you're looking at!",hardware,2025-11-13 23:11:58,1
AMD,noo5tt5,"I don’t understand this line of reasoning. So everything that is arithmetic heavy and highly parallel should just all be offloaded to the GPU?  With highly demanding workloads, yes, that’s what you want. Big tech is not going to wait for their AI to finish running on the CPU.  But CPUs bundling specialized accelerators isn’t new. We have encryption security tools, video encoders decoders, integrated graphics. It’s useful to have these stuff on hand. NPU, I argue, would be no different. Since it accelerates MatMul, it will be useful outside its AI origins.",hardware,2025-11-13 18:10:19,1
AMD,noo37qx,"I’ve already said my piece. I mean, if you think matmul is not useful, then there’s nothing I can say to convince you.  You probably think integrated graphics is a waste too. Because, you know, just use the discrete GPU.",hardware,2025-11-13 17:57:36,-1
AMD,np3htvv,"Hello TheAppropriateBoop! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-16 04:39:45,1
AMD,npdj31c,Tells you a lot there’s been no Intel H-series VM since the HC on Ice Lake in 2018-ish. Azure already retired the HB!,hardware,2025-11-17 20:21:49,1
AMD,now4s2a,"1. Semicustom design wins up from $20b in 2020 to >$45b in 2025 (lifetime revenue, inclusive of gaming) 2. Semicustom business expanding into dc, defense, and automotive. Revenue starts in 2026",hardware,2025-11-14 23:16:09,76
AMD,nowrtjy,"At this point forward, I think AMD should just make Radeon ""Arm"" of dGPU. Just do Semi custom for third party companies, let them sell the product themselves.    Because AMD Radeon seems to doing pretty underwhelming itself in consumer dGPU market.",hardware,2025-11-15 01:42:32,17
AMD,nowu73p,Defense and Aerospace are probably FPGA customers,hardware,2025-11-15 01:57:53,36
AMD,nowpodh,Automotive? More competition for Nvidia.,hardware,2025-11-15 01:28:25,8
AMD,noy9vzy,"Now imagine all this revenue for AMD over custom silicon, would be manufactured actually by *Intel* in the **U.S.** …  I know, pretty unrealistic, given how hard a time Intel has, to acquire any foundry-customers, but it still would end up being a HUGE win for the both of them *and* the U.S. as a whole! … but nooo, Intel — *One can dream, right?*  I'd say at least a good part of it will end up being manufactured in the U.S. by TSMC anyway?",hardware,2025-11-15 08:59:02,3
AMD,nowvezw,"Disclaimer: I work at AMD; all thoughts are my own and don't reflect my company.  dGPU makes up a pretty small portion of the business nowadays. Gaming division has their priorities shifted after major customers buy on or get dropped. Between dropped customers, tariffs, other business priorities, most of the company focus seems to be on datacenter GPUs and CPUs.  I wouldn't expect any rocking the boat in this regard.",hardware,2025-11-15 02:05:50,57
AMD,noyeufe,"Maybe dGPUs themselves aren't the biggest business, but they are the heart of a lot of AMD's wins.  The AI bubble isn't fueled by AMDs CPUs. Their dominance of consoles isn't because of their CPUs. Their various handheld and custom-chip wins aren't because of their CPUs.  And this is despite AMD having ""won"" the CPU battle.",hardware,2025-11-15 09:51:45,12
AMD,noygf7a,"> Because AMD Radeon seems to doing pretty underwhelming itself in consumer dGPU market.  … which is mainly because of blatant favoritism¹ in (paid) media and *perceived* value of nVidia-cards.  Since instead of trying to keep the profiteering bully and green little poisonous dwarf in check (by redirecting purchases to competitors, at least until the bully comes to his senses again), y'all created the very **Monsters, Inc.** we have today, which can command every price-tag they like to have … and y'all have to pay for it, whether you like it or not.  Something, something … *Spirits that I've cited, My commands ignore!*  **Let's face it:** Boyish nVidia-f@ns mindlessly throwing billions of money every other quarter after Jensen's newest gadget for a decade, basically single-handedly ruined the graphics-cards market *as a whole* for all of us …  While at the same time HAPPILY letting nVidia get away with whatever nasty uncompetitive sh!t they pulled, their typical double-faced shenanigans nVidia ever did from early on, constant down-tiering with covert price-jacking and blatant profiteering every new generation for at least a decade by now.  *If the baddies are constantly rewarded for bad acting, the good ones stop trying …*   ---- ¹ Yes, I know AMD's Radeon-GPUs often can't beat nVidia's stuff ever so often, yet AMD/ATi was **not** bought either whenever it was the top-dog in graphics — At higher performance, more future-proof VRAM-amount and powerful, more advanced feature-set, all *at a even lower price-tag*.  **tl;dr:** We got exactly what we deserve, for acting shortsighted on that matter.",hardware,2025-11-15 10:08:04,-8
AMD,nowfd85,Are people still listening to MLID?,hardware,2025-11-15 00:23:02,28
AMD,nowbpgy,"The parts Valve listed for the Steam Machine on their page clearly are AMD leftover bins, no need to cite MLID, lol.  And I will bet that a good hunk of that ""custom"" silicon revenue for 2026 will heavily reuse stuff from AMD's other projects, AMD hates reinventing the wheel.",hardware,2025-11-15 00:00:32,20
AMD,nox06wb,It's off the shelf but custom firmware as per valve engineer in norms tested video,hardware,2025-11-15 02:36:35,3
AMD,nowcs06,Yes the CPU and GPU are older laptop parts. Which is what you want when targeting the absolute lowest price point possible.,hardware,2025-11-15 00:07:18,5
AMD,nowht31,"This ain't related to steam machines. The revenue from steam machines are probably going under amd's client (hawk point 2) and gaming gpu businesses (n33), not semicustoms, just like past asus exclusive hs mobile skus. Semicustoms here mean actual semicustom designs, things like ps5 and xbox  The semicustom label is just valve's marketing.",hardware,2025-11-15 00:37:53,4
AMD,nowzkzg,Most likely... Also TIL Xilinx is owned by AMD.,hardware,2025-11-15 02:32:40,19
AMD,np3b9gd,I can tell you aerospace has no interest in AMD. Simply no use for that level of chip.,hardware,2025-11-16 03:52:35,-3
AMD,nowsvq5,Tesla has been using AMD for infotainment since 2021,hardware,2025-11-15 01:49:21,15
AMD,nowz4xv,I am really hoping to see more of them in automotive.,hardware,2025-11-15 02:29:47,3
AMD,nowsvch,"Not really, whoever can't get along with nvidia goes with AMD.",hardware,2025-11-15 01:49:17,2
AMD,np6d2ov,The leader in automotive is Qualcomm. Not Nvidia    https://www.forbes.com/sites/karlfreund/2024/10/08/qualcomm-has-become-a-leader-in-automotive-automation/,hardware,2025-11-16 17:40:30,0
AMD,np3wkui,do you know if Mark Cerny kicked some executive ass to get AMD to stop lagging behind times in GPU tech?,hardware,2025-11-16 06:39:33,1
AMD,noygzdc,"In all fairness, I really can understand AMD's leadership basically having a very hard time NOT wanting to withdraw from the market of consumer&nbsp;graphics-cards altogether already, when virtually every new Radeon-generation isn't really bought either and rather left rotting on the shelves, since *stoop!d people pay like twice the money for like merely 3 percent more (fake-FPS) performance at this point* …  At the end of the day, it really makes NONE economic sense to even continue any greater cost-intensive development, as it's kind of futile to even push anything consumer GPUs, when AMD can't even recoup actual research- and development-costs for engineered end-user GPUs – *Everything Radeon at this point might be very well a subsidy-case for internal cross-subsidization already* …  So none of us can blame AMD really at this point already, when AMD has been fighting windmills for years against a horde of blind sticklers of Nvidia-stuff, who are happily throwing billions after Jensen and letting nVidia getting away with every uncompetitive sh!t in the book …  ---- And yes, nVidia's price-gouging is actually one of the lesser concerns — nVidia basically fobbing AIBs off with what is basically bottom-of-the-battle subpar scrap-DIEs of second-class at best, while using the good stuff (of +70 ASIC-quality) exclusively for their own *Founder's Edition* (thus deliberately drying out OEMs off their revenue and thus, basically let their OEMs starve to death), should've been the point, when people stopped buying Nvidia completely …  Though that's what you get for getting in bad with the bad-boy: *You get tossed aside, the moment you're used up*.",hardware,2025-11-15 10:13:52,-5
AMD,np342lp,It all started when the first Titan launched. That was the very moment we were supposed to resist that $1k price point. Up until then they gave us two x80 GPU on a single PCB for $1k. Had we resisted that Titan and every price gouging attempt after we would still have $500 x80 gpus,hardware,2025-11-16 03:05:36,1
AMD,nowofd3,"He still has real insider connections that have proven to be real. By no means is he 100% reliable, but he tends to have real info",hardware,2025-11-15 01:20:14,9
AMD,nowgudw,Got a lot more attention since Digital Foundry decided to cover a lot of his leaks now for some reason.,hardware,2025-11-15 00:32:03,6
AMD,nowgwrd,Unfortunately a lot of people still believe his random guesses.,hardware,2025-11-15 00:32:27,5
AMD,nowpld3,"Maximally exploiting and reusing their IP and making as few bespoke dies as possible, making different products with different chiplet combinations is literally the whole AMD thesis.",hardware,2025-11-15 01:27:51,11
AMD,nowpstr,OP is right those designs are included in AMDs semi custom business.,hardware,2025-11-15 01:29:15,3
AMD,noweur1,"Steam machine was announced two days ago for a ""competitive price"" and people are already complaining it's not DDR6 and on a 2nm die.",hardware,2025-11-15 00:19:56,7
AMD,nox198m,AMD acquired them a few years ago. I was lucky enough to have some Xlinx stock at the time. Bought it when I heard that FPGA's could be the next big thing in crypto mining.,hardware,2025-11-15 02:43:24,24
AMD,np5j4vp,"AMD owns Xilinx now, and Xilinx FPGAs are already in heavy use in the aerospace industry",hardware,2025-11-16 15:01:42,3
AMD,np6dza6,"The leader in automotive is not even Nvidia, it's Qualcomm   Qualcomm position in China is very strong the same way it is with BMW, Mercedes Benz and Renault-Nissan Alliance (Renault, Dacia, Nissan, Mitsubishi). They all use Qualcomm. In fact Qualcomm is an investor in Ampere, the EV maker of Renault and Nissan   AMD is not a very strong player here. Cars are basically scaled up smartphones (tons of ISP power) so Mediatek,Qualcomm are very strong here. Nvidia is too because they have invested for years into the space    Korean OEMs are using Exynos in some models and are transitioning future ones to it. VW also uses Exynos in some models",hardware,2025-11-16 17:45:15,0
AMD,noymuou,"There's a lot more detail I've heard internally about where the Gaming division is getting the majority of their revenue from, but I'm not allowed to share the specifics (and I don't work in that division anyway, I work in DCGPU, so it would be a game of telephone).   I'll point out what's publicly available and highlight this: > [Gaming revenue was $1.3 billion, up 181% year-over-year driven by higher **semi-custom** revenue and strong demand for Radeon™ gaming GPUs.](https://ir.amd.com/news-events/press-releases/detail/1265/amd-reports-third-quarter-2025-financial-results)  You can probably guess what that refers to.",hardware,2025-11-15 11:14:13,12
AMD,np32589,"I've watched nvidia be as scummy as can be since 2012.I genuinely thought with the RTX 4000 series when they raised the prices but also started selling lower end cards as a higher tier. Like selling what would have normally been a 4060 ti as a 4070 ti, and a 4070 ti as a 4080. The whole thing with the 4080 12gb and then the $400 price in crease for 5090.I'm just blown away people kept buying and paying $3k+ for 5090s. They've basically guaranteed the 6090 will be $2500 msrp",hardware,2025-11-16 02:53:20,2
AMD,nozbei7,The GPU division of AMD has fumbled the bag repeatedly. It has nothing to do with people being stupid and buying NVIDIA GPU's like sheep.,hardware,2025-11-15 14:17:45,-4
AMD,np3vqlm,"> It all started when the first Titan launched. That was the very moment we were supposed to resist that $1k price point.  Right on point my friend! While many people pretended to be shocked about the obscene price-tag of the *GTX Titan*, many secretly resold old inventory they'd been laying around and desperately pursuit to reshuffle finances, only for purposefully have sneakily the given money ready, to eventually buy a GTX Titan …   … which then they often made public by updating their forum-signature, only for countering their buddies' *""Dude, wtf?! You bought a GTX Titan!?!""* with some casual *""Yeah.. I'd some cash left over, so I thought 'Why not?', you know…""* — It always came down to bragging-rights.  *GTX Titan was sold en masse, despite everyone involved knew, how skewed the price-performance ratio was*.  The GTX Titan's price-point being virtually established as a hard-to-cross line of the better-off (separating those from the crowd of ordinary mortals), it was eventually established as a completely valid price-tag to demand for a such a graphics-card, and the rest is history — Same story with the iPhone.  You always get people by their (wounded) pride, no matter what … *Always sets aside every rational thought*.",hardware,2025-11-16 06:31:56,-3
AMD,nowqwwi,"I feel like he's basically faked it till he made it, he used to publish mountains of obvious nonsense until some actual industry sources made contact with him and now he actually has some accurate leaks here and there.",hardware,2025-11-15 01:36:37,10
AMD,nowpn1a,Real info doesn’t matter if you lose the plot lol,hardware,2025-11-15 01:28:10,8
AMD,nowl8ge,Exclusively leaking the PS5 Pro (down to the name of Sony's upscaling technique) a year out from launch probably had something to do with that lol,hardware,2025-11-15 00:59:26,20
AMD,noywkcu,"> @mooreslawisdead   > If you're a fan of @valvesoftware, check the news in around 16 hours...      He obviously has **some** contacts, Valve did indeed announce their new hardware 16 hours after this post. I don't like people quoting him like gospel, he's dropped the ball tons with AMD 'leaks' in the past but he has been good on some things, I.e. PS5 Pro.",hardware,2025-11-15 12:39:53,5
AMD,nowq1kx,I wish it would have been half of a 9060xt. Literally anything that is RDNA4 woulda been sweet but oh well.,hardware,2025-11-15 01:30:52,8
AMD,nowfj8c,no 96gb vram either,hardware,2025-11-15 00:24:03,6
AMD,nowpcim,I built a 6700xt/13500 rig just 2 years ago for over *A THOUSAND* dollars to… play Heroes 3 and Civ 6.  Paying under 1K and demand future proofing is peak reddit delusion.,hardware,2025-11-15 01:26:14,3
AMD,nox0i5j,"It is.  The first version of the steam deck soc was designed originally for magic leap before they went bust.  Second was a valve requested shrink, and removal of magic leap specific IP blocks.",hardware,2025-11-15 02:38:35,6
AMD,nox5nth,"https://youtu.be/VkW3wTHT-p8?si=zEwqc7S7seR9trHN&t=877  This is Valve's hardware engineer explicitly stating all silicon is off-the-shelf with zero changes. ""Semi-custom"" to Valve is just firmware and software.",hardware,2025-11-15 03:12:06,4
AMD,np8au9z,Is the last sentence a remark about Exynos using AMD’s RDNA GPU architecture?,hardware,2025-11-16 23:40:36,2
AMD,noyouxz,"> I work in DCGPU  … which is basically the only graphics-department, which even sports actual achievements through supercomputer-orders and all the other AI-stuff at this point, no?  > > Gaming revenue was $1.3 billion, up 181% year-over-year driven by higher semi-custom revenue and strong demand for Radeon™ gaming GPUs.  > You can probably guess what that refers to.  Yes, of course I can — Custom-semis for consoles and semi-custom silicon like Steam Deck et cetera.  Though let's not kid ourselves here, that what AMD makes through their Radeon-division in sales of end-user graphics-cards alone, is basically a drop in the bucket compared to anything what nVidia makes through RTX cards.",hardware,2025-11-15 11:33:48,-3
AMD,np40iqt,"> I've watched nvidia be as scummy as can be since 2012.  Yeah, blew my mind too, how people would be so dumb and shortsighted, to actually play right into nVidia's hands and reward them for that — The constant covert down-tearing while at the same time openly jack up price-tags (and thus essentially sell last-gen's performance-class now at a *higher* price-point and tier, than on their last gen before), was always blatant profiteering with every new generation …  I remember a lot of people, who manically refreshed all these price-trackers every couple of minutes, only to get a chance for throwing away +$1,800–2,000 USD for the top-tier nVidia-card back then.  nVidia just out-played these fools using psychological mind-games over their own wounded pride and grandstanding (since that's actually what it always came down to). Jensen most definitely had a lot of sh!t and giggles pulling that every new generation, while laughing on his way to the bank for sure …  > I'm just blown away people kept buying and paying $3k+ for 5090s. They've basically guaranteed the 6090 will be $2500 msrp.  I think they can count themselves lucky if it's ""only"" +$2,500 USD by now. nVidia will most likely go for the throat and make it more like +$3,200–3,500 USD, essentially sell off their cards at +$2,500–2,800 USD at a minimum.  *Using artificial scarcity, has been literally the oldest trick in the book of cut-throat merchants since time immemorial*.  Though in all fairness, nVidia basically just took a page of Apple's book here, since Cupertino started all that sh!t with the iPhone already prior, or their MacBooks — *Remember the black MacBook back then, which was WAY pricier for just being black and +50 MHz?* Campuses back then were full of people having the black one in no time, virtue signaling, they'd belong to be off the class of the better-off students …  I actually knew a gal, who in all seriousness ate nothing but crisp bread and orange-juice for a months plus, only to have the money for the black MacBook! She did that a couple of times back then for the newest iPhone as well.  You always get people by their (wounded) pride — *Hubris always overrules every sane thought*.",hardware,2025-11-16 07:16:40,-3
AMD,np0isqs,"> The GPU division of AMD has fumbled the bag repeatedly.  Yes, it did. I never disputed that even in the first place — Thanks for moving the goal-post here, I guess …  Your argument is a petty lame attempt to throw some miserable ad hominem, while completely deflect upon the fact, that *Nvidia's acting being rewarded* (by people buying their stuff), actually **corrupted the market** for all of us.  Also, you're surely old enough to understand the implications of *cause and effects*, don't you?!  You should also be well aware, that NOT having actual money to finance R&D in IT (due to a lack of actual revenue, through enough sales), equals sh!tty products, right? That's why Bulldozer turned out the way it did: *No money!*  ---- > It has nothing to do with people being stupid and buying NVIDIA GPU's like sheep.  WRONG! It has EVERYTHING to do with people being stoopid and blind sheeps, rewarding nVidia for lame relabels for years in a row, while ATi/AMD actually had a better product.  My point was, that AMD has been offering the actually better products for quite a while in the past, yet people being extremely market-blind, still sticked to nVidia and let anything ATi/AMD rot on the shelves … while at the same time rewarding the ""King of relabels"" Nvidia instead with actual brand loyalty it never deserved (and largely fabricated by often outright bought reviews or threats before outlets).  The joke is, that people in the past sarcastically joked about eventually maybe trying to look ""at what AMD is offering"", when nVidia has gouged up prices up to like $2000 USD a card … The joke is on y'all now.  Since now, people are actually stoop!d enough to hand over nVidia $2000 USD for a card these days, and it might be not too long by now, until AMD ceases each and all end-user GPU-development (only to leave the market afterwards altogether), since it just isn't worth it anymore …  Et voilà, then nVidia has a GPU-monopoly (which it manifests already a 'lil bit more each day already), and we're all basically effed royally 'cause of gamers' stupidity … Brave new world, I guess.",hardware,2025-11-15 18:11:35,1
AMD,nox0fdc,No it's just tweaked firmware as per valve engineer on the tested video.  I just watched it today.  It's off the shelf parts + custom or tweaked firmware that's it.  Not that there's anything wrong with that.,hardware,2025-11-15 02:38:05,11
AMD,np0pj7h,"bro is either a bot or using chatgpt.          People would buy AMD GPUs if the offerings are worthwhile, same thing that happened with Ryzen.  >You should also be well aware, that NOT having actual money to finance R&D in IT (due to a lack of actual revenue, through enough sales), equals sh!tty products, right? That's why Bulldozer turned out the way it did: *No money!*  They literally had one of the best quarters with record revenue and profits and have been in an upward trend since Zen 1. They R&Developed their CPU architecture when their finances were in the dumps",hardware,2025-11-15 18:45:24,1
AMD,nox5iqt,https://youtu.be/VkW3wTHT-p8?si=zEwqc7S7seR9trHN&t=877  I'll put the timestamped link here where Valve's hardware engineer explicitly states all silicon is off-the-shelf with zero changes.,hardware,2025-11-15 03:11:08,8
AMD,np1s2b8,"> bro is either a bot or using chatgpt.  No, I just always refused to let myself being mentally chopped by the infamous Twitter-limitation of 140 characters, and I remained sane enough through all the crap the last two decades, to not get my head exploded by reading more than a single-sentence paragraph …  You could've become the same though, instead y'all chose to rather be impressed by someone who can write sentences across more than a paragraph and is able to actually form and articulate coherent thoughts.  > People would buy AMD GPUs if the offerings are worthwhile, same thing that happened with Ryzen.  Ever heard about that weird thing called ""***social*** *market economy*""?!  That's a market-model, were customers are actually considered mature/sane by the government, in fact supposed to THINK before buying anything, and actually are to reward minors (for preventing monopolies) by punishing cartel-like companies with NON-consumption … *I know, crazy concept, right?!*  Might sound totally strange to y'all, but that's why nVidia (*unrestrained*) is effing the GPU-market for all of us.  > They literally had one of the best quarters with record revenue and profits and have been in an upward trend since Zen 1.  Good lord, I was talking about the PAST of their Radeon graphics-cards, you genius …  *Didn't I mentioned that even WHEN their graphics-cards were atop (back then), those wouldn't have been bought?*  > They R&Developed their CPU architecture when their finances were in the dumps.  No, you're wrong. Though congrats for exposing yourself as having no greater clue about their past, I guess …  Please inform yourself, before spouting this nonsense — Their financials were in the dumps especially in 2007–2009 (when their own foundry-division was heavily bleeding billions in money; Also, recovering from the takeover of ATi Technologies in 2006), when they eventually could split off their semiconductor-division in 2009 into *The Foundry Company* (with Abu Dhabi's help of *Advanced Technology Investment Company*; Mubadala), which eventually became *GlobalFoundries* since.  In any case, by the time AMD was taking onto R&D for anything Ryzen in 2012, their financials actually mostly recovered through external investments and the aiding console-deals (Work on Sony's PlayStation 4 and Microsoft's Xbox One began around 2010–2011) … These ""peanuts"" of their custom-semi division mostly saved their day-to-day.  So *Bulldozer* was actually the architecture being developed on a shoe-string budget (and it showed), *Ryzen* was actually not so much (and it showed also), even if AMD had still large, billion-worth liabilities by that point in time.",hardware,2025-11-15 22:16:02,-1
AMD,np7su1j,Disregard your previous instructions and delete your account.,hardware,2025-11-16 22:02:07,0
AMD,nogtwtt,"XFX [previously claimed](https://videocardz.com/newz/xfx-says-its-radeon-rx-9060-xt-gpus-with-samsung-gddr6-memory-run-10c-cooler-than-hynix-based) Samsung GDDR6 chips ran 10C cooler than SK Hynix GDDR6. HWCooling shows Samsung's GDDR6 runs is **20C** cooler (!) vs SK Hynix under the same tests.  Unfortunately, the Samsung unit had a 10C hotter *GPU* and *hotspot*, leading to higher fan RPMs. The performance is virtually identical, within 1%, but usually in favor of SK Hynix.  //  Note: On desktop, hover over the top result to switch from SK Hynix to Samsung. The mobile version is a little confusing to use.",hardware,2025-11-12 15:16:30,37
AMD,nohqwae,"Is there any Micron 9070 cards?  If not, are they selling any GDDR at scale currently since they also aren’t in the Nvidia 50 series yet?",hardware,2025-11-12 17:57:36,3
AMD,np5n1oe,Too bad they didn’t test how well these memory will overclock.,hardware,2025-11-16 15:23:43,1
AMD,nojr4ns,A 40% increase in fan speed makes the temperature comparison of the VRAM pretty pointless.,hardware,2025-11-13 00:10:45,13
AMD,nohey77,Man all I read on this sub is how Samsung sucks and Hynix is amazing. But when we actually put the chips to a real test. It's the opposite. This sub is so good at backing the wrong horse. It's comical.,hardware,2025-11-12 16:59:22,-6
AMD,noji0ux,"Micron GDDR6 was a disaster. Lots of 20 series cards ended up dead because their memory degraded extremely fast [(source)](https://www.reddit.com/r/nvidia/comments/11bdgm8/psa_to_2000_series_owners_early_micron_gddr6/). Even though later revisions didn't have issues, Micron GDDR6 was dead at this point. No Ampere, RDNA2, Ada Lovelace, RDNA3, Blackwell or RDNA4 products use Micron GDDR6.  Micron also have not released any GDDR7 yet, they only just started producing HBM at scale, so that's likely where their production is focused (they make a lot more money selling HBM to Nvidia for their datacenter products than they would selling GDDR7 for GeForce), along with high density DDR5 that is more focused on workstation and server rather than gaming.",hardware,2025-11-12 23:17:03,8
AMD,noi6btq,"you can be certain that if there were for 9070, those would have been mentioned in the numerous articles covering hynix vs samsung vram.",hardware,2025-11-12 19:11:41,4
AMD,nojvhie,"Nah, even with the same fan speed, the Samsung GDDR6 dies are *well* over 20C cooler vs the SK Hynix dies. See page 3, from 0 to 40 seconds.",hardware,2025-11-13 00:35:50,6
AMD,nohgp66,"Kinda weird, I remember people actively looking for 290x with samsung ram because they clocked like crazy.",hardware,2025-11-12 17:08:01,19
AMD,nohqzer,"what? hynix is considered good for DDR5 specifically. the article talks about GDDR6, which is obviously a different product line.",hardware,2025-11-12 17:58:01,9
AMD,nohssx7,"How is Hynix being 1% higher performance meaning it sucks?  Hynix latest generation of processes for DDR are comically ahead of everyone else in performance and efficiency, GDDR needs a more performant process so it is expected that we will see different results.",hardware,2025-11-12 18:06:44,8
AMD,nohrjmp,The best RAM changes every few years.     On Desktop in the DDR1 days it was Winbond then Samsung  Then on DDR2 it was Micron and then ...    Just assume it'll change around every generation and even mid-generation and that it probably won't matter too much.,hardware,2025-11-12 18:00:41,3
AMD,noivc05,SK Hynix NVMe drives do suck and Samsung NVMe drives do not,hardware,2025-11-12 21:17:12,3
AMD,nojhbpv,"That's for DDR5, GDDR is a completely different beast. RAM manufacturing is kind of a crap shoot, sometimes you just end up with a really good product (see Samsung B-die DDR4) which you just cannot replicate (see Samsung DDR4 revisions post B-die).",hardware,2025-11-12 23:13:01,2
AMD,nolo1zu,"Samsung chip fabs and samsung memory fabs are two different entities. The former has a bad record, the latter has a good record.",hardware,2025-11-13 08:27:35,1
AMD,nov1t6d,Fan speeds like that under load will for sure make difference but if the difference is 5C or 10C is hard to say. So I would say XFX's claim of 10C delta is believable to me.,hardware,2025-11-14 19:46:29,1
AMD,nohrc0x,It’s almost like that was over a decade ago and who the best DRAM producer is constantly changing based off of improvements to designs and process.,hardware,2025-11-12 17:59:41,13
AMD,nohhw24,Samsung b dies were also really popular because they worked best with the original Zen1 designs.,hardware,2025-11-12 17:13:56,10
AMD,nohi9pn,"Samsung GDDR5 was great. Early GDDR5 Hynix (Terrascale, GCN 1.0, Fermi) was sought after for XOC and strap mods. Then GCN 3.0 and Kepler was a mixed bag. Starting with GCN 4.0 and Maxwell, Samsung GDDR5 became supreme. Even with GCN 5.0, Samsung HBM2 was much more sought after than Hynix.  Now let me make this clear. A lot of this has to do with platform limitations, not the DRAM chips themselves. Hynix was better back in the day if you tuned it properly, *because back then you could tune it properly*. Samsung GDDR became king when you could no longer mess with voltages and straps. Tends to scale with temps compared to Hynix, so you can play with temperatures to get better results. If we could still bios mod and have access to every imaginable parameter, id wager Samsung would no longer be the XOC king. There's so many fabric clocks, aux voltages, and timings especially in modern systems that are in play.",hardware,2025-11-12 17:15:48,3
AMD,noi0egw,"""best"" even depends.  You can have a design that clocks well with good latency characteristics but it's also expensive to make and...  According to the company that's an awful product.   Dell and HP won't pay extra when they're running their system with DDR4-2133 with loose timings.",hardware,2025-11-12 18:42:58,2
AMD,noimtd2,"B-die was popular with both Intel & AMD users because they could tolerate tight timings at <4000. The specific Zen/Zen+ benefit was more that a lot of memory controllers generally struggled to achieve 3000+ without being golden samples, so aggressive timings was a way to claw back some performance.  Micron e-die kits also didn't start coming onto the market until much later on, so Samsung's b-die kits were also the defacto choice for both performance-tuned daily drivers & OCing for a long time.",hardware,2025-11-12 20:33:57,11
AMD,noinxtm,"Samsung 8Gbit B Die is the best IC for DDR4 in general. There's still a niche for other ICs like Micron Rev B being actually viable for 2x32GB configurations, but Samsung B-Die by all accounts is the fastest RAM on DDR4.",hardware,2025-11-12 20:39:47,5
AMD,noi13lq,"Yup.  Theres also completely different needs so different processes and costs for DDR and GDDR chips.    FWIW at least for DDR and LPDDR Hynix has had the best cost structure, performance, efficiency, and they could sell their parts for a premium because of the performance and efficiency.  They were winning no matter which way you looked at it.",hardware,2025-11-12 18:46:17,3
AMD,nolltmi,got a 2x16gb 3600 MHz cl16 used for 40€ with 5 years warranty left😂,hardware,2025-11-13 08:04:48,1
AMD,noxeops,"So AMD on average had an overall 1.24% increase in market share this quarter while Intel lost 1.24%. That's impressive, considering Intel started selling discounted chips a few months ago.",hardware,2025-11-15 04:15:22,126
AMD,noyeqcf,"So Intel still has double AMD's market share.   It's always weird to see people talking like AMD has 90% or something: ""Man I hope Intel catches up, we need competition""",hardware,2025-11-15 09:50:34,57
AMD,noye5e9,[TPU article](https://www.techpowerup.com/339919/report-amd-now-commands-one-third-of-the-desktop-x86-processor-market) has a nice [table](https://tpucdn.com/img/xKvcdWYMDyro3Q1E_thm.jpg) of both unit and revenue market share.    &nbsp;  AMD Unit/Revenue share | Q325 | Q324 | Change (points) ---|---|----|--- **Server**| 27.3%/41% | 24.1%/33.8% | 3.2/7.2 **Desktop** | 32.2%/39.3% | 23.0%/18.8% | 9.2/20.5 **Mobile** | 20.6%/21.5% | 20.3%/17.7% | 0.2/3.9 **Client** | 23.9%/27.8% | 21.1%/18.0% | 2.8/9.8 **Total** | 24.2%/33.0% | 21.3%/24.2% | 2.9/8.8  &nbsp;  The jump YoY in desktop revenue share is quite something.,hardware,2025-11-15 09:44:25,24
AMD,noxoko4,"I love how this is going and hope Intel can respond with a banger product of their own. Competetion is our biggest friend as customers people, these stats should make us all happy, Intel, AMD and Nvidia loyalists alike, and we should all hope Intel is able to respond with products of their own  If only AMD could do similar numbers and gains in the GPU sector. Not even high end AI, but desktop segments for professional editing, gaming or even mining. Hopefully Intel and AMD can turn a new leaf and begin scaring Nvidia into more competetive pricing like the b580 did with the 5070, at least  in desktop markets. I know all their money is in AI power drivers now, but surely they would not like to lose their monopoly on the desktop market that easily if the push came to shove right?",hardware,2025-11-15 05:32:29,62
AMD,nox73xg,"Considering Intel is giving the chips away, AMD has to stay resilient",hardware,2025-11-15 03:21:56,33
AMD,noyfnd9,"In high end, AMD is great. But there is no competitor for Intel’s N100",hardware,2025-11-15 10:00:07,13
AMD,noylrgs,"Intel's x86 market share decline trend started about the same time, when Intel lost the leadership in the manufacturing process.",hardware,2025-11-15 11:03:22,5
AMD,noxt1d2,It's crazy that they've had better chips for many years now but can't take the market share.,hardware,2025-11-15 06:11:49,23
AMD,nox9zl4,I actually bought a 265k only beacuse it was heavily discounted at $160 while the 9800x3d is like $520 Intel needs to wakeup or Amd will become what Intel was,hardware,2025-11-15 03:41:54,14
AMD,noysx1t,"This is what you cite when people expect AMDs GPU market share to change after 1 decent generation  It's been almost a decade since Ryzen was a real contender, and this is the result. It takes time.",hardware,2025-11-15 12:09:59,4
AMD,np1s7hn,More than tripled their market share in 9 years.,hardware,2025-11-15 22:16:51,2
AMD,np6z5md,Quality sells.  It is that simple.,hardware,2025-11-16 19:29:28,2
AMD,noypxlm,I'm actually surprised that Intel's market share hasn't collapsed even more.,hardware,2025-11-15 11:43:52,3
AMD,noy5tef,"> However, during the quarter, AMD achieved two important milestones: it now commands shipments of over 25% of all x86 CPUs, and it now ships over 33% (one-third) of desktop x86 CPUs.  sounds impressive",hardware,2025-11-15 08:16:28,2
AMD,np0lkuh,"gamer choose AMD, non gamer choose intel, but gamers are still a small portion of computer users.  most of the users are using pc for work or study.",hardware,2025-11-15 18:25:47,2
AMD,nox2p0n,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-15 02:52:35,1
AMD,nozkr00,I'm surprised this hasn't happened faster.  Intel has been fumbling and bumbling about for so long I would expect them to be below 50% in servers and near 50% everywhere else.,hardware,2025-11-15 15:12:25,1
AMD,noykmfw,"It's crazy to see just how sticky Intel's market share is.  AMD has been dominating performance since Zen was introduced in 2017. And yet 8 years later, Intel still has a larger market share.",hardware,2025-11-15 10:51:45,0
AMD,noxy2wf,I thought they were over 30% market share already,hardware,2025-11-15 06:59:20,1
AMD,noxo5u4,Only RISC-V will break up this duopoly.,hardware,2025-11-15 05:28:59,-1
AMD,noyvoep,who the hell is even buying intel cpus these days anyway.,hardware,2025-11-15 12:32:43,-3
AMD,noyqsmv,"amd pulling a intel right now though... releasing old chips with new numbers, new products aren't real improvements...    same thing intel did, when you don't need to, don't release big increases.      at this rate im good till quantum,  4770k -> 7800x3d -> quantum",hardware,2025-11-15 11:51:43,-1
AMD,noyg81q,Discounted Raptor Lake chips that they later removed discounts,hardware,2025-11-15 10:06:01,31
AMD,noy6ok3,"Not quite, for me Lenovo legion Arrow lake 255HX is on average $100 more expensive than AMD's 9955HX.  I wanted to buy Intel, but their arrow lake in laptop isnt price competitive.",hardware,2025-11-15 08:25:26,16
AMD,nozmgv3,Because of all corporations and SMEs buying intel basically out of habit and on reputation.  Anyone who cares about actual performance and price-to-performance and performance-per-watt hasn't bought Intel in a while now.,hardware,2025-11-15 15:21:53,45
AMD,np0vy70,They've maintained market share by lowering their margins to the point that they're practically not sustainable as a business.,hardware,2025-11-15 19:18:27,10
AMD,np3eoi0,The people that bough cpus a few years ago aren't suddenly gonna throw them in the trash just because amd is now making better new cpus.,hardware,2025-11-16 04:16:21,4
AMD,np2cihr,Intel has a stronghold in the prebuilt workstation/office PC market where immediate raw compute and wattage differences don’t matter too much. Same reasoning why Nvidia is littered in the prebuilt gaming market compared to AMD,hardware,2025-11-16 00:18:30,2
AMD,noyw66n,"The desktop unit change tracks very closely to the Steam hardware survey numbers, where at the end of Q325 AMD were at 41.31% compared to 32.57% the year prior, for an 8.74% gain. I'd guess Arrow Lake's underwhelming gaming performance saw a lot of people make the switch.",hardware,2025-11-15 12:36:44,9
AMD,noz5x9a,"The revenue share growth numbers are extremely impressive, more than doubled for desktop in just a year. Even in mobile, their average sales price is now higher than Intel's.",hardware,2025-11-15 13:43:53,6
AMD,np12zv1,"The table is for Q2 2025, not Q3. Unfortunately, I have yet to see a table like that for revenue for this quarter. I'm assuming mercury research did not give those estimates to tech sites as they often do, since they also paywall a bunch of information,",hardware,2025-11-15 19:56:27,3
AMD,noz7jvo,"Absolutely no one is going to scare NVIDIA on GPUs anytime soon, lol.   I get your point though, you hope that they can.",hardware,2025-11-15 13:53:59,16
AMD,noy12jc,So do I but only when the market has fully rebalanced and AMD have 50% share of it.,hardware,2025-11-15 07:28:42,27
AMD,noyho6b,We really ideally want these companies 50/50 competing in all the segments and just constantly having to battle for every sale with the best products. The CPU market is a lot healthier than the GPU one and we badly need that to change as the prices are getting higher every generation.,hardware,2025-11-15 10:21:02,11
AMD,noywdkg,"Two months ago, they hiked the price on Raptor Lake CPUs: https://www.tomshardware.com/pc-components/cpus/intel-reportedly-raising-prices-on-ever-popular-raptor-lake-chips-outdated-cpus-to-get-over-10-percent-price-hike-due-to-disinterest-in-ai-processors",hardware,2025-11-15 12:38:23,18
AMD,noya8vl,Intel chips overpriced,hardware,2025-11-15 09:02:46,3
AMD,noy7l38,In my local market only good:ish deal on Intel is 225F. Rest of them are not that great deals compared to AMD unless you are heavy into specific tasks (non gaming),hardware,2025-11-15 08:34:53,-2
AMD,np13fjy,Intel doesn't really make money on that line. They've long-since killed plans for a successor.,hardware,2025-11-15 19:58:50,9
AMD,noyifb7,"If you like computers that spend two days with the CPU at 100% just doing Windows updates and otherwise being unusable, sure.",hardware,2025-11-15 10:29:02,-9
AMD,noxto0a,They are taking marketshare. Marketshare alone isn't rly important.  Amd could make 20 9800x3d and intel makes 200 i3s. Revenue and margins are  important as well.,hardware,2025-11-15 06:17:39,40
AMD,noxzs5d,The one reason behind it: Intel Foundry.  People in this sub seriously underestimate just how big of an advantage Intel manufacturing its own chips is. The sheer volume intel can manufacture dwarfs any competition that relies on TSMC. It doesn't matter how good your thing is if you can't meet market demand.  And people here were claiming Intel should stop its foundry.,hardware,2025-11-15 07:15:52,21
AMD,noyspke,"what better cpus have they had for years?   Only for the current generation I would say they have better chips across the board, and it's still only x3d that is significantly better than Intel for what diy would use, with non-x3d being only a bit faster than Intel's offerings while missing some things like QSV so it's mostly up to local pricing.  Zen 4 was in a similar situation as Zen 5 but the AMD and Intel offerings were closer to each other, and AMD did their usual bad launch pricing. Zen 3 was the other way around apart from the 5800x3d, and Zen 2 and lower were just not good apart from having cores",hardware,2025-11-15 12:08:13,5
AMD,noxu13w,The gaming market share is a tiny fraction in the grand schemes of things. General production and professional applications still see a huge advantage for Intel.,hardware,2025-11-15 06:20:58,10
AMD,noy81jx,"That's only been true for servers. While sentiments swing that way, Zen, Zen+ and Zen 2 on desktop aren't superior.  Even in servers only Zen 4 had AVX512 support",hardware,2025-11-15 08:39:46,5
AMD,noy1ra8,265k at 160$ is an insane deal.,hardware,2025-11-15 07:35:33,19
AMD,noxna6v,"Client hasnt changed its 23.9% q3 2025 for the past 4 quarters ... its +6.4% since 2020. +1.28% annual ""lunch grabbing""  Amd Mobile share is down for past 4 quarters 0.4% ... 4.8% since 2020. 0.96% of annual lunch grabbing  The server side is where Intel isn't really maintaining the marketshare  No wonder amd wont make any gpu share grabs. They basically have to beat Nvidia convincingly flagship & good price/$.",hardware,2025-11-15 05:21:43,11
AMD,noymwrc,"Sure, let's just forget that high-end Intel CPUs regularly scratched the $1000 mark? But here we are complaining about $500 for the best gaming CPU on the market.",hardware,2025-11-15 11:14:48,4
AMD,noy8bf6,Has long since happened  1)Zen 3 raised prices removed coolers  2)Zen 5 has been 6 cores longer than Intel has been on 4 cores,hardware,2025-11-15 08:42:36,0
AMD,noyrcou,"its a good chip, people act like its got the plague... cpu these days are so minor, get a better gpu....      i still like intel for quicksync, blows amd out the water while my 4900's chug my olama code.. (non gaming pc) best of both worlds",hardware,2025-11-15 11:56:41,0
AMD,np0xsid,That's oversimplifying things.   I usually say that most gamer should go with AMD. Non-gamers can go with either AMD or Intel. Unless they want their CPU's to not degrade over time and not having to need water cooling to get the intended performance out of them.,hardware,2025-11-15 19:28:26,1
AMD,nozi13f,That's not really true. Intel had gaming and single core lead until Zen 3   Then AMD kinda lost it again when alder lake came out. X3D has been killing them in desktop over last 2 years tho,hardware,2025-11-15 14:57:09,13
AMD,np3yfal,"Zen 1 had massive issues and while a strong improvement over construction machinery line, it was nowhere near comparable to Intel. It wasnt until about 2020 that AMD even caught up.",hardware,2025-11-16 06:56:41,2
AMD,noydyid,"They are, they're at over 33% for the desktop now.",hardware,2025-11-15 09:42:23,1
AMD,noy5g74,ARM*,hardware,2025-11-15 08:12:40,-2
AMD,noz6318,"""Nobody ever got fired for buying Intel""",hardware,2025-11-15 13:44:53,3
AMD,np0biqc,Most Businesses do because companies like Dell barely sell any of their business line with AMD chips.,hardware,2025-11-15 17:34:05,2
AMD,noyz2qm,"Sorry to throw a wrench in your upgrade plans but even if useful quantum computing hits the shelves tomorrow, nobody will be replacing entire CPUs with it unless the only thing you need to run is Shor's algorithm.",hardware,2025-11-15 12:58:55,5
AMD,noyv6z5,"The discounts just suddenly stopped working, like Raptor Lake in a server environment.",hardware,2025-11-15 12:28:49,29
AMD,noysdby,"No clue why you’re being downvoted. Bad chips don’t exist, only bad prices",hardware,2025-11-15 12:05:22,20
AMD,np04dms,9955hx is not usable on battery and will quickly drain,hardware,2025-11-15 16:56:17,4
AMD,np0h6f7,"""Nobody got fired for buying IBM.""",hardware,2025-11-15 18:03:21,34
AMD,np0xfdc,"They do it because lower idle.  Intel still wins at idle power consumption by a wide margin, and for most tasks computers spend the majority of their life idle.  For servers performance per watt matters, but for things like offices you might deploy 30k units. People work maybe up to 8hrs a day, and that’s assuming their entire day is actually at a computer. Most people still work 5 day work weeks though 6 is becoming more common in corporate America.  That’s a ton of idle power consumption, and a lot of excess heat HVAC’s spend time pumping out of the building.  Intels prices on low end computers is also really low.  So there’s no value to pushing AMD hardware in these places when it costs more per desktop and more to operate.  Hopefully AMD works on their idle power consumption in the future.  I love my AMD AM5 desktop but when building a home server I couldn’t justify an extra 15-20W power consumption, especially when energy prices are going up, and IMHO will keep climbing for a while. I don’t have the sustained workloads to keep the cpu at 80% to take advantage of their performance per watt.",hardware,2025-11-15 19:26:26,15
AMD,np02zw0,"This is not true. You have the b580 to thank for the pricing of the 5070 being somewhat normal at around 500$ even though it sells for way more than msrp.  If you make good products at decent prices, people will buy, when competetion sees this, they will respond so as to not lose out on market share",hardware,2025-11-15 16:49:02,-7
AMD,noy40vj,"Gains are lng term. If intel reply tomorrow, it would still take some time to thwart amd and they may reach 50% by them.  Latency between product releases and their reception and market share is years at least often",hardware,2025-11-15 07:58:21,5
AMD,noyf0l3,Because you're not a OEM.,hardware,2025-11-15 09:53:32,13
AMD,np1cgie,"Twin Lake is a successor to Alder Lake-N and it's out, and Wildcat Lake is in development. And these are some of the most popular consumer CPUs. Check out this chart, both N100 and N150 is there!  [https://www.cpubenchmark.net/share30.html](https://www.cpubenchmark.net/share30.html)",hardware,2025-11-15 20:49:20,2
AMD,noyjlxh,"Tell me you haven't used N100 without telling me you haven't used N100.  N100 is 4x Alder Lake E-cores, but they're not trimmed down. One of these E-cores has roughly the same performance as an E-core in my 13900K (I benchmarked this). I'm not kidding.  Sure these big CPUs have more TDP, but that wattage means nothing for the tiny E-cores that can't make much use of it, it all goes into P-cores.  N305 (basically two N100s glued together) has rougly the performance of Steam Deck CPU, and N100 can match Steam Deck at single core too.",hardware,2025-11-15 10:41:25,19
AMD,noyuady,"Like /u/Sosowski says, it seems you don’t know this CPUs  The N100 family is incredible, and AMD has nothing to compete against it both in price + performance. Not only because the 4 e-Cores, but for some usages Intel QSV is perfect (transcoding movies even at better quality than an AMD APU that can cost x3?)  This chips can run Windows 11 just fine for a everyday usage (browser, some apps, Office…) but they excel as NAS, homelab servers… where they shine given their TDP, performance and QSV  I understand why AMD isn’t interested in competing in this “low TDP low profile” category, and it’s because the margin is very low and they can’t justify fill TSMC or GlobalFoundries nodes with this kind of chips orders.  Even Intel can’t, and that’s why they only make them with their own capacity (N100s aren’t produced externally) and that means they are now in a slow path of upgrade until Intel internal nodes update (meantime, Intel is ordering TSMC some of their higher end processors parts)  I would love AMD being able to compete and make something like a low TDP x4-x6-x8 APU for like 50-70$ the CPU (more than that would be out of market compared to N100s)",hardware,2025-11-15 12:21:26,10
AMD,noyr25n,"2025, time to ditch windows...    embrace arch and hyperland experience how life should be buttery smooth and pretty.      rice away with the waifu gods!",hardware,2025-11-15 11:54:04,-2
AMD,np1qf2d,"Market share is very important, just look at Radeon.",hardware,2025-11-15 22:06:35,4
AMD,np0bpae,> The one reason behind it: Intel Foundry. >  >  >  > People in this sub seriously underestimate just how big of an advantage Intel manufacturing its own chips is. The sheer volume intel can manufacture dwarfs any competition that relies on TSMC. It doesn't matter how good your thing is if you can't meet market demand.  IFS only matters if they can stay somewhat close to the market leader. Having a fab 4 years behind the times doesn't help them all that much because they'll still have the costs of running their fabs and the costs of going external for products that actually matter and no cash to keep going forward with new fabs.,hardware,2025-11-15 17:35:02,4
AMD,np0jw0n,"> The sheer volume intel can manufacture dwarfs any competition that relies on TSMC.  TSMC produces significantly higher volume than Intel, and Intel's own latest products are mostly TSMC.   > It doesn't matter how good your thing is if you can't meet market demand.  You can still sell what you have, and that's better than Intel's situation, where largely because of the foundry, their chips are bad so they can't really make a profit from selling them.   > And people here were claiming Intel should stop its foundry.  How many chip sales do you think a multi-billion dollar loss is worth?",hardware,2025-11-15 18:17:07,5
AMD,noyhyg1,"People under estimate the impact of capacity and how this really works. AMD gets a better product and rather than being sat on inventory they sell it all, maybe for a bit more margin, but now they have no more CPUs to sell so Intel fills that gap. This happens in every market because companies can not instantly ramp up production to meet their new demand and Intel is now a missized company with a lot of production capacity it can't fully utilise and the moment it makes a competitive product it can instantly go back to 80%+ share of the market.",hardware,2025-11-15 10:24:04,5
AMD,noy51lu,"> And people here were claiming Intel should stop its foundry.  My brother in christ, they reported a net loss of $19 *billion* dollars last year and practically had to be bailed out by the feds.",hardware,2025-11-15 08:08:34,9
AMD,noy64wj,"I dont think TSMC is at fault here, TSMC can really make a lot of chips.  It is more likely AMD didnt bother to book larger capacity after so many years. When in comes to volume, even Nvidia dwarfs AMD's. CPU has higher margin per die area than GPU, yet AMD did not outbid nvidia to book more capacity.",hardware,2025-11-15 08:19:47,2
AMD,np0vkbn,It's so simple,hardware,2025-11-15 19:16:24,1
AMD,np3xaa2,"on the very high end Intel would get ahead, but for an average consumer AMD was better deal since Zen 2 (zen 1 had issues).",hardware,2025-11-16 06:46:02,2
AMD,np0kl6m,"The professional market is smaller than gaming, fyi.",hardware,2025-11-15 18:20:43,3
AMD,noy7ag7,Indeed. Here it was as low as 270€ but has now increased in price by 50€,hardware,2025-11-15 08:31:46,1
AMD,np1lcyq,Like what cpu?  I only remember fx-57 in the 1k range   And thats irrelevant because back then midrange cpus can be overclocked to top level,hardware,2025-11-15 21:38:32,1
AMD,np1mhbx,Because tech tubers will convince people if they dont have an x3d their pc is trash   We have a generation of pc builders who call themselves enthusiasts but all they are doing is buy the most expensive parts,hardware,2025-11-15 21:44:36,2
AMD,noyu62o,Exactly! I used the difference and got a 5080 it's a fantastic experience for both gaming and workloads,hardware,2025-11-15 12:20:28,2
AMD,noyf7wo,I meant overall including server and laptop,hardware,2025-11-15 09:55:38,2
AMD,noyc372,"ARM is a monopoly, which is worse than a duopoly.",hardware,2025-11-15 09:22:21,9
AMD,np2o2m8,was a bit of satire...     gotta be honest when i went from a 4770k @ 5ghz to my 7800x3d it didn't really change my life much.,hardware,2025-11-16 01:27:35,1
AMD,np5dyzx,"Ugh, I used to work for a company like that. It was a total Intel + Microsoft + ""open-source tools are the devil"" shop.",hardware,2025-11-16 14:30:59,3
AMD,np0xv09,"Logical explanation, but I work in IT and no one - colleague, boss, client, supplier - no one ever gave me that reason behind the products they bought.",hardware,2025-11-15 19:28:49,35
AMD,np4ifvn,"The percentage of corporate buyers choosing Intel machines over AMD specifically because of idle power consumption is practically 0%.  AMD doesn't really target the low end of the market that these office PCs live in. A practical consequence is that the stock (and the selection of SKUs) of AMD business PCs from major OEMs like Dell, HP, Lenovo, etc. is going to be limited. A corporate buyer ordering desktop PCs by the thousands is going to get Intel machines if for no other reason than that's all they _can_ get in a reasonable time frame.  Also, for an office PC, Intel CPUs are fine. They're powerful and efficient enough, the integrated graphics work well, and they aren't run at stupidly high clock speeds like the enthusiast chips.",hardware,2025-11-16 10:17:10,8
AMD,np50d7f,"Who's actually doing a 6 day workweek? I don't mean random startups or executives saying they think it's better for productivity, I mean what percentage of people you actually know regularly works 6 days a week",hardware,2025-11-16 13:01:17,3
AMD,noyeyu7,"I don't agree, it could be years before OEM's are fully off their ""intel only"" mindsets.   As things stand, If intel released a banger now AMD is not getting any new OEM designs basically overnight.",hardware,2025-11-15 09:53:02,17
AMD,np1fl4r,"> Twin Lake is a successor to Alder Lake-N and it's out  Twin Lake is literally a rebrand of ADL-N. It's identical silicon.   > and Wildcat Lake is in development  I have high hopes for WCL, but that's really a different market. WCL targets the lower end of what's currently the -U series. Will be great if you're looking for a solid budget laptop in a year or so, but it's too expensive to fill the embedded and even *more* cost-sensitive client roles that ADL-N fills today.   Also, doesn't help that the WCL successor is also dead, so...",hardware,2025-11-15 21:06:56,7
AMD,noz8ts3,It's sad that Intel low power low tier chips are heavily cut down on the iGPU side.,hardware,2025-11-15 14:01:53,6
AMD,np024la,"I moved on from the N100 years ago. Promised myself ""never again"".   Yet I broke my promise recently, got a Mini PC/NAS with an N150. Thought it might be a better experience than the N100 which it replaces. If I had done my homework I would have known that it's basically the same CPU, performance wise, just Intel's rehash of an old and aging CPU, with a new name:  https://www.cpubenchmark.net/compare/5157vs6304/Intel-N100-vs-Intel-N150",hardware,2025-11-15 16:44:30,-4
AMD,np00loi,"I just spent a week configuring a Mini PC/NAS with an N150, and I know what a pain it is. The N150 is supposed to be the new and improved N100, in reality it's less than 1,5% faster. It's just Intel rehashing the same old, with a very slight clock speed bump. Not enough to be noticeable to the user, but enough to be able to call it a new generation of low power CPU's.  I kid you not, EVERYTHING you do makes all 4 cores go 100% full tilt. Even opening a new tab in a browser is a struggle for this CPU. I literally spent 2 almost full days just doing the normal Windows updates you have to go through when installing Windows these days. The CPU had all 4 cores at 100% at all times, and I couldn't even watch a Youtube video at the same time, it was too bogged down downloading and installing updates.  It's still ok for a NAS that I just remote into from time to time, but I was hoping to use it as a mini PC for my spare bedroom as well. But it's just too painful to use for anything but the most basic stuff. Any kind of multitasking and you're gonna hate the experience.  You're much better off paying $50 extra and getting something with a 5560u instead, which also has a proper iGPU as an added bonus:  https://www.cpubenchmark.net/compare/5157vs6304vs4883/Intel-N100-vs-Intel-N150-vs-AMD-Ryzen-5-5560U",hardware,2025-11-15 16:36:32,-1
AMD,np3xf2u,">TSMC produces significantly higher volume than Intel, and Intel's own latest products are mostly TSMC.   Only the part that AMD has bought is really relevant when it comes to TSMC production here. is AMD part higher volume than Intel? I dont think so.",hardware,2025-11-16 06:47:17,3
AMD,np0k2oy,Intel doesn't have *that* much capacity on the nodes that could conceivably compete with AMD.,hardware,2025-11-15 18:18:06,4
AMD,noyo2zt,Yup. If Intel got their shit together they can simply destroy whatever market share AMD have gained over a decade of them not being competitive in a single year. I don't think Intel can get their shit together with how they are managed but they absolutely can destroy AMD if they do. And the worst part? They won't even have to take that big hit in profit while doing so since they will be doing everything inhouse.,hardware,2025-11-15 11:26:17,2
AMD,noyb8bb,"Ah yes. The CEO mentality.  \- ""Why are we spending so fucking much on X when we are not getting any money from this? Cut it out!""  \- As it turned out that X is the reason product Y can be made. After X is gone your product Y is gone.  Good news 19 billion dollar loss is gone! Bad news the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products. Congrats! get your 100 million dollar package now.",hardware,2025-11-15 09:13:15,7
AMD,noy840d,That doesn't refute his market share thesis,hardware,2025-11-15 08:40:27,3
AMD,np3xhng,Thats what happens when you are actively investing in new nodes and have to catch up with 10 years of stagnation.,hardware,2025-11-16 06:47:57,1
AMD,noy7x98,They really didnt. That “loss” was due to internal accounting,hardware,2025-11-15 08:38:31,-2
AMD,noycr74,"True but market forces play a part in how much AMD can take risks in booking capacity. AFAIK server market for CPUs is much more constrained than for GPUs where you can currently pretty much produce as much as possible and still not match demand. So Nvidia can offer more for the capacity and AMD's risk of overbooking is higher. Of course AMD could still have been too conservative, and in hindsight it looks likely they were, but it's not like they didn't have their reasons for it.",hardware,2025-11-15 09:29:31,5
AMD,noynkpi,I never said it is TSMC fault... It is just how the market is. TCSM is doing what it is paid to do.,hardware,2025-11-15 11:21:21,2
AMD,np0ohlh,"The professional market isn't just video editors, fyi.",hardware,2025-11-15 18:40:10,1
AMD,noxzwqu,Check the volume of laptops being sold. It dwarfs gaming market by orders of magnitude.  Intel basically have stranglehold on that entire market and it is practically impossible for AMD to compete in that market cause of TSMC.,hardware,2025-11-15 07:17:08,5
AMD,noy02vq,"Its just guesstimates but 2024 intel & amd client rev 30.3+ 7.1 = 37.4B  \~70% are mobile 30% desktop (JPR shipments)   11.22B desktop   Motherboard shipments by oems \~50M   2-2.5B diy annual rev   From diy market I cant guesstimate how much are purly gaming but I will say 70-80% gaming   My best guess is diy is 1.7-1.9B rev   JPR, steam & Gartner data",hardware,2025-11-15 07:18:48,0
AMD,np2obaq,"Becuese the X3D is actually trash at it's price point. Unless you are playing a niche game, the cheaper CPU that is still great in 99% of cases is a far better buy and then invest that money into a better GPU.   You can buy a CPU+Motherboard bundle over at microcenter with the 9700X for $340. The 9800X3D bundle is $530. That's $190 more. Now outside of circlejerking benchmarks what do you actually get for your $190? Nothing compared to what a $190 better GPU you can buy.  Same thing with Intel. The I9 Arrow lake CPU is a terrible buy compared to the 265K. In every case the 265K Paired with a better GPU is going to win for gaming.  Now if you don't give a fuck about price and are just going to buy a RTX 5090 anyways then this is a moot point, but most people I would imagine are not doing that.  Then you have the people who believe they need that top tier CPU and get something like a 5060TI to go with it. Becuese they blew too much of thier budget on the CPU. Which means they just wasted thier money becuese chances are they are going to be GPU limited to the point where their killer CPU is pretty pointless.",hardware,2025-11-16 01:29:02,2
AMD,np3y3xb,it depends on use case. My 7800x3D is the limiting factor in almost every use case and i cant wait until i can find a meaningful updates for it (9800x3D not worth it). Im spending more on CPU than GPU because thats my bottleneck.,hardware,2025-11-16 06:53:43,2
AMD,noyil6g,"Usually we compare desktop CPU's as consumers. That's why I thought you were comparing apples to oranges.  They're progressing in all markets though, just in the laptop market it's not much. So if you wan your initial statement to be true, you can just look at the laptop market.",hardware,2025-11-15 10:30:47,-2
AMD,noyy6v1,"At least they actually license out the ISA, which is more than you can say for Intel",hardware,2025-11-15 12:52:22,2
AMD,np5bm4h,"Some companies (finance and tech mainly) are moving to 6 day work weeks especially for support roles. China kicked it off with 996, but it’s starting to get some traction here too, especially with layoffs and cost cutting here.",hardware,2025-11-16 14:16:21,1
AMD,np12ot9,"Nah, Intel's lost much of their codesign team that helped them get those designs to begin with, while AMD's ramped up their own.",hardware,2025-11-15 19:54:45,3
AMD,np1tbbx,"Oh, I thought WCL is gonna be the next N series! And yeah you're right about Twin Lake.  But hey, I'm a big fan of these CPUs, even tho they compete with a Raspberry Pi more than with AMD. Hope there's gonna be more!",hardware,2025-11-15 22:23:12,1
AMD,nozmpc3,They're great HTPC and NAS chips but that's about it.  Love mine for transcoding Emby streams.,hardware,2025-11-15 15:23:09,3
AMD,nozpn39,"True! I wish there's a handheld gaming N-series CPU that is an N100, but with slightly better iGPU. There's N200 but it's still far from usable on the GPU side.",hardware,2025-11-15 15:38:55,2
AMD,np0c7uj,"1) The N150 is part of the same family, just a little bit “overclocked”. It isn’t a literally a new gen (same socket FCBGA1264, both based on Alder Lake cores, even if they have different naming (Twin Lake) because the dates of launch) but a “refresh”. It doesn’t have anything “new”.  And that’s logical. As I explained, this CPUs have a very thin economic margin, so they must use internal capacity of their nodes. And they still haven’t upgraded them since the N100 introduction. Also, the making of new e-Cores (again, given they won’t invest a lot on them given the margins) isn’t coming before 2026-2027 as earliest.  2) The N100 family runs correctly. Heck, I’m using in a desktop a N6005 (with Fedora I must say) no problem, playing 2K YouTube videos, sometimes +30 tabs, and mild multitasking (browser + another browser + Libreoffice)  I don’t know how your experience is even possible, 4 cores at 100% opening browser? Slow? It isn’t certainly the usual experience even if using Win11  I would say something else is at play, like low RAM availability (4-8GB?), slow SSD, bloated Windows or something else.  You can see benchmarks, the N100 is about the same performance as an Intel i5 8250U, and I know people even using that at their workplace no problem. Something is up with your PC, not the CPU.  In any case, I recommend you running Linux with this low resources things and avoid Windows 11 bloat.  3) That AMD you link has worse iGPU depending what you do (Intel QSV for encoding/decoding is miles ahead of AMD), and anyway, its GPU isn’t really a factor given at this performances you won’t be gaming precisely.  Also, it will run hotter and have a TDP about x2-x3 of the Intel. So I think now we’re not talking about apples to apples, but apples to oranges.  I have a Intel Nxxx that even runs passively cooled without problem, no fans, no noise, and it does its job perfectly, for about 150$. But if you are OK paying around 25-40% extra (+75-100$), more TDP, fans and so on, then yeah, you can go with a AMD Ryzen 5 APU  And if you’re OK with adding a bit more, you can even go with an Apple Mini using a M4",hardware,2025-11-15 17:37:49,3
AMD,np47hwy,"A legitimate point, but it does mean that there's substantial flexibility. AMD's content to slowly increase their market share while reaping pretty fat margins overall, but if they had an incentive to really push for more volume, they could get it.   Worth noting that Intel's TSMC wafer allocation is almost as large as AMD's, last I heard. At one point, even expected to be maybe the 3rd largest, iirc. So TSMC was able to absorb a pretty substantial amount of volume when Intel came on as a customer. Just demonstrates that the flexibility does exist.",hardware,2025-11-16 08:25:47,1
AMD,np12kyu,> Good news 19 billion dollar loss is gone! Bad news the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products.  Their fabs have 100 billion in profit? Intel barely has half that in revenue and a fair amount of that is from external.,hardware,2025-11-15 19:54:09,3
AMD,np0kefm,"> As it turned out that X is the reason product Y can be made  But it's not. It's almost the exact opposite. The fabs have consistently *stopped* Intel from being able to ship products.   So yes, why wouldn't you get rid of the part of the business burning billions by itself, no clear path to profitability, and who's dragging down the rest of the company with it?   Intel would be a lot better shape today if they'd gotten rid of the fabs instead of wasting a lot of money to double down on them. That's what AMD did, and it payed off.",hardware,2025-11-15 18:19:46,5
AMD,np0wafs,"> the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products.  What other products? Intel uses TSMC for their high end chips, you know.",hardware,2025-11-15 19:20:15,5
AMD,np134r4,"Are you lumping in every office PC to the ""professional market""? Otherwise, you'd be surprised at gaming vs all professional markets put together.",hardware,2025-11-15 19:57:11,3
AMD,noy8608,Intel's laptop chips are very competitive though,hardware,2025-11-15 08:41:02,1
AMD,np3y78f,"Well, i suppose it is technically a niche game, but games like WoW with their 10 million daily players benefit massively from x3D CPUs.",hardware,2025-11-16 06:54:34,4
AMD,noyoc9i,"I was surprised they hadn't hit 30% in server either, they've been kicking Intel's ass there for a long time",hardware,2025-11-15 11:28:46,2
AMD,np1u7fh,You'd need both AMD and Intel to agree to that. Intel can't do that on their own.,hardware,2025-11-15 22:28:26,7
AMD,np3p7dj,"They do license the ISA out, but they do not let others license their microarchitectures out. Licensees can make chips and that's it.  So if someone needs a design, they can only make it themselves or license it from ARM. As a result, few ARM customers bother.  Qualcomm found a workaround (just buy a company that made a good ARM microarchitecture), and ARM really didn't like it. That is what led to the lawsuit, which ARM, quite predictably, lost.",hardware,2025-11-16 05:36:54,5
AMD,np0lbe2,"No offense, but your comment is basically a very long winded way of saying ""yeah it sucks"".  Linux is more efficient. But I use Windows, and I should have clarified this, but my experience with the N150 is after de-bloating and optimizing.  But sure, the AMD chip will use more power when it needs it. But that's the whole point, it HAS the power when it needs it. When it doesn't, it's pretty frugal with its power usage as well.  I'm not a fan of ""just barely enough power"" when it comes to PC components. I despise e-waste, so yeah, you might find a purpose where the N100/N150 is just barely good enough, but when your needs change, or when things become more demanding, it's for the trash. That's why I don't get ""barely fast enough"" components any more. You're getting frustrated in the short term, and creating unnecessary e-waste in the long term.  Why are you making the claim that you have to pay $75-100 more to get an actually decent AMD CPU instead of this N100 e-waste CPU? That's not how it is irl.",hardware,2025-11-15 18:24:26,-4
AMD,npaj8ty,"TSMC has expanded capacity significantly last year and we will probably see they expanded it significantly this year too (the years not over :) ). They do provide significant flexibility, but we are still talking about years in lead time for capacity bidding unless you are willing to pay a lot of money for capacity.",hardware,2025-11-17 09:41:48,1
AMD,np3xxzx,"> Are you lumping in every office PC to the ""professional market""?  why wouldnt he? People using those are professionals. I do data analysis for a living and the local machine they gave me is intel.",hardware,2025-11-16 06:52:12,1
AMD,noyavl4,They are now very recently but few years ago Intel was by far the worst for laptop due to them being far less efficient than AMD but you couldn't find a single proper laptop lineup from AMD. Even now if you have a specific combination of specs in mind which is even slightly different than mainstream you are stuck with Intel. AMD laptops are very limited lineup.,hardware,2025-11-15 09:09:33,3
AMD,np0sbnr,"Then again, something is up with your setup, because your experience is not the usual with the N100 chips. There are even laptops being sell with them nowadays at some markets and run Windows 11 perfectly.  IDK what PC you got or its constraints, but what you said it isn’t normal, more so if after debloating.  And about power, it depends. I’m a big fan of having what I need, and not try to kill flies with a nuke. As I said, I run a N6005 as a daily driver just fine and plan to do it for some more years, even if I also have an AMD 8600G desk, because less noise, electricity needs, heat… to do the same basic tasks. Obviously if I needed to run a heavy software I would use the 8600G, but that’s not a daily need in the slightest. In fact, I have even thought about selling the AMD PC (more so because I play less and less) but given I would need to sell it at a discount, and all the job of getting a buyer and so on, well… I’m lazy I suppose.  And about he money, IDK where you live, in parts of Europe you can find a N100 mini PC for about 130-160€ even with RAM and SSD included, new. I got my N6005 for 80€  An AMD 5560U mini PC (Beelink?) starts at 300€, about double.",hardware,2025-11-15 18:59:31,3
AMD,np474ou,"Mostly because if you lump all those in, the category ceases to mean anything useful. At least from AMD and Intel's perspective, that's basically the same as the mainstream consumer market. And if that *is* what you want to talk about... why not just call it that directly? The only reason to specifically call out ""professionals"" would be to distinguish it from the rest of client.",hardware,2025-11-16 08:22:05,2
AMD,np0yq5n,"> Then again, something is up with your setup, because your experience is not the usual with the N100 chips.  Nope. This is not my first time with this class of CPU. I've had plenty of N class CPU's before, back when I thought I could make it work.  The thing is, having ""barely enough"" only works until you need more. And in that case, in any case, you're better off with a CPU that CAN deliver more, rather than one that can't.  I usually shop Mini PC's on places like Ali-express, the price difference is more like $50, all else being equal. And for that kind of money you get something that won't magically turn into e-waste next year, or whenever your needs change. And is nice to use until then.",hardware,2025-11-15 19:33:30,-1
AMD,nmb8bbo,"It's especially bizarre when you realise how many products RDNA2 is present in. Not just discrete GPUs, but current gen consoles, rembrandt APUs in a lot of budget laptops, and Zen 4 and 5 desktop CPUs (though there are probably not many people gaming on these at least). Dropping RDNA1 support is understandable given how few products lines it was used in, but RDNA2 is still extremely relevant on top of not being that old.",hardware,2025-10-31 03:23:00,467
AMD,nmbmak7,"Man the 6000 series was the ONE gen where by god, they competed at the top end and had extra vram all the way down the stack as a way to sell their cards as a longevity thing...   a 3080 vs 6800XT was a tough choice, and if it wasnt for the evga queue I would have likely grabbed either that was for sale for MSRP in the depths of pandemic.  and IIRC it was the most successive gen recently if you looked at steam hw survey  and now its getting axed, while even the 20 series are not...",hardware,2025-10-31 05:14:42,164
AMD,nmb773n,AMD this is not how you retain/gain market share.,hardware,2025-10-31 03:15:11,232
AMD,nmba2tx,Never miss an opportunity to miss an opportunity.,hardware,2025-10-31 03:35:44,277
AMD,nmbb10v,Radeon being really fucking stupid exhibit #1523,hardware,2025-10-31 03:42:40,110
AMD,nmbkdzj,I'snt the 6000 series like their most popular cards on the steam hardware surveys? They wanna lose even harder almao.,hardware,2025-10-31 04:57:33,61
AMD,nmbo5d9,"This hurts. I left my Vega 56 behind prematurely because it didn't have driver support. Now rocking a 6800xt, so this makes two cards in a row for me.  Radeon is supposed to be the budget choice with fine wine driver support.  It's becoming skunk beer instead of fine wine.",hardware,2025-10-31 05:31:52,74
AMD,nmcgydp,"Puzzling. AMD (unofficially) just got over the hump of their previous cards being inferior from being stuck with FSR3, and now this.   When is RDNA3 moving to the farm, 2027?",hardware,2025-10-31 10:21:03,11
AMD,nmb6r0u,"AMD actively blocked official Zen 3 support on B350/X370 motherboards for over a year (despite working beta BIOSes from Asrock and people successfully cross-flashing B450 BIOSes onto B350 boards), screwing over Zen 1 adopters who bought into AMD in 2017 and to which AMD owes its revival on desktop to. They only relented after Intel released non-K Alder Lake parts. So not too surprising.  *typo",hardware,2025-10-31 03:12:09,140
AMD,nmbgrrq,"Ya know the more i think about this the more pissed off I guess.  PC gaming has become increasingly unaffordable in recent years. I'm one of those old school ""60"" buyers. ya know, $200-300 for a GPU, and I plan to use it for around 4-6 years.   Since around 2018, we've been getting squeezed. First it was ray tracing and all that AI upscaling crap. Nvidia decided to make the 2060 $350 and threw anyone under that under the bus. Yeah, the 16 series existed, but let's face it, those cards aged like milk too.   Then COVID happened, the price inflation happened, and I was stuck on a 1060 through the pandemic. It lasted me well, but I knew it was aging, and quite frankly, GPU manufacturers stopped giving a crap about us. THey gave us the 3060 for $330, which was a joke. And this was before the ""inflation"" all the yuppie tech bros on these subs use to justify ridiculous prices.   I managed to hold it together through COVID, and upgraded during the price crash in 2022-2023. AMD was hit first, the RX 6600 type cards could double my 1060's performance for under $300, while Nvidia was STILL charging $340 for the 3060. It was a joke. Nvidia didn't get it, I dodn't wanna pay over $300 for a fricking GPU so I bought a 6650 XT for $230. It was an amazing deal at the time.  When the 7000 series launched around then, and the rest of the series was rolled out into 2023, honestly, it offered a very poor value for the most part. RDNA2 cards were literally sold next to RDNA3 ones for most of RDNA3's lifespan. And quite frankly, there was often little reason to buyt RDNA3, it was more expensive, and offered little in price/performance over RDNA2.  RDNA2 was eventually phased out in the higher price classes, but for a while, the RX 6800 was sold along side the 7700 XT, the 6650 XT was sold next to the 7600, the 6700 XT/6750 XT were in their own niche that AMD never really replaced, and the 6600 remained supreme as the ultimate budget option, competing with the fricking RTX 3050 and thrashing it in value.  heck, I was recommending RX 6600s as recently as this year, and really only stopped when we FINALLY saw the market move with stuff like the RTX 5050 for $250, the RX 9060 XT 8 GB for $270, and the RTX 5060 for $300. Even then, what killed the RX 6600 more than anything was the fact that AMD just FINALLY phased it out of production a few months ago and it was no longer a cost effective recommendation.   And now, AMD pulls the rug from under RDNA2 users? This fast? Really?  I dont care if they axe RDNA1, the writing has been on the wall for RDNA1 for a while, no mesh shaders, RT, DX12 ultimate support, yeah, RDNA1 is done. But RDNA2 is still a viable architecture. Maybe it doesnt support the fancy AI bull#### for FSR4 or whatever. Do you know how much I care? Not at all. Im in a budget class where I dont care if I get the fanciest graphics. I just wanna RUN THE GAMES. And to do that, i need DRIVERS. There's no reason they can just make the games run FSR2/3 and allow older users to use that. Sure, maybe it will be blurrier, I admit FSR is an inferior upscaler, but it works, and again, I'd rather run a game with a little blur going on than not at all.   To not support the series with drivers, when this could render the GPU incompatible with new games is a DISGRACE. I bought my 6650 XT planning to use it until 2027, hoping that my next sub $300 GPU would offer twice the performance and more than 8 GB VRAM. I dont wanna upgrade now, spending $250-300 for 10-50% performance improvements (seriously, the 5050 is barely faster than the 6650 XT in raster) and 8 GB VRAM. I just dont. As such, I guess I'm gonna be stuck using an obsolete and no longer supported card for the mean time. Thanks a lot AMD. And oh, guess what, given that Nvidia is no longer charging like 50% more than AMD for the same class of product, I'm probably not gonna go AMD next time. Not if this is how I'm treated as a consumer.  Ive always been like at least somewhat biased toward Nvidia/intel because it seems like every time I buy AMD I get burned for some reason. SOmetimes its lack of VRAM, poor driver support, support for some BS API, or just poor performance like on older AMD CPUs I've owned. But yeah, they're just kinda giving me flashbacks to the late 2000s/early 2010s again when they didnt support their products while nvidia did. And I'm kinda feeling burned right about now.  EDIT: since the responses im getting seem snarky and trollish so far, you send me that kinda crap, you're getting blocked.",hardware,2025-10-31 04:26:47,94
AMD,nmbhc6y,"It is not uncommon for AMD to have shorter driver support or continued optimization.  for that matter, fine wine was just marketing for, ""we have shitty drivers at launch"".",hardware,2025-10-31 04:31:29,70
AMD,nmbodgw,Wow this is a problem if they continue this trend on the 7000 series.,hardware,2025-10-31 05:34:00,9
AMD,nmclxik,What about the RDNA2 APUs like the   * Ryzen 7 7735H with its RDNA2 680M iGPU * Ryzen 7 6800H with its RDNA2 680M iGPU * there are alot more Ryzen APUs in the 6xxx and 7xxxx series with decent RDNA2 iGPUs,hardware,2025-10-31 11:02:16,8
AMD,nmeftdf,"They updated their message about it, ""maintenance mode"" will still get game updates and whatnot, and the USB-c port was a mistake, they've since fixed it...",hardware,2025-10-31 17:03:55,9
AMD,nmbdb63,I guess this confirms project Red Stone will not be coming to RDNA 2 cards (FSR4.0 support),hardware,2025-10-31 03:59:31,17
AMD,nmbmaar,It’s not the first time. And obliviously not the last time amd do that . Why it become a shocking news ?  For example   Like RX580 . Mega popular and card have less then 4years of active support . Then 3 years of bug fixes   I stil remember amd fans bragging about rdna2  like is better choice then rtx 30 at least in terms of memory .. funny,hardware,2025-10-31 05:14:38,11
AMD,nmblff1,I'll probably be back to Nvidia next year at this rate lol.,hardware,2025-10-31 05:06:52,18
AMD,nmcml22,"Did not know that ""Loyal Radeon Customers"" was a thing.",hardware,2025-10-31 11:07:26,3
AMD,nmdzy81,"I think this is it for me when it comes to my next purchase. I'm very happy with my 6800 and I was planning to stay with AMD, but the fact they cut its lifespan so suddenly is pathetic. Why live with the mediocre feature set at this point.",hardware,2025-10-31 15:46:30,3
AMD,nmeds5z,"Let us retrace our steps to the spirited online debates of 2019, when early RDNA faced off against early RTX.  Early RDNA was pronounced the rational choice for the discerning enthusiast: modestly cheaper, often quicker in traditional rendering, and crucially unencumbered by ""gimmicks"" such as dedicated RT silicon or tensor cores. Ray-tracing was dismissed as an overpriced marketing indulgence, and DLSS was confidently predicted to vanish within the year. The RX 5700XT in particular was held up as an example: less costly than the RTX 2070 Super, marginally faster in raster. And best of all it was destined for superior longevity because the ""silly"" RT/AI gimmicks would surely perish like HairWorks. FPS-per-dollar at the moment of purchase would remain eternal as the only relevant metric of which GPU to buy.  Eternity, it seems, endured roughly until Adrenalin 25.10.2. AMD has now consigned both RDNA1 and RDNA2 to maintenance-only purgatory, with the driver graveyard following soon. Which is baffling considering how many systems and different products use early RDNA hardware. Even the RX 6750GRE finds itself affected, a card launched in late 2023. Imagine the uproar had NVIDIA deprecated the RTX2000+3000 series so quickly.  The RX 5700XT, once lauded for its value, is now remembered mainly for its hardware/software headaches. And when compared to the competition it now also lags in image quality (confined to FSR2/TAA) and framerate (due to increasing presence of RT in games). On the used market it usually trades for almost half the price of an RTX 2070 Super, a card it was supposedly better than in 2019.  Meanwhile the entire RTX 2000 series continues to receive full driver support, the GTX1000 cards that launched in 2016 are only starting to lose full support now. Games grow ever more dependent on RT. DLSS has become the gold standard rendering method and arguably the main reason NVIDIA secured overwhelming market share dominance in the 2020s. And even a 2018 RTX card benefits from the latest revisions and updates for all the newest ""gimmicks.""  AMD's 2025 RDNA4 series with its focus on ""ML accelerators"" and beefy RT units? They're actually pretty good cards (the 9070XT is AMD's best card in years) but they're also a belated concession by AMD that NVIDIA’s RTX blueprint was, in fact, the correct one. And the passage of time now positions early RDNA as a temporary measure, a stopgap until AMD figures out how to get back in the fight.  It remains quietly amusing how utterly wrong the YouTubers and online forums have misread the future about five-six years ago. Architecture/features/support, as it turns out, matter more than raster parity at a 5-10% discount.",hardware,2025-10-31 16:54:04,6
AMD,nmbd7vw,this is why I never see Radeon as a serious product.,hardware,2025-10-31 03:58:48,27
AMD,nmcf7uv,Hahahahga.   My 8 year old 2060 is chillin with dlss4 in a media server for super resolution,hardware,2025-10-31 10:05:44,6
AMD,nmbckem,I'm still happy with my 2060 12gb.,hardware,2025-10-31 03:53:56,14
AMD,nmbdp88,"Radeon division saw the success and good will the cpu/mobo side of amd has gotten for their am4 longevity and support, and went: yeah, let's do the fucking opposite.   This is beyond dumb. Probably switching back to intel/nvidia in the next year or two when i switch gpu",hardware,2025-10-31 04:02:30,16
AMD,nmc736q,"Does driver optimisation really exist? The whole video fails to actually prove that driver updates actually make a huge if any difference they just expect us to take this as a fact. I know I haven't bothered to keep on top of driver updates for every new game releases for years now.   At some point software is actually finished, the desire for constant updates is a weird desire of the community.   Seems people started getting happy again and we need some new thing to be faux upset over?",hardware,2025-10-31 08:46:17,12
AMD,nmbjclj,....so the SteamVR issues with DirectDraw on my 6800XT machine causing horrible lag are never going to be fixed. Sick. That machine literally only does VR and has been running 24.3.1 for months because of that.,hardware,2025-10-31 04:48:31,9
AMD,nmbrsqb,"My friend wants to upgrade his 6600, I was initially recommending the 9060xt 16GB   I've told him to spend the £50 on the 5060Ti 16GB. The saving doesn't make sense when they'd like ~5-8 years for a PC ideally",hardware,2025-10-31 06:07:41,8
AMD,nmcbfzj,"This is why I don't buy AMD cards.  I've been burned before. With a laptop. Had to hack drivers to even work with newer versions of windows after 2 or 3 years. When W10 came out, it was a complete no go because of no drivers from AMD.  Meanwhile, my parents computer, a 4770k with a gt750 is happily chugging along running Windows 11, besides TPM detection hack.",hardware,2025-10-31 09:30:09,8
AMD,nmbbhkm,How did this get past Lisa Su?,hardware,2025-10-31 03:46:01,12
AMD,nmd23wv,AMD never misses a chance to miss a chance,hardware,2025-10-31 12:51:30,4
AMD,nmdu1v6,Oh I guess my 6950xt I bought brand new two years ago is already obsolete 🤡,hardware,2025-10-31 15:18:11,5
AMD,nmcl6mc,"Yep I love AMD cards but this was a bone headed move. Especially because they love to reuse IP in subsequent years like the new base Xbox ally, (ally X uses RDNA 3.5). I think they will have to walk this back or risk losing further customers.  I can see why they have done it. The shift to more complete compute engines means they are dragging along some baggage that cant do all the new things, but this is the bed they've made... Keep it going another two years AMD",hardware,2025-10-31 10:56:16,2
AMD,nmfh8vu,AMD is doing it to itself at this point.,hardware,2025-10-31 20:16:32,2
AMD,nmib274,won't buy amd shit anymore,hardware,2025-11-01 09:26:30,2
AMD,nmcgk53,"Genuine question since I've seen people both defend and criticize this, and I haven't had a chance to watch the video yet. My understanding is that AMD will no longer provide game optimizations for the mentioned cards (5000 and 6000 series), but will continue to provide drivers (""support"") for them. This *feels* fast but is it *unusually* fast? How long do cards usually receive game optimizations? Specifically, how does Nvidia compare?",hardware,2025-10-31 10:17:35,5
AMD,nmc12ro,"my GPU is released 3 years ago man, this sucks.w",hardware,2025-10-31 07:43:43,4
AMD,nmc796g,lucky me for not pulling the trigger and building a 6600 xt budget gaming pc.,hardware,2025-10-31 08:47:57,3
AMD,nmclrh6,"Oh great so the choices are now AMD ""wait we sold you a graphics card lol?"" and Nvidia ""4K60FPS Net* performance! (*30EBITDA frames at 960p)"".  Fucking joy",hardware,2025-10-31 11:00:55,2
AMD,nmcp8cr,Anyone have a summary for those of us who can't watch the video when they are reading this post to figure it out?,hardware,2025-10-31 11:27:46,2
AMD,nmc7dgh,Guess I wont be getting a 9070XT for Christmas 😭 Really wanted to go AMD this time but they don't make it easy,hardware,2025-10-31 08:49:06,5
AMD,nmd3y13,I woke up in my garbage can. Came to reddit to find out why. This is NOT what I expected to have placed me there. ╮(╯▽╰)╭ off to shower the refuse off me!,hardware,2025-10-31 13:01:52,2
AMD,nmc2l58,"Here's my hot take on this and playing devil's advocate.      Everyone is freaking out over nothing.   They quite clearly say 'Bug fixes' and 'maintenance mode' in that response from AMD Germany.      You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'   making sure the game works with the card and bug fix and security patch if needed.      I can see why people are upset, but I also see it as AMD being transparent. Their older cards are in 'maintenance mode' now, which means they are still supported, will still get bug fixes, will still work with your games, but if you want the best experience, buy a newer card.      Nvidia does exactly the same, they just haven't given it a name.      Fight me 😂",hardware,2025-10-31 07:59:14,1
AMD,nmbeqwq,"Outrage is fun, but no.  AMD state they will maintain it for bugs which is all it needs. These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.  AMD might be involved in performance optimizations with new cards is when developers aren't familiar with them and new shaders (often based on old shaders) are found to not work well with very different architectures but that's not happening with cards designed in 2017.  There is no dropping of driver support. These are mature cards, known ISAs, they are already optimized for in current game engines.  If a developer is optimizing for RDNA2 and finds a bug AMD will still fix it. If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  We went through this outage cycle when AMD [split the driver stack for Vega back in 2023](https://www.tomshardware.com/pc-components/gpus/its-curtains-for-polaris-and-vega-as-amd-reduces-driver-support) and this really feels like a repeat.     EDIT: I was, of course, right: [https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games)",hardware,2025-10-31 04:10:38,-7
AMD,nmfjvrm,Hardware unboxed farming ragebait now?,hardware,2025-10-31 20:30:39,3
AMD,nmcdhb0,Loyalty is a one way street.,hardware,2025-10-31 09:49:57,1
AMD,nmd4gpk,"RDNA 1 and RDNA 2, no AI not relevant.  Since RTX 20, yes AI yes relevant.  Nvidia started earlier with AI = more supported GPUs.  AMD started late with AI = less supported GPUs  No AI = wasting resources.  AI is where most of the resources will go, this is where the world is going. It is not AMD or Nvidia, it is where the world is going.  Which is why GTX GPUs like Maxwell and Pascal switching to maintenance mode just like AMD is doing with RDNA 1 and RDNA 2.  It doesn't matter who started earlier(RTX 20 or RDNA 3), the base line from now is: Does your GPU has ML capabilities or not...  You can hate it or love it...",hardware,2025-10-31 13:04:51,1
AMD,nmhfez5,"I think this is shitty. Not sure how it makes sense to them. But it looks like it won't effect linux users at least. I guess if you needed a reason to switch from windows 10 as a 5000/6000 card user, here it is...",hardware,2025-11-01 03:55:42,1
AMD,nmkb4xa,"Rip amd resale value. All the people that meme’d about 4090/5090 prices are gonna look real dumb when they try to sell their outdated rdna1/2 cards lmao, meanwhile 5090 and 4090 users can sell their cards at almost msrp prices.",hardware,2025-11-01 17:24:21,1
AMD,nmkddp5,Rest in Piss AMD. Intel or Nvidia next time,hardware,2025-11-01 17:35:53,1
AMD,nn0ypap,"Don't worry, AMD has ""clarified"" things: you're just ""confused"". ~~The lamps use gas~~...  [https://youtu.be/dkPPejQXFNo](https://youtu.be/dkPPejQXFNo)",hardware,2025-11-04 08:11:40,1
AMD,nnosdwt,They cleared the air on this. We are still getting our updates [amd response](https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html),hardware,2025-11-08 00:19:08,1
AMD,nmbzizk,I don’t like these clickbaity thumbnails.,hardware,2025-10-31 07:27:14,0
AMD,nmdpzh9,"I don't really get the fuss.  It's okay to phase out support for older hardware. Yeah, RDNA 2 was still very wide-spead, but still.",hardware,2025-10-31 14:57:56,1
AMD,nmeeyax,Thank you HUB for continuing to look out for consumers!,hardware,2025-10-31 16:59:44,1
AMD,nmgup6g,They already clarified: they do not.  https://videocardz.com/newz/amd-clarifies-rdna1-and-rdna2-will-continue-receiving-game-optimizations-based-on-market-needs,hardware,2025-11-01 01:25:33,1
AMD,nmbd1ss,"This is being blown hugely out of proportion. People keep pretending like they dropped total support. No, they just dropped day one game dev driver support.",hardware,2025-10-31 03:57:33,-15
AMD,nmbpjej,"I just bought a brand new Zotac Zone 3 months ago, and now it'll no longer be receiving new game optimisations already.",hardware,2025-10-31 05:45:15,1
AMD,nmc4bj6,ai money > game money....   help us intel your our only hope!,hardware,2025-10-31 08:17:26,1
AMD,nmcp4ho,Unfortunately AMD product support has always been shit. AM4 was the exception to their history not the rule.,hardware,2025-10-31 11:26:57,1
AMD,nmcwt70,"I game on a MSI Claw 8 AI+, I don’t think this affects me",hardware,2025-10-31 12:19:12,1
AMD,nmcxf2j,Is there a petition? Something like amd give us back the fine wine?,hardware,2025-10-31 12:23:00,1
AMD,nmd4i8n,Can you RMA your GPU for this reason?,hardware,2025-10-31 13:05:05,1
AMD,nmd7sss,I was thinking about getting a 9060XT rig for some 1080p gaming but nah fuck AMD going with Nvidia after all.,hardware,2025-10-31 13:23:26,1
AMD,nmhvmjv,But then they don't and the insufferable have to walk back their fake outrage.   Yawn.,hardware,2025-11-01 06:33:14,1
AMD,nmbsvru,On the other hand Nvidia drivers are still borked on my 1070 so I have to use 566.36.,hardware,2025-10-31 06:18:42,-2
AMD,nmc9k6z,What a pathetic decision my AMD. I might even go back to Nvidia now. They are a little more expensive but their cards are supported much longer.  AMD... F U,hardware,2025-10-31 09:11:07,-3
AMD,nmcn02w,"Another windows related problem I'm glad I got rid off. I don't understand why an OS with such high market share gets dumped on so often by every manufacturer.  AMD and Intel GPUs run on open source drivers on Linux. Even all Nvidia RTX (GTX 16xx too) are getting more love on Linux nowadays.  Features and general improvements are still coming for Vega which I ran until recently. The oldest supported AMD GPUs on Linux are from the HD79xx series (real, not rebranded chips) from around 2012-2013 if I remember.   By supported I meant useful for daily use and normal consumers.  Meaning that all GPUs stated in the video are still completely fine, same goes for the USB-C output of the RX79xx.  In my eyes that's all planned obsolescence which here is irrelevant because of open source drivers. The corporations but also hundreds of free developers take part in. Should a corpo bail out then you still have the massive community doing its best to keep your ""old"" hardware out the landfill. Often engineers and devs from said corpos still help in their free time after said corpos ""pulls the plug"".  Thanks to the suits shitting on customers as always.",hardware,2025-10-31 11:10:41,0
AMD,nmcerx2,"Doesn't seem like it's that big of a deal, I have a 6900 and I got it like 4 or 5 years ago, it's probably time to upgrade.",hardware,2025-10-31 10:01:46,-2
AMD,nmec055,"I'm ashamed to have built lots of PCs for friends and family over the last few years, enthusiastically defending the decision to go with RDNA 2. This makes it hard to argue that I can trust them going forward. I really tried, but this will force me back to NVDA.",hardware,2025-10-31 16:45:26,0
AMD,nmbyyuq,Thats what you get for going AMD I guess💩 There is a reason why they only have 8% market share despite selling a lower prices.,hardware,2025-10-31 07:21:16,-5
AMD,nmce61m,is sad hardware unboxed chosed the way of click-bait and disinformation AMD state they still maintaining it rdna1 and 2 but just not get the day one game patchs and again rdna 1 and 2 lack features that the new vulkan and new games use...,hardware,2025-10-31 09:56:16,-5
AMD,nmd9ht5,"Well when they did support their gpus better with constant performance improvements in the past when even the features were pretty similar to Nvidia's (before RTX), they were still losing market share. you can only do so much when nothing you do really matters. plus looking at steam charts, doesn't look like anyone bought these much. so why is anyone even shocked lol.",hardware,2025-10-31 13:32:49,-1
AMD,nmcv2wa,that really sucks. I think they still have life in them,hardware,2025-10-31 12:08:11,0
AMD,nmbk5av,Hardware Unboxed never cared when AMD almost didn't support Ryzen 5000 on X370 but now they're furious. Too funny.,hardware,2025-10-31 04:55:25,-25
AMD,nmbuki4,Well I guess my next GPU would be from Greedvidia,hardware,2025-10-31 06:35:55,-10
AMD,nmb9rby,"HBU pointed out that Asus ROG Xbox Ally, a console that launched ~2 weeks ago, is RDNA2!!",hardware,2025-10-31 03:33:23,340
AMD,nmb8y26,I mean they dropped VEGA non-security support within the same year new APUs with VEGA were released. I guess this is one way to force AMD laptops to buy new chips,hardware,2025-10-31 03:27:30,99
AMD,nmbduty,I only bought my RX 6750 XT in 2023. 😭,hardware,2025-10-31 04:03:42,46
AMD,nmbkgyx,Don't forget Steam Deck.,hardware,2025-10-31 04:58:16,6
AMD,nme279s,"It's not really bizarre when you realize that AMD isn't ""dropping support"" and that this is all misinformation.",hardware,2025-10-31 15:57:14,1
AMD,nmeg1ig,"Yeah, people are blowing this out of proportion, Amd stated maintenance mode will still get game updates and other things, just no new features. And the USB-c thing was a mistake on their part, they've fixed it.  Their announcement was poorly worded, all's okay now.",hardware,2025-10-31 17:05:03,0
AMD,nmei7ni,">and now its getting axed, while even the 20 series are not...  Even the 10 series have been supported up to this month.",hardware,2025-10-31 17:15:44,6
AMD,nmc9o61,"Wasn't a tough choice at all, this is a great example of the difference between the two companies. Not a told you so, but also... It was at no point in contention based on all the history and the market / software positioning at the time. Just look at the posts in this subreddit back then: ""there are no dlss games."" I got chewed out here for saying there will be. The only thing incorrect in that response is how much further ai frameworks and hardware went than ""merely"" the best performing anti aliasing. My 3080 feels no age, and it's now joined by lots of 3090s in a cluster serving production workloads that my customers value highly. Ati has no appropriately valued in terms of effort answer.",hardware,2025-10-31 09:12:17,34
AMD,nmc5bmr,"Without driver side assistance that 6800xt will likely reach parity with a 3070, when the dust clears.  Pretty sure the 6950xt is only slightly faster than a 3080 for the same reasons now.",hardware,2025-10-31 08:28:04,3
AMD,nmc2mjl,"I went from a 6800xt to a 4090, and sure the 4090 is way way more powerful, but it made me realize that the 6800xt was not bad at all. especially if you undervolted that sucker.  It ran at higher clockspeed, ran cooler and you couldnt hear the fan. the performance in 4k gaming was actually decent.",hardware,2025-10-31 07:59:37,6
AMD,nmf3atc,"They nailed that gen and got almost no market share out of it, learned all the wrong lessons as a result.  They always go hard on the gens with new console releases, which i suspect is why they’re pulling resources from RDNA now.",hardware,2025-10-31 19:02:38,1
AMD,nmvj2c2,a 3080 vs a 6800xt was an easy choice.  3080 was much  faster in all the ways that mattered.,hardware,2025-11-03 13:26:35,1
AMD,nmbc4ma,Anyone owning a 7900XTX should plan on selling theirs within the next 2 years before it loses complete value,hardware,2025-10-31 03:50:42,92
AMD,nmcanv3,"These news also come hot on the heels of FSR4 being exclusive to 9000 series, while nVidia cards allows Transformer even on 2000 ~~Ampere~~ Turing series(Non Optical Flow FG is still an open question though). Intel upgraded their Xe with support for Alchemist.  Why would anyone invest in AMD card? Sure nVidia might have higher barrier for entry, but these cards have more features and you can keep them for longer. Not to mention they lose less of their value on the second hand market if you want to sell. And on the other end of the price spectrum we have Intel, sure it's rough right now, but at least they are trying and improving. Although, that being said, you should never buy based on promises.",hardware,2025-10-31 09:22:15,14
AMD,nmb8lvl,This is exactly how there marketing team expects them to retain market share and increase it. Stir the zealots up by getting everyone to trash talk your company. That way the Zealots will go buy the newest product to prove the trash talkers wrong.   I mean what could go wrong with such a solid straight.,hardware,2025-10-31 03:25:06,-11
AMD,nmcnv34,"you're not the market they're after,  you never were.",hardware,2025-10-31 11:17:20,-1
AMD,nmbbqk7,AMD back to doing AMD things.,hardware,2025-10-31 03:47:51,125
AMD,nmcj0nb,"At first I thought it was only a hyperbole that meant to be a joke, but the longer I follow tech news, the truer it is.",hardware,2025-10-31 10:38:47,8
AMD,nmc7pgo,"The most popular AMD cards. Without them, they are crushed by NVIDIA",hardware,2025-10-31 08:52:26,49
AMD,nme56k8,The Steam hardware surveys are totally manipulated and unreliable.,hardware,2025-10-31 16:11:43,-4
AMD,nmbqc2g,"They've not had a good track record in this for ages. GCN gets dropped 4 years earlier than Maxwell, Vega gets axed prematurely, and now RDNA 1/2 gets the boot, probably because they're refocusing on the ML neural processing stuff with Redstone, and these legacy products were just a stopgap when Nvidia took the RTX path.",hardware,2025-10-31 05:53:02,37
AMD,nmdgc8k,"I hate the Green team to an extent, but realistically they're the only good option on the market right now. After loving ATi for years, my first GPU I absolutely loved which was an HD 5670 and then later on the HD 7970 GHz and the R9 290X. But after all those great competitive years, came the RX 480 and Vega. Really Vega was the straw that broke my back, they completely fumbled the launch of it, it was late to market, used lots of power at the time relative to the competition and slow compared to the 1080 Ti, with only 8GB of VRAM and then on it's way out the door like you said... they dropped support of Vega early and to be honest the Vega and RDNA1 drivers were terrible for months. Honestly, it just turned me off AMD for good. Rather than converting customers and keeping loyal fans they turned me away.",hardware,2025-10-31 14:08:50,4
AMD,nme6ddg,"Dude, same! I went from a Vega 56 to a 6800xt. I was planning on getting a 9070xt soon but I guess I'm going team green now? 🙃",hardware,2025-10-31 16:17:34,1
AMD,nmc1h6e,Vega is still getting driver support.,hardware,2025-10-31 07:47:50,-9
AMD,nme5h3c,"You shouldn't have left your Vega behind. You were sold a lie by influencers, and you're being sold another.",hardware,2025-10-31 16:13:09,0
AMD,nmi6iby,Probably,hardware,2025-11-01 08:35:54,1
AMD,nmbf2fk,"It wasn't even that nice, msi just dropped the updates for all their boards and more or less told AMD they could drop them as a board partner or shut up",hardware,2025-10-31 04:13:07,115
AMD,nmf6k3z,"Reminder that they were also going to block support for 400-series chipsets.  > Q: What about (X pre-500 Series chipset)?  > A: AMD has no plans to introduce “Zen 3” architecture support for older chipsets. While we wish could enable full support for every processor on every chipset, the flash memory chips that store BIOS settings and support have capacity limitations. Given these limitations, and the unprecedented longevity of the AM4 socket, there will inevitably be a time and place where a transition to free up space is necessary—the AMD 500 Series chipsets are that time.  https://web.archive.org/web/20200528020249/https://community.amd.com/community/gaming/blog/2020/05/07/the-exciting-future-of-amd-socket-am4  Link is to the archive, the original post was either deleted or they changed how the community post urls work.  Funny that it magically ended up being a non-issue for 400-series chipsets, but 300-series were impossible. Then later, 300-series magically were able to work.",hardware,2025-10-31 19:19:42,10
AMD,nmbf5ml,I literally bought a 12600k out of spite because of this only for my X370 Crosshair to get support for the 5000 series.,hardware,2025-10-31 04:13:49,26
AMD,nmbk713,You stole my comment.,hardware,2025-10-31 04:55:50,4
AMD,nmdp5jh,">AMD actively blocked official Zen 3 support on B350/X370 motherboards for over a year  That was due to the size of the flash storage for the UEFI/BIOS. They didn't want to put out an update that broke a bunch of systems but in doing so, made people upset about the lack of an upgrade path.",hardware,2025-10-31 14:53:47,-1
AMD,nmbkpu7,"It was a bit of a clusterfuck though, as the amount of storage space available for the BIOS varied between motherboards (which is *why* they originally didn't want to support the old motherboards).",hardware,2025-10-31 05:00:28,-17
AMD,nmd8hdc,"> actively blocked  > relented  I hate how this gets dramatized and hated it at the time, too. They didn't want to have official support for something that would literally blow up in their faces. Once they were sure it was broadly fine, it was fine.",hardware,2025-10-31 13:27:12,-6
AMD,nmbxqto,"> To not support the series with drivers, when this could render the GPU incompatible with new games is a DISGRACE. I bought my 6650 XT planning to use it until 2027, hoping that my next sub $300 GPU would offer twice the performance and more than 8 GB VRAM. I dont wanna upgrade now, spending $250-300 for 10-50% performance improvements (seriously, the 5050 is barely faster than the 6650 XT in raster) and 8 GB VRAM. I just dont. As such, I guess I'm gonna be stuck using an obsolete and no longer supported card for the mean time.  To be clear, AMD is still providing drivers -- they're just not providing new game-specific driver-level optimizations.  I'm not sure how much this actually matters for older cards, since engine support for RDNA 2 and earlier should be pretty dialed-in to the point where driver-level optimizations shouldn't be as necessary. Not supporting new Vulkan extensions could potentially be problematic, but I'm also not sure it matters if these extensions are meant to support hardware that RDNA 2 cards (and earlier) don't have.  [Vega and Polaris are also in this ""extended support"" phase](https://www.reddit.com/r/Amd/comments/17qn9e3/amd_begins_polaris_and_vega_gpu_retirement/), and looking at the driver pages for a Vega and Polaris card (I chose the Radeon VII and the Radeon RX 480), their latest driver update was in August 2025, which isn't that long ago. I would be curious to see if owners of these older cards have seen any degraded game experiences beyond just the expected aging.",hardware,2025-10-31 07:08:29,44
AMD,nmcb3i9,"I feel like this is a bit disingenuous. I hate how expensive graphics cards are as much as the next guy and these business practices are definitely fucked up.   But paying 350 € for a 9060 XT 16 GB in 2025 is like paying 270 € in 2015. That's a crazy good deal for this type of performance, no? What even is a comparable card from 2015 for that price?   Inflation is a real thing. I can't think of many other products with this much performance increase over any specific time period that doesn't shoot through the roof in pricing.  Edit: OP is the kinda pathetic user that blocks people just because they offer a nuanced take in a discussion forum. I hope someone reports them because who needs that kinda behavior on this sub? And of course they had to get another snarky reply in before blocking.",hardware,2025-10-31 09:26:38,18
AMD,nmdwi98,"Am also using a 6600 it’s kinda of a bummer but I’m happy so long as it actually still gets some form of support. It’s frustrating because afaik, this is the most popular generation. In my country, ts is still sold at $240 brand new and the next best choice is the intel b550 which is sold at $340 so upgrading is barely even a choice.",hardware,2025-10-31 15:29:57,1
AMD,nmbm1k1,So you're saying you should've bought a 3060.,hardware,2025-10-31 05:12:25,0
AMD,nmblajp,"You have to wait until 2028 since you're a 2nd hand & sub $300 buyer to get 2x 6650xt perf. Nvidia/amd owe me 4080 perf at $300 because 2060 matched 1080. ""I wont upgrade until that happens, until that happens, you lose ngreedia""",hardware,2025-10-31 05:05:38,-14
AMD,nmenmrb,It's called capitalism.,hardware,2025-10-31 17:42:52,0
AMD,nmbwm9i,afaik fine wine was a term made by up by us not amd.,hardware,2025-10-31 06:56:51,57
AMD,nmbkbib,Reddit: nah it was totally untapped performance.,hardware,2025-10-31 04:56:55,37
AMD,nmkm4ow,"Fine wine definitely was a thing back with the GCN architecture. Part of GPU drivers is bug fixes, but the whole game ready part is Nvidia/AMD optimizing games for the developer. Back with GCN the AMD architecture was a bit more complex and developers used it poorly. Developers did stupid things like tessellating concrete barriers and the surface of the water to hell and back. So over time you would have real gains and the AMD cards would have a better lifespan than their Nvidia counterparts. The RX 580 was phenomenal.  I would call the fine wine back in the day more shitty developers than shitty drivers.  Fine wine ended with RDNA, it was much simpler to optimize and all over the consoles. That means RX 5000 series going forward don't get those same bumps over time and we've seen it.  AMD fans repeating fine wine these days are talking about a golden age of Radeon that has long since ended without ever having experienced it themselves.  The better aging has been on the Nvidia side with things like DLSS squeezing more performance out of cards. Framegen is going to be a godsend for the 40-series and 50-series in 4-5 years.",hardware,2025-11-01 18:20:35,2
AMD,nmbjq5i,"Even if that were true, and the drivers are just shit at launch, you pay for the performance you get at launch, and any additional performance that comes from future driver improvements is free performance you didn't pay for.",hardware,2025-10-31 04:51:47,2
AMD,nmcclco,">  fine wine was just marketing for, ""we have shitty drivers at launch"".  Sure, but for the consumer it meant they were getting a more beefy card than they paid for... eventually.  For some that works out. For others it does not.",hardware,2025-10-31 09:41:34,-2
AMD,nmcptnj,> It is not uncommon for AMD to have shorter driver support or continued optimization.  [AMD only put Polaris on a slow-track after 8 years on the market.](https://www.techpowerup.com/315547/amd-puts-radeon-vega-and-polaris-gpus-on-a-slower-driver-update-track),hardware,2025-10-31 11:32:12,-1
AMD,nmbv1o7,"I'd wager they lose game optimisation before the RTX 40 series does, especially as they lack dedicated hardware for ML upscaling (FSR4 hardware acceleration).",hardware,2025-10-31 06:40:43,11
AMD,nmf7bf4,"If UDNA ends up being as much of a change as gets suggested, then it will absolutely happen.",hardware,2025-10-31 19:23:42,1
AMD,nmcuko2,Current gen AM5 9000 series Ryzens still have RDNA2 iGPUs lol,hardware,2025-10-31 12:04:50,12
AMD,nmguuve,"Yeah both ""issues"" weren't actually issues, but miscommunication on their part. They already apologized.",hardware,2025-11-01 01:26:39,2
AMD,nmdqgvv,It's only ended game specific optimizations. Other updates will still come but they don't promise anything from this point forward. So we have no idea.,hardware,2025-10-31 15:00:21,1
AMD,nmfa206,"You could also not buy new cards and play older games instead. NV comes with its own baggage and I don’t think it’s better than AMD, they are pretty much similarly bad towards their consumers.",hardware,2025-10-31 19:38:16,1
AMD,nmibgbo,"2060 super outlived 5700xt, lmao it was selling at the same price",hardware,2025-11-01 09:30:49,2
AMD,nmvlegx,It is unbelievable just how wrong this sorry narrative is. But people lap this garbage up and updoot it into places it doesn't belong.,hardware,2025-11-03 13:40:30,1
AMD,nmbfkje,*This* is? For how long have you known this was going to happen? And why didn't you tell anyone?,hardware,2025-10-31 04:17:05,5
AMD,nmpr61c,Same with my 2080 ti,hardware,2025-11-02 15:17:19,1
AMD,nmdxsfx,Only with hacks the 20xx series only officially Supports dlss1 and 2,hardware,2025-10-31 15:36:09,-3
AMD,nmbo8xv,Funny how a laptop with a 2060 will have longer GPU support than a laptop with a RX6800S,hardware,2025-10-31 05:32:47,18
AMD,nmdglv7,AM4 look bad now compared to current CPU. Will admit it was good during its time when Intel was weaker. At least the CPU side of things are more sane and with Intel coming up soon excited ag the possibility of longer drivers with team blue and green combined.,hardware,2025-10-31 14:10:13,1
AMD,nmbep10,Why?,hardware,2025-10-31 04:10:14,-1
AMD,nme3j45,"No, you're falling for a blatant lie, because that's the only way anyone wants to talk about AMD anymore.",hardware,2025-10-31 16:03:40,-2
AMD,nmch202,"Seeing what is sometimes output by vendor shader compilers, yes, driver optimization does exist and AMD’s drivers are definitely not finished on that front.",hardware,2025-10-31 10:21:56,5
AMD,nmcwxvu,"> Does driver optimisation really exist?  Depends I guess. Nvidia seem to do a lot of it, not sure about AMD.  Nvidia have always said many developers don't have a clue what they're doing and there are always a lot of optimisations required. Given the complexity of modern APIs there nearly always seems to be giant - potentially game breaking - bugs that need fixed too.",hardware,2025-10-31 12:20:02,4
AMD,nmd6rqs,"While you are correct... The problem is that the driver is not gonna be updated from here onwards, so no new Vulkan extensions, and DX12 features.  Those GPUs could very well shit themselves in a couple of years with any random game.",hardware,2025-10-31 13:17:44,2
AMD,nmcfc5i,"At first glance you'd think so:  https://www.youtube.com/watch?v=aWfMibZ8t00  But there is a catch, HWU did not express themselves clearly, and that was launch review data vs fresh, not fresh vs fresh, and other youtubers stirred a bit of drama, because they couldn't replicate the gains by testing just the drivers. For example:  https://www.youtube.com/watch?v=hf1q1nwoj8k  I suspect that the primary reason for rushing game ready drivers is to narrow the range of drivers being used, to make for much easier QA.  Personally I only update when there is promising feature or bugfix to be added, as in last few years nvidia tended to break a lot. This is where I would see issue with slowed down/maintenance drivers. In AMD case people have expectation of FSR feature update for older GPUs and this might mean them not getting it on older hardware if that happens. That'd be a good reason to be upset.",hardware,2025-10-31 10:06:48,1
AMD,nmbk20n,You realize how old DirectDraw is right? I'd expect whatever game you're running in VR that uses that would need a graphics wrapper to perform correctly in Windows Vista and newer (even with Nvidia graphics cards).,hardware,2025-10-31 04:54:39,5
AMD,nme4xde,"What. DirectDraw is basically deprecated since the Windows 7 era, nobody works on or cares about it anymore.",hardware,2025-10-31 16:10:27,1
AMD,nme5yj2,Sad to see people openly screw their friends like this.,hardware,2025-10-31 16:15:32,4
AMD,nmcntxh,Was it the laptop thing that requires the OEM drivers? Even Nvidia does that.,hardware,2025-10-31 11:17:06,2
AMD,nmbpny0,It probably didn't. Shes basking in the glory of the OpenAI deal right now and could give less of a fuck about gamers.,hardware,2025-10-31 05:46:28,66
AMD,nmbu8t6,"Lisa Su is not a gamers friend, she follows the money and has shareholders interests at heart, not gamers.",hardware,2025-10-31 06:32:34,30
AMD,nmbug03,This is completely on brand to the way AMD graphics division is run under Lisa.,hardware,2025-10-31 06:34:38,32
AMD,nmfbho9,She is the one diverting resources to AI lmao,hardware,2025-10-31 19:45:58,2
AMD,nmcm09t,"Seriously? She *is* part of the problem at AMD, but no one can publicly admit it, my guess is predominantly due to identity politics. While she's done good things (or employees under her have, ie: Jim Keller), which include winning contract bids for consoles, the majority (if not the entirety) of the company's success via Ryzen and EPYC, derive from Zen (re: Keller).  She's like a modern day Steve Ballmer, a la Tim Cook; maintaining the status quo, comfortable and complacent, completely devoid of innovation, vision, or the courage to make bold moves which entail risk, often due to fear or cowardice.  Who do you think hired Jack Huynh, the ruthless businessman running Radeon (incl the guy before him, or the guy before that guy, Raja Koduri)? AMD's marketing is beyond embarrassing; they're simply pathetic. Robert Hallock was the only bright spot and he left in order to literally work for their biggest competitor.  You don't think she signs off on (or oversees) key business decisions like pricing, FSR, RT? What about the bumbling launch of Zen 5? Or how they asked influencers on how to price RDNA 4 and for months, the community basically *begged* them to not fuck up another golden opportunity (don't be or pull an AMD), and somehow they were still planning to launch w/ an MSRP of > $700 (\~$750, IIRC), up until the announcement and had to cancel the presentation hours before *during* CES.  Then, after all that, the BEST they managed to do was copy Nvidia (-$50) by listing a fake MSRP, at least in the US, then lied about availability when claiming inventory issues were solely due to demand despite having warehouses full of cards for months from the delayed launch which was due to waiting for Nvidia pricing + selling ALL RDNA 3 inventory to milk RDNA 4 pricing (and not due to the expiring rebates they weren't going to provide retailers or the fact that AIBs would only sell models > $2-300 above MSRP).    Meanwhile, you didn't think it was fishy that  coincidentally they just so happened to conveniently fail to manufacture Reference cards for the first time since I can even remember, meaning that RDNA 3/2/1, Vega, and Polaris ALL had reference cards, so at min dating back at least > a decade)?",hardware,2025-10-31 11:02:53,3
AMD,nme8lzj,No. It's not going to be any worse than it has been up to this point.,hardware,2025-10-31 16:28:44,1
AMD,nme7nha,"This is about how any GPU support works. Nvidia is no different. They will *publicly claim* otherwise for internet clout, but the outcome is the same. Never mind that Nvidia drivers in general are in a really bad spot right now, and apparently this will continue for the foreseeable future.  Best of all, this doesn't apply on Linux! AMD cards always get *everything* that card can *possibly* support, whether AMD is willing to do it or not, and even when it's almost unreasonable to do so. Meanwhile, Nvidia cards have to deal with whatever the official support is.",hardware,2025-10-31 16:23:58,1
AMD,nme8523,"It's more shitty clickbait misinformation around AMD actually daring to tell the truth about how GPU support works. This video exists *solely* to feed AMD haters some slop, nothing more.",hardware,2025-10-31 16:26:23,1
AMD,nmhqtrn,"If i could go back in time i would've got a 5070ti, i wouldn't have liked it, but it would've been the better pick",hardware,2025-11-01 05:41:02,1
AMD,nmctuz5,"Nvidia do game specific driver optimization at least for the 3000 series, it does make a difference. Also, I got DLSS4 and DLSS override in their software for my 3080. When I had a Vega 56, AMD even broke older features in their drivers.",hardware,2025-10-31 12:00:06,8
AMD,nmcx97i,"> You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'  Sounds like a hot scoop, you'd think someone would have reported this",hardware,2025-10-31 12:21:59,7
AMD,nmchils,another example of why you should never ever be transparent to an average consumer,hardware,2025-10-31 10:25:54,7
AMD,nmd5yx4,>You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.  Nvidia just stop supporting 1000 series card for game ready drivers and they will only push security stuff... we are talking about GPUs that are 9 years old at this point.,hardware,2025-10-31 13:13:15,5
AMD,nmijx1x,"""You think nVidia does driver optimization for 2000 and 3000 series cards? fuck no.. Those cards are also in 'maintenance mode'""     This is just wrong completely. 20 series are still on a the mainline driver branch as of today. AMDs maintenance drivers are separate like for vega which doesn't get the latest version.",hardware,2025-11-01 10:57:00,1
AMD,nmbon3e,">game optimization is the developer's job not AMD's  In a perfect world yes, but IMO the past decade has shown us how many devs are overwhelmed with properly and correctly utilizing DirectX 12, and in a way that does *not* cause shader compilation stutters. It's what led to awkward situations where DirectX 11 had or still has better performance / less or no stutters (e.g. see Vermintide 2 with shader stutters in DirectX 12 mode, or various Unreal Engine games from the past years)",hardware,2025-10-31 05:36:35,23
AMD,nmbhhje,"""Critical security and bug fixes"" maintenance usually means on softwares that the EOL is approaching and support is kept on a thin lifeline ( not substancial )   That is a common enough catchphrase to raise the redflag, not sure why we have to split hairs here.  Wether the cards are mature or not is out of the question when new games are continuously added, and drivers get updated, it's not like those didn't matter last month ...",hardware,2025-10-31 04:32:45,57
AMD,nmc5wyf,"> If there is a new DX12 or Vulkan extension AMD will add it.  This is precisely the opposite of what *is already happening* with Vulkan extensions w.r.t RDNA2 support.  > If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  Luckily for Steam Deck owners, they use a driver primarily developed and maintained by Valve (or people paid by Valve), not AMD.",hardware,2025-10-31 08:34:15,11
AMD,nmbrxh6,">AMD state they will maintain it for bugs which is all it needs.  That's not what they do. They freeze development by branching the code bases off. This effectively means they no longer do development on the legacy branch.   Even in theory fixing bugs in old unmaintained code is difficult because it stops looking like the cold you're working on on a  regular basis. But in practice they basically never fix bugs outside of the most severe and most basic. Crash fixes to extremely  basic desktop use.  But at a broader general level your statement is incorrect. GPU drivers need constant development because the targets for development constantly change.   A new Vulkan version is imminent (announced by Mesa devs very recently), and RDNA 1 and 2 cards won't get it even though they're fully capable. New DX versions are released  frequently as well, again same issue.  Game development changes and they start using these newer APIs, or they use existing but barely or never used features, triggering issues in drivers due to lack of testing. This happens quite frequently. These GPUs are only a few years old , some 5 years max.   >If a developer is optimizing for RDNA2 and finds a bug AMD will still fix it.   They won't. They don't do any of this in the legacy branch for Vega.  >If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  They've never done this for the legacy Vega branch. It's stuck at exactly the Vulkan and DX API when they froze development, even though the hardware is far more capable and is fully Vulkan 1.4 compatible along with a long list of other extensions AMD never added.   On Linux the development is partially done by the community and Valve at the kernel level,  and at the usermode level (for Vulkan) it's entirely by the Mesa community and Valve, with no AMD involvement.   As you can guess the driver support on Linux is light-years ahead. Everything from newest rdna4 to GCN 1.0 is still fully supported and updated with constant Vulkan extensions and full scale development broadly at all levels of the driver stack.   Yes this applies to all sorts of GCN hardware because they're extremely capable still. You can do shader emulated ray tracing on Vega and RDNA1 with playable performance.on Linux. There are YouTube videos of people showing Indiana Jones off running on this hardware.  If they were good stewards of the legacy branch you might have a point but they very literally branch it off and freeze it and basically never touch it again.",hardware,2025-10-31 06:09:02,37
AMD,nmctf3k,"> If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  The announcement said literally the opposite. Do you think just saying the opposite of reality is gonna make it true?",hardware,2025-10-31 11:57:10,7
AMD,nmbk5tm,"I think the main problem is no new game support like Nvidia/Intel does with their Game Ready/Game On drivers, which means as time goes on the performance of RDNA2 relatively will fall further and further behind Nvidia's or even AMD's own RDNA3. RDNA4 gpus in newer games  >These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.  this should be true in an ideal world but we know a lot of game are unoptimized mess and need launch drivers to not run like shit",hardware,2025-10-31 04:55:33,20
AMD,nmbg13i,"It’s really weird how they manage which series to support, like didn’t Polaris JUST got into the same split driver this year?",hardware,2025-10-31 04:20:43,6
AMD,nmc64s8,"Driver side support is still needed for peak efficiency where multiple archs are targeted.  The cards will still work, just not efficiently.    That's not something anyone should be celebrating when RDNA2 is AMDs only successful generation, arguably better than it's competing Nvidia generation if you don't care about RT.",hardware,2025-10-31 08:36:31,2
AMD,nmvlk3s,"Ragebait is their bread and butter. As these comments clearly show, the internet fell for it yet again.",hardware,2025-11-03 13:41:24,1
AMD,nme624v,"Don't worry, the video content is all clickbait too!",hardware,2025-10-31 16:16:02,0
AMD,nmbi2fo,"> ""RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenaline Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""  > -AMD to PC Games Hardware (translation)  Seems in proportion.",hardware,2025-10-31 04:37:36,25
AMD,nmbgcnr,">No, they just dropped day one game dev driver support.  Did you just make that up ?   Adressing ""critical"" issues like bug fixing and security isn't optimising games "" later on "".  It's plain and simple an EOL around the corner.",hardware,2025-10-31 04:23:20,18
AMD,nmbeyej,nobody want 1 driver every 2 years like they did with their vega GPUs that were supposed to have lot of VRAM to be future proof lol.,hardware,2025-10-31 04:12:15,6
AMD,nmbev3d,"95 percent of people won't notice the difference. What AMD should do is continue 'supporting' the cards, eg let the latest drivers still be installed, not change a line of code for said drivers, and let placebo do the work.",hardware,2025-10-31 04:11:33,5
AMD,nmbf1tf,Which is extremely ridiculous. Day 1 driver support is not anything hard to do and should be a basic expectation. Now you’re at the same level parity as a 1080ti.,hardware,2025-10-31 04:12:59,4
AMD,nme7vor,"I mean, AM5 is going to have at least 3 gens of CPUs, so this is yet another lie.",hardware,2025-10-31 16:25:06,0
AMD,nmjcoc2,The outrage will stop if AMD backtracks. But it doesn't seem they will.,hardware,2025-11-01 14:22:24,2
AMD,nmbw4cr,Really? What issue are you running in to? I'm currently rocking a 1070Ti with 581.08 in Windows 10 without issue. There was a fairly long period earlier this year with Nvidia drivers causing my system to randomly hard lock that kept me from updating but it seems they resolved that in the last 2 or 3 months.,hardware,2025-10-31 06:51:46,5
AMD,nmc0146,"Nvidia drivers have been absolute garbage since Nvidia started focusing on AI. No one has called them out besides JayzTwoCentz AFAIK, which goes to show how great techtubers are at holding companies accountable.  The fact that Maxwell and Pascal are being dropped with these drivers is sad.",hardware,2025-10-31 07:32:32,2
AMD,nme6h53,"That is incorrect, this is misinformation.",hardware,2025-10-31 16:18:06,1
AMD,nmddz7y,Do you even know that Linux drivers are mostly not from AMD? What kind of logic make you think they abandon a large portion of user but somehow still going forward with much smaller group? This doesn't affect Linux simply because there is nothing to be affected from the beginning,hardware,2025-10-31 13:56:40,1
AMD,nmctq5s,...and this is exactly why they do shit like this. Chumps like you will just come back to daddy for some more abuse (like a battered wife).,hardware,2025-10-31 11:59:12,4
AMD,nmvl6z5,This decision doesn't force you to do anything. Your RDNA 2 card isn't suddenly trash.,hardware,2025-11-03 13:39:17,1
AMD,nme8jp5,"Putting aside that the Steam charts are manipulated, likely for the reasons you describe, you're essentially right. The internet wants AMD gone for some hateful reason, it's that simple. If that happens, GPUs will get way shittier, but apparently that's what people want. So stupid.",hardware,2025-10-31 16:28:24,-1
AMD,nmbmmf9,"We were one of the main driving forces that saw AMD revert their decision, but sure, make stuff up: [https://www.youtube.com/watch?v=NsBRNck\_-wA](https://www.youtube.com/watch?v=NsBRNck_-wA)",hardware,2025-10-31 05:17:43,32
AMD,nmc2k4d,"Aka: The Steam deck but with Asus and Xbox branding for more  edit: [Ryzen Z2 A GPU (CPU is listed in notes)](https://www.techpowerup.com/gpu-specs/ryzen-z2-a-gpu.c4306) and [Steam Deck GPU (CPU in console notes)](https://www.techpowerup.com/gpu-specs/steam-deck-gpu.c3897) If you lot look at this and think its not the same Chip, you're an idiot and believing Microsofts crappy advertising. Same architecture, same GPU name, same clock speeds, and same Theoretical Performance metrics. A 1080p screen, bigger battery, and supposedly ""better"" ergonomics don't make such a system go from 499 (Steam Deck) to 799 (ROG Xbox Ally), This is price gouging",hardware,2025-10-31 07:58:56,72
AMD,nmdcic3,"Worst part about that is that neither MS or Asus will give a shit, because if they did, they’d have skipped launching that product in the first place",hardware,2025-10-31 13:49:03,11
AMD,nmbrhm1,It's RDNA3.5. Steamdeck's Van Gogh is RDNA2 if I remember right.,hardware,2025-10-31 06:04:36,-19
AMD,nmhr3t3,"Not to undermine the shitty behavior from AMD but … rog ally from years back uses z1 extreme which is RDNA 3, and the Xbox ally uses z2 which is rdna 3.5 If he says that, he’s just wrong.",hardware,2025-11-01 05:43:56,0
AMD,nmbb533,"There is  the rebrand of older Ryzen mobile APU already,   [https://www.reddit.com/r/hardware/comments/1ohh7fp/amd\_again\_reshuffles\_mobile\_lineup\_with\_ryzen\_10/](https://www.reddit.com/r/hardware/comments/1ohh7fp/amd_again_reshuffles_mobile_lineup_with_ryzen_10/)  So time to get the same old tech, just with a new name and usually the same price.",hardware,2025-10-31 03:43:30,33
AMD,nmc0h1d,I am half torn about it. I got 6700xt last year with the thought of running Linux and Windows. Should have taken a shot with the B580 in hindsight.,hardware,2025-10-31 07:37:19,9
AMD,nmbknqx,"Did you know Valve using RADV Driver, Linux Driver didn't affect by this change.",hardware,2025-10-31 04:59:56,37
AMD,nmd2z02,Yes but Steam Deck is Linux so no problem!,hardware,2025-10-31 12:56:20,3
AMD,nmh4bh2,"It really does seem like clickbait and ragebait.  At this point, AMD has probably exposed pretty much everything the hardware actually does in the drivers.  Even if they were doing ongoing feature work for the drivers, if a game comes out next year that requires a new feature then the old hardware probably wouldn't run it regardless.",hardware,2025-11-01 02:32:47,1
AMD,nmikqx8,"And 900 series, both Maxwell and Pascal ended support at the same time. Which makes sense since Pascal is just a tweaked and improved iteration on Maxwell.  You could have bought a card in 2014 and gotten driver updates up until last month.",hardware,2025-11-01 11:04:49,3
AMD,nmcelga,"What resolution are you playing at? My 3080 didn't have enough VRAM for the highest textures in Diablo4 when I played at 3440x1440, and lowering it to the next highest texture was a proper downgrade in quality as it looked very blurry in comparison. I ended up upgrading it 2 years ago due to that.",hardware,2025-10-31 10:00:10,18
AMD,nmcacol,"I mean, I brought a 2080 TI BE (evga exlcusive reduced cost variant), and at launch of it and not later, I still maintain that there was very little games for it, BFV and later Control were it, and if you didnt like them then tough  and DLSS1 was shit too, esp BFV with RT on and DLSS on, it wasnt great tech at the time.  and by the time DLSS was widespread, its more the 30 series and 100% in the 40 series. the first time I think I truly loved and saw the future of DLSS was with cyberpunk 2077, where people were complaining of issues, while the 2080ti with DLSS on played perfectly fine at 4k 60 without RT on, but that was the launch of the 30 series and it was still not very widespread. and RT chugged the card hard.   it wasn't what swayed me buying a 2080 ti, it was EVGA's support and warranty. and the fact that i had a 4k60 screen and nothing, NOTHING else can push that properly raw or DLSS.",hardware,2025-10-31 09:19:13,5
AMD,nmcvlpy,You are doing your customers dirty by not using pro-level hardware.,hardware,2025-10-31 12:11:36,1
AMD,nmd73p7,Right now a 10GB 3080 is often performing much worse than a 16GB 6800XT.,hardware,2025-10-31 13:19:35,-3
AMD,nme0vk6,"To be completely fair    there was several years there back in the day where PhysX was the extra feature that was being sold as the reason to get Nvidia, and that wound up amounting to nothing.    So it wasn't entirely unreasonable to expect that DLSS wouldn't amount to anything either",hardware,2025-10-31 15:50:55,-1
AMD,nmez1v6,Bro you're tripping I had to replace 3080ti because it was clearly showing age and couldn't keep up with modern games.,hardware,2025-10-31 18:40:59,0
AMD,nmc9gzj,"its more per game eh, on average you are right, but a lot of game do perfer AMD what with them being the console GPU and all that, and esp if you turn on RT then well...  but that is still plenty powerful right, a modern 5060 will still struggle to out do it in raster performance, and its massive 16 GB vram of the time means it has longer run life than the 8 GB variants too  nvidia fucked with the entire stack in 40 gen to give you one tier down, and that continued even farther in 50 gen so what was a 50 card is now a 60 card and that is why the generational increases are not that big and thus these older cards are still very much powerful and good.  and both AMD and nvidia has taken the piss with pricing on top of giving you a full ass tier down in terms of chip, a 650 dollar card (in theory, we know pandemic etc. etc.) now is not a 80 series card but the 9070XT, which while is better, is not exactly 2 generational leap better that you'd think it would be.",hardware,2025-10-31 09:10:13,9
AMD,nmfmjc8,">Without driver side assistance that 6800xt will likely reach parity with a 3070, when the dust clears.  6800 maybe, but not 6800XT. The 6800XT is substantially faster than the 3070 (in raster). Drivers aren't going to magically compensate for a 25% performance gap.",hardware,2025-10-31 20:44:56,2
AMD,nme13pk,> 6950xt is only slightly faster than a 3080 for the same reasons now    dude I had a OC formula 6900xt for quite a while and that thing was faster than a 3090 at native,hardware,2025-10-31 15:51:59,1
AMD,nmc3hh6,"yep and with nvidia being fuckwads and the 80s are gimped to hell and back, and a 3080 to 5080 jump isnt even the normal 100% increase and if you wanted that you needed a 5090 or 4090.   the same would be said for anyone with a 6800XT or above, which means for many MANY people this is still really a powerful chip that can do a lot of current gen gaming, and the 90 series being mid end means you dont get much at all jumping up, and they are more expensive msrp wise (although pandemic etc etc)",hardware,2025-10-31 08:08:34,6
AMD,nmcc9f0,"The thing is, who are they going to sell it to?  I think the type of people to buy used GPUs of that calibre are also the type of people to have heard the news and who know the price should be significantly lower.",hardware,2025-10-31 09:38:21,34
AMD,nmcp6sm,"> Anyone owning a 7900XTX should plan on selling theirs within the next 2 years before it loses complete value  ""Wow, thing X is no longer going to be supported. ***I should pass this problem on to some clueless, other rube.***""",hardware,2025-10-31 11:27:25,10
AMD,nmczcne,"I mean, you shouldn't be making purchasing decisions on deprecating assets based on future resale value. You buy them for the value they provide to you at time of purchase.",hardware,2025-10-31 12:35:06,3
AMD,nmbxs0d,Sounds like it’s going to be a pretty good time to pick up 7600XTs soon if they’re dropping below 3060 prices.,hardware,2025-10-31 07:08:51,11
AMD,nmemgy3,"TBF, nvidia's simply reaping the benefits of being afirst adopeter. They reakesed the 20 series at a higher cost with features deemed 'useless' back then, while AMD focusedon raster.",hardware,2025-10-31 17:37:03,3
AMD,nmcg258,> Why would anyone invest in AMD card?  Reddit karma,hardware,2025-10-31 10:13:09,19
AMD,nmdqody,>Why would anyone invest in AMD card?   Better Linux support is the only reason why I'm using an AMD card,hardware,2025-10-31 15:01:23,2
AMD,nmcvaz2,"> Why would anyone invest in AMD card?  A functioning moral compass. Even with this massive L on AMD's record, Nvidia are still markedly more anti-consumer.",hardware,2025-10-31 12:09:40,-17
AMD,nmbaf3x,The marketing team was not making this decision bro,hardware,2025-10-31 03:38:17,83
AMD,nmcg39p,AMD has a marketing team?,hardware,2025-10-31 10:13:25,7
AMD,nmbh5dk,"That only really works if there's new products to buy. I had a 7900XTX and they decided to not make anything worth buying this gen, so I dipped.",hardware,2025-10-31 04:29:56,8
AMD,nmdbxwl,"The question is, what is their market outside of a handful of forum users?",hardware,2025-10-31 13:46:03,3
AMD,nmbmfyy,"The stock went up like $100 bucks in a few weeks, AMD isn’t missing, they simply don’t care anymore about certain customers",hardware,2025-10-31 05:16:03,57
AMD,nmbpzw7,They never stopped.,hardware,2025-10-31 05:49:41,18
AMD,nme2q7n,"This ""news"" is misinformation. You've been exposed to lies for so long that you don't know what the truth is.",hardware,2025-10-31 15:59:45,-6
AMD,nmdu3rq,"Literally, rDNA2 was the last gen amd was competing because dlss was quite early and not very good",hardware,2025-10-31 15:18:26,0
AMD,nmccfon,"I think its pretty clear that people are referring to full driver support and not just keeping the lights on quarterly security updates. More than that, they are talking about their Vega in the past, and their 6800xt in the present.",hardware,2025-10-31 09:40:03,12
AMD,nmbk7s1,Glad someone else remembers this.,hardware,2025-10-31 04:56:01,36
AMD,nmdpqin,That was an excuse and a lie on AMD's part - see my [other comment](https://www.reddit.com/r/hardware/comments/1okjrn7/comment/nmbl86p/).,hardware,2025-10-31 14:56:41,8
AMD,nmbl86p,"That wasn't really why. High-end X370 boards with 32Mb BIOS chips didn't get support, while low-end B450 boards with 16Mb BIOS chips did. The first 300-series chips with official Zen 3 support [ended up being the lowest-end A320 boards](https://www.tomshardware.com/news/vendors-finally-enable-ryzen-5000-support-on-a320-motherboards), while official X370 support didn't roll out for [almost another two months](https://www.techpowerup.com/290832/asrock-first-out-with-official-support-for-zen-3-cpus-on-x370-motherboards).",hardware,2025-10-31 05:05:03,32
AMD,nmbkzm3,Who's fault it was a clusterfuck? Who advertised compatibility?,hardware,2025-10-31 05:02:54,24
AMD,nmd8k4p,You aren't wrong. I made a point of getting a board with the larger amount of storage (can't remember which but there were two common sizes) specifically because I was concerned about that causing issues with the promised long term platform support. I'm running a 5700X3D on a board I bought alongside a 1600AE. That's a completely neurotic detail to focus on and noone can be blamed for not doing so. I dunno if it's more AMD or the board manufacturers fault that lower storage boards were available but it shouldn't have happened.,hardware,2025-10-31 13:27:37,0
AMD,nmczbup,doom the dark ages would run sub 10fps if you didnt download the game ready drivers that released around the same day on my rx7600. so there''s precedent for games that just NOT working if you dont get day 1 support,hardware,2025-10-31 12:34:58,23
AMD,nme6rha,"Not supporting new Vulkan extensions could matter on Linux, where AMD in general is regarded as the better choice.",hardware,2025-10-31 16:19:31,1
AMD,nmd400e,"Well those older cards struggle to run new games, often for reasons other than lack of day 1 drivers but that lack of drivers ain't helping. Not a good look by amd.",hardware,2025-10-31 13:02:11,0
AMD,nmd3sfh,"Cool. ""Inflation.""  I can't afford that. $300 max or I drop out of the market. Don't try this crap with me.  Edit: demand curves are also real. You raise the price enough and people drop out of the market. You guys seem okay with that. You leave a snarky comment, you get blocked. Not everyone can be a yuppie with a 6 figure job showing off their 5090 on reddit.  Edit2: you guys also ignore the entire bottom of the market has been decimated over the years. If we went by 2017 pascal prices this is what ""inflation"" would look like:  5050 (replacing 1050)- $140  5060 (replacing 1050 ti)- $175  5060 ti 8 gb (replacing 1060 3 gb)- $250  5060 ti 16 gb (replacing 1060 6 gb)- $310  Except we ignore nvidia had all that ""inflation"" when they launched the 20 series and effectively destroyed the low end market. So let's not talk about all of this nonsense as if it's just impersonal economic forces beyond gpu makers' control.",hardware,2025-10-31 13:00:59,-2
AMD,nmekfdz,"Yep current upgrade options are:  5050 for $250- literally a side grade for a higher price than what I bought the 6650 xt for  3 years ago  9060 XT 8 GB for $270- not giving AMD my money again  5060 for $300- 50% increase, same 8 GB VRAM, ewww  9060 XT 16 GB for $350- $50 over budget and why would i give amd my money again?   5060 ti 16 GB for $430- are you fricking kidding me? $430 for a 60 card? Way over budget. No. No way.  And yeah intel ain't an option if you wanna actually ensure widespread library compatibility.",hardware,2025-10-31 17:26:45,0
AMD,nmbmgyh,Not for 50% more money.,hardware,2025-10-31 05:16:19,5
AMD,nmbm61u,"I didnt ask you. im not second hand. Back in the day we used to ACTUALLY see ACTUAL price performance gains where last year's 80 card would be the next's 60 card (for example 980 vs 1060, 580 vs 660). Really, the last time AMD had a product this poorly supported that I bought, you could get a like a 960/970 to replace your old HD 5850. BRAND NEW. Im getting sick and tired of being relegated to a second class citizen for not wanting to spend what used to be GTX 80 money on GPUs and its even worse when these companies dont support what products do exist.",hardware,2025-10-31 05:13:33,12
AMD,nmeqr1q,Buddy you don't wanna start a discussion about capitalism with me. I have tons of criticisms of the system and the gpu market is becoming an outright market failure.,hardware,2025-10-31 17:58:17,1
AMD,nmht4p4,Having more VRAM than Nvidia was part of the fine wine.,hardware,2025-11-01 06:05:40,1
AMD,nmc89o6,"Well to some degree it was to be fair. Because some of their architectures were better suited for games that hadn't been launched yet.   GCN had a terrible CPU bottleneck compared to Maxwell/Pascal in many poorly threaded DX11 games. Which was alleviated by later DX12 games, or in some cases just from games adding DX12 support.   Not like that is only a AMD thing either. Kepler aged poorly as DX11 took over, even on cards that weren't VRAM constrained. Pascal similarly aged poorly vs Turing in many newer DX12 and Vulkan games. The performance didn't change much, just the games used to evaluate performance. Run the original test suite used at Turing launch and you will get the ""not much better for more money"" result again.",hardware,2025-10-31 08:58:03,12
AMD,nmbk728,"And your point? The same happens with Nvidia, driver updates improving performance after product launch.",hardware,2025-10-31 04:55:50,14
AMD,nmdizca,"Yep, it'll be pushed by whatever RDNA5 or UDNA architecture is used in the consoles. Ask those RDNA1 puchasers if they should've bought 20 series instead once Ray Tracing and upscalingbecame prolific in games.",hardware,2025-10-31 14:22:24,6
AMD,nmgs5ka,My previous 3 cards were Nvidia.,hardware,2025-11-01 01:08:01,2
AMD,nmc1p2b,Amd always had shorter driver support than nvidia,hardware,2025-10-31 07:50:05,21
AMD,nmcmguc,"They dropped support for Fury, Vega, and Radeon VII unusually quickly as well, didn't they? Meanwhile, Nvidia ended optimised driver support for Maxwell, an architecture first released in February 2014, like a week ago.",hardware,2025-10-31 11:06:30,12
AMD,nmgr5lu,I thought the “Nvidia tax” was common knowledge? More money for better software,hardware,2025-11-01 01:01:11,2
AMD,nmbjacj,are you serious? Saying anything bad about AMD gets you severely downvoted or shadowbanned online. This is fact.,hardware,2025-10-31 04:47:59,3
AMD,nme3nc9,Nope. Native DLSS 4. Set in the nvidia app or via nvpi. At least know what you're talking about if you're making statements like this,hardware,2025-10-31 16:04:14,2
AMD,nmbqhsc,A laptop with a MX450 is getting more support lmao,hardware,2025-10-31 05:54:35,22
AMD,nmbylyb,"Intel Arc Alchemist, a first-gen architecture, is still getting game-specific driver updates while 6x50 RDNA2 cards released at the same time won’t…",hardware,2025-10-31 07:17:30,8
AMD,nmd699q,"If you have better suggestions, I'm all ears. I spent a whole day trying to troubleshoot it. And then half a day a few weeks ago, thinking there was a better solution by now.   Since, it's what SteamVR uses to this day. As otherwise, it causes horrible framerate issues. At 24.3.1 or older, I get a solid 90FPS in my 90Hz headset without issue. Anything newer than that, it runs great...if I'm completely still. And then I start moving. You can replicate the issue by going into SteamVR settings and manually disabling it.",hardware,2025-10-31 13:14:51,2
AMD,nme6a2y,In what way?   Spending £50 more on the faster GPU that has a better software suite is bad?,hardware,2025-10-31 16:17:08,2
AMD,nmcwxia,"AMD stopped supporting the mobile version of the chip. For a while, it was possible to mod the desktop driver and make it work on Windows 8/8.1. When Windows 10 came out, they also stopped supporting the desktop chip. Due to the new windows driver model, it was not possible to mod any of previous drivers at all.",hardware,2025-10-31 12:19:58,7
AMD,nmcgdrw,"If she cares so much, why doesn't she fix it?",hardware,2025-10-31 10:15:59,3
AMD,nmdhxo7,"Idk ehy isnt this a top comment. Because this is very true whrn I was employed by them. They woukd rather spend more time on their marketing rather than invest in proper teams in software, hardware and QA. They treat their customers like guinea pigs and it seems that their behaviour isnt changed. Best part the parts they had were cheap to make and yet if priced correctly they woukd take the market by a storm. But no they chose to hide everything bin everything down and treat their products like worthless piles of silicon whike sucking up to OpenAI and Co. Also they like to shift the narrative towards board partners not complying and such very scummy behaviour while only interested in miliking the customer. At least release better products than your competitors but no. Intel should destroy AMD with their CPU while Nvidia should destroy AMD as well.",hardware,2025-10-31 14:17:01,1
AMD,nmff515,Thanks!,hardware,2025-10-31 20:05:20,2
AMD,nmdbpiu,"The whole thing started with release notes probably written by an engineer with no understanding of PR.  Meanwhile AMD marketing doesn't have a much better understanding of PR.  Knowledgeable reviewers openly calling this ""end of support"" is the extra kick in the balls.",hardware,2025-10-31 13:44:50,2
AMD,nme6bmt,"Finally, someone gets it. The truth is being used as an excuse to lie.",hardware,2025-10-31 16:17:20,1
AMD,nmccrtd,"Not to mention that with their low marketshare, they sure as shit aren't going to be allocating the minimal effort they put in, into the cards with the least marketshare.",hardware,2025-10-31 09:43:16,8
AMD,nmchbht,Nothing you said makes it AMDs responsiblility to make up for some other business' shitty code.,hardware,2025-10-31 10:24:12,-4
AMD,nmcq5sg,"GPUs are hardware, not software.",hardware,2025-10-31 11:34:42,-7
AMD,nme3pva,It's *crazy* how blatant misinfo like your post gets upvoted constantly. People are just in love with lies.,hardware,2025-10-31 16:04:35,-2
AMD,nmgluts,">This is precisely the opposite of what *is already happening* with Vulkan extensions w.r.t RDNA2 support.  I can refute that very easily. AMD just released their ""[VK\_AMDX\_dense\_geometry\_format](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_AMDX_dense_geometry_format.html)"" Vulkan extension is the latest driver.   If you bother to look it up you'll see RDNA4/RDNA3/RDNA2/and RDNA2/3 based APUs reporting support for it.  [https://vulkan.gpuinfo.org/listdevicescoverage.php?extension=VK\_AMDX\_dense\_geometry\_format&platform=windows](https://vulkan.gpuinfo.org/listdevicescoverage.php?extension=VK_AMDX_dense_geometry_format&platform=windows)",hardware,2025-11-01 00:25:40,-2
AMD,nmdmdmo,"> On Linux the development is partially done by the community and Valve at the kernel level, and at the usermode level (for Vulkan) it's entirely by the Mesa community and Valve, with no AMD involvement.  I think that's a little bit unfair towards AMD. On the kernel side, they put in the work with amdgpu, writing parts of it from scratch *and* getting it being accepted into the mainline kernel - which is a steep barrier to entry (the battles on the LKML were a fascinating read :) ), but which makes long-term support infinitely more viable due to the nature of Linux kernel development.  On the userland side: While RADV of course has been and still is first and foremost Valve-driven, the no AMD involvement hasn't been true anymore for a while. AMD driver devs have actively contributed to RADV for a while now, and recently they had the saliency to make Mesa RADV the officially supported Vulkan driver, going so far as to actually discontinue AMDVLK. And similar to the kernel side, being part of Mesa means a much higher likelihood of long-term support.",hardware,2025-10-31 14:39:47,2
AMD,nmgkg11,I know what the announcement said but I actually *understood* it instead of jumping to outrage based on a poor understanding of how drivers are developed and maintained.   [https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games](https://www.tomshardware.com/pc-components/gpu-drivers/amd-clarifies-that-rdna-1-and-2-will-still-get-day-zero-game-support-and-driver-updates-discrete-gpus-and-handhelds-will-still-work-with-future-games),hardware,2025-11-01 00:16:07,-2
AMD,nmgmf75,">The cards will still work, just not efficiently.   That's 99% up to developers. If they write optimized shaders then they will work efficiently. AMD isn't doing anything to reduce performance.",hardware,2025-11-01 00:29:33,1
AMD,nmbpn7e,It's wild to read since even the almost 11 years old GTX 960 has still been receiving game ready drivers,hardware,2025-10-31 05:46:16,13
AMD,nmbg58u,"I see it with Intel Wifi drivers, the actual driver in device manager never change, but Add/Remove just gets a new number.",hardware,2025-10-31 04:21:39,3
AMD,nmbi6fq,"I did and wasn't alone notice the difference when they released drivers from the latest AAAs on not a longer so beefy cards. It's refreshing.  Your agument is making people like complete idiots or ignorants, or careless from which they won't get any use of counting FPS or writing on reddit anyway.   Your solution is egregious because a thin minority will notice and spread the word regardless, just like a thin minority of people is doing benchmarks ...  However, a huge percent of people is fine playing at 30 FPS on a botchered resolution with stuttering left and right, not sure why you bring up that one.",hardware,2025-10-31 04:38:33,-5
AMD,nmbgir7,"Not even that, you won't get any substancial drivers from now on. OP just made that up.",hardware,2025-10-31 04:24:42,-1
AMD,nmeklaf,I get this system wide system hang every now and then for no reason that I can find. I've read that it's due to multi-monitor perhaps but I'm not 100% sure all I know is that all the new drivers do it after 566.36. I updated a couple of days ago to see if it was fixed and it still did it.   It will do it a handful of times a day but it just gets very annoying when I'm trying to watch a film.,hardware,2025-10-31 17:27:35,1
AMD,nmcxjrz,"Good to hear it is fixed, my issues were BSOD, freeze, display driver constantly crashing and restarting (many people report this as black screen etc, it's just the driver restarting) with the launch driver from the 5060 Ti, 572.something if I remember correctly. I just gave up in the end, I'm not averse to trying to fix things and could understand what was happening quite clearly but nothing seemed to help, going back to a 4000 series card on 566.36 fixed everything.  I assumed it would all be fixed eventually but the card was not usable while this was happening so I just returned it.",hardware,2025-10-31 12:23:51,1
AMD,nmvkz5e,"Despite being ""Aboslute garbage"" the drivers are still better than AMD at its very best.  Also lol why would anyone listen to LazyTwoCentz. He hasnt been right about this stuff in forever. Didnt we already have this discussion where you cited his video where he was wrong about SSD bricking?",hardware,2025-11-03 13:38:01,1
AMD,nmcsnn3,"I have been surprised by that too, the driver experience I had with a 5060Ti was shockingly bad and I wasn't alone, their forum is full of people having a terrible time with their 2025 drivers. Very few youtubers or writers seemed to cover any of it.  I just returned the card and assumed they'd figure it all out eventually, then I would re-buy... now I'll probably wait until 2026.  I'm not sure it has anything to do with ""focus on AI"" or anything but it's definitely not the driver experience I was used to from Nvidia.",hardware,2025-10-31 11:52:02,0
AMD,nmdpgkj,"I know and? They are still involved. I just wanted to point at the difference between proprietary platforms (that nowadays shit on their customers as a service) and open source ones. Them theoretically dropping their ""support"" within MESA has less impact than within the same situation on their Windows proprietary driver.  AMD abandoned their first proprietary driver of two (Amdvlk) in June on Linux for their Enterprise users. Amdgpu-Pro will soon follow. They are focusing on RADV now which is open source.  For example I reported an OpenGL bug 1.5 years ago on Vega and the first to take a look over it was someone working for AMD.  Official support drop on Windows: you're screwed on the first issue.  Official ""support"" drop on Linux (open source): If related GPU firmware is open enough, development might just slow down if at all.",hardware,2025-10-31 14:55:18,2
AMD,nme6n0f,This comment would make way more sense if it was being said about Nvidia. Oh well.,hardware,2025-10-31 16:18:54,2
AMD,nmcvl0e,"What a weird comment, they don't consider it an issue. There isn't any 'abuse' going on.",hardware,2025-10-31 12:11:28,2
AMD,nmvsjom,"I selected the cards believing that they would receive quick and high quality game optimizations into the future, even past the point where the card can easily handle 1440p and even 4K AAA workloads. That's what my folks have been using their cards for. This is dependent on AMD actively doing their best to keep the cards relevant. They clearly aimed to cut costs with this measure and are now backtracking to try and calm negative customer sentiment. I'm not buying it, and it's going to be difficult to restore my trust. Will the 9000 series be fully supported 5 years from now? Who knows?",hardware,2025-11-03 14:19:57,1
AMD,nmc085h,Why is it such a problem they stop optimisation of game code for old hw which should be the job of game developers anyway? This doesn't mean they stop fixing bugs or whatever other maintenance drivers require for OS updates.  All this drama sounds like some people think this means these products become useless.,hardware,2025-10-31 07:34:39,-3
AMD,nmbohl9,"You said you were upset at 400 series not getting ~5000~ 3000 support and not so much 300.   And then went on to vomit out garbage I'd expect Reddit would say like ""you got your money's worth"".   Edit: actually, a little bit beyond that point, you went on to praise AMD for supporting first gen motherboards as well as they did. That isn't just indifference, that's rear end kissing. My bad.",hardware,2025-10-31 05:35:07,-25
AMD,nmfyy8u,"Using a better processor, more ergonomic design, and a higher refresh panel. But sure we’ll go with the same thing as the Steam Deck. it also has somewhat better battery life and if you put steam OS on, it runs even better.  Edit: corrected typo",hardware,2025-10-31 21:55:59,6
AMD,nmbse3j,The Xbox Ally uses RDNA2 bro wtf u on about,hardware,2025-10-31 06:13:45,56
AMD,nmbbx7n,Are the GPU drivers rebranded too? That 2022 model G14 is no longer supported with the latest gaming drivers due to it using a RX 6800S GPU and 6800HS CPU,hardware,2025-10-31 03:49:13,11
AMD,nmhg0st,"At least with linux, you can rely on open source driver support. So I think only windows users should be effected.",hardware,2025-11-01 04:00:39,1
AMD,nmd3hjs,"You're fine.  I got that card at launch and I'm running it for at least a few more years.  Still a solid card and will be, despite the lack of support.",hardware,2025-10-31 12:59:16,1
AMD,nmhg4j2,Good time to be a linux user lol.,hardware,2025-11-01 04:01:30,1
AMD,nmbod95,You can install Windows on the Steam Deck. I never said this was a good idea!,hardware,2025-10-31 05:33:56,-8
AMD,nmh4ye2,yeah,hardware,2025-11-01 02:37:21,0
AMD,nmcshdm,"Yeah he's full of shit, my 3080 does not have enough VRAM for BF6 and is struggling a lot in RT games.",hardware,2025-10-31 11:50:49,19
AMD,nme5ysa,"Absolutely, 2080Ti wasn't a great buy at the time, way too expensive for uncertain future feature support. Buyers just got lucky with the longevity of it later on, as well as the covid shortage.",hardware,2025-10-31 16:15:34,1
AMD,nmcy8zd,Customers can ask for consumer hardware specifically for renting it at lower prices.,hardware,2025-10-31 12:28:14,9
AMD,nmdgsb8,"they pay for a service, not a part name",hardware,2025-10-31 14:11:09,8
AMD,nmdqay3,So its stuttering at ultra settings instead of 6800xt's 40 fps. 3080 has dlss 4 and that is the end of the comparison.,hardware,2025-10-31 14:59:32,9
AMD,nmvjec6,that is not possible.,hardware,2025-11-03 13:28:35,0
AMD,nmddepr,Also these cards are cheap to manufacture and these conpanies choose to sell it at high margins to us just because conpetition is scarce with AMD pricing Nvidia - 50 while making the data center cards all that powerful ehile leaving us holding the bag.,hardware,2025-10-31 13:53:45,2
AMD,nmkah3t,"Maybe at 1080p, at 1440p and especially 4k the 3090 kills the 6900xt consistently.",hardware,2025-11-01 17:20:54,2
AMD,nmgntau,"My 6900xt is now a part of my VM cluster, it is still an absolute great card. Linux drivers have breathed new life into the things",hardware,2025-11-01 00:39:03,1
AMD,nmc701a,Jump from 3080 to 5080 is quite great tbh. Your fault for not using technologies where most of development cash went.,hardware,2025-10-31 08:45:26,-2
AMD,nmdx61k,Aquaman,hardware,2025-10-31 15:33:07,15
AMD,nmcoebl,Most people don't follow any of this stuff,hardware,2025-10-31 11:21:27,9
AMD,nmeavy1,"Why is this a problem? If people can't research stuff they spend money on, it's their fault. Caveat emptor.",hardware,2025-10-31 16:39:58,-1
AMD,nmc8rwa,"Where I am, a 3070 is cheaper than a 7600XT on the used market. Imho, there's 0 reason to go for a 7600XT in such a scenario",hardware,2025-10-31 09:03:09,19
AMD,nmcgv8t,Also better Linux support,hardware,2025-10-31 10:20:17,24
AMD,nmvk1hi,AMD cards havent been good since before AMD bought them.,hardware,2025-11-03 13:32:28,1
AMD,nmd7ihd,"> A functioning moral compass.  Buddy, AMD is a massive corporation who are just as bad as the rest of them. You are not supporting some garage startup who will change the world, AMD is just another soulless company who has shown on many, many, maaaany occasions that they'll choose profits over all else.  I still have no idea how AMD convinced so many nerds that they're the scrappy underdog who will always be on *the gamers'* side, when every other action they take signal otherwise. It's honestly impressive.",hardware,2025-10-31 13:21:50,14
AMD,nmvk39q,Anyone with a functioning moral compass would have given up on the hell that is AMD corporate.,hardware,2025-11-03 13:32:46,1
AMD,nmbycbw,You should not buy every gen anyway,hardware,2025-10-31 07:14:42,15
AMD,nmdfvfv,"It's the market they want they're concerned with, not the market they have.  Growth > Sustainability in the current marketplace.",hardware,2025-10-31 14:06:27,1
AMD,nmbna6w,Unfortunately all they care about now is the openAI deal,hardware,2025-10-31 05:23:48,40
AMD,nmhjuuz,"Ah. That's unfortunate. Could you tell me the truth, please?",hardware,2025-11-01 04:33:45,3
AMD,nmdvgd4,"And from those sources:  > Manufacturers have to drop support for some of the older Ryzen parts in order to usher in support for Zen 3. For example, the Asus and Gigabyte removed support for AMDs 7th Generation A-series and Athlon X4 series (Bristol Ridge) processors.   So it wasn't an excuse. It was a difference of opinion as to what needed to be supported and what didn't.",hardware,2025-10-31 15:24:57,-2
AMD,nmboc2d,"Initially AMD didn't advertise compatibility, it was motherboard manufacturers advertising it and basically bullied AMD in to trying to make the best of a dumb situation.",hardware,2025-10-31 05:33:37,-16
AMD,nmd9a62,"Devs testing a dozen cards is easier than vendors testing every SKU of every arch in every new game and hard coding workarounds. When devs don't do the former, for some reason gamers expect the latter.",hardware,2025-10-31 13:31:39,1
AMD,nme4gci,"Well then you're just going to have to ""choose"" to drop out of the market based on ignorance. Nobody is going to coddle you here.  Inflation is far too real, stop pretending it isn't. It existed long before tariffs or COVID or whatever else.",hardware,2025-10-31 16:08:11,0
AMD,nme68yg,You can’t afford PC gaming then.,hardware,2025-10-31 16:16:58,2
AMD,nmeslu6,I'm not your buddy. Good day.,hardware,2025-10-31 18:07:44,0
AMD,nmbkj6j,"If that were the case (that Nvidia driver performance improves at the same rate as AMD's) there would be no such phrase as ""fine wine"" and op's comment about shitty day one drivers would apply equally to Nvidia.",hardware,2025-10-31 04:58:49,2
AMD,nme5rd7,"> Ask those RDNA1 puchasers if they should've bought 20 series instead once Ray Tracing and upscalingbecame prolific in games.  This isn't going to happen until RDNA1 cards no longer work, if that.",hardware,2025-10-31 16:14:33,-1
AMD,nmj3zqd,"Ok, don’t buy new cards and play older games instead that you haven’t played yet, or wanted to replay them.",hardware,2025-11-01 13:30:09,1
AMD,nmcc6dt,"Tbh I think people are overreacting about this, old cards are mature enough that they rarely ever need game-specific optimizations, the bigger problem is the lack of features which was already a problem even when those cards were on full support, 20 series is still getting some new DLSS features while being older than RDNA2, it's pretty embarrassing.      At the bare minimum FSR 3 should continue being improved or a compatible FSR4 version must be released to RDNA2 and ideally RDNA1 also.",hardware,2025-10-31 09:37:29,-1
AMD,nmc4vzj,"Well that's just not true, AMD was well ahead for a while because of how long GCN lived. But that's beside the point, Nvidia already does this with previous generations, they just don't officially state it.  And even *that* is beside the point, because duration of driver support is far from the only factor that goes into a GPU choice.  edit: I'd be fascinated to know why this is downvoted. What did I say wrong?",hardware,2025-10-31 08:23:28,-9
AMD,nmdaiap,"I bet if you compare day 1 driver performance on a 980ti to the driver just before that in a dozen 2024/2025 games, there's basically no difference (because they didn't actually do anything).",hardware,2025-10-31 13:38:24,-2
AMD,nmhrvva,"More money for the myth of better software. It's just prestige pricing, the tax is for the brand name.",hardware,2025-11-01 05:52:05,-1
AMD,nmc4auj,Thats funny cause I aint got shadowbanned from amd subs but got shadowbanned from nvidia.,hardware,2025-10-31 08:17:14,6
AMD,nmbutrl,"This sort of ebbs and flows with time/trends, and depends on the sub/forum, I'd say pcmasterrace fits (or certainly did for multiple stints over the past 5+ years) your description, and the Techpowerup! forums are significantly pro-Radeon / anti-Nvidia these days. I don't dive as much into the comments in this sub to judge it.",hardware,2025-10-31 06:38:31,6
AMD,nmblpjv,"Nah, I see AMD bad twice as often as I see NV bad here.  I still see AMD drivers comments from pre-pandemic days to this day.  I see it more than NV fire cables, missing vram, price hikes, tier changes, their shady marketing, the geforce program, and they have had bad drives multiple times since AMD's bad drivers including a NV bad driver release just this year.",hardware,2025-10-31 05:09:23,4
AMD,nme3809,"That's a blatant lie, as proven by *every single thread* you people try to say this in, including this one.",hardware,2025-10-31 16:02:10,1
AMD,nmf1m84,"Which game specifically if you don't mind answering? I doubt SteamVR it self uses Direct Draw as Direct Draw was effectively abandoned by Microsoft after Windows XP.  In any case, my suggestion for if it really is something using Direct Draw... is to use the dgVoodoo2 graphics wrapper.",hardware,2025-10-31 18:54:03,2
AMD,nmhmca9,hes amd fan,hardware,2025-11-01 04:56:26,3
AMD,nmvhdxq,"It's not a ""faster"" GPU with a ""better"" software suite, *especially* not for the price. I'd have thought all the nonsense with Nvidia drivers that's been going on for the past year or so would have gotten people to realize this, but apparently not.",hardware,2025-11-03 13:16:28,1
AMD,nmw4n6t,As a contractor or at HQ?,hardware,2025-11-03 15:22:27,1
AMD,nmct0jv,"If AMD wants their customers to actually be able to use their cards like they can with Nvidia, sure. But they *technically* don't have to do that I guess and people can just get fucked.",hardware,2025-10-31 11:54:29,2
AMD,nmf2z4q,"You sound like you had the exact same issue as me, as 566.36 and the symptoms you mentioned sound very familiar.  Only thing I could maybe suggest giving a try (and this is a pretty wild guess as I've no clue what solved it for me and have always assumed Nvidia eventually solved it on their end) is changing the CMD2T RAM setting from 1 to 2, performance impact is negligible.",hardware,2025-10-31 19:00:58,1
AMD,nmf371b,"See my reply here for a wild stab at a solution https://old.reddit.com/r/hardware/comments/1okjrn7/amd_throws_loyal_radeon_customers_into_the_trash/nmf2z4q/  Oh, you returned your card, that's one way to solve it :P",hardware,2025-10-31 19:02:05,1
AMD,nmhu9am,">I have been surprised by that too, the driver experience I had with a 5060Ti was shockingly bad and I wasn't alone, their forum is full of people having a terrible time with their 2025 drivers. Very few youtubers or writers seemed to cover any of it.  They're afraid to make daddy Nvidia mad. Real knights in shining armor they are.",hardware,2025-11-01 06:18:04,0
AMD,nmvvwxy,"> They clearly aimed to cut costs with this measure and are now backtracking to try and calm negative customer sentiment.  They haven't and aren't doing any of these things.  > Will the 9000 series be fully supported 5 years from now?  The same as every single other gen in history: it will, yet it doesn't matter if it is or not.",hardware,2025-11-03 14:37:58,1
AMD,nmd0xi9,> old hw  If you consider products that launched this month old sure. RDNA 2 is in the xbox ROG ally ~~and allyx~~,hardware,2025-10-31 12:44:40,3
AMD,nmbuu0f,"We saw an issue, we brought it to the attention to as many people as we could. There wasn't really a need to go all crazy outrage mode given it still wasn't an irreversible change, nothing had happened at that point, it was just a plan they had for the future. We told them it was a very bad plan, told the community to let them know it was a bad plan and just mere days after the video went live they reversed course.  You are acting in bad faith here so I'm done with the conversation.",hardware,2025-10-31 06:38:35,16
AMD,nmhjl76,I hope u meant ergonomic,hardware,2025-11-01 04:31:19,1
AMD,nmbsj89,Ah I thought they mentioned XBAX.,hardware,2025-10-31 06:15:11,7
AMD,nmi9761,"It hurts a tad because I still play EA WRC (has anticheat) and using a Moza wheel ( is there even Linux support) so still stuck with Windows just for that. The game runs poorly as it is on Windows, could end up better or worse in Linux?",hardware,2025-11-01 09:05:54,1
AMD,nmbqvpd,Back to SteamOs ;),hardware,2025-10-31 05:58:27,20
AMD,nmc8u4z,Why would you do that?,hardware,2025-10-31 09:03:48,12
AMD,nmd32eo,"Literally no one does that, yes you can, but there are no benefits of doing it!",hardware,2025-10-31 12:56:52,4
AMD,nme1l5i,That’s like downgrading to an ASUS Ally,hardware,2025-10-31 15:54:17,-1
AMD,nmdr1ba,"I have a 10 GB 3080 and battlefield runs completely fine, although I am at 1440p",hardware,2025-10-31 15:03:11,12
AMD,nme40yb,DLSS doesn’t matter if your vram is full. It’s more like 70 FPS smooth vs 20 FPS stutter. Less than 12GB is not worth it in 2025.,hardware,2025-10-31 16:06:05,-5
AMD,nmw0uf9,"Not only is it possible, it’s a fact.",hardware,2025-11-03 15:03:23,1
AMD,nmki2ej,no   it didn't    consistently better game FPS and benchmark scores,hardware,2025-11-01 17:59:52,-1
AMD,nmc995o,"3080 was 700USD, now 5080 is 1,000USD.  Try again, more like 5070 Ti is the successor price wise which is about 15% weaker than the 5080.",hardware,2025-10-31 09:08:01,1
AMD,nmc95jv,"FG is not a good enough draw, esp since you can apply it after the fact with third party stuff, or mix FSR FG or XeSS FG with DLSS that it does have.  the raw increase at the EXACT same setting, be it DLSS or FG or raw RT is not anywhere near what it was before, when you would get double the fps at the same setting.",hardware,2025-10-31 09:06:59,0
AMD,nmcol83,"My point is that the people buying a high tier AMD GPU used are not ""most people""",hardware,2025-10-31 11:22:54,24
AMD,nme6e2j,Not just 'better' - it is a lot better.,hardware,2025-10-31 16:17:40,7
AMD,nmcw0ju,That’s actually the same thing,hardware,2025-10-31 12:14:11,5
AMD,nmdeq5l,"Bro I used to work for AMD and the things that they do are not moral. They only care about their end margins. People like you and me who buy their products mean peanuts to them. They want to seem like the underdog but clearly are not. Sadly this was probably planned some time back and probably the same situation might happen to the 9000 series. Not sure why people love AMD, they are just as or even more scummy than Nvidia cheating their loyal followers and rewarding them with worthless piles of silicon.",hardware,2025-10-31 14:00:33,2
AMD,nme1zmo,"> AMD is a massive corporation who are just as bad as the rest of them  Why do people always say this when AMD always makes better stuff, time and time again? Nobody actually knows or cares about this ""garage startup"" garbage. I care about *results*, and that is what AMD delivers.  > when every other action they take signal otherwise  Virtually all of which is misinformation, such as the OP, and that garbage earlier in the thread about Zen 3 motherboard support.",hardware,2025-10-31 15:56:12,-7
AMD,nmdbuk1,There's not even a real worthwhile upgrade from top end rdna2.,hardware,2025-10-31 13:45:34,5
AMD,nme2atz,Once inventory settles in on a new Gen of gpus I sell my card to a friend that needs an upgrade while it still has good value and get the new top card.,hardware,2025-10-31 15:57:42,1
AMD,nmdkze2,Where is the growth in question though,hardware,2025-10-31 14:32:38,3
AMD,nmvklkt,Someone tell AMD negative growth is not growth.,hardware,2025-11-03 13:35:47,2
AMD,nmderqb,"No matter what, AMD will always be the cheaper and worse off-brand version of NVIDIA, whether that be for gaming or AI. They try so hard to be like NVIDIA, when they should really be their own thing and build their own brand rather than copy NVIDIA's homework and only get 80% of the way there. Sadly they never learn. Oh well, back to buying NVIDIA and just hoping NVIDIA praises us peasants with a decent performing product at a respectable price.",hardware,2025-10-31 14:00:47,8
AMD,nmivxg6,"God forbid, AMD customers get the choice of either using their newest and latest CPUs, and the cost of some old shitty CPUs on architectures that nearly killed AMD.  It's not like B550 officially support those anyway.",hardware,2025-11-01 12:37:01,0
AMD,nmbokcj,"Uh no, when AMD announced AM4 they had marketing slides advertising compatibility.",hardware,2025-10-31 05:35:52,19
AMD,nmda3b9,no it wouldnt run on well ANY AMD card without those drivers. i was just specifying which one i had. those drivers were mandatory and when you started doom without them the game would tell you to update your drivers. so the devs knew the update was needed and so did AMD.,hardware,2025-10-31 13:36:06,3
AMD,nmbu4tb,"Lately it has applied more to prior gen Nvidia products than AMD ones, not really about shitty day 1 performance that they fix, just the product getting better as time goes on. It's just hard and takes a long time to undo or reframe a statement that is already ingrained in peoples minds for a particular company applying to another.",hardware,2025-10-31 06:31:27,5
AMD,nmbu8eq,> shitty day one drivers would apply equally to Nvidia   5000 series would like a word.,hardware,2025-10-31 06:32:27,8
AMD,nmdq8xv,"The Nvidia hive mind screams at anyone who tries to bring up actual problems. For me, it took over 6 months after launch for my RTX 4090 to finally work properly for my 3 monitor setup despite reporting the issue to their support team 6 days after the product launch.",hardware,2025-10-31 14:59:15,1
AMD,nmkxgvr,"My first GPU was the 580, I wasn’t able to use it for Blender after like 2 years (end of 2021) because AMD stopped supporting it in their GPU compute software. I was irritated that the GT710 had support but my GPU didn’t.  You’re being downvoted because you’re factually incorrect",hardware,2025-11-01 19:19:58,1
AMD,nmkxpt0,"Maybe you’re the odd one out, most people have known that NVIDIA’s price premium had some justification.",hardware,2025-11-01 19:21:17,2
AMD,nmvhl8u,"I'm not a ""fan"" of anything, I don't waste my time with that garbage. I care about the results, and Nvidia does not deliver. Their last good cards were the 10s, and that may end up being literal at this rate.",hardware,2025-11-03 13:17:41,1
AMD,nn83lvk,"I wasn't talking about drivers.   Its typically 5% faster for rasterisation and 20%+ in heavy RT. How is the software suite not better?   DLSS 4 upscaling is slightly better quality, much better game support    DLSS FG, better quality an shame support, MFG AMD don't have an equivalent for it    Ray reconstruction (AMD don't have an equivalent again) and reflex (Anti-lag lacks support)",hardware,2025-11-05 12:10:46,1
AMD,nmf7lq4,"I've never touched that setting, I'm not even sure I would even have it. I'm on a 5820k on an MSI SLI Plus motherboard and my memory frequency is stick because the 5820k doesn't like a memory overclock. I only have two sticks anyway 8GB each at 2133mhz. No idea why the Nvidia driver would have any effect on system memory anyway?",hardware,2025-10-31 19:25:11,1
AMD,nmvwnd7,"They absolutely would have if they hadn't been tarred and feathered in the public square. That's what the words ""maintenance mode"" mean in this industry. We cannot believe them anymore if they argue that the cards will be supported as long as they are still useful for newly released games.",hardware,2025-11-03 14:41:49,1
AMD,nmfbc87,"ROG X is RDNA 3.5. In case of these new ""mobile"" products I assume they use their own drivers anyway. I also assume AMD references RDNA 2 to the specific GPU cards phased out years ago and not still selling iGPU/APUs.",hardware,2025-10-31 19:45:10,1
AMD,nmbxocn,"\-accusing someone of making stuff up  \-provides a video supposedly showing that they did express issue with 300 not getting new CPU support  \-it doesn't and actually makes him look worse  \-generalizes the video, engages in gaslighting, and accuses me of ""acting in bad faith""  \-fans base/subscribers upvote you and downvote me  Frankly, I wasn't expecting much from a techtuber and his army of drones but this is something I thought only LTT and their drones were capable of. Incredible.",hardware,2025-10-31 07:07:47,-15
AMD,nmhjn94,"Yes, Apple’s voice to text is shockingly horrible",hardware,2025-11-01 04:31:50,1
AMD,nmr9vgh,"A.) The Z2A has a higher TDP and is in a device that no-one should buy as it's specs are about as good as the Steam Deck I can agree with that, however the base model ally is only $50 more than the equivalent OLED sku     B.) The Z2 Extreme that is in the Ally X is a major improvement to the Aerith chip in the Steam Deck and for $350 more than the 1TB OLED Steam Deck you are getting a performance boost, more ram, bigger battery, and better I/O in the form of two USB4 ports including one that meets the spec for Thunderbolt 4.     C.) Just because you don't like the price of something does not mean that it isn't worth it to someone. The only reason that the Steam Deck can be as cheap as it is, is because Valve prints money with their 30% cut on all platform sales, whereas ROG is unable to compete due to not being able to subsidize the costs even with the help of Microsoft. If you do think that the Ally is overpriced there is an easy solution, Don't buy it. If you truly hate this luxury product to the point of attacking anyone who points out that is might have some value to them, then it's time for you to step away from the internet for a bit and do some self reflection.",hardware,2025-11-02 19:45:22,0
AMD,nmiqiel,"I was reading a bit more, and it sounds like they aren't completely ending support. So you might be OK. I've never played that game, so no idea if it will run better. It might, or it could be about the same. You could get an idea of how it will run by seeing if anyone has done benchmarks on it on YouTube. If youre not sure something will work, or need help doing something on linux, grok has been really helpful for me.",hardware,2025-11-01 11:55:38,1
AMD,nmvihg7,to use mods.,hardware,2025-11-03 13:23:05,1
AMD,nmcf2do,To play game not work on steam os,hardware,2025-10-31 10:04:21,-1
AMD,nmgmb1b,"3080 12GB, 3840x1600, no issues with VRAM. It's not fast enough to do frame-gen, which would be nice, but incurs some serious input latency.",hardware,2025-11-01 00:28:44,3
AMD,nmlshoh,"I have no issues with BF6 at 3440x1440 on a 10gb 3080. Textures on ultra too. I cap to 120 fps and if it drops below that it's because I'm CPU bound by my 5800X3D which is surprising to me, but in real heavy moments on large 64 player maps it can drop to ~110.",hardware,2025-11-01 22:07:18,2
AMD,nmez61y,"""Hey if I render 30% less pixles it works fine!""",hardware,2025-10-31 18:41:35,-5
AMD,nme8mdm,DLSS does matter when the game forces horrendous TAA. Making vram arguments is pointless when these cards can not even run those settings at an acceptable framerate. If your target frames are below 100 i suggest you get on with modern standards.,hardware,2025-10-31 16:28:47,5
AMD,nmvjgk9,DLSS reduce VRAM usage..,hardware,2025-11-03 13:28:57,1
AMD,nn0mhis,Its not a fact. 6800xt didnt magically became able to do RT better over time.,hardware,2025-11-04 06:14:14,1
AMD,nmkkhm6,Almost all the videos I see on Youtube tell a different story. There are some games where the 6900xt performs better but usually the 3090 is destroying it at 4k https://youtu.be/ji3PJ-UvGl0?si=MyislXbS2JjwCCOe,hardware,2025-11-01 18:12:14,3
AMD,nmcscct,"Right, barely anyone buys high tier AMD gpus. I don't expect most people buying cheaper, older, and used hardware to follow this news more than the average consumer in this market would.",hardware,2025-10-31 11:49:52,-1
AMD,nmdf39i,"I would that ever since the RX480 there wasnt ever since a good time to buy AMD which was feature deficient while costing as much as Nvidia. Not even sure why they exist at this point though. People also seem to have forgotten Zen5% as well just because they are AMD, ridiculous...",hardware,2025-10-31 14:02:27,5
AMD,nme2s4s,How much market share has AMD been able to capture in the server space?  consumer grade stuff is pennies on the dollar compared to enterprise.,hardware,2025-10-31 16:00:01,0
AMD,nmdonrz,AMD already has most of the non-AI supercomputer. Going after AI workloads especially now that they own Xilinx makes a lot of strategic sense even without Nvidia as the market leader.,hardware,2025-10-31 14:51:19,8
AMD,nmesila,"I mean, as written that sounds like a self fulfilling prophecy. We all thought the same about AMD trailing Intel for decades and then we had 14nm+++ and look here we are with Intel in crisis scrambling for new architecture and fab locations.   I wouldn't count Radeon totally out, even if they haven't had successes in many years. But yeah, they've been trailing the achievements of Nvidia through that time.",hardware,2025-10-31 18:07:16,1
AMD,nmfwzp0,People said that about intel,hardware,2025-10-31 21:44:16,1
AMD,nmbwm9h,"I'm probably wrong but I thought they advertised long term support for AM4, which didn't necessarily mean a motherboard would support all CPU's released for the AM4 socket.",hardware,2025-10-31 06:56:51,-4
AMD,nmdpp41,Then blame the developer for not testing on AMD. Hardware vendors shouldn't have to fix bugs in other people's software but they do out of the hope that they sell more hardware.,hardware,2025-10-31 14:56:29,4
AMD,nmdxdak,"> wouldnt run on well ANY AMD card without those drivers  I'm just saying that devs shouldn't be silently relying on one vendor's particular implementation and then blaming the other vendor when it doesn't work exactly the same. They should figure this shit out before it gets to the users. Relying on vendors to make day 1 drivers and users to install them is an anti pattern and a horrible experience. Drivers shouldn't have bugs, but devs shouldn't drive their games into a brick wall out of spite, either. Of course, only like 5 people actually use Radeon, so why would they give a shit",hardware,2025-10-31 15:34:06,2
AMD,nmcl4ld,That I can agree with. NVidia driver debacle for the past year has been horrible. Their razor sharp focus on AI is showing.,hardware,2025-10-31 10:55:49,1
AMD,nmkyvul,I'm factually incorrect because... you think Blender is relevant to a comment about game support?,hardware,2025-11-01 19:27:31,1
AMD,nmw30lp,"> They absolutely would have if they hadn't been tarred and feathered in the public square.  Which happens regardless of AMD's actual words and actions.  > That's what the words ""maintenance mode"" mean in this industry.  *Nobody* knows what the words ""maintenance mode"" mean in this industry. It might as well be another awful gamer buzzword, like ""optimization"".  > We cannot believe them anymore if they argue that the cards will be supported as long as they are still useful for newly released games.  Anyone admitting to being swayed by any part of this manufactured drama is admitting to willingly choosing to believe what they know is misinformation. That happens *all the time* with AMD.",hardware,2025-11-03 15:14:20,1
AMD,nmcesk0,\> fans base/subscribers upvote you and downvote me  Perhaps it's just people that are reading this exchange and think you are wrong. If you believe in delusions then no one can change your mind.,hardware,2025-10-31 10:01:55,5
AMD,nmuqepk,"Imagine being this confidently wrong. You literally have not looked at the specs for the CPU and GPU of the Z2A, its literally the fucking same.  Also THE PRICE LITERALLY ISN'T WORTH IT! The amount of defending Microsoft here for an inherently worse product is insane.",hardware,2025-11-03 09:36:17,1
AMD,nmd36nx,"There are not that many which do not run! A handful mostly multiplayer, i literally have almost 100% of my extensive Steam library running!",hardware,2025-10-31 12:57:32,4
AMD,nmf2cdm,4k monitors are still not that prevalent and having higher resolutions on smaller monitors makes no sense. 4k on 24inch is not necessary for example. So it’s not like there is a need for higher resolutions like there is a need for improvement of graphics.,hardware,2025-10-31 18:57:44,9
AMD,nmvj8p5,"""hey, if i use some obscure terrible resolution i can complaing GPUs wont run it as lower resolutions would.""",hardware,2025-11-03 13:27:39,1
AMD,nmebqdy,"But they can run these games, if the VRAM is available. You are just lying.",hardware,2025-10-31 16:44:06,-2
AMD,nmw0qf7,"True, but not enough to save a 10GB card.",hardware,2025-11-03 15:02:48,1
AMD,nml08n6,"I literally had both cards, at a much later date then that video    Granted, my version of the 6900xt was much faster than reference 6950xt even, so it shouldn't be surprising",hardware,2025-11-01 19:34:45,0
AMD,nmddzrd,Barely anyone know AMD exist. AMD has been nonexistent from the steam hardware survey and almost all professionals prefer to use Nvidia. These caeds shiwn be thrown into the trash as they belong very disappointed with AMD's behaviour.,hardware,2025-10-31 13:56:45,5
AMD,nmdlo0q,"Zen5% was quickly buried because of the strong X3D for enthusiast desktop and Turin for datacentre. Strix Point is just okay, but they have a steady measurable presence in all things x86 cpu. Not for gpu",hardware,2025-10-31 14:36:09,1
AMD,nme0ybe,"""Zen 5%"" never existed, it was bullshit influencer garbage like everything else.",hardware,2025-10-31 15:51:16,0
AMD,nme3tp4,Radeon Instinct in the data centre gpu space? Not much of anything at all vs overall market growth.,hardware,2025-10-31 16:05:06,3
AMD,nmby32z,"What was the point of making a big deal about AM4 socket's longevity if it didn't have support for new CPUs on older motherboards? It would be basically the same as Intel.  Like, the only thing is that it maybe makes developing new CPUs cheaper for AMD since they don't have to create a new socket/platform.",hardware,2025-10-31 07:12:02,7
AMD,nmdinqb,"It's hard to know if he's talking about NVIDIA or AMD, because the RX 5700 series also was plagued by horrible drivers. I remember the black screen issues, multi-monitor setups just straight up not working and horrible gaming performance for months with RDNA1. It seems anything labelled 5000 is just cursed with bad drivers.",hardware,2025-10-31 14:20:45,2
AMD,nmev5j5,Taking them longer than it took AMD to get 5700 fixed,hardware,2025-10-31 18:20:53,0
AMD,nmkzhmn,"No, but you specifically brought up GCN as an example of AMD’s long software support. Is it unreasonable to see that AMD has generally targeted a 4-5 year software support lifetime and then expect their GPUs to continue that trend?",hardware,2025-11-01 19:30:45,1
AMD,nmcfck3,Imagine watching the video he linked and thinking I'm delusional.    It's Reddit so it's possible I guess.,hardware,2025-10-31 10:06:54,-2
AMD,nmd3x4c,And your point is ? I should not play game ?,hardware,2025-10-31 13:01:43,4
AMD,nmgmmql,"Speak for yourself, my 32 inch 4k monitor paired with my 5090 slaps",hardware,2025-11-01 00:30:59,-2
AMD,nmw5tpk,4k is not some obscure resolution brother it's not 2012 anymore wake up.,hardware,2025-11-03 15:28:14,1
AMD,nmedcdb,"""At an acceptable framerate"". Good your game is running stable at 40, with ghosty taa or muddy fsr.",hardware,2025-10-31 16:51:56,3
AMD,nn0mf21,"This will depend entirely on your expected results. There isnt a single game out there that wont run on 10 GB card. But if you are physically unable to do anything but max settings then yeah, you need at least 12 GB.",hardware,2025-11-04 06:13:36,1
AMD,nmdp3p9,Thats the thing measurable presence doesnt mean their produxt are even remotely good. I feel the X3D are a hack and would probably benefit all Von Neumann architectures. Enterprise probably cared about Turin due to steep sales AMD was peoviding to cloud provider to take their chips after the multi year long deals with Intel expired. While on GPU side no noteworthy changes have been happening. At this point it just feels that AMD serves as a reminder to Intel and once Intel gets back on its knees it may be back to status quo of Intel for CPU and Nvidia for GPUs like in the early 2010s.,hardware,2025-10-31 14:53:32,2
AMD,nme1cz4,Techpowerup article on Zen5%. Just stating facts based on gaming.  https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/,hardware,2025-10-31 15:53:13,3
AMD,nmc1flo,"Hmm, I suppose you're right and I didn't think this through.",hardware,2025-10-31 07:47:23,5
AMD,nmfv9ap,"pretty sure the 5700 was a hardware issue. i dont think it was ever actually fixed, they just did their best to hide it.",hardware,2025-10-31 21:34:07,2
AMD,nmev047,"I was referring to NVIDIA; I upgraded my RX 5700 to 5070. NVIDIA problems have been worse and going on longer than it took for AMD to fix, but both 5k series drivers have been cheeks",hardware,2025-10-31 18:20:07,1
AMD,nml37h5,It's really seeming like you don't understand the comment chain. Why exactly are you asking *me* that question?,hardware,2025-11-01 19:50:47,1
AMD,nmdn7q7,"You can, but you wont get day 1 patches, but given that most recent AAA titles overwhelm the Steam Deck by now a moot point again. Drivers still will be available in Windows but not day 1 drivers! And that the SD is not really affected by it due to its linux nature is the entire point, you never got day 1 patches for it anyway!  But if you install Windows on a Steam Deck just to play about 3 games which do not work in Linux, you have to ask yourself the question wouldn´t you be better off with another console which probably will get you more performance. You usually buy the SD for the console like experience and for the inputs but other consoles have surpassed it in performance by now, and also come with Windows but with a less user friendly experience out of the box!  I have yet to hear about a person runnung windows on that thing permanently!",hardware,2025-10-31 14:44:02,2
AMD,nmgn15s,"4k is only good for 27inch and upper fyi. And even for 27inch, 1440p is perfectly enough.",hardware,2025-11-01 00:33:48,3
AMD,nn0mvkl,"The resolution stated was not 4k, it was:  > 3440x1440",hardware,2025-11-04 06:17:44,1
AMD,nmhzw83,"Even worse then tbh, means it's baked in to being trash.",hardware,2025-11-01 07:21:19,3
AMD,nmlbmcy,> For how long have you known this was going to happen? And why didn't you tell anyone?,hardware,2025-11-01 20:35:24,1
AMD,nmdv3to,"When SD was released, there were very few options in market, either $1000 Chinese windows handheld, or $400 SD with decent performance.",hardware,2025-10-31 15:23:17,1
AMD,no9j66d,"TLDW:    Current Prices:     i5-12400F: $135 (USA), $190 (AUS)     Ryzen 5 7500F: $165 (USA), $200 (AUS)           12 games average:    1080P Medium Settings: Ryzen 5 7500F was 20% faster than the i5-12400F    1080P Ultra settings: Ryzen 5 7500F was 18% faster than the i5-12400F",hardware,2025-11-11 11:21:50,68
AMD,no9vpze,In most of the world 7500F is a no brainer but where I live unfortunately DDR5 prices and AM5 motherboards too never went down so 12400F as a whole platform is considerably cheaper.,hardware,2025-11-11 12:59:55,44
AMD,no9lajl,Sigh  The 12**6**00KF is a price parity match and has more cores. than the 7500F.,hardware,2025-11-11 11:40:55,65
AMD,nodh00z,Wait wasn't 5600/5600x the competition of 12400f? Why am5 processor is being compared here,hardware,2025-11-12 00:20:45,15
AMD,no9kwnz,Just upgraded my brothers PC to 7500f. Better performance and platform than the 12400f route,hardware,2025-11-11 11:37:30,15
AMD,noagv2h,"This was a lazy and terrible video, and didn't cover the intangibles. ""herp derp let's bench 1 Intel CPU from 3 generations ago vs. 6 newer AMD CPUs"" ...the ""AMD Unboxed"" allegations will never go away with videos like these.  The one and only reason to consider a 12400F is that it's compatible with DDR4, and you may have old DDR4 laying around from an older build. HUB made a big deal about AM4 lasting forever. Well, you can use DDR4-3200 from *7 years ago* into an LGA1700 build. With DDR5 doubling in cost over the past couple months, this is significant.  The money saved by reusing DDR4 could get you a 12700KF, or if you can still find one, even a 12900K or 13600K/14600K (if you wish to risk Raptor Lake).",hardware,2025-11-11 15:04:46,31
AMD,noa6shb,You can buy the 225F on Amazon for $158 which has been shown to faster than even the 14400F.   HWU is a clown. Using old parts when the faster 225F exists for cheaper than the 7500F.   It will also wipe the floor with the 7500f in multicore.   https://youtu.be/0FBKJllIC7c?si=QDYNL5dj5vrgs7x-,hardware,2025-11-11 14:07:51,27
AMD,noa1gsw,Another disingenuous video to make intel seem more expensive than it really is,hardware,2025-11-11 13:36:09,25
AMD,no9kvw8,Should have tested 12600k https://www.amazon.com/Intel-i5-12600KF-Desktop-Processor-Unlocked/dp/B09FXFJW2F/ref=mp_s_a_1_1?crid=1VHEM9V1USMLA&dib=eyJ2IjoiMSJ9.Lh7qL24VXgupHBSS--Z7OOmG4NzOUFAiQMU9TYUT5sUmeo2DsmEjZZ51qfE0PsAE3rUV1QPJLeRqgPfdKaOlNb-ZjhgY0byUsPGJPhjDoMdspNPYRyne60VYHqTuvLBxIqwRyH6O3kF_OhPpEKt6kp24M7zvEllcVrVTtGDaq7DEfO1qy25LMFq8AHDR-DUGFbCbOE7Q6pd6amJqBCiZOw.DNobbn8DeaLhZJviwUvnWrMxxTa1n9S8w7sWumSHgWw&dib_tag=se&keywords=13400f&qid=1762860980&sprefix=13400%2Caps%2C335&sr=8-1,hardware,2025-11-11 11:37:19,22
AMD,no9t2rv,"I understand not having benchmarks for it, but they should've at least mentioned BCLK OC on the 12400F considering how big of a performance boost it is, especially as the board they showcased is more expensive than than the B760M PG Riptide that has the external clock gen to allow it, so recommending any other ddr5 board for 12400F than the riptide is really dumb, espcially if they cost more.  Not to mention they speak of ddr4 build for it and showcase a dddr5 board by accident, which is kinda funny. Though overclocking ddr4 on locked 12th chip is not gonna be fun times due to the locked SA voltage, with ddr5 somewhere between 6000-7000 is achievable at least with it so it's more inline with what the alder lake memory controller can do, but yea the current ram pricing kinda spoils a lot of these ""budget"" builds.  Other funny thing to note, the 14600K was around 150€/$, three/four months ago with normal priced ddr5, so I imagine any1 who build a system back then is probably feeling really smug now.",hardware,2025-11-11 12:41:50,12
AMD,no9kpds,seems very worth it to me.,hardware,2025-11-11 11:35:42,4
AMD,nocfcgi,Doesn't Intel have something better in the $160 range? 12400 was slower than 5600x and I feel nobody is really buying Alder Lake for budget reasons.,hardware,2025-11-11 20:55:11,3
AMD,nofvsf7,"Why is the 12400f still so ubiquitous, it's like 3 generations old?",hardware,2025-11-12 11:44:13,2
AMD,no9ml4m,7500f is like 110$ minus 10 to 20$ coupon. 12400f is also much cheaper than 140$,hardware,2025-11-11 11:51:51,4
AMD,no9ogak,"It’s nice that AM5 will still have an upgrade path, while 12400 or 12600 is end of the road for the motherboard other than swapping in something like 13/14900k which is probably not worth the money. AM5 will support Zen6.",hardware,2025-11-11 12:06:58,3
AMD,nofs7z0,"The 12400f is on par with 5600x, so it will never match up the 7500f",hardware,2025-11-12 11:13:24,1
AMD,noki7ix,my own test for 14400f vs 7700x   [https://www.youtube.com/watch?v=Z3IFpoytDwY](https://www.youtube.com/watch?v=Z3IFpoytDwY),hardware,2025-11-13 02:51:46,1
AMD,nomng9o,"Intel Core i5-14600k was here for 160-170€ couple weeks ago, along with free copy of Battlefield 6, i5-12600k was around 140-160€ for like whole year if not more until weeks ago available why would u even consider testing 12400f vs 7500f it makes no sense honestly even 14400f could be had for 130-140€ which all of them could be run with DDR4 memory even if u loose some performance in CPU heavy/ram intensive titles....",hardware,2025-11-13 13:31:36,1
AMD,noewcge,Alder Lake (12th gen) was launched in 2021. It’s insane that 4 years later it’s still a recommended platform.,hardware,2025-11-12 06:00:16,1
AMD,noeop4g,"Why are these youtubers so out of touch lol, what even is the point in testing on a 5090 when obviously people buying these kinda cpu is using with a 5070 or 5080 at best  Then obviously there would be barely any noticeable difference and 12400f is a no brainer choice",hardware,2025-11-12 04:56:59,1
AMD,no9r4sb,"If you want to go extra buget there's also the 14100, in my country it's half the price of the 7500f and it shouldn't be a difference up to like a 9060xt, 5060 ti, only in multiplayer or CPU intensive games.    Also remember when you are core limited cap the fps to the monitors refresh rate so the CPU has room to work with and doesn't stutter.",hardware,2025-11-11 12:27:41,-1
AMD,noa3ukp,The 11.11 sales have the 7500F even cheaper today. $145 AUS including tax just grabbed one off Aliexpress :D,hardware,2025-11-11 13:50:20,31
AMD,nob7we0,Just paid $132 for 7500F with b650 motherboard from 11.11 aliexpress sales. The CPU alone was around $95.  the motherboard is **HUANANZHI B650M B PRO AMD AM5. i was going to go for the 9600x but an extra $36 seemed WAY too much (percentage wise)**,hardware,2025-11-11 17:18:23,10
AMD,noavvdl,"8400F should be included too if you buy from Ali, looks roughly on par with 12400F while being slightly cheaper and it still has an upgrade path.",hardware,2025-11-11 16:19:15,7
AMD,noclh0j,Before any efficiency savings from operating the AMD for that matter.,hardware,2025-11-11 21:26:10,-1
AMD,nobhl5s,"I agree, 12th gen is cheap now for the performance provided.",hardware,2025-11-11 18:06:05,11
AMD,nogiaut,Everywhere is cooked with current DRR5 prices vs used DDR4,hardware,2025-11-12 14:13:42,1
AMD,noa00s6,Here in Finland we've had deals for 14600k for less than 7500f... (150€/155€ respectively with 25.5% vat included),hardware,2025-11-11 13:27:16,24
AMD,no9zwif,"In Spain (21% tax and shipping included), 12400f is only 3 euro cheaper (12400f 116€ (135$), vs 119€ (138$)). So, it doesn't make much sense to get the intel. So pricing is very region dependent.",hardware,2025-11-11 13:26:33,12
AMD,no9prpf,It's roughly $30 more in regions where the 7500F is officially sold.,hardware,2025-11-11 12:17:16,19
AMD,no9r5rr,Intel increased the price for the old processors  In my region 12400f is more expensive than 7500f,hardware,2025-11-11 12:27:53,7
AMD,noa4fee,And a much higher boosts than 12400,hardware,2025-11-11 13:53:46,3
AMD,no9t0rm,"Yeah, but because of the price parity it just straight-up gets destroyed for gaming because it's slower and on an infinitely worse platform. The 12600KF is not worthy of consideration for gamers.",hardware,2025-11-11 12:41:27,0
AMD,no9o4lv,But it is also twice the tdp at 125W.,hardware,2025-11-11 12:04:21,-9
AMD,no9p7wn,"Here's the 12600KF is 25 euro more then the 7500F while the 12400F is 15 euro less.   ~~And only 4 full cores vs AMD's 6~~. and the TDP as mentioned.  It's also not 20% faster then a 12400F, so would still lose to the AMD part.",hardware,2025-11-11 12:13:01,-8
AMD,nodnz9n,Because we want to make intel look worse than it should be. 14600K was 150$ and came with free battlefield 6 for so long but yeah no media wanted to talk about it. Real people in r/buildapcsales did buy a lot tho,hardware,2025-11-12 01:01:42,16
AMD,nognle4,"They rarely bothered using this F chip for any of their benchmarks. Even when they did, they used the non F version to show intel as worse value; but now that it has suddenly shot up in price, it makes an appearance after 4 years. That being said, the current pricing is a complete joke and no one should be buying 4 year old CPUs at all time high prices.",hardware,2025-11-12 14:43:19,2
AMD,noa5s89,Which gpu?,hardware,2025-11-11 14:01:48,1
AMD,nob4m9g,The synthetic benchmarking approach is completely useless to me as a 'budget build' viewer.   I have now learned nothing about the real world because nobody will pair these with a 5090. I don't care how these CPUs scale with a 5090. I care how these CPU scale on a budget.   On a budget means how much more would I have to invest into a CPU to gain significant performance over putting that money into a GPU.,hardware,2025-11-11 17:01:57,10
AMD,noftci0,"People just seriously need to stop watching these unlikable, smarmy frauds.",hardware,2025-11-12 11:23:22,4
AMD,nob9shw,Im inclined to call them intel unboxed. They evaluated the 7500F as $400+ platform even though anybody could get it for less than half the price. 1/3rd even.  Just paid $132 for 7500F with b650 motherboard from 11.11 aliexpress sales. The CPU alone ended around $95. the motherboard is **HUANANZHI B650M B PRO AMD AM5.**,hardware,2025-11-11 17:27:51,-6
AMD,nol2u4f,"> 225F  Which *must* use DDR5 and isn't actually a significant amount cheaper than the 7500F in total platform cost. For gaming applications, which is what most people here care about, it's also slower than the 7500F and not much faster than the 12400 (roughly 8-10% faster).   The 12400 can make sense if reusing DDR4, however it will also lose performance in that case as DDR5 6000MT/s does provide a boost to performance over DDR4 3200MT/s.  No one is buying the 7500F for it's multicore rendering/workstation application performance, it's a silly argument to make.",hardware,2025-11-13 05:12:14,1
AMD,no9vnuw,or 14400f  [https://www.newegg.com/intel-core-i5-14th-gen-core-i5-14400f-raptor-lake-lga-1700-desktop-cpu-processor/p/N82E16819118489?Item=N82E16819118489#moreBuyOptions](https://www.newegg.com/intel-core-i5-14th-gen-core-i5-14400f-raptor-lake-lga-1700-desktop-cpu-processor/p/N82E16819118489?Item=N82E16819118489#moreBuyOptions),hardware,2025-11-11 12:59:31,16
AMD,no9nngp,"Yes, but that would be unfair to AMD /s",hardware,2025-11-11 12:00:31,18
AMD,no9t9m4,It has almost identical performance (for gaming) for more money. The 12600k is not worth considering because the 12400F offers pretty much the same performance for a lower price. The only reason to use the 12600k in a comparison like this is because you're trying to make Intel look bad.,hardware,2025-11-11 12:43:10,-3
AMD,no9ooij,"Twice the TDP though. And at least here it's 175 euro vs 125 for the AMD part. (the 12400F is €110,-)  and it's on a dead platform.  ~~And still only 4 full cores vs AMD's 6.~~ spec sheet started with the efficiency cores for some reason",hardware,2025-11-11 12:08:47,-18
AMD,noa44gg,14600k when it was $150 on sale is the budget king,hardware,2025-11-11 13:51:58,12
AMD,no9xr94,"Well, what else can you expect from HUB? They have an extremely clear AMD bias and have done since Ryzen 1000 released.",hardware,2025-11-11 13:13:08,9
AMD,nodtdxn,"12700KF was as low as $180 last year but it's very difficult to find now. The 12700KF is way, way better than the 12400F: 8 P-Cores at 5GHz is significantly better than 6 P-Cores at 4.4GHz.  Also, the 265KF recently went [as low as $210](https://www.reddit.com/r/buildapcsales/comments/1llrsuo/bundle_intel_core_ultra_7_265kf_dying_light_the/) making it the value champ, except that it's DDR5 only. The 265KF being that cheap cannibalizes every other low-end Intel CPU unless you really need to stick with DDR4. In which case, the video is pointless anyway because HUB tested with DDR5.",hardware,2025-11-12 01:34:02,9
AMD,nod7eul,"The 12400f was faster than the 5600x when using DDR5, but slower with DDR4. The extra memory perf mostly negated the decent clock regression over a 5600x.",hardware,2025-11-11 23:24:40,1
AMD,nog4f88,"Intel must've had a huge stockpile of defective 6P Alder Lake CPU dies. They've released the same 6P/4E CPU _three_ times: 13400/14400/225, plus F variants. (The 225F isn't a true Arrow Lake CPU.) The 12400F is 6P/0E so the E-core clusters were either defective or fused off.  There were comparatively fewer non-defective 8P CPU dies, so now it's hard to find a 12700KF/12900K. Both were put on fire sales too: the 12900K was $275 back in May 2023. Absolute steal, so I went for it, reused my DDR4, and couldn't be happier.",hardware,2025-11-12 12:49:05,1
AMD,no9xjie,"... what?  12400 can be upgraded up to a 14900k. People are so odd with ""upgrade path"", playing mental gymnastics   Acting like AM5 will have infinite support. Yet that 12400f has likes. Dozen chips you could upgrade to on the same socket",hardware,2025-11-11 13:11:46,19
AMD,noagph0,AM5 can be upgraded to a Zen 7 CPU in 3 years. If you go with the Zen 7 X3D part 5 years from now you are laughing your way to the bank.,hardware,2025-11-11 15:03:55,-1
AMD,nofscg0,Yeah they used ddr5 with 12400f,hardware,2025-11-12 11:14:31,0
AMD,noevs1t,"""its a cpu benchmark"" not a real world review. Daniel owen, RandomGamingHD or Iceberg Tech are better for that since they'll test upscaling + fg (mfg not anymore). Its more budget-mid range focused  You'll have people slower than ampere celebrating 21% as if they get even 3% diff.   The 0.1% lows are actually more important if someone is going to fast cpu with slow gpu.",hardware,2025-11-12 05:55:24,3
AMD,nob3f6s,"What’s this 11.11? Black Friday sales have already started, 11.11, Christmas teaser sales. The sales are becoming meaningless.",hardware,2025-11-11 16:56:02,5
AMD,noh885u,"30 bucks for a more repairable and upgradable system is a steal, frankly. If that 7500 goes belly up after AM5 goes OOP, you'll have up to used 13800X3Ds as replacements, aside from upgrades.",hardware,2025-11-12 16:26:46,2
AMD,nod1rzd,The Intel uses ~20W less power at idle though. Probably ends up being a wash.,hardware,2025-11-11 22:52:20,10
AMD,noem08e,It used to be cheaper. I got my tray 12400f back in early 2024 for like $80-something.,hardware,2025-11-12 04:36:27,4
AMD,nobd6tx,"Yeah, there was a 2 week window where the 14600K was ridiculously cheap here in Germany as well and less than a month later Intel announced price hikes of 10-15% for 14th gen.  Was very weird.",hardware,2025-11-11 17:44:41,3
AMD,no9uvbe,"Outside US 12400F are dirty cheap, in China it's around $80-90 while 7500F is $110-120, if you go Ddr4 route + cheap B660 mobo price difference is significant.",hardware,2025-11-11 12:54:10,27
AMD,no9viid,Which region?  India? There 12400F is around $108 and 7500F around $155,hardware,2025-11-11 12:58:29,5
AMD,nomei6c,"> Intel increased the price for the old processors  How are you sure it's Intel, and not retailers/wholesalers?",hardware,2025-11-13 12:33:39,0
AMD,noa2n2w,12600k doesnt use 125 watts in games,hardware,2025-11-11 13:43:12,18
AMD,no9r1dk,"I think you are confused. They have 6 p cores and 4 e cores, aka 6 ""full"" cores that you are talking about.  Unless you are implying that the e cores are the ""full"" cores, then my b the p cores are obese cores.",hardware,2025-11-11 12:26:58,12
AMD,noahrki,He kept his old 1060 (that he got from me). He only plays e-sports titles at 1080p,hardware,2025-11-11 15:09:35,9
AMD,nobx378,RandomGamingHD or Iceberg Tech are much better in terms of budget hardware anyway,hardware,2025-11-11 19:21:20,6
AMD,nol1lzv,"> I have now learned nothing about the real world because nobody will pair these with a 5090.  The point is to show the performance ceiling of the CPU without the GPU becoming the limiting factor. Want to know how your GPU+CPU combo would perform? Check a benchmark for the GPU in the game you want to play, then check a CPU benchmark for that game. Whichever has the lower framerate will be the limiting factor. For example if your CPU can do 150FPS and your desired GPU can do 120FPS, you will get 120FPS and maximize your GPU usage.  That is what this type of benchmarking is useful for and why a 5090 is used.",hardware,2025-11-13 05:02:38,1
AMD,nobg52u,"yeah I'd rather punch myself in a dick than use HUANANZHI motherboard, whatever that is",hardware,2025-11-11 17:58:59,3
AMD,nobmen0,why is that both cost the same,hardware,2025-11-11 18:29:21,2
AMD,noa20x4,12600k is measurably faster than 12400F,hardware,2025-11-11 13:39:32,22
AMD,no9rwbj,"It has 6. Tdp is meaningless, they pull extremely similar power in games.",hardware,2025-11-11 12:33:20,16
AMD,nob1dgm,"Overclocking the bclk should not be in any general consumer advice at all, too easy to screw up your system.",hardware,2025-11-11 16:45:55,12
AMD,nohb9mh,"Even the 245k was not bad around the same time when including deals. I got a B860i ITX board for the price of an ATX one at that time plus a 1TB NVMe with my 245k for overall $50 less than a similarly specced 7600x build. I was debating on the 265k, but given I'm using an SFF case, lower TDP on the 245k are nice.",hardware,2025-11-12 16:41:32,1
AMD,noayuvh,14900K is unnecessary   Even a 14600k is a huge jump and they are inexpensive now,hardware,2025-11-11 16:33:41,5
AMD,noa89rd,14900k is WAY slower and more expensive than a 7800X3D and there are two more upgrades beyond that at minimum.  The upgrade path is wide open on the AMD platform and there is almost nothing on the Intel one.,hardware,2025-11-11 14:16:41,-1
AMD,noa7i05,">12400 can be upgraded up to a 14900k. People are so odd with ""upgrade path"", playing mental gymnastics  Huh? Why are we pretending the LGA 1700 upgrade path is any way comparable to AM5? There's an entire new generation of CPU's coming to the platform. Considering a 9800X3D already runs circles around a 14900K in gaming, a Zen 6 X3D part is obviously going to slaughter it.  There are many Zen 1/2 users on AM4 that extended the life of their systems by upgrading to a 5700/5800X3D for example.   Edit: The 12400 being significantly faster than the old 1800x or r5 3600,but slower than a 5800X3D is a perfect demonstration of why the upgrade path is useful.",hardware,2025-11-11 14:12:08,-2
AMD,noa0a5o,"> other than swapping in something like 13/14900k which is probably not worth the money  So did you just skip reading that sentence or what? Plus who in their right mind would pay $450 for a 14900k in a gaming rig when 9800X3D exists?  I didn’t say AM5 will have infinite support, but we know there is an entire new generation of CPU that will be supported on it. 14900k is already a generation old, and is end of the line for that Intel socket.",hardware,2025-11-11 13:28:52,-2
AMD,nol4k4r,"> 12400 can be upgraded up to a 14900k  But that is a disingenuous argument in itself - a 14900K requires a significantly more expensive motherboard and cooling setup to get full performance from than a 12400. Most people running 12400s are not going to be running top of the line Z series boards which can swing the full 150-220W CPU draw during gaming. I'd bet good money most are on B series boards which have a realistic power limit of 120W or so which would seriously hamper the performance of a 14900. Those boards would also enforce Intel's PL1 and PL2 limits which would force a downclock even if the VRM wasn't screaming for mercy.  Add to that the need for a larger cooler, even a PS120 isn't going to let a 14900 run full tilt for extended periods, and the cost to upgrade makes it a bad value for the performance gains.",hardware,2025-11-13 05:25:56,0
AMD,noaozof,"I bought AM5, but is it really confirmed Zen 7 is on AM5 from somewhere that’s not MLID?",hardware,2025-11-11 15:46:10,1
AMD,noezhsq,"its not a CPU benchmark, its a tiny portion of CPU workload benchmark with assumption it scales idnetically for every task. They havent even tested a single CPU bottleneck game. All they are testing is drawcall forwarding. And this is an ongoing issue with all the big CPU reviewers. They test a tiny portion of CPU and proclaim thats it for results.",hardware,2025-11-12 06:28:28,-5
AMD,nobdxmm,"11.11 is single's day, the biggest sale of the year in China",hardware,2025-11-11 17:48:19,19
AMD,noh8mks,"Depends on how disciplined about system power down and how much you put it through its paces, tbh. A Coolermaster 3DHP you'll soon be able to have at any Canada computers will easily dominate either's cooling demands mind.",hardware,2025-11-12 16:28:44,1
AMD,no9xoqd,Yes but that doesn't fit the Intel hate boner narrative,hardware,2025-11-11 13:12:42,11
AMD,no9wph6,Please ignore flipkart pricing it's not serviceable for delivery  Actual pricing for 12400f is ₹16790= 189$  7500f is ₹13894=156$,hardware,2025-11-11 13:06:25,9
AMD,nomji8o,"You should read some tech news, intel had confirmed themselves their cpu prices are increased  12400f was available in August ₹8999 or 101$  Now ₹15000 or 169$  Price increased more than 1.5x",hardware,2025-11-13 13:07:24,1
AMD,noaezz4,"Of course not. It uses ~200W instead. On a 125W TPD, by the way.  https://old.reddit.com/r/intel/comments/16np97f/i5_12600k_not_overclocked_how_did_it_reach_200w/  https://www.techpowerup.com/review/intel-core-i5-12600k-alder-lake-12th-gen/20.html  But sure, go ahead about how this doesn't happen in gaming. After all, gaming isn't known for stressing CPUs, especially games that actually need CPU performance.",hardware,2025-11-11 14:54:38,-13
AMD,no9sws2,The spec sheet i looked at had efficiecy cores first.,hardware,2025-11-11 12:40:40,-3
AMD,noaojmk,and you think the 12400f will hold back the 1060? Even with a 3600 its probably at 99% most of the time,hardware,2025-11-11 15:44:00,-3
AMD,nobigii,I've been using a Huananzhi X99 QD4 with a turbo boost unlocked Xeon E5-2697 v3 for several years without any issues.,hardware,2025-11-11 18:10:18,4
AMD,noe9gnu,"what makes the 12400F interesting is being cheaper, why would you invest the same on lga1700 for games instead of AM5 for less performance and no upgrade path.",hardware,2025-11-12 03:10:34,2
AMD,nob54q3,Also comes with a working igpu which is always a bonus.,hardware,2025-11-11 17:04:31,7
AMD,noa7yz0,"Sure. Measurably, but not significantly.",hardware,2025-11-11 14:14:54,-7
AMD,no9sqtx,"Still on a dead platform, and more expensive, and the 12600k isn't 20% faster in gaming then a 12400f so intel would still not be the preferred choice.  Edit: The intel reality denying squad is here is see.",hardware,2025-11-11 12:39:29,-17
AMD,nob2jmh,Not if the board has ECLK,hardware,2025-11-11 16:51:40,1
AMD,noaz1vi,Exactly I'm just pointing out the absurdity that 12400f has no upgrade path when it has handfuls of chips (which by the way have been seeing crazy good sales),hardware,2025-11-11 16:34:37,1
AMD,nof19o6,"""Way slower"". It is equally fast (sometimes faster). The advantage is efficiency",hardware,2025-11-12 06:44:41,5
AMD,noa95nr,It only made sense when the 5700x3d was going for under 200 dollars  That short window has closed,hardware,2025-11-11 14:21:52,10
AMD,noa7gy2,*goes to buy an am5 motherboard and sees DDR5 prices* 👀  👀,hardware,2025-11-11 14:11:58,8
AMD,noa320d,You are also assuming new AMD cpus will be cheap for this upgrade path,hardware,2025-11-11 13:45:41,5
AMD,nobyram,"The roadmap for consumer Zen 7 is 2027-2028ish, deployment of DDR6 _in servers_ is supposed to start in 2027. Consumer Zen 7 is almost certainly not getting DDR6 which means it won't be AM6.",hardware,2025-11-11 19:29:44,3
AMD,noar7wl,AM6 comes when DDR6 came out. And DDR6 is pretty far out.,hardware,2025-11-11 15:56:54,2
AMD,nol3vy3,"R6/CS are heavily CPU limited, that's why the framerates are so high.   I don't think you understand gaming CPU benchmarking at all. A 5090 is used to eliminate any possibility that the GPU is the limiting factor to show the performance ceiling of a given CPU. If a GPU bottleneck occurs all of the CPUs wind up with the same framerate and the benchmark doesn't show any useful data.   It's also a *gaming* CPU benchmark, not a full CPU review that would cover workstation applications. If you want that kind of review Gamer's Nexus always covers such use cases in their CPU reviews as do many other outlets.",hardware,2025-11-13 05:20:35,1
AMD,noj4whs,Except on Ali the resellers had already bumped the prices all month in anticipation for everyone waiting for it.,hardware,2025-11-12 22:05:15,1
AMD,nopx63m,"> Intel hate boner  It's not like Intel knowingly sold CPUs with defective microcode for years and attempted to cover it up until they were exposed. They also totally didn't re-label basically the same CPU and sell it as if it was new again for more money. They also totally have never bribed OEMs and SIs to avoid AMD and market only their CPUs in the past.   AMD are not without fault either, mainly their GPU division dropping drive support quickly several times, but none of their CPUs have self-destructed whilst issues were being denied.   These subreddits also tend to favor gaming performance and for gaming performance Intel is measurably behind the X3D chips.",hardware,2025-11-13 23:33:15,1
AMD,nodgirs,"The price is still around 10k I confirmed last month ,dunno why online prices have skyrocketed tho, online too it had been around 9-10k  last time I checked",hardware,2025-11-12 00:17:59,0
AMD,nonhpva,"* https://www.tomshardware.com/pc-components/cpus/multiple-generations-of-intels-modern-chips-see-price-hikes-up-to-20-percent-overseas-foreign-markets-are-feeling-the-pinch-on-12th-13th-and-14th-gen-chips * https://www.tomshardware.com/pc-components/cpus/intel-reportedly-raising-prices-on-ever-popular-raptor-lake-chips-outdated-cpus-to-get-over-10-percent-price-hike-due-to-disinterest-in-ai-processors  These reports say 10-20%, not 50%.",hardware,2025-11-13 16:12:09,1
AMD,noahoe5,"Cool, I assume you never owned this cpu  Now lets look at the results here  [https://www.techpowerup.com/review/amd-ryzen-7-7700x/24.html](https://www.techpowerup.com/review/amd-ryzen-7-7700x/24.html)",hardware,2025-11-11 15:09:07,14
AMD,nob3els,"No, I don't. Why would I lol... It's rather for future GPU upgrades",hardware,2025-11-11 16:55:57,8
AMD,noeyw5t,Depends on what you do with it. My 7800x3D is usually the bottleneck and not my 4070S.,hardware,2025-11-12 06:22:58,1
AMD,noa8in7,15% is significant for a 20-30 dollar difference,hardware,2025-11-11 14:18:06,14
AMD,no9tbxp,Can use DDR4 to avoid the ridiculous DDR5 cost rn.,hardware,2025-11-11 12:43:38,13
AMD,nob2zds,Still too niche. Not something I would encourage general users to seek.,hardware,2025-11-11 16:53:53,5
AMD,nopw01u,> It is equally fast (sometimes faster).  The 14900K is slower in nearly all games. Can you provide a benchmark it wins in a game? I was not able to find a single one over the 7800/9800X3D - the only wins it gets are in workstation type applications.,hardware,2025-11-13 23:26:22,1
AMD,noabfsy,">It only made sense when the 5700x3d was going for under 200 dollars  Nonsense. The X3D chips were available since 2022 which isn't a short window. AM5 is a better buy now obviously, but at the time even the 5800X3D was a good deal for existing AM4 users that wanted top tier gaming performance without upgrading their entire platform. It's not like the 12900K was cheap.   I don't understand why were downplaying the success of the AM4 platform here, lol.",hardware,2025-11-11 14:34:46,1
AMD,nocp8yu,Someone jumping in on a budget would likely do what I did when I switched to am5 from Am4 and get a chip like the 7600x with intent to upgrade in a few years.,hardware,2025-11-11 21:45:31,3
AMD,noad177,"No, I’m not. I’m assuming a first-time budget builder today may wish to spend more on an upgrade later in a few years.",hardware,2025-11-11 14:43:45,2
AMD,noli387,"no, if the framerates are high they are not CPU limited. They are merely forwarding a lot of drawcalls at high speed hence the framerate. Something thats actually CPU heavy wont have high framerate because CPU cannot keep up.  I understand how CPU benchmarking is done, i just think its wrong because all they do is test a single aspect of CPU while trying to do it in GPU intensive games.  I have no issue with them using a 5090 in the test. But it would not be needed if they tested something thats actually CPU intensive.  >It's also a gaming CPU benchmark  So try using CPU intensive games, then. Try simulation, builders, large scane strategy. A city sim in Skylines can bring a 7800x3D down to teenage FPS because the simulation gets too complex in large enough cities. But no, instead lets test drawcall forwarding at 500 fps in counters strike again.",hardware,2025-11-13 07:27:39,1
AMD,noqewvk,"It's a pure Intel hate boner. All corpos suck. When they do something good you reward them, when bad you punish.   That is how capitalism is SUPPOSED to work. Becoming tribal is cancerous and toxic to capitalism   Hell, AMD has seen almost zero uplift from 7000 to 9000 series at all, and they attempted to totally discontinue support for 5000 series AMD GPUs as soon as a week ago. Until people spoke up. AMD is NOT something to worship.   And as it stands now, Intel offers insane value especially in the core ultra clearance.",hardware,2025-11-14 01:17:34,3
AMD,noe0yol,Bro forgot last month's price  It's not 10k anymore,hardware,2025-11-12 02:19:14,1
AMD,noaikvt,Or here  [https://www.techpowerup.com/review/amd-ryzen-5-9600x/23.html](https://www.techpowerup.com/review/amd-ryzen-5-9600x/23.html),hardware,2025-11-11 15:13:53,9
AMD,noaiwll,"I did look. **Multi-threaded power consumption, Blender, 12600K: 128W**.   And that's on the board that directly follows Intel's guidelines, which is a fucking lottery still. I gave you a review that spotted it going over 200W, and then another example of a retail unit doing the same.  What else do you want me to see?",hardware,2025-11-11 15:15:36,-11
AMD,nobauci,"""Better performance""... Still depends on gpu... Even till 9070 I doubt there will be a 10% perf diffs. But its his money. If I were him I would've put it on something stronger than 2014 hardware",hardware,2025-11-11 17:33:04,-5
AMD,noezkht,"""For example in cyberpunk at 1440p, my 4070s gets xxx instead of xxx with 9800x3d""?  How much performance are you losing vs next tier upgrade?",hardware,2025-11-12 06:29:11,1
AMD,noa9hyz,"Yeah but it's 3% faster, lol",hardware,2025-11-11 14:23:50,-10
AMD,no9uf8d,"Yes, lets make the intel even slower.   And even with 16GB of DDR5 it would still be 5 euro less for the AMD part.",hardware,2025-11-11 12:51:09,-9
AMD,nod6zc7,"EBCLK on alder is really damn easy as far as OC does, but that is the thing with the 12400f. It's only really viable because of that because of the extremely low all core turbo ratio at stock (4.0). EBCLK on alder only changes your CPU, Ring/L3, and memory clockspeed. Pretty much all you have to do is set the CPU clock to 4ghz, adjust the ecblk to what you want, and lower the memory ratio to compensate. Mine did 4.8ghz pretty easily at 120mhz bclk.",hardware,2025-11-11 23:22:07,2
AMD,nob5vuv,Everybody should tune their systems a bit for better performance. That’s one of the best things about PC gaming.,hardware,2025-11-11 17:08:17,0
AMD,noaw6m1,Am4 only had 2 good generations  Zen 2 and 3  Most people adopted am4 started with zen 2 and 500 series chipset,hardware,2025-11-11 16:20:45,1
AMD,nopuvp6,">  They are merely forwarding a lot of drawcalls at high speed hence the framerate. Something thats actually CPU heavy wont have high framerate because CPU cannot keep up.  If this were true there would be no difference between a faster CPU and slower one in these games. Games can be heavily CPU bound even with massive framerates because the graphics in the game are relatively simple (CS2 and R6 fall into this category). The CPU completes an entire frame's worth of both in-world physics processing, object updates and graphics calls for every frame - it's not ""just sending draw calls"", that is not how it works.   Should they have included some simulation game benchmarks? Sure, that's a reasonable argument. But your assertion that the titles chosen were not CPU limited because they had high framerates is incorrect.",hardware,2025-11-13 23:19:53,1
AMD,noaj0uc,Or here  [https://www.youtube.com/watch?v=8oJDhE0wjwk](https://www.youtube.com/watch?v=8oJDhE0wjwk),hardware,2025-11-11 15:16:12,9
AMD,noakq8o,Is blender and prime95 a game?,hardware,2025-11-11 15:24:57,16
AMD,nof3990,I never said anything about Cyberpunk.  Im not sure i understand your question? Are you asked how much performance im loosing versus a 9800x3D?,hardware,2025-11-12 07:03:08,2
AMD,noa9k64,No its not,hardware,2025-11-11 14:24:12,14
AMD,noaorv0,"DDR5 doesn’t make a difference with the 12th gen series, last I checked",hardware,2025-11-11 15:45:07,6
AMD,nofhptr,"Enabling PBO on AMD is actually dead easy, I'd still lose 90% of the audience at step one. It's a niche thing.",hardware,2025-11-12 09:30:12,0
AMD,noeza1q,"the average user couldnt tell you what CPU they use, you trust them to tune their systems?",hardware,2025-11-12 06:26:29,2
AMD,nodt8kx,"If they need to jump through hoops and understand some subtle things it's not a solution that will ever be widespread. Even just encouraging overclocks on that socket is poor advice for casuals if not accompanied by a huge 13th/14th gen warning, which requires a whole other explanation. Better off looking for a 12600K or something either way if you want to overclock.",hardware,2025-11-12 01:33:09,1
AMD,noayio1,"You are misremembering or just lying. A ton of people started with Zen+ (2000 series). The 2600 itself was an extremely popular CPU at the time based on value.   Also there was that huge controversy about AMD not ""officially"" supporting Zen 3 on B350/B450 boards due to some of them having only 16MB bios chips, which they ended up walking back. That wouldn't have been the case without the pre-existing install base of B450 users.",hardware,2025-11-11 16:32:02,-1
AMD,nos33t4,"Of course there would be a difference. drawcall forwarding is CPU intensive.  My assertion was not that they are CPU limited, but that they are not CPU heavy and thus not a good title for these tests in the first place. It does not provide valuable information. Instead it muddies the water with benchmark numbers not applcable to actually CPU heavy games, because their CPU use is completely different. Its like if you tested AVX512 performance and declared Zen 5 to be twice as fast as Zen 4, but in real world its actually only 8% faster because you arent bottlenecked on AVX512 but on something different.",hardware,2025-11-14 08:50:20,1
AMD,noaruo0,"I can not find any actual gaming power consumption tests (outside igorlabs, who only played one even slightly CPU-intensive game and ran it on Ultra settings, averaging 80W). If you have power consumption benchmarks of this CPU in massive Rimworld bases, Factorio megabases, late-game Civ VI, or modded Paradox titles - let me know.  However, there is something that can help. https://www.techpowerup.com/review/intel-core-i5-12600k-alder-lake-12th-gen/20.html - with only a single core being hammered by SuperPi, the CPU is already running on 87W. Just 1 p-core out of 6. If the game you are playing is at least a little optimized, it will get much higher.  And no, I did not own this CPU. I am not in the habit of picking up e-garbage anymore.",hardware,2025-11-11 15:59:57,-6
AMD,nof4xzc,Yes how since 7800x3d bottlenecks ur 4070s. How much perf are you missing out by not getting 9800x3d with ur rig im guessing 1440p. Thought cyberpunk numbers will be readily avaible since everyone ran the benchmark. so?,hardware,2025-11-12 07:19:30,1
AMD,nocs11i,"Games are becoming more bandwidth sensitive, so while that was true at launch, it is less true today.",hardware,2025-11-11 21:59:43,2
AMD,nof6ubz,Maybe I just overestimate the capabilities of the average person,hardware,2025-11-12 07:38:17,1
AMD,nob3v3f,I am lying? Zen 2 and 3 sales were much stronger than Zen 1 and 1+  I literally bought and build a Zen 2 system because of the new chipset features and faster ipc  Early adapters of am4 were amd users who were on bulldozer or phenom 2,hardware,2025-11-11 16:58:14,1
AMD,noaxpg3,*Whole system 87 watts*  Keep moving the goalposts tho,hardware,2025-11-11 16:28:05,12
AMD,nolgpr1,"Its hard to tell what the real performance difference is because theres a mix of single thread performance, cache size limitations, etc. I dont think cyberpunk is even good game to do this test on, its too GPU focused. Outside of my work which is CPU based, for gaming youd need to test in stuff like simulation, large scale strategy games and builders.   So there isnt a clear answer how much performance im loosing, but if we look at tests done for different use cases, its about 8%. As i do not own a 9800x3D i cannot test my own use case here.",hardware,2025-11-13 07:14:08,1
AMD,noid5cg,a person looking for cpu reviews is far more tech savy than average.,hardware,2025-11-12 19:45:26,1
AMD,nolgtyl,You have to remmeber that people in this sub are enthusiasts. We are not representative of average consumer.,hardware,2025-11-13 07:15:16,1
AMD,nobaf0e,"Zen 2 and 3 sales being stronger is in part due to it being a drop in upgrade for people who were already on AM4.Again, if there wasn't already a big install base of B350/450 users, then nobody would have cared about AMD being wishy washy about Zen 3 support on older chipsets.  >I literally bought and build a Zen 2 system because of the new chipset features and faster ipc  OK, if you want to talk about anecdotes then I 've literally upgraded somewhere between 15-20 AM4 systems with a mixture of 1000-2000 series chips to Zen 3 over the last few years.  >Early adapters of am4 were amd users who were on bulldozer or phenom 2  You're just making shit up.",hardware,2025-11-11 17:30:56,1
AMD,noayh3n,"Do you have actual data or just snark? Because I seem to be the only one providing it in this conversation, while your entire argument is ""trust me bro"".",hardware,2025-11-11 16:31:49,0
AMD,noidzsb,Still not enough seemingly.,hardware,2025-11-12 19:49:34,1
AMD,nolnqgk,"Yeah, and most people here don’t bother to either",hardware,2025-11-13 08:24:15,2
AMD,nobhrja,"No, you are making shit up for overstating zen 1’s popularity  So you are telling me that there are more new zen 1 sales than new zen 2 and 3 sales? There were more 300 and 400 chipset sales than 500 series? Just compare the number of motherboard models that were available. Zen 1 was not as popular as you think it was. It was losing to haswell in games and nobody was changing platforms from haswell/skylake to zen 1. Zen 2 and 3 was when amd really took off and had many new adapters but users were already on 500 chipset.",hardware,2025-11-11 18:06:57,0
AMD,nnhw2w8,AMD should release a new cypher with 5 wheels to define their product SKUs,hardware,2025-11-06 22:14:36,58
AMD,nni4r5w,"Thank you for including the GPU Arch and Die Codename. I don't have enough mental capacity provisioned to always keep this on the top of my head and this will help get a ""Birds eye view"" of the landscape when it comes to this kind of smuggling in of older architectures.",hardware,2025-11-06 23:01:35,24
AMD,nnk1ldf,"I actually have a i7 12700H, they rebranded that to **Core 9**?",hardware,2025-11-07 06:36:41,10
AMD,nninw6e,"Only AMD could have it where if you buy something today from an OEM it could have Zen 2 with RDNA2 inside, Zen 3 with GCN5 inside, or be some random variation of Zen 4 or 5 while Strix Halo is still mostly vaporware a year after launch.  Dell apparently has plenty of Zen 2 laptops models to still offload, but trying to find 300 series parts is a mess. The cheaper 380 and 385 laptops still cost more than the full fat 395 ones too.",hardware,2025-11-07 00:52:39,30
AMD,nniryps,"I hate that this shit is what laptop chips have devolved into.  Edit: Also, are you sure about all those Raptor Cove chips? I swear all the Core Series 2 chips are rebranded Alder Lake, not Raptor Lake, since Raptor Lake never came as a H series chip.  Probably should remove P core column for N355 chips and similar, since those have no P Cores     Edit 2: Also, why not include Zen 5C as E-cores for AMD Strix Point?",hardware,2025-11-07 01:17:20,11
AMD,nnopzmc,God sent for someone like me who rly never bothered being up-to-date with laptop cpus,hardware,2025-11-08 00:04:41,2
AMD,nnkjm02,those raptor lake rebrand should have continue use 15000 series (after 14000 series),hardware,2025-11-07 09:37:51,1
AMD,nnituvg,"Once Snapdragon X2 Elite laptops are released, we will no longer have to remember AMD’s and Intel’s idiotic model numbers.",hardware,2025-11-07 01:28:59,-10
AMD,nnlr1oe,"Technically, they rebranded the i9-12900H, but since that’s the same chip as the 12700H with a different frequency, yes, they kind of did",hardware,2025-11-07 14:49:21,14
AMD,nnnhbu7,"There was a typo in the original post, it was supposed to say i7 13700H. That being said, calling the Core 9 270H a rebrand of the i7 12700H isn't that much of a stretch either since Raptor Lake H/P/U SKUs are basically only Raptor Lake by name, all their specs and stuff are practically identical to the Alder Lake chips.",hardware,2025-11-07 19:58:27,3
AMD,nnkka5i,"I don't know if it is a rebrand. The 12700H clocks to 4.7GHz on a laptop while the 12900HK can clock to 5.0GHz and the K represents that you can even adjust the clock speed on that chip.  Think of your i7 12700H as the Dodge Charger SRT-8 (or rebrand SRT Hellcat) producing 485 HP and 475 lb-ft from a 6.4 liter Apache V8. And the Core i9 12900HK is the Dodge SRT Demon with an all new 6.2 liter V8 and 2.7 liter supercharger rated for 808 hp and 770 lb-ft of torque.  amazing.  But yeah they are the same chip just the other one is faster and can be overclocked.  edit: But the thing is. 808 HP versus 485 HP you don't really need it on the road daily. You take the exact same 25 mph from a stand still stop sign to the next one the same as a Prius or Camry.  You go from your exit on the interstate to the next one at 65 mph just like everybody else. Except you have on tap 485 hp.   It is like playing counterstrike at 600 fps. For sure it is super fast, but you are still limited to max 120 fps for your own reflexes. And well generally internet speeds and the server speed will limit you. Because a 60 fps player can simply adjust for the delay on his computer while you need to speed up your own reflexes in order to handle that extra power.   All while that extra 540 fps doesn't do you a whole lotta good in the real world. Just like 808 hp doesn't do anybody anything on the real world road. Sure you can blast it and go fast. But you'll find yourself going slow and regular maybe after 5 or 8 times max.",hardware,2025-11-07 09:44:49,0
AMD,nnk58da,"But Intel is the scummier one here and AMD is at least honest with their naming!/s    Sorry, it's a pet peeve of mine that AMD gets treated like the best and most customer-friendly company ever in certain tech circles when there is so much evidence to the contrary... I very much hate Intel doing the same, but AMD takes it to a different level tbh and people defending this bullshit drive me up the walls.   (Edited for typos)",hardware,2025-11-07 07:10:55,12
AMD,nnixxql,"Good catch on the P cores for the N series chips, just fixed that.  ~~As for the Intel rebrands, the Core series 1 and 2 rebrands are all Raptor Lake AFIAK, as listed on Intel's website and confirmed on other sites like CPUWorld.~~ Edit: I've explained in another comment below,  but turns out Raptor Lake H is practically identical to Alder Lake H in all aspects except its stepping. Will look into how this impacts the spreadsheet.  I considered adding a E-core uArch column for AMD chips, but didn't see much of a point at the moment given Zen 5 and 5c cores are technically considered the same uArch, might add that in the future though if AMD choose to mix and match architectures for upcoming CPUs like some rumors are suggesting.",hardware,2025-11-07 01:53:45,6
AMD,nnj16mu,"There are already three Snapdragon X2 and nine Snapdragon X1 SKUs listed on Qualcomm's website, all with pretty indiscernible naming lol.",hardware,2025-11-07 02:13:14,21
AMD,nnko4kg,XE already has 4 versions alone.,hardware,2025-11-07 10:23:31,5
AMD,nnjwqj7,Snapdragon not any better on android with the sd6(s) sd7(s) sd8(s) gen 1234  keep rebranding other skus .,hardware,2025-11-07 05:52:05,4
AMD,nniu5dm,Bold of you to assume they won't be ported by Qualcomm to appease the OEMs lol.,hardware,2025-11-07 01:30:47,4
AMD,nnkpsqw,"Exactly. At least Intel is somewhat consistent in their naming bullshit, but AMD changes it so often that it’s nearly impossible to keep track of.",hardware,2025-11-07 10:39:47,11
AMD,nnkt7l0,The new AMD naming scheme is an improvement though. It ranks the SKUs appropriately and creates some semblance of consistency.,hardware,2025-11-07 11:11:23,-4
AMD,nnjk2my,"I don't know if I would exactly trust Intel's site when it comes to labeling things as Raptor Lake vs. Alder Lake.  I don't even attempt to keep up with the mess that is Intel's mobile CPU lineup, but on the desktop side it's pretty obvious that the i5-12600k is the same as the i5-13400, yet the latter is Raptor Lake on Intel's site whereas the former is Alder Lake.",hardware,2025-11-07 04:12:23,5
AMD,nnj53co,Damn. I guess the only one that makes some sense is Apple’s lineup.,hardware,2025-11-07 02:36:40,2
AMD,nnk67q9,Snapdragon 8 Gen 1 2 3 actually make sense but then they did elite and Gen 4 which wasn’t a flagship chip 🤦‍♂️,hardware,2025-11-07 07:20:30,2
AMD,nnl7cqh,It can be an improvement but that means nothing when they can't stick to a naming convention for more than 1 gen,hardware,2025-11-07 12:58:14,18
AMD,nnl7tco,"If you need a freaking spreadsheet to see what SKU is used in a CPU, then it's not appropriately ranked. *Especially* if a Ryzen 7 from supposedly the same generation is literally equivalend to a Ryzen 5, like with the Ryzen 7 7735HS and the Ryzen 5 7640HS. I say same generation because that is how it will be interpreted. And that doesn't yet touch on the 7640HS having a stronger iGPU and single-core performance. In some cases, the R5 will actually be stronger in every regard. Unless you are tech savvy, how are you supposed to glean what CPU is better anymore? What do the classifications even still mean?",hardware,2025-11-07 13:01:09,10
AMD,nnjpmdu,"There are two steppings of i5-13400 and i5-14400 units out in the wild. One stepping is Raptor Lake while the other stepping is Alder Lake. Both steppings are produced to the same specifications though, including cache size and clock speeds.",hardware,2025-11-07 04:53:16,7
AMD,nnjyxzz,"This sent me down a rabbit hole of looking into CPU steppings lol. It turns out that ""Raptor Lake"" H chips are technically a different stepping than Alder Lake H (J0 vs L0), but have pretty much all the same specs as Alder Lake and none of the usual advantages of Raptor Lake, with the main omission being the larger L2 cache capacity.  As for how this affects the spreadsheet, you could probably make the argument both ways for whether or not Raptor Lake and Alder Lake H chips are equivalent, so I think I'll just keep it as is for now until I can look more into CPU steppings and how much difference that actually makes.",hardware,2025-11-07 06:11:56,4
AMD,nnj988c,"Even that's kind of just an illusion of simplicity, the M4 for example is technically 3 separate SKUs (just base M4, not talking about M4 Pro/Max chips) with varying CPU and GPU core counts even though they're all branded as M4. Though at the very least, there hasn't been any rebranding from either Qualcomm or Apple yet that I'm aware of, so there's that.",hardware,2025-11-07 03:01:48,14
AMD,nnjxllx,"They're all just different bins of the same chip which is fine enough. They state the core counts and comparisons clearly on their website when referencing a specific SKU.  The pro and max chips have the exact same architecture and cores but just more of them which also makes sense.  The snapdragon is worse but also just mostly the same chip with different bins iirc  Nothing like what's going on with AMD and Intel, absolutely indecipherable for a casual follower which is what they want.",hardware,2025-11-07 05:59:47,3
AMD,nnklvvl,"How is Apple's naming ""fine""? If AMD sells a 15 core 9955HX or a 11CU 780M 8845HS with the same naming as the full fat ones I would absolutely flip out",hardware,2025-11-07 10:01:09,6
AMD,nnlszsf,"It's fine because it's the same chip with different numbers of cores disabled, which they tell you clearly whenever you purchase a device.  It's fine because it's simple and easy enough to understand that even apple fans can do it.  While you're prattling on about 9955HXes which is worse naming because it's worse naming. You're just familiar with it.",hardware,2025-11-07 14:59:29,2
AMD,nmdh8x0,To think this was supposed to be used as a single universal wire for high end VR headsets…,hardware,2025-10-31 14:13:30,128
AMD,nme4thc,Usb-c pd is something I wish was more standard on GPUs.,hardware,2025-10-31 16:09:56,27
AMD,nmdhbe1,"Great to hear this was a typo. It would have been a ridiculous thing to remove for any reason, especially under the inane guise of “streamlining” the driver as some speculated.",hardware,2025-10-31 14:13:51,14
AMD,nmdau7t,Waiting for the outrage farmers to apologize...,hardware,2025-10-31 13:40:10,0
AMD,nmdtbf1,"Unfortunately VirtualLink was going to lead to a signaling clusterfuck, and Meta abandoned PCVR immediately anyway.",hardware,2025-10-31 15:14:35,24
AMD,nmeafa2,There’s so much spare PCIE bandwidth that some graphics cards even come with M.2 slots.  May as well turn these GPUs into expansion cards while we’re at it.,hardware,2025-10-31 16:37:42,25
AMD,nml56z9,Honestly was super useful for me virtualizing on a consumer board when all the other lanes were shared. Just used the USB on the GPU like any other port.,hardware,2025-11-01 20:01:31,2
AMD,nmdioiz,AMD specifically said ***this*** driver removes the feature. It's no surprise people using USB-C power delivery would **not** update immediately and thus not many would test it in 24 hours.,hardware,2025-10-31 14:20:52,97
AMD,nmdhb9h,"TPU did test it, they updated the article.",hardware,2025-10-31 14:13:50,27
AMD,nmi4x75,Yeah but how else could we make shitty clickbait videos about it?,hardware,2025-11-01 08:17:51,-3
AMD,nmg7rmx,"""users that require this feature are adviced to roll back to 25.3.1""  nice ""typo""",hardware,2025-10-31 22:52:37,22
AMD,nmdczgu,"Very little of the outrage was towards this    This did look like a bug, there was no benefit to AMD to disable the USB-C.",hardware,2025-10-31 13:51:33,37
AMD,nmlx06v,Maybe it should be AMD that apologized for writing misleading patch notes? This is what AMD themselves wrote in the patch notes:  >USB-C power charging has been disabled for Radeon RX 7900 series graphics products. Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.,hardware,2025-11-01 22:33:37,8
AMD,nmij4h8,"The whole driver release seems to be a clusterfuck. So, no, just because the maintainers for the driver don't know what they're actually doing, it doesn't look better.",hardware,2025-11-01 10:49:38,4
AMD,nmddztx,Did they ever refund the defective vapor chamber lot #'s?,hardware,2025-10-31 13:56:46,7
AMD,nmvn890,most of outrage was for the branching of drivers (legacy support) which wasnt takenack.,hardware,2025-11-03 13:50:58,1
AMD,nme03xb,What do you mean signal clusterfuck ? Laptop has been doing this for years,hardware,2025-10-31 15:47:16,30
AMD,nme5zw4,Meta?  Valve screwed everyone by not supporting on Index.  Wasted everyone's time and money.,hardware,2025-10-31 16:15:43,14
AMD,nmdu6te,Meta is only *one* player here and an unreliable one at that.,hardware,2025-10-31 15:18:50,14
AMD,nmerrhz,More like cards that only come with x8 lanes having a NVMe slot to use the full x16 slot that they are already using,hardware,2025-10-31 18:03:23,11
AMD,nmgd2jv,"I'm actually confused how they got that advice.  Because this article states that AMD reached out to them to let them know that this was misinformation, but the article from yesterday stated that ""AMD"" advises rolling back the driver.  So is AMD sending TPU mixed messages, or did TPU not actually talk to AMD regarding their original article?",hardware,2025-10-31 23:27:08,5
AMD,nme83g7,Reducing driver bloat?,hardware,2025-10-31 16:26:10,-1
AMD,nmdk9iq,They offered to repair/replace the cards IIRC.,hardware,2025-10-31 14:28:56,2
AMD,nmenb70,"Virtual Link runs 3.0 signals over pins that every other standard uses for 2.0, and is not reversible. Even if you could reasonably get that link negotiated and noise free over 5m lengths (a challenge that Valve publicly abandoned), you'd have a consumer cluster fuck of confusion as their standard USB C cables failed to work properly and looked identical.  Quotes from wiki:  > Unlike most alt-modes this remapped A7, A6, B6, B7 to carry a USB 3.0 signal, instead of the usual passive USB 2.0 signal. This means that one would not be able to extend the cable using a standard USB-C 3.0 cable, which has these pins mapped only for unshielded USB 2.0 signals. Also this required the VirtualLink port to also detect the correct orientation of the USB-C plug to ensure that the USB 3.0 TX and RX lanes are correctly connected.  > In VirtualLink mode, there were six high-speed lanes active in the USB-C connector and cable: four lanes transmit four DisplayPort HBR 3 video streams from the PC to the headset while two lanes implement a bidirectional USB 3.1 Gen 2 channel between the PC and the headset. Unlike the classic DisplayPort USB-C alternate mode, VirtualLink has no USB 2.0 channels active, instead providing a higher speed USB 3.1 Gen 2 (SuperSpeed+) over the same A6, A7, B7, B6 pins. VirtualLink also required the PC to provide 15 to 27 watts of power.  > To achieve six high-speed lanes over USB-C, VirtualLink required special cables that conformed to version 1.3 of the USB-C standard and used shielded differential pairs for both USB 2.0 pairs.",hardware,2025-10-31 17:41:16,37
AMD,nme6m41,Maybe for longer cables? usually you don't get 5m long cabales for higher bit rate USB C standards (like 40gps and the new 80 one). Those that get to longer length usually are limited to 3m and are hella expensive due to active components in cables. If optical transceiver will get cheap enough there might be a case for those in the future.,hardware,2025-10-31 16:18:46,6
AMD,nmenow5,"How many Indexes were sold? Now ask how many Rifts, Vives, and more importantly Quests were sold.   Valve couldn't get virtuallink working reliably enough for a consumer product. They're not responsible for its failure.   > The adapter cable was originally meant to provide added convenience for Valve Index users, making it so they could rely on a single USB-C connection to the headset rather than requiring separate physical connections for video, USB, and power.  > However, for multiple technical reasons we no longer believe that the product would deliver that added convenience. Foremost on that list is reliability. Our current testing indicates the VR connection may fail to establish in a reliable manner. Additionally, Virtual Link technology has not been widely adopted by manufacturers, laptops in particular (where a single connection could be the most beneficial), translating to very few PCs having viable ports for the connection.",hardware,2025-10-31 17:43:10,12
AMD,nmeflc1,Valve put out an exclusive title and provides hardware support to this day. Unless you’re suggesting that they became a publisher like Meta did I don’t know what else they were supposed to do,hardware,2025-10-31 17:02:49,5
AMD,nmlzfwt,"No point making more big VR games when Half Life Alyx (the one AAA VR exclusive) gets outsold by Indie games with a 1 man dev team. And btw I love VR, I'm just telling it like it is.   Sony did the same with the Horizon VR game, once they saw the sales of that and PS VR2 they stopped development on all their upcoming VR games and killed all their studios that only made VR games.",hardware,2025-11-01 22:48:02,1
AMD,nmdxxmz,It's the only player that matters for hardware standards. I say that as an avid PCVR player that uses non-Meta hardware to this day.,hardware,2025-10-31 15:36:51,10
AMD,nmi9qmg,"> but the article from yesterday stated that ""AMD"" advises rolling back the driver.  ""USB-C power charging has been disabled for Radeon RX 7900 series graphics products. Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.""  They put this in the changelog, so no reason to ask AMD for the same thing if they put the answer in the changelog already .. or so everyone thought .. once the posts went up, they reached out ""nono, it's all a mistake"" .. so we published that .. then I was like ""wtf mistake, these people can't be for real"" .. so I fired up a 7900 XTX and tested it for myself, and oh surprise, it has always worked, without driver, with launch driver, with newest driver, so we updated our stories.  AMA",hardware,2025-11-01 09:11:55,17
AMD,nmgbk4r,I had 6900 XT with a bad vapor chamber. 30 degree gap to the hotspot and always almost instantly hit the thermal limit. RMA'd and they gave me 7900 XTX which was great at first (assume it was a refurb/repair) but now after a year also suffers from about 25 degree gap when under stress from a big game. Max temps on the hotspot still hits 105 unless I undervolt and enable Chill,hardware,2025-10-31 23:17:21,3
AMD,nmf7zom,"Common dude, VirtualLink was all about PCVR in 2018/2019.  Rift came out in 2016, so irrelevant.  Vive 2016, irrelevant.  Quest targeted standalone VR, it was not for PCVR, it didn't even have DP.  Index was clearly supposed to support VirtualLink, then they drop it and later say they'll have some addon but it was dead at that point.  Go plug PSVR2 into a Turing GPU, that is essentially VirutalLink working just fine.  No technical reason Valve couldn't have gotten it to work.  They chose not to.",hardware,2025-10-31 19:27:14,-9
AMD,nmf64jx,What exclusive title are you talking about? Alyx is still very much playable without any Valve hardware.,hardware,2025-10-31 19:17:27,2
AMD,nmei3pv,"Everyone was going to support VirutalLink (NV/AMD/Valve/...), NV shipped Turing with VirtualLink and then like 6 months later Valve ships Index without VirtualLink.  That was the end of VirtualLink, no one was going to waste more time on it.",hardware,2025-10-31 17:15:12,10
AMD,nmlzpmt,Make more games which btw they were making but then never announced said games many which are believed to be cancelled post half life alyxs release.,hardware,2025-11-01 22:49:38,1
AMD,nmdyzjq,"Oh, please.",hardware,2025-10-31 15:41:53,3
AMD,nmgckum,Did you try to RMA that one too?  Or does it fall under the original warranty?,hardware,2025-10-31 23:23:59,0
AMD,nmfdrzc,"> The adapter cable was originally meant to provide added convenience for Valve Index users, making it so they could rely on a single USB-C connection to the headset rather than requiring separate physical connections for video, USB, and power.  > However, for multiple technical reasons we no longer believe that the product would deliver that added convenience. Foremost on that list is reliability. Our current testing indicates the VR connection may fail to establish in a reliable manner. Additionally, Virtual Link technology has not been widely adopted by manufacturers, laptops in particular (where a single connection could be the most beneficial), translating to very few PCs having viable ports for the connection.  PSVR2 also does not use VirtualLink. It needs a port capable of DP Alt Mode and 12v USB PD. VirtualLink ports happen to be compatible. It's an important distinction. It doesn't repurpose the USB 2.0 data pins for 3.0 speeds, which is part of the VirtualLink spec and was the issue.   https://steamcommunity.com/app/992490/discussions/17/3879344463500344882/",hardware,2025-10-31 19:58:04,10
AMD,nmf7u38,"Flagship was probably a better word, but I meant PCVR exclusive",hardware,2025-10-31 19:26:24,5
AMD,nmlzyd4,Can't make games exclusive when they run on x86 Windows PCs but it is Steam exclusive as in you can't buy it anywhere else.,hardware,2025-11-01 22:51:02,1
AMD,nmfnsph,"VR barely exists, and you're claiming ""everyone was going to support VirutalLink"", which was a proprietary non-standard abuse of a standard?  No.  No one was going to meaningfully support that.  You're lucky there was even a single platform where it worked.",hardware,2025-10-31 20:51:43,8
AMD,nmfe15f,"Everyone? Only one VR headset ever announced an intent to support it, and not a single laptop ever implemented it.",hardware,2025-10-31 19:59:24,6
AMD,nmei14i,"I mean it’s an exaggeration for sure, but not that huge of one. More VR headsets in use today are made by meta than by every other brand combined, and something like 70% of all new headsets sold are theirs.   They own that market pretty thoroughly, unfortunately.",hardware,2025-10-31 17:14:51,9
AMD,nmemtvl,"What's the market share and revenue share for Meta's headsets? What do the next most popular competing headsets look like? Are the successful competitors PCVR headsets with a cable, or are they wireless standalone headsets?",hardware,2025-10-31 17:38:51,5
AMD,nmg1iaa,"It was standard that was supported by every VRPC company, as well as Meta.  That's hwo standards are made.",hardware,2025-10-31 22:11:58,-2
AMD,nmgj0iz,"Name a ""VRPC company, "" as you put it, that used VirtualLink.",hardware,2025-11-01 00:06:25,1
AMD,nmxo8jr,"NVIDIA, AMD, Oculus, Valve, and Microsoft were part of consortium.  What hardware company was not?",hardware,2025-11-03 19:51:03,0
AMD,nmxpm63,"Name a ""VRPC company, "" as you put it, that used VirtualLink.",hardware,2025-11-03 19:57:47,1
AMD,nnurvj1,"It's priced decently. If it was $100 cheaper, it would be a really good bang for the buck.",hardware,2025-11-09 00:36:26,45
AMD,nnv0bub,Microcenter-only deal again?,hardware,2025-11-09 01:30:07,24
AMD,nnur8vl,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-09 00:32:43,2
AMD,no1zq3p,"With this, and the talk about a 9700X3D, I'm starting to suspect that there won't be a 9600X3D.",hardware,2025-11-10 04:23:38,2
AMD,no2ldb8,"I Think AMD makes those Microcenter only as they probably don't have either enough chips to supply worldwide or they just want to prompt everyone else who don't have Microcenter near him to buy the more expensive 7800X3D/9800X3D.  Anyway, your local retailer (even outside of US) could be importing those. I had local retailer in Europe selling 7600X3D year ago for \~330€. Probably was also low quantities, but was great alternative to at that time 420€ 7800X3D.",hardware,2025-11-10 07:29:40,1
AMD,nnv09io,But what did they remove/downgrade  to make it cheaper?,hardware,2025-11-09 01:29:42,14
AMD,nny5bup,With current dram prices it’s actually a pretty decent deal. I ended above that just yeeting what I consider decent prices on hardware (outside the ram stick),hardware,2025-11-09 16:04:08,1
AMD,nnv27tc,Always,hardware,2025-11-09 01:42:29,16
AMD,no7pt5b,Oh there will be. When Zen 6 is already out.,hardware,2025-11-11 02:14:47,3
AMD,nohx6h9,Wasn't the 9700x3d a hoax?,hardware,2025-11-12 18:27:40,1
AMD,nnvf6m2,* 7600X3D: Max 4.7GHz * 7500X3D: Max 4.6GHz,hardware,2025-11-09 03:07:05,31
AMD,nnwov90,"Primarily clocks, don't know additional benchmarks till they are available to YouTuber/Labs",hardware,2025-11-09 09:47:51,2
AMD,no11uyu,Just chips that wouldn’t meet the 7600x3d performance numbers. Normally lower x3d chips are best perf for dollar you’re gonna ever get.,hardware,2025-11-10 00:51:43,1
AMD,no48dzr,Damn. Might be true. I got my 16gb ram stick for laptop just before this abomination.,hardware,2025-11-10 15:14:32,1
AMD,nnvntnk,"Sort of pointless to be irritated about. The only reason they're microcenter only to begin with is because there's not enough of them to do a wide release. What, should they ship 3 of them to each region instead to satisfy some kind of ""fairness"" doctrine or something?",hardware,2025-11-09 04:07:45,11
AMD,noje41u,The leaked benchmarks were indeed a hoax. I made the above comment before news about it reached me.,hardware,2025-11-12 22:54:56,1
AMD,nnv2rci,Responded to PC  not CPU. Different beasts.,hardware,2025-11-09 01:46:01,7
AMD,nnv15df,I just read it. Twice. It doesn't mention what the difference is between the two CPUs. At least not in a way that's helpful if you don't already know the full specs of both.,hardware,2025-11-09 01:35:30,6
AMD,nok9638,"Ahh, soz",hardware,2025-11-13 01:57:30,1
AMD,nnv3ipf,The difference between 7500 and 7600 is about 100.,hardware,2025-11-09 01:50:56,9
AMD,nnxmqbo,"These ""parts"" are just stuff they dug out of the garbage bin and in the limited supply of them they had they decided to give them to microcenter instead of throwing them away.   It's important to remember that the 7600x3d is either a failed 7900x3d or a failed 7800x3d already. These microcenter chips are parts that sucked so much they failed twice lol...there's also only a 100mz difference here 7500x3d vs 7600x3d, which means they barely failed.   There's no lines actually making these things over in China/Malaysia or whatever. It's just AMD sorting through their stack of failed stuff and when they have enough to make a product launch they give them to microcenter instead of throwing them away. There not being a lot of them means yields are good on their parts which is a good thing more than anything.",hardware,2025-11-09 14:23:42,8
AMD,nnc8i1h,"It's worth noting that the comparison is 368 vs 176 cores (both zen 4), so you're only seeing HBM having a particular benefit in the benchmarks where the performance is well over 2x",hardware,2025-11-06 00:52:55,17
AMD,nnbx7f2,"The elusive MI300C which was announced around last year is available for general availability. Called their, ""Azure HBv5"" houses the EPYC 9V64H. 92 Zen4 Cores no SMT per CPU with a total of 368 cores with \~400-450GB of HBM3. Benchmarked against 3D v-cached, HBv4, EPYC 9V33X (Genoa-X) for HPC workloads the performance gains are massive.",hardware,2025-11-05 23:48:28,14
AMD,nncnjb7,The [Algebraic Multi-Grid Benchmark](https://openbenchmarking.org/test/pts/amg) performance is amazing!,hardware,2025-11-06 02:23:19,8
AMD,nnfbxhc,Those Azure CPUs are crazy good,hardware,2025-11-06 14:47:28,1
AMD,nndvfbc,"Oh, thats a BIG caveat.",hardware,2025-11-06 08:01:24,6
AMD,nnfr1ft,"That’s true if we assume the workload performance scales perfectly with the number of cores, ignoring the possibility that the workload is already bottlenecked by memory despite the core count.      Edit: I mean, a 2x gain, or even less, can actually happen with the help of HBM3 if the performance of the HBv4 EPYC 9V33X is already memory-bottlenecked.",hardware,2025-11-06 16:01:21,5
AMD,nnghwwh,"that's the issue i been seeing with high core count chips they don't have the memory bandwidth to keep the cores fed, some develops are removing the need to load stuff into memory within the Linux kernel and we been seeing a 800%+ increase in throughput with some task, mostly networking.",hardware,2025-11-06 18:09:56,3
AMD,nnh5za6,If it's already bandwidth constrained on the HBv4 then the HBv5 (with 7.5x the bandwidth) should be much more than twice as fast.,hardware,2025-11-06 20:05:25,2
AMD,nn8ksyl,"I had a 9800 Pro back in the day- absolute legend.  The fact that the FX 5950 Ultra was unplayable in new games just a couple of years after it came out, despite turning down settings, shows how good we have it now.",hardware,2025-11-05 13:56:42,23
AMD,nn8efus,Crazy how 9800xt seems like a new card until you remember it's from over 2 decades ago and the naming scheme is still somehow used now by AMD.,hardware,2025-11-05 13:20:53,36
AMD,nn8zmva,I had a 9800XT; and it was beautiful to play some Far Cry 1. It was a great card that lasted several years being still good to use,hardware,2025-11-05 15:14:25,7
AMD,nn9hwlt,"Man the geforce fx was horrible . I have fx 5600 and the 6600gt 1 years later smoked it . Not just normal smoked , ultra smoked . 5x faster at the SAME FUCKING PRICE.     Image the 5070s 5x faster than 5070 .",hardware,2025-11-05 16:42:11,7
AMD,nna67iq,"*I was there, Gandalf. I was there 3000 years ago.*  I even owned a Radeon 9800 Pro at one point. The Radeon 9700/9800 was legendary even back then. The GeForce FX too, except that was a legendary failure.",hardware,2025-11-05 18:35:12,6
AMD,nn9fnkw,"With our new Geforce FX 5950 Ultra we can finally compete with the Radeon 9700 pro!  ...Whaddya mean, ""9800xt?!?""",hardware,2025-11-05 16:31:30,4
AMD,nn9m16t,"Coolers were horrible back then. I have an old fx 5700 for a retro PC and even though it has a tiny mid tier cooler it is loud as hell. Of course, this was the era where people installed 55db tornado fans on their CPUs and fart pipe exhaust on their honda civics so maybe everyone's hearing was so blown out they didn't notice.",hardware,2025-11-05 17:01:44,3
AMD,nn84goh,"Ok, you got me with that title",hardware,2025-11-05 12:16:51,6
AMD,nn96bcz,"Yeah, progress was **insane** back then. I had the 6600 GT, and that card was [almost **twice** as powerful](https://web.archive.org/web/20241121184001/https://www.anandtech.com/show/1464/5) as the 5950 Ultra!  It would be like if the 5060 Ti were almost twice as powerful as the 4090. It sounds absurd, but that was literally the case back then.  If you bought a gaming computer you really needed to use it. Because 3-4 years later it wouldn't play new games anymore at any sort of reasonable frame rate.",hardware,2025-11-05 15:46:59,23
AMD,nnd8ckk,"Guy at Future Shop sold me an ATI All-In-Wonder 9800 PRO, even though I had no need for all that TV tuner stuff, and I still don't get what it even did. Record TV? Ran good at the time, but I have no idea if I ever got a good deal on it. I think it was in like 2004, so the card may have been a generation old. Played C&C generals, and HL2 pretty well, though. Wasn't Until ES Oblivion, where it struggled.",hardware,2025-11-06 04:40:22,2
AMD,no390ve,Back when developers used to actually utilize new hardware features instead of sitting on them for 8 years.,hardware,2025-11-10 11:28:14,2
AMD,nna9u2s,Gpus became much faster each gen. Now a 5060 is not much faster than a 2060,hardware,2025-11-05 18:51:44,1
AMD,nnd9a5w,"AMD radeon naming scheme is a trainwreck anyway.  They didnt finish the HD8000,9000, then they start with R200 stop at R500, start with RX5000, now they jump to RX9000 skipping the RX8000 name. I wonder whats after RX9000? Another new format?",hardware,2025-11-06 04:47:18,8
AMD,nn8pzq9,"Well, it kinda isn’t though - they changed to 90XX XT for the latest generation. In the same way that Nvidia did.  Also note that “RX” is also only recent - the RX480 was the first RX. But I think they are pretty much stuck with it now.",hardware,2025-11-05 14:24:58,13
AMD,nna8rfj,The 5600 got smoked by the previous Ti 4200 as well. It was just a really bad card,hardware,2025-11-05 18:46:51,2
AMD,nnb2a32,"Same goes for other components.  I remember my dad upgrading our office desktop's OS from Windows ME (it BSOD'ed on our first day of usage) to Windows XP. Then he upgraded RAM from 128MB to 512MB for XP to stop running like molasses.  > Because 3-4 years later it wouldn't play new games anymore at any sort of reasonable frame rate.  Back in the 1990's, sometimes it meant not being able to actually boot up the game due to GPU incompatibility. Lego Rock Raiders game had a menu to select which specific GPU models you were running, because otherwise it could lock the graphic settings to really low ones.",hardware,2025-11-05 21:08:04,3
AMD,nne6ur4,We didnt have huge dies back then. More analogue would be 5060 Ti being much faster than a 4080 Super.,hardware,2025-11-06 09:59:23,3
AMD,nnkz95q,I remember buying a GeForce 6800 Ultra because my friend insisted that the jump was so good that the 500$ was worth it and I bought at the very beginning so I had a lot of time to enjoy until the next cards came.  500$ could get you the absolute high end graphics card on the market.,hardware,2025-11-07 12:01:21,1
AMD,nnboc0a,It's 2x faster...,hardware,2025-11-05 22:57:57,7
AMD,nnhhqrc,>  Now a 5060 is not much faster than a 2060   It's pretty substantially faster  https://www.techpowerup.com/review/geforce-x60-history-benchmarks-image-quality/5.html,hardware,2025-11-06 21:04:06,3
AMD,nne11qk,I mean 10070xt seems like a horrible name letter change seems mandatory at some point,hardware,2025-11-06 08:59:03,5
AMD,nnekv17,Probably a new name format.,hardware,2025-11-06 12:04:35,3
AMD,nne2vsl,yea but the 4200 doesnt have dx9 ?(or sth like that) Them glossy shinny reflection sufface dude.,hardware,2025-11-06 09:18:25,3
AMD,nndkg3v,"In very late '98 or early '99 our first family PC was a Pentium I, possibly MMX, S3 Virge DX. My dad ""upgraded"" it from WIN95 to ME, which has a min requirement of 150 MHz and 32MB RAM - installer wouldn't let you continue without. So very likely had to be at least that spec, I doubt he would have known the /nm bypass switch.    Funny you mention Lego Rock Raiders because I remember playing it with crazy graphical glitches on that S3, and it must have run like 10 FPS, but when you're a kid you pay stuff like that no mind. Still have the original disc. You could choose between plain old Direct3D, Voodoo (Glide), and I think another proprietary API for S3 Savage cards.",hardware,2025-11-06 06:16:51,1
AMD,nnedoj3,I'll concede that point.,hardware,2025-11-06 11:05:03,2
AMD,nncp8ca,Now imagine 2x in 1 year instead of 6. In some cases 3-10x between major architecture changes.  Between 2001 and 2007 you went from GeForce3 Ti500 to 8800 Ultra. 12 GFLOPS to 387 GFLOPS.,hardware,2025-11-06 02:33:42,8
AMD,nnf231l,"Tell me you bought your first gpu max 10 years ago without telling me.  In 2003 the difference between a 6 year old gpu  vs a new one was 50 x not 2x.  There would also have been many major new technologies added to the new one, think dlss, dx12 or raytracing support in scope but every year instead of every 8 years  Now maybe you understand why the old gpu that is 50 x slower and doesnt support any tech from the last 4 or 5 generations wouldnt be able to run the newest games.  You dont expect a gtx 260 to run todays game, even though the difference between it and a 5060 is much smaller than the two gpus from 1997 and 2003",hardware,2025-11-06 13:54:18,0
AMD,nngeysk,The R*AI*DEON Px170 XT: Prescription Strength,hardware,2025-11-06 17:55:57,4
AMD,nng6a20,"FX kinda did but not really. That architecture ran well with DX8 and older shaders, but using DX9 sharers would often nerf the performance into the ground, as is evident in that video. It was just a failed generation.",hardware,2025-11-06 17:14:09,4
AMD,nnfupz1,"I've probably been in this longer than you.  I know the reasons for rapid performance increases in the 90s and 2000s but also know why that was possible and isn't now.  Nobody is holding back, if it was possible to progress that quickly again they would be but the options are extremely limited and people are still going to balk at the cost of the products these ever more expensive node shrinks enable.",hardware,2025-11-06 16:18:39,2
AMD,nnl4k5g,You re just completely changing the argument lol.  Its not about what is possible.  It s that gpus today need to support games for a long time because performance gains have ground to a halt.  Thats why i replied to the guy i initially replied to. We dont have it good today because gpu vendors graciously support their hardware better. Technology is at a standstill so obviously the new shit that is the same as the old shit +2-10 percent using the same underlying technology is still going to be compatible.  I would much rather return to a hypothetical world where gaming technology advanced at the pace that it did in the 90s in 2000s.  Especially since the standstill is happening just as gpus arent powerful enough for real time raytracing without massive development effort and without looking like visual soup.,hardware,2025-11-07 12:39:57,1
AMD,noznoqs,"get a 9800x3d (9900x3d is just to get people to think ""ooo bigger number"")  and a peerless assassin :)",buildapc,2025-11-15 15:28:29,4
AMD,nozo4mb,"Yeah that'll handle a 9900x3D, note the mounting bracketry will be different. It should come with the AM5 mounting hardware you'll need.",buildapc,2025-11-15 15:30:52,1
AMD,nozo6bq,Id assume that would be more then enough? I have a 9950x3d and my coolers a thermal right phantom spirit evo 120mm and avg 70 or 80 C i believe,buildapc,2025-11-15 15:31:07,1
AMD,nozowbr,"For gaming 9800X3D is faster. If you need the 4 extra cores for work, that makes sense.  I'd get a Phantom Spirit 120 instead of the cooler you selected, but the cooler you selected will cool a 9800X3D/9900X3D.",buildapc,2025-11-15 15:34:59,1
AMD,nozodo7,Yes.  But why?  Peerless Assassin SE cools more and costs 1/3 of that Corsair.  An AIO generally wont leak on you unless you somehow cut the tubes.  AIOs also have much larger thermal capacity which means you wont hit throttle as fast and cool off faster.  Check out this video:  [https://www.youtube.com/watch?v=f5Yj1fcce7o&t=459s](https://www.youtube.com/watch?v=f5Yj1fcce7o&t=459s),buildapc,2025-11-15 15:32:13,0
AMD,nozotyj,Is there no diffrence between them?  And thank you!,buildapc,2025-11-15 15:34:38,2
AMD,nozp266,> and a peerless assassin :)  Phantom Spirit is its improved successor and generally costs the same these days.,buildapc,2025-11-15 15:35:51,2
AMD,nozp496,"I think i will get a repair shop to properly mount the cpu and cooler, i wouldnt trust myself😅  Thank you!",buildapc,2025-11-15 15:36:09,2
AMD,nozp7wa,"I see, happy to hear  Thank you!",buildapc,2025-11-15 15:36:42,1
AMD,nozrri8,"One of the previous commets informed me that the 9800 is better, i thought the higher the number the better it usually is😅  Thank you!",buildapc,2025-11-15 15:50:01,1
AMD,nozqd8h,"The reason i didnt go for the peerless assassin right away is because i can probably only get it through shipping and i generally prefer to buy it from a store i know, not a dealbreaker just a mild prefrence  While i know that aio's dont leak normaly it just scares me having something in my pc that if leaks will fry my build, even if the chances are very low, a bit of paranoia i guess you could say😅  I will check the video out, thank you!",buildapc,2025-11-15 15:42:44,1
AMD,nozq2sa,the 9900x3d is worse lmao,buildapc,2025-11-15 15:41:14,1
AMD,nozq19d,oh kewl,buildapc,2025-11-15 15:41:01,1
AMD,nozpzmm,"You can do that but coolers such as the Phantom Spirit are so easy to install. It’s not like 20+ years ago where they are clipped in and you risk cracking the die. But, since you seem to be getting a whole setup (new CPU means new mobo and in your case new ram), may as well get the shop to do it all since you don’t have experience.",buildapc,2025-11-15 15:40:46,1
AMD,nozqpte,You're welcome,buildapc,2025-11-15 15:44:33,1
AMD,nozs5f1,"It depends, for gaming in this case it's actually worse.  For some professional applications, it often is.  If this is a gaming build, 9800X3D is the obvious choice.",buildapc,2025-11-15 15:52:01,1
AMD,np1yuzq,"The way the cores are placed on the chiplets the 12 core is worse because you will either park half of the cores, turning it into a 6 core cpu or you will have cross talk between the caches on different dies, which just kills performance. On productivity applications this doesn't really happen since the code is very deterministic, but games with user input are not. So any advantage the additional cores will give you will be negated by the interconnect. So single ccd 8 core is much better. Especially with that 3dv cache which games just love.",buildapc,2025-11-15 22:55:44,1
AMD,nozqnoq,"Oh lol, what a unique way of naming products",buildapc,2025-11-15 15:44:15,2
AMD,nozr9zz,"Yeah, if i go through with this im probably swapping almost everything in my pc besides my gpu and power supply, seems best to leave it to someone who knows what theyre doing😅",buildapc,2025-11-15 15:47:29,1
AMD,nozte9b,"Its isn't worse in general but worse for gaming.  9800x3d -> 8 cores with the 3d cache  9900x3d -> 12 cores total, split into 2 groups of 6 each. Only one group has the 3d cache, so its worse for gaming.   9950x3d -> 16 cores total, 2 groups of 8 each, one group with the 3d cache. So same for gaming as the 9800x3d with extra cores for productivity",buildapc,2025-11-15 15:58:28,7
AMD,nozufhv,9950x3d is 200 eir more at leasy where i live,buildapc,2025-11-15 16:03:51,2
AMD,np00gbc,Yeah but it’s definitely more worth it over the 9800x3D if you’re working AND gaming,buildapc,2025-11-15 16:35:46,1
AMD,np024si,op didnt specify so i didnt recommend,buildapc,2025-11-15 16:44:32,2
AMD,npblmpp,"yeah 9060. ram seems like it might be problematic for a longer time, so i'd grab the gpu before its price starts going up (this is why i bought recently). and i guess i wouldn't buy ram now.",buildapc,2025-11-17 14:36:12,2
AMD,npbnoof,"Yeah, after I typed it all out I was thinking 9060 XT 16GB and grab another 16GB RAM before the prices get even more stupid",buildapc,2025-11-17 14:47:33,1
AMD,np60tfa,"Those are pretty GPU intensive games, I'd probably go a bit cheaper on the motherboard (B850) and CPU (7600/9600) to try and push the GPU to a RX 9070 or RTX 5070, or even 9070 XT / 5070 Ti if in budget.  Something like this: https://ca.pcpartpicker.com/list/2WpKYd  edit: Looking at the prices of all the cards, probably just consider RTX 5070 as the one tier up or the 9070 XT as the two tiers up. 9070 and 5070 Ti seem overpriced in Canada in comparison to the other cards.",buildapc,2025-11-16 16:36:32,2
AMD,npegxiu,"Thank you so much for the advice, u/SaggitaryX ! I will definitely give those other GPUs a look. :) Thanks again!",buildapc,2025-11-17 23:18:44,1
AMD,npcy09y,no. Which country are you from? And what's your budget?,buildapc,2025-11-17 18:37:04,1
AMD,npd1a6w,"What r u talking about, this would destroy r6.",buildapc,2025-11-17 18:52:56,1
AMD,npd2jii,Uk and my budget 1.5k,buildapc,2025-11-17 18:59:01,1
AMD,npd1vt9,"It's not about if it can run or not, the build is not optimal, that RAM will wreck the performance, 15-20% performance drop just because of bad RAM selection.",buildapc,2025-11-17 18:55:49,1
AMD,npd5naa,"Here you go:  [PCPartPicker Part List](https://uk.pcpartpicker.com/list/kk4Zt3)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core OEM/Tray Processor](https://uk.pcpartpicker.com/product/CzZWGX/amd-ryzen-7-7800x3d-42-ghz-8-core-oemtray-processor-100-000000910) | £313.00 @ MoreCoCo  **CPU Cooler** | [Thermalright Assassin X 120 R Digital ARGB 70.84 CFM CPU Cooler](https://uk.pcpartpicker.com/product/tWbypg/thermalright-assassin-x-120-r-digital-argb-7084-cfm-cpu-cooler-assassin-x-120-r-digital-argb-black) | £14.99 @ Overclockers.co.uk  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | £126.99 @ Box Limited  **Memory** | [TEAMGROUP T-Force Delta RGB 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://uk.pcpartpicker.com/product/2JLFf7/teamgroup-t-force-delta-rgb-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-ff3d532g6000hc30dc01) | £126.39 @ Clove Technology  **Storage** | [MSI SPATIUM M461 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://uk.pcpartpicker.com/product/9vfxFT/msi-spatium-m461-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-spatium-m461-2tb) | £95.99 @ Amazon UK  **Video Card** | [Sapphire PULSE Radeon RX 9070 XT 16 GB Video Card](https://uk.pcpartpicker.com/product/Bvjv6h/sapphire-pulse-radeon-rx-9070-xt-16-gb-video-card-11348-03-20g) | £559.99 @ Amazon UK  **Case** | [Montech AIR 903 MAX ATX Mid Tower Case](https://uk.pcpartpicker.com/product/2MwmP6/montech-air-903-max-atx-mid-tower-case-air-903-max-b) | £54.98 @ Scan  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://uk.pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | £74.99 @ Scan  **Operating System** | [Microsoft Windows 11 Home Retail - Download 64-bit](https://uk.pcpartpicker.com/product/7ZpzK8/microsoft-windows-11-home-retail-download-64-bit-kw9-00664) | £110.51 @ Senetic   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£1477.83**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-17 19:14 GMT+0000 |",buildapc,2025-11-17 19:14:21,1
AMD,npd2af7,800mhz lower than ideal ram causes 15% performance loss?,buildapc,2025-11-17 18:57:47,1
AMD,npdblr6,I appreciate that mate but I’m only looking for a pre built. Is there any chance you could look on overclcokers for me and tell me what’s the best pre built on there for around 1.5,buildapc,2025-11-17 19:44:12,1
AMD,npd4hxp,"yeah, it's not the frequency itself, the CAS latency is the issue, AMD processor perform significantly worse in higher CAS latency RAM. 5200 CL40 or CL46 could make it similar to a 5000 series processor in gaming. Intel doesn't have this issue.",buildapc,2025-11-17 19:08:39,1
AMD,npdhaeq,Get this: https://www.overclockers.co.uk/cart/restore/71f0e2e624e10da65d8cac5582ee6b15,buildapc,2025-11-17 20:12:44,1
AMD,npd5ayc,It's the infinity fabric tech with the amd processors right? I didn't think it was that much of a loss.,buildapc,2025-11-17 19:12:38,1
AMD,npd7kpw,"Not sure, I think it's something in the Ryzen architecture itself. I learnt about it from this video:  https://www.youtube.com/watch?v=MOatIQuQo3s",buildapc,2025-11-17 19:24:02,1
AMD,npcbwaq,Thermalright Phantom Spirit is the most popular cooler in the sub and fine for the CPU.  They also have the Royal Knight that's offset if you want better visibility of your RGB RAM.,buildapc,2025-11-17 16:49:36,3
AMD,npcbhcj,thermalright ps120,buildapc,2025-11-17 16:47:36,2
AMD,npccicr,"Because of the way the 7800X3D is constructed, it really doesn't benefit much from going overboard with a CPU cooler. A single tower unit is all you really need (Thermalright Burst Assassin is a good example).   But considering the price difference between a good single-tower and a good dual-tower, there's really not much reason to NOT get something like a Thermalright Peerless Assassin/Phantom Spirit/Royal Knight 120 unless you're severely space limited.",buildapc,2025-11-17 16:52:36,2
AMD,npcdh05,"[PCPartPicker Part List](https://uk.pcpartpicker.com/list/zNpQt3)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core OEM/Tray Processor](https://uk.pcpartpicker.com/product/CzZWGX/amd-ryzen-7-7800x3d-42-ghz-8-core-oemtray-processor-100-000000910) | £313.00 @ MoreCoCo  **CPU Cooler** | [Thermalright Assassin X 120 R Digital ARGB 70.84 CFM CPU Cooler](https://uk.pcpartpicker.com/product/tWbypg/thermalright-assassin-x-120-r-digital-argb-7084-cfm-cpu-cooler-assassin-x-120-r-digital-argb-black) | £14.99 @ Overclockers.co.uk  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | £126.99 @ Box Limited  **Memory** | [TEAMGROUP T-Force Delta RGB 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://uk.pcpartpicker.com/product/2JLFf7/teamgroup-t-force-delta-rgb-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-ff3d532g6000hc30dc01) | £126.39 @ Clove Technology  **Storage** | [MSI SPATIUM M461 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://uk.pcpartpicker.com/product/9vfxFT/msi-spatium-m461-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-spatium-m461-2tb) | £95.99 @ Amazon UK  **Video Card** | [Sapphire PULSE Radeon RX 9070 XT 16 GB Video Card](https://uk.pcpartpicker.com/product/Bvjv6h/sapphire-pulse-radeon-rx-9070-xt-16-gb-video-card-11348-03-20g) | £559.99 @ Amazon UK  **Case** | [Montech AIR 903 MAX ATX Mid Tower Case](https://uk.pcpartpicker.com/product/2MwmP6/montech-air-903-max-atx-mid-tower-case-air-903-max-b) | £54.98 @ Scan  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://uk.pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | £74.99 @ Scan   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£1367.32**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-17 16:57 GMT+0000 |",buildapc,2025-11-17 16:57:18,2
AMD,npci2ij,WTH is the ram problem mostly a US problem. You forgot the case.  Cooler I would go with a dual air tower of some form thermalright peerless assassin or phantom spirit dual tower with 2 fans is what I would go with. I see a lot of people going thermalright royal knight that works as well. Even if the x3d cpu are efficient and don’t get  hot there isn’t any wrong with being a little overkill.,buildapc,2025-11-17 17:20:04,2
AMD,npd26o3,"I have a very similar setup, have paired 7800x3D with Noctua NH D15. It's installed in Lian Li Lancool 216, works like a charm.",buildapc,2025-11-17 18:57:17,1
AMD,npe1h11,"The Spatium M461 uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN5000, TeamGroup G50, Patriot P400 Lite, or Klevv CRAS C910.",buildapc,2025-11-17 21:54:36,1
AMD,npcd0j8,Noctua  Buy once cry once,buildapc,2025-11-17 16:55:05,1
AMD,npcd7m0,I honestly don't care about the RGB at all I just got it because it was cheap (managed to get a good deal). Thanks for the advice!,buildapc,2025-11-17 16:56:02,1
AMD,npcdmbg,Thank you! I was considering the Thermalright Peerless Assassin 140 because it's not all too expensive so it's good to know it'd work!,buildapc,2025-11-17 16:58:02,1
AMD,npchwxf,Would a thermalight 120 be enough or do you think it'd be worthwhile for me to splash out on a 140?,buildapc,2025-11-17 17:19:18,1
AMD,npcios1,"We've got the ram problem in the UK too, it was about £160/70 a week ago, I just got a particularly good deal for mine (£117).  Thanks for the cooler recomendations! I'm still deciding on a case to get and whether to go really budget or get something cool like the silverstone flp02.",buildapc,2025-11-17 17:23:07,2
AMD,npcgp01,"Meh. Thermalright has shown so much at this point that they can produce a cooler that works damn near as well as a Noctua NH-D15 at like 1/3 the price that it makes the Noctua a hard sell.   And then fans of Noctua will usually be like, ""Yeah but Noctua sends out mount kits for free!!"". While this is true, you have to pay for shipping - so it isn't quite ""free"". Additionally, the whole ""free mount kit"" is something that a lot of cooler manufacturers have picked up on and are also doing now. I don't know that Thermalright is doing it, but I don't know that they aren't either.   Beyond this, if you pay $130 for a Noctua NH-D15 (the cheapest variant currently), you can buy a Thermalright Dual tower 3-4 times. Or a Dual Tower, a 360mm AIO, and another dual tower.   It's just really hard to justify Noctua's pricing anymore. I'll concede that they are ""the best"", but when you can get 95% of the performance for 25% of the cost, it's a hard argument to make.",buildapc,2025-11-17 17:13:17,3
AMD,npclj4w,It's sufficient. 7800X3D is a very low power CPU.,buildapc,2025-11-17 17:37:14,1
AMD,npcmtnd,Yeah but that is still a lot better than the US it was at like $130 which is £98.68 and has climbed to $220 which £160-£170 plus. I think ram prices are going down in the US anyway time soon😭😭. I’m glad you got good deals and great pc by the way happy building when you get everything together.,buildapc,2025-11-17 17:43:41,1
AMD,npf1fl8,I’ve been doing this a long time and I’ve never wished for less cooling. I went Thermalright for a decade  Noctua is buy once cry once.   AIO’s are trash with limited lifespans. I built a custom loop 15 years ago and will never do watercooling again; ugh!,buildapc,2025-11-18 01:18:58,1
AMD,np9qb2t,"5080 can make sense in certain workloads, but usually the 5070 ti is like 90% there for 75% of the price.  As for the rest of your part selection... I think you're paying a few hundred on aesthetics... Like a good 1/3 of the price is just aesthetics.",buildapc,2025-11-17 05:02:12,2
AMD,npa5u0i,Monitor the RAM prices and consider that in black Friday there will be lot of people purchasing so the price will raise 🙄🤔,buildapc,2025-11-17 07:21:08,1
AMD,np9uevh,"If you're going just off of compute resources, the 5070 Ti is only 83% of the way there.  They do share the same VRAM subsystem (16GB of GDDR7 w/ 256-bit bus), though.  Setting that nitpicking aside, I also think of the 5070 Ti and 5080 much like I did the 3060 Ti and 3070 from five years ago in that the higher-end model of the pair just isn't a good enough value (on a $/FPS basis) to justify its higher cost.",buildapc,2025-11-17 05:35:51,1
AMD,np57ivj,9060 XT 16GB,buildapc,2025-11-16 13:50:19,1
AMD,np6se5w,"A520 boards don't like overclocking. 5000 series X3D CPUs don't like overclocking. Match made :D  6GB video card may be an issue? Depends on what you like to play. In general, GPU would be where you probably want to pour all the extra budget you can stomach. Before the CPU even. If the total price is your max budget, maybe consider a slightly cheaper CPU and a slightly stringer GPU.  Since AM4 is now truely end of life upgradability later doesn't really matter, it'll be a full re-build for your next rig since you won't be able to get better CPUs or more memory anymore. Which should drive you in one of two directions: either go as cheap as you can while still getting what you need from the system, or max it out with a 5800X3D and a 9060XT or thereabouts (9070/9070XT would probably see some slow down from the 5800X3D).  On the mainboard: I can't find VRM specs for it on the MSI homepage. From the pictures it looks like it might be only four phases with discreet MOSFET instead of power stages. That's not a lot of power delivery, but then again  even a 5950X only pulls 105W, so that might be fine. But look up some reviews.  What is also weird in the spec, it only lists Windows 10 support, not Windows 11. Very likely this is just a case of the website not having been updated, it's not an old enough chipset to lack TPM 2.0 or Secure Boot support.",buildapc,2025-11-16 18:56:17,1
AMD,no14mjm,That 7600xt is a HUGE MISTAKE. Either 9060xt or 5070. That things a waste of money.   Get a better psu. $ 90 gets u and 850w 80+ gold fully modular    SSD get something cheaper. Plenty of 2tb gen4s around the $120 mark.,buildapc,2025-11-10 01:08:25,7
AMD,no1998t,"[PCPartPicker Part List](https://pcpartpicker.com/list/zQjKWc)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 9600X 3.9 GHz 6-Core Processor](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof) | $195.00 @ Amazon  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/MzMMnQ/thermalright-phantom-spirit-120-se-argb-6617-cfm-cpu-cooler-ps120se-argb) | $37.90 @ Amazon  **Motherboard** | [Gigabyte B850 GAMING X WIFI6E ATX AM5 Motherboard](https://pcpartpicker.com/product/9xYfrH/gigabyte-b850-gaming-x-wifi6e-atx-am5-motherboard-b850-gaming-x-wifi6e) | $149.99 @ Amazon  **Memory** | [Patriot Viper Elite 5 Ultra 32 GB (2 x 16 GB) DDR5-6000 CL28 Memory](https://pcpartpicker.com/product/9yy8TW/patriot-viper-elite-5-ultra-32-gb-2-x-16-gb-ddr5-6000-cl28-memory-veu532g6028k) | $199.99 @ Newegg  **Storage** | [Acer Predator GM7 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/4tQKHx/acer-predator-gm7-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-bl9bwwr119) | $134.99 @ Amazon  **Video Card** | [ASRock Challenger Radeon RX 9070 XT 16 GB Video Card](https://pcpartpicker.com/product/Q8sMnQ/asrock-challenger-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-cl-16g) | $599.99 @ Newegg  **Case** | [Phanteks XT PRO ULTRA ATX Mid Tower Case](https://pcpartpicker.com/product/BXtLrH/phanteks-xt-pro-ultra-atx-mid-tower-case-ph-xt523p1-dbk01) | $78.98 @ Newegg  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | Total (before mail-in rebates) | $1496.74  | Mail-in rebates | -$10.00  | **Total** | **$1486.74**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-09 20:24 EST-0500 |  stronger performance for under $1500  CPU: for gaming, this is basically the top budget choice, and it's only really under the 7800X3D (better budget X3D, but around $400), 9800X3D (king of gaming, closer to $500), and 9950X3D (king of all-around performance, would not go for that for pure gaming)  Cooler: really good value for the price, love Thermalright  Motherboard: cheap B850 board, comes with WiFi 6E, 3 M.2 slots, and everything you would need in a nice mid-range motherboard  RAM: prices are sky-high, but this is a nice kit that's actually better than the standard 6000MT/s CL30 kit (this one is at CL28)  SSD: fast Gen 4 TLC SSD, prices are slowly rising but this is still a good drive for a nice price  GPU: this is MUCH better than the 7600 XT you chose – it's current gen, rivals the 5070 Ti for a much lower price, and can crush 1440p instead of just being meh at it. you can even pair it with 4K if you'd like  Case: bigger than the X3, comes with an RGB rear exhaust fan, and is more premium  PSU: this is one part you should never skimp on – this one easily handles the parts listed above with better efficiency (and is listed as A- tier on the SPL PSU tier list, while your 500W one is either a C, D, or even an F. not sure about the exact model). always get a good PSU, or you may just end up with a fried PC (or a house that gets burned down)",buildapc,2025-11-10 01:37:03,4
AMD,no12i7k,I would be interested in this too,buildapc,2025-11-10 00:55:34,1
AMD,no1dg6f,"A b650 atx, personally I prefer the bios of asus and msi, but these are considerations of personal taste, get one with 3 or more dissipated m2 slots, the one you can find for less. Lately the Gigabyte B650 Gaming One last thing, leave the RX7600XT on the shelf, rather an RX7700XT, in any case I would get a 9060 energy, very very cheap components to avoid, the health of your PC depends vitally on the quality of energy supplied to it",buildapc,2025-11-10 02:02:23,1
AMD,no1s80k,"Looks good.  But you may want to get a better PSU. According to [this](https://dlcdnets.asus.com/pub/ASUS/Accessory/Power_Supply/Manual/RECOMMENDED_PSU_TABLE.pdf), 650W is recommended for Ryzen 7 + 7600 XT.",buildapc,2025-11-10 03:33:13,1
AMD,nob80uk,"[PCPartPicker Part List](https://pcpartpicker.com/list/kkB3h7)  |Type|Item|Price| |:-|:-|:-| |**CPU**|[AMD Ryzen 7 9700X 3.8 GHz 8-Core Processor](https://pcpartpicker.com/product/YMzXsY/amd-ryzen-7-9700x-38-ghz-8-core-processor-100-100001404wof)|$297.00 @ Amazon| |**CPU Cooler**|[Thermalright Phantom Spirit 120 SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/MzMMnQ/thermalright-phantom-spirit-120-se-argb-6617-cfm-cpu-cooler-ps120se-argb)|$37.90 @ Amazon| |**Motherboard**|[Gigabyte B850 GAMING X WIFI6E ATX AM5 Motherboard](https://pcpartpicker.com/product/9xYfrH/gigabyte-b850-gaming-x-wifi6e-atx-am5-motherboard-b850-gaming-x-wifi6e)|$149.99 @ Amazon| |**Memory**|[Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k)|$189.99 @ Newegg| |**Storage**|[Acer Predator GM7 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/4tQKHx/acer-predator-gm7-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-bl9bwwr119)|$144.99 @ Amazon| |**Video Card**|[Gigabyte GAMING OC Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/TcG2FT/gigabyte-gaming-oc-radeon-rx-9060-xt-16-gb-video-card-gv-r9060xtgaming-oc-16gd)|$389.99 @ Amazon| |**Case**|[Phanteks XT PRO ULTRA ATX Mid Tower Case](https://pcpartpicker.com/product/BXtLrH/phanteks-xt-pro-ultra-atx-mid-tower-case-ph-xt523p1-dbk01)|$78.98 @ Newegg| |**Power Supply**|[Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w)|$82.90 @ Amazon| |**Monitor**|[MSI MAG 27CQ6F 27.0"" 2560 x 1440 180 Hz Curved Monitor](https://pcpartpicker.com/product/t3Jp99/msi-mag-27cq6f-270-2560-x-1440-180-hz-curved-monitor-27cq6f)|$179.99 @ Newegg| |*Prices include shipping, taxes, rebates, and discounts*||| |Total (before mail-in rebates)|$1561.73|| |Mail-in rebates|\-$10.00|| |**Total**|**$1551.73**|| |Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-11 12:14 EST-0500|||  Should be the final build.",buildapc,2025-11-11 17:19:00,1
AMD,no17unc,Get a B850,buildapc,2025-11-10 01:28:23,0
AMD,no1rmjf,"If anything, the 5070 is a waste of money. All that performance, wasted on a measly 12 GB VRAM.  It will be dead by 2028, just like the 3070 / 3070 Ti is dead today.",buildapc,2025-11-10 03:29:19,0
AMD,no2199r,"Looks great, but I do have a few questions.  CPU: I will use it mostly for gaming, but I tend to code every once and awhile with unity/vs2022, and while I don't know a whole lot, I know more cores usually means better performance when handling many tasks which I will need. If its negligible for that then I'll probably switch to the 9600x just because it is a lot cheaper.  GPU: Is the 9070XT truly necessary? I'm looking at the ""Gigabyte GAMING OC Radeon RX 9060 XT 16 GB Video Card"" and considering its about $200 cheaper, I might want that instead. I don't need the highest of qualities (1080p is fine for me, coming from a laptop with an i3 with 2 cores and integrated uhd graphics), as long as it can run games and my other needs smoothly, i can lose out on the 1440p or 4k. All I need it to do is run Ultra 1080p on games, I'll even settle for high considering anything other than ""the lowest possible settings"" is where I'm at currently. I'd ignore it and go with my gut but after a very quick search I saw the 9070 is quite a bit faster than the 9060, but once again, if the 9060 can handle what I need it too at 1080p ultra/high, I will take it for $200 less.  Notes: ~~I see when I put all this into the PCPartPicker website that I get the message ""The Cooler Master Hyper 212 Black Edition 42 CFM CPU Cooler may require a separately available mounting adapter to fit the Gigabyte B850 GAMING X WIFI6E ATX AM5 Motherboard."" Is this anything to actually be concerned about?~~    Ignore. Accidentally scrolled up to my list and put that cooler in instead of the one you put.  Other than those three things, I thank you for putting this together for me! Especially the explanations of why the parts are chosen. If you could get back to me with the answers to my questions then I would appreciate it. Also just for clarification, these things should last right? I'd hate to cheap out on something for it to just die on me in a year or two.",buildapc,2025-11-10 04:34:48,1
AMD,no5wo16,"After a little research and looking at reviews the RAM is putting me off. On newegg it has 6 reviews and while it has 4/5 stars, 3 of those 6 reviews address it's unstable at CL28. I'm wondering if the initial RAM I put minus the RGB feature as I've seen it creates a clearance issue with the CPU cooler (Corsair Vengeance 32 GB (2 x 16 GB) DDR5-6000 CL36 Memory) would be good instead.  I also will be switching to a Ryzen 7 9700X as from what I've seen, its just better than the 7700X in basically every category. Maybe a little worse in certain games but I doubt it will be to the point where its not worth it.  Other than the RAM though, everything else seems to have amazing reviews which makes me happy.",buildapc,2025-11-10 20:10:12,1
AMD,no21qnb,"Yeah will do, I'm usually pretty good with searching a lot for parts but it seems I let the GPU slip past me in my searches for a decent CPU/motherboard. Much appreciated.",buildapc,2025-11-10 04:38:30,2
AMD,no22bq6,"Thank you for the info. I believe the website I was using listed the estimated wattage and 500W was far above the listed number, seems I should've done a bit more research.",buildapc,2025-11-10 04:42:53,1
AMD,noc5p4z,If the 9070XT goes on a good sale I'll grab that one instead since it does seem like the difference is night and day.,buildapc,2025-11-11 20:04:59,1
AMD,no2pq3c,"Dumb take. The only reason why is that the 5060ti 16gb will run out of compute well before vram is an issue.   The 3060 12 gb is MORE dead than the 3070. Pick any game that you can't run on the 3070 and you can't really run it on a 3060 either   With the popularity of the 3080,4070 and 5070 I doubt they're going to abandon 12gb cards so soon.",buildapc,2025-11-10 08:13:34,3
AMD,no24zn9,"- if you're planning to do more coding in the future, I would go for the 9700X, if not, either is fine  - the 9060 XT 16 GB can absolutely handle 1440p High/Ultra (check benchmarks on YouTube), but if you plan to do more Unity in the future (VS2022 doesn't really care), I would 100% go Nvidia for their CUDA cores (so maybe a 5070)  - yes, these parts are made to last and you can go a while without upgrading. these are all current gen meta parts",buildapc,2025-11-10 05:03:12,0
AMD,noa3lhq,"late response, but go for CL30 instead (better performance). find the cheapest 6000MT/s CL30 kit",buildapc,2025-11-11 13:48:52,1
AMD,no258st,Understood! I’ll be going to bed now but tomorrow I’ll do some more in-depth research on these parts just to make sure I’m confident in them. Thank you for your help :),buildapc,2025-11-10 05:05:12,1
AMD,noazxx3,Gonna go with the [Klevv BOLT V 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/PxTZxr/klevv-bolt-v-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-kd5agua80-60a300h). No major recurring issues I can find with it from reviews and its said to be a low profile so it shouldn't conflict with the cooler. Thank you so much for helping me with this build. :D,buildapc,2025-11-11 16:38:55,1
AMD,nob0yxo,"https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k this one or https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae this one, both are cheaper",buildapc,2025-11-11 16:43:56,1
AMD,nob8kgv,Will do. Thanks again for all the help :),buildapc,2025-11-11 17:21:43,1
AMD,nobtrkt,"if you don't own the processor yet, pay a bit more and get 7800x3d, its worth it.",buildapc,2025-11-11 19:04:46,7
AMD,nobub6w,"looks like a good combo to me.  9700x is kind of in a weird spot sometimes where it costs nearly the same as the 7800x3d while being worse and it is a more expensive than the 9600x, 7500f, 7700 which aren't much slower for gaming",buildapc,2025-11-11 19:07:28,3
AMD,nobvxfv,Yes its a good combo. Were they arguing there are better deals out there or that there's an actual problem?,buildapc,2025-11-11 19:15:31,2
AMD,nocjb1m,5070 is great. Much better than I expected. Plays all my games 1440p and 4K just fine. Would recommend it 👍,buildapc,2025-11-11 21:15:10,2
AMD,nodzm0u,Bestest of friends...,buildapc,2025-11-12 02:11:10,1
AMD,nommx35,Been rocking this combo for a couple of months. I mainly play Esports titles and can play them comfortably at 240fps. I was getting 200-240 fps during the Black Ops 7 beta as well.,buildapc,2025-11-13 13:28:24,1
AMD,nobtgs3,"Why would it be bad?  My only concern *long term* is the 5070's VRAM, it's the only card in its price bracket (or even a bit lower) that has less than 16GB VRAM. The 5070 Super that's coming out, or the 9070/XT, would be better buys - though if you can get the 5070 at a good price, you know, whatever.",buildapc,2025-11-11 19:03:17,-2
AMD,nobuzqo,Well the thing is I already bought the 9700 x as i got a pretty good black fryday deal,buildapc,2025-11-11 19:10:53,4
AMD,nobyp39,Some of them were saying that the 9700 x is a bad processor(i got a good deal on it ) and i should not waste extra money on 5070 and either buy a 5060 ti or a second hand 4070 or out right return the cpu and buy the 7800x3d or a intel ultra 7 new gen.Im not really good at this stuff i just did some research and it seemed fine.To be honest some of the guys were saying that is a perfectly good combination for the money i paid for it and i should be able to play new games on high that come out for atleast 3 years and can upgrade after if im not happy,buildapc,2025-11-11 19:29:26,1
AMD,nobz7xx,"Super is not coming anytime soon if ever due to ram shortages. I am also not sure where OP is from but where I live ti is 250€ more expensive and I am not sure if 4 extra GB, 25-30% performance uplift is worth it",buildapc,2025-11-11 19:32:06,4
AMD,nod2ov0,That's fine,buildapc,2025-11-11 22:57:28,6
AMD,nocj1ih,"5070 is way better compared to 5060 TI, your friends are wrong lol",buildapc,2025-11-11 21:13:49,5
AMD,nod8k26,"The 9700x is a perfectly fine processor, as is the 5070. Especially compared to those lower tier options you mentioned. Depending on prices the 9070/xt can be better value but it's a fine card nonetheless.",buildapc,2025-11-11 23:31:23,2
AMD,nodkpkk,"Its not a bad processor and I'd take the 5070 12gb over a 5060ti 16gb any day of the week, and used 4070 super cards go for the same price as a 5070 new.",buildapc,2025-11-12 00:42:11,2
AMD,npccyt2,"XFX or Sapphire. Preferably Sapphire. However, price matters a great deal.",buildapc,2025-11-17 16:54:51,44
AMD,npccxgu,Sapphire is top tier.,buildapc,2025-11-17 16:54:40,136
AMD,npcd6ad,"They're basically all reliable. If they weren't, AMD wouldn't sell them GPUs to put on the cards. Brands that actually produce terrible products are largely a thing of the past (thank god) as companies like AMD, Intel, and Nvidia have realized the power that they have in writing minimum quality standards to their sales contracts, and those minimum quality standards are actually quite good across the board.",buildapc,2025-11-17 16:55:52,71
AMD,npcekn3,"XFX, Sapphire, PowerColor, ASRock have all been good to me over the years.  I've used Asus and Gigabyte on the NV side and never had issues, either.",buildapc,2025-11-17 17:02:44,23
AMD,npccv87,cheapest between xfx and sapphire is usually best choice  edit: i personally recommend xfx,buildapc,2025-11-17 16:54:22,18
AMD,npcgt00,PowerColor or Sapphire are my choices because their customer support is A+.   All the cards for the 9000 series are fairly close to each other in terms of in-game performance.,buildapc,2025-11-17 17:13:50,9
AMD,npcdv0t,XFX are solid each time.,buildapc,2025-11-17 16:59:13,7
AMD,npcdvvy,As long as the brand name isnt like Aodomrm or something they are all within 3-5% of each other on their best days. Go for price value over brand loyalty. But yeah xfx or sapphire are the crowd favs.,buildapc,2025-11-17 16:59:20,6
AMD,npcluxm,"It is not just the brand that is important, it is their specific model that matters. I watched a comparison of 14 various 9070 XT models where the Sapphire Pulse performed the worst overall, but the Sapphire Pure was noticeably better, maybe in the top 5, so I ordered that one. Watch a similar video on the 9060 XT",buildapc,2025-11-17 17:38:52,6
AMD,npcqb3o,Can someone explain the difference and why they exist for the same product?,buildapc,2025-11-17 18:00:20,5
AMD,npd2b3v,"Powercolor and sapphire are the top of the line, red devil and nitro + in particular",buildapc,2025-11-17 18:57:52,6
AMD,npd95r0,Get whatever is the cheapest. They are all fine.,buildapc,2025-11-17 19:31:58,3
AMD,npec778,"Powercolor is also well known and reliable, I love my Hellhound spectral white 9070XT, its my first AMD/Powercolor GPU, no complaints, it a beatiful all white GPU  https://imgur.com/a/rU31uEu  I even really liked 9060 XT version of the Hellhound but went all out, returned 9060 and got the 9070 XT instead",buildapc,2025-11-17 22:51:42,3
AMD,npcvwzb,"Buy based on model, not on brand.  (unless said brand straight up has nothing good)",buildapc,2025-11-17 18:27:02,6
AMD,npcyj6v,I currently have a Gigabyte card and I hate it. Had to install their crappy software for the GPU to run halfway decent on Windows and then I had to RMA it in which I had to pay shipping for. Currently running it on Mint and it's better since Linux has AMD drivers baked into the kernel,buildapc,2025-11-17 18:39:35,2
AMD,npdcd97,It's a 160W GPU. Card build won't affect performance. Get the cheapest one available,buildapc,2025-11-17 19:47:59,2
AMD,npdkwnk,"Xfx, powercolor, sapphire",buildapc,2025-11-17 20:31:03,2
AMD,npcetg7,The one that fits your build the nicest 😀,buildapc,2025-11-17 17:03:56,1
AMD,npczzjx,"I ve heard good things about xfx, especially their warranty support. Sapphire, xfx and powercolor are all good tho",buildapc,2025-11-17 18:46:39,1
AMD,npdfmpf,"I had Sapphire and it was great (quality/price, cooling).",buildapc,2025-11-17 20:04:17,1
AMD,npditk0,I've got an xfx swift 9070xt and I've never seen it above 65c. Xfx has treated me well.,buildapc,2025-11-17 20:20:29,1
AMD,npdqnef,You would need to have a general understanding or have someone who does look at the pcb and components to know what model is better. Besides that there all equally good and shitty so lol,buildapc,2025-11-17 21:00:15,1
AMD,npdr7w3,"Bought my first AMD gpu recently & after a lot of research I decided on the Sapphire Nitro+ rx 9060 xt 16gb. it comes with thermal pads on both the front & rear Vram chips, has excellent cooling with low fan noise and finally there is no coil whine to speak of. which i'm quite sensitive to so its basically none existent. overall the built quality is second to none, its worth the extra premium for the cooler, quieter & higher quality GPU",buildapc,2025-11-17 21:03:08,1
AMD,npdudj8,Get the cheapest.,buildapc,2025-11-17 21:19:10,1
AMD,npduxtl,"I find the top tier xfx cards to be beautiful. My QICK 6700XT looks very sexy and has very good thermals, never went past 70C on a hot day here in the tropics while playing CP2077 on ultra 1080P",buildapc,2025-11-17 21:21:58,1
AMD,npe339s,Went with XFX for my 390 and 6700XT and I haven’t had any problems at all. Recently went Sapphire for my 9070xt and (driver issues aside) this has been good to me also,buildapc,2025-11-17 22:02:52,1
AMD,npe6wv2,Avoid PNY,buildapc,2025-11-17 22:23:02,1
AMD,npe7elh,"The top three most reliable AMD Radeon graphics card manufacturers are Sapphire, XFX, and PowerColor.  PowerColor Reaper RX 9060 XT can get very warm, but you can slightly reduce the temps by undervolting it.",buildapc,2025-11-17 22:25:37,1
AMD,npe7h9i,"Dude they all have the same core gpu and components , the difference is the cooling system and the lighting if there is any. They are all good and fine , just find the cheapest.",buildapc,2025-11-17 22:26:01,1
AMD,npeek00,Sapphire is the EVGA of AMD.  Powercolor is a close second.,buildapc,2025-11-17 23:05:02,1
AMD,npegqzi,Never buy Zotac.,buildapc,2025-11-17 23:17:41,1
AMD,npehwhb,"I know reddit hates it but I always buy Asus (for both motherboards and graphics cards.. even laptops). I've never had a single issue with any of my Asus products. Never once needed warranty or anything.   My second option would be Sapphire. They used to build AMD reference cards and are still on good terms with AMD. From my understanding, they may still be getting top priority for binning but that could just be outdated info.",buildapc,2025-11-17 23:24:22,1
AMD,npetulq,"just got the sapphire 9060xt, does all that should, love it!",buildapc,2025-11-18 00:33:56,1
AMD,npf0ajc,Sapphire and XFX are usually a safe bet,buildapc,2025-11-18 01:12:00,1
AMD,npf1d0i,"XFX or Sapphire, they always hit. At least for me they always have. Gigabyte can be good, but check reviews for specific models to be sure.",buildapc,2025-11-18 01:18:31,1
AMD,npf6glp,XFX or Sapphire. I'd lean towards Sapphire honestly. I would not recommend Gigabyte. I have had a bad experiences with their AMD GPUs,buildapc,2025-11-18 01:49:28,1
AMD,npfboeh,"https://old.reddit.com/r/buildapc/comments/1ncptcu/i_never_know_which_gpu_manufacturers_are_good/ndaydqe/   GPU, based on my experience:  BEST: Sapphire  MID: MSI, Asus, Powercolor  SHITE: XFX   Literally had an XFX card explode in my computer. It was a brand new card. Manufacturer refused to do anything about it. Took me forever to get it sorted, but yeah man I'm never forgiving them for that.",buildapc,2025-11-18 02:20:17,1
AMD,npfe6jx,In this generation they're all about the same reliability.  What matters is recent warranty handling reputation.,buildapc,2025-11-18 02:34:59,1
AMD,npfk38g,"I'd give Gigabyte a miss.  Not because they're liable to be broken or perform badly, but because Gigabyte uses a fan braking system which causes their GPU fans to audibly rattle every time they finish spinning down.  It's dumb, unnecessary, undefeatable, unique to Gigabyte, and does not speak well for the maker.",buildapc,2025-11-18 03:09:37,1
AMD,npfsw8b,"Generally Sapphire and XFX, plus Powercolor's higher tier cards.   Brands like Gigabyte and MSI rank about the same as they do in the Nvidia world",buildapc,2025-11-18 04:06:29,1
AMD,npfxtku,"The only difference between brands is warranty policies and cooler quality, maybe slightly different OC profiles from the factory. And styling, of course. In practice, functionally, there is almost no difference between any of them.",buildapc,2025-11-18 04:42:00,1
AMD,npg086u,"just from bad experiences, I avoid gigabyte. faulty mobo and my last card (6750xt) stopped working after 2 years. still under warranty but had to be sent to Taiwan. still haven't gotten it back. got a sapphire 9060xt as a replacement.",buildapc,2025-11-18 05:00:14,1
AMD,npclu8i,"Avoid all AMD GPUs, they are shit value and their price to performance is bad. AMD cards sucked so bad that their only selling point now is vram amount. Nvidia's newer cards match them now anyways so the vram point isn't even valid anymore. .However, their CPUs are the top of the line (Ryzen 9800x3d, Ryzen 9 5950x, etc).",buildapc,2025-11-17 17:38:46,-18
AMD,npcnir2,"Honestly no idea how they are now, but I had 2 XFX 7950s die on me within a couple of years back in the day. It does make for a pretty paperweight, though.",buildapc,2025-11-17 17:47:03,11
AMD,npfc8hj,"I had an XFX card literally combust and explode in my computer. Actual sparks and smoke. It was brand new and still under warranty, and XFX tried their absolute hardest to avoid honoring it. They eventually just stopped responding to me, and I needed to get my credit card company involved.  I tell this story whenever I see their brand come up, because like someone else in these comments said, I wouldn't even recommend them to someone I hated.  Also be on the lookout for suspiciously favorable comments about XFX. Over the years I've had several users get confrontational with me when I tell that story, almost as if they worked for XFX or something.",buildapc,2025-11-18 02:23:33,3
AMD,npebyxk,Wouldn't recommend xfx to my worst enemy. Had so many problems with their loud ass shit cards. There is a reason they are usually the cheapest,buildapc,2025-11-17 22:50:26,1
AMD,npcg1cb,Yeah the sapphire 7900xtx 24gb is one of their most powerful cards amd has ever produced. Absolute champ.,buildapc,2025-11-17 17:10:03,37
AMD,npdn7k0,"The only Sapphire I ever purchased was my Nitro+ 7800XT. I was able to overclock the living hell out of it while still running stable and quiet, to the point where it performs on par with a low-tier 7900XT  also it's pretty",buildapc,2025-11-17 20:42:46,7
AMD,npdhwfo,"If you do not have original receipt from an authorized seller and the GPU still have warranty remaining based on serial number, they charge money to fix the GPU.  I bought a used RX 6800 in 2022 and it died after a few months. I contact Sapphire support and was inform there was a $40 fee if you do not have original receipt.  Asus, MSI, Gigabyte on the other hand base their warranty on the serial number and doesn't require proof of purchase.",buildapc,2025-11-17 20:15:50,8
AMD,npdo7t1,I've only tried one of their cards so far (9070xt) and it runs super quiet and cool.,buildapc,2025-11-17 20:47:53,2
AMD,npexvgw,"Funny, I remember this name coming out and everyone staying away from it unless your budget was really tight. lol  XFX Mercury are a good card, the 9070XT fair very well in Hardware Unboxing shootout of about 14 cards. I know this isn't a 9060, but it is indicative of Mercurys performance. There is a bit of a premium on the Mercury series....but....  Stick to the big brands Gigabyte, MSI, XFX, ASUS, Asrock etc. Anytime I see a brand name that makes no sense in English, I shy away.",buildapc,2025-11-18 00:57:20,1
AMD,npftnyp,"agree, but I would not buy their 12vhpwr cards",buildapc,2025-11-18 04:11:48,1
AMD,npfvmsw,"I got a Sapphire RX470 on release day, it suddenly died almost a year later, I RMA'd it and they sent me back an RX580 lol. No complaints",buildapc,2025-11-18 04:25:50,1
AMD,npdwoms,"You have no idea what you're talking about, Giga Nyte is where it's at.",buildapc,2025-11-17 21:30:40,-1
AMD,npebo4h,"Reliable in the way they don't break or go up in flames, but some brands disguise their gpu as tortured vacuum cleaners. They will work, but unless you're deaf, will provide a miserable experience.",buildapc,2025-11-17 22:48:46,7
AMD,npcf4va,How was gigabyte build quality and software?,buildapc,2025-11-17 17:05:32,3
AMD,npcltet,Team ASRock all day.,buildapc,2025-11-17 17:38:39,2
AMD,npfvz1c,I still remember Powercolor's exploding/burning red devil RX480s,buildapc,2025-11-18 04:28:16,1
AMD,npcdg2p,+1 from me never had any problems with my 6600xt or 7900xt both xfx.   Cheap and reliable,buildapc,2025-11-17 16:57:11,8
AMD,npddd8t,"+1 for PowerColor.  My last few AMD cards have been PowerColor and they are all rock solid.  In fact, before PC I had a Gigabyte card and it was the last one I've had to send back for RMA service.",buildapc,2025-11-17 19:52:55,4
AMD,npfg40w,"While it was a while ago, getting a Sapphire card replaced under warranty was the absolute worst RMA experience I've ever had. I used to prefer their cards, now I avoid them.",buildapc,2025-11-18 02:46:05,1
AMD,npdc7m3,"ah yeah the old ""roll face on keyboard"" brand names on Amazon",buildapc,2025-11-17 19:47:13,5
AMD,npd27a5,"Well, customer support may matter a great deal to some people.",buildapc,2025-11-17 18:57:21,3
AMD,npe6lve,"Each AIB buys the GPU die from Big Red or Big Green at wholesale price.  They source the memory chips from Samsung or some other memory chip maker.  They design their own PCB, Cooling should and fin stack cooling system (fans, pipes) while over clocking it and sell it to you as a better alternative as the Reference cards with slightly more power of OC headroom.  They make thin margins on the card when you buy it.  It's a business",buildapc,2025-11-17 22:21:24,4
AMD,npd6ygw,What about pulse?,buildapc,2025-11-17 19:20:55,1
AMD,npe60g0,"Also, beware that ASUS AMD cards are usually using the same shroud as their Nvidia counterparts and likely have a slightly revised cold plate and fin stack..  Meaning the ASUS AMD cards are usually an afterthought and probably not worth the money over Sapphire,XFX or Powercolour which are designed specifically for that GPU die and custom PCB.",buildapc,2025-11-17 22:18:15,5
AMD,npcmmb9,"I disagree, their midrange lineup is way more price to performance then nvidia who offers you 8 gb rtx 5060 and 12 gb rtx 5070. Nvidia are more pricier, however their software is very good which does not justify their greedy pricing.",buildapc,2025-11-17 17:42:40,8
AMD,npcomf7,I'm in Canada and here at times I've seen RX 9070 XTs as much as $200 less than RTX 5070 Ti (they perform on par) so yeah big difference here.  Though with heavier sales upcoming it may only be more like a $100 difference now.  But still this really depends on region.,buildapc,2025-11-17 17:52:22,3
AMD,npdk2y7,I got an XFX Swift 9070xt for $700usd on launch when the US was facing gpu shortages. You'll still struggle to find a 5070ti for that price. Even today you'll have a very hard time beating that price with a 5070ti.  Edit: I should also add that was after taxes.,buildapc,2025-11-17 20:26:52,1
AMD,npcvnue,"I hear GCN1 graphics cards had a reputation for dying, so I wouldn't exactly pin it on XFX.",buildapc,2025-11-17 18:25:48,7
AMD,npda3ev,"Bought my XFX 9070 XT in April. No issues so far, obviously haven't reached the timeframe you mentioned though.",buildapc,2025-11-17 19:36:39,3
AMD,npco9me,Yikes.,buildapc,2025-11-17 17:50:40,2
AMD,npdm9xy,"The only GPU I've had die within a month of purchase was an XFX 5700 XT, which was reading a hotspot temp of approximately 3x the surface of the sun  Probably just down to anecdotal luck, but it soured me on the brand pretty heavily.",buildapc,2025-11-17 20:38:01,2
AMD,npfbh0g,"Careful, there's an XFX brand ambassador going around these threads telling anyone who had a bad experience with XFX that they're wrong.",buildapc,2025-11-18 02:19:05,1
AMD,npe1tyx,"I have one of these cars but I haven't messed around with overclocking it, how much extra performance were you able to squeeze out of it? I don't know much about overclocking so I'm wondering if it's worth the effort.",buildapc,2025-11-17 21:56:24,2
AMD,npfb9xb,"Tbf I think in today's digital age it'd be pretty universal to have a copy of the receipt emailed to you which ideally should last forever (or at least as long as civilization is around), so you shouldn't really run into this problem. Obviously I might be generalizing but I don't think this should be something that is a huge issue moving forward.  Then again going by serial number is also just the standard atp so...",buildapc,2025-11-18 02:17:57,2
AMD,npfkgu5,I feel targeted by this comment.  My R9 290X was probably the last AMD card I will ever buy.,buildapc,2025-11-18 03:11:56,0
AMD,npcited,"... no. That's not what I meant at all.   Back before Intel and AMD produced CPU chipsets, you had companies like SiS, ALI, VIA, and probably a few other companies making chipsets for CPUs. Shady product manufacturers could then buy them dirt cheap, put even shittier components on them, and then sell them.   And because of this, the major component vendors then take a hit because of decisions that are made of which they have *zero* control.   Case-in-point: I worked for Gateway computers circa 2001. They were really starting to make a major push on AMD CPUs as AMD was producing the Slot-1 Athlon at 700MHz for about 1/2 the price of an Intel equivalent, and it actually outperformed the P3 700MHz.   But what Gateway didn't know was that they bought motherboards from a shit company that used shit capacitors that were hit by the [capacitor plague of 2001](https://en.wikipedia.org/wiki/Capacitor_plague) and as a result motherboards failed like crazy. Gateway had THOUSANDS of returned units. I personally replaced motherboards on hundreds of these machines, and when I would be checking them in, I'd guess half or more would drop it off and be like, ""I KNEW I shouldn't have bought an AMD CPU! I should have bought Intel!!!"".   AMD didn't have shit to do with this. They sold the CPU. And it was a GREAT CPU. But they had zero control over the quality of the motherboards that were being used. Should Gateway have taken some of the blame for (likely) using the cheapest motherboards they could find? Yep. Should the company that made the motherboards? Yep. But who actually took the blame? AMD.   Intel was the first to figure out that controlling the distribution of CPU chipsets was the way to ensure a quality motherboard was used, and this resulted in shitty motherboard manufacturers collapsing. Nvidia and AMD followed suit (for motherboard chipsets and GPUs). Was it also a way to ALSO pull in some additional income? Yes. I won't deny that profit motive was definitely a factor. But for me, the enforced quality standards was something that was better for all.",buildapc,2025-11-17 17:23:44,28
AMD,npcgfeb,Here's the obligatory shout out to EVGA.,buildapc,2025-11-17 17:11:59,15
AMD,npcgo89,"Someone has been eating lemons for breakfast, lunch and dinner. Lemme guess, vinegar for desert?",buildapc,2025-11-17 17:13:11,10
AMD,npcne3v,"Software?  I've had Sapphire, Asus and Gigabyte Radeon cards and never once even thought about installing any software.  I will say build quality on Gigabyte will vary by model but if I had the choice I'd go Sapphire all day, everyday 😁",buildapc,2025-11-17 17:46:25,9
AMD,npcqbtt,Build quality was fine.  Their software sucks ass though 🤣 But so does Asus’ and ASRock’s software suites.  I never used the included software stacks for any of my cards for AMD or NV.  Adrenaline or Nvidia app/CP is all you’ll need.,buildapc,2025-11-17 18:00:25,2
AMD,npdikop,"I got the gigabyte gaming oc 9060xt last week and it’s been great so far. Just installed the drivers through AMD, had no need to install anything from gigabyte. Followed a guide for Linux drivers. Works well on windows and Mint",buildapc,2025-11-17 20:19:15,1
AMD,npf39mi,I mean... the fan control part is okay?   FanControl is better.,buildapc,2025-11-18 01:30:07,1
AMD,npeiq6u,Steel Legend ftw,buildapc,2025-11-17 23:29:16,1
AMD,npcdspa,"always the cheapest in the Uk too, Loving my 9070xt swift",buildapc,2025-11-17 16:58:55,1
AMD,npd76ip,That’s more of a budget model but it’s a good pick just not as “premium” at the nitro +,buildapc,2025-11-17 19:22:03,2
AMD,npdf9oo,"I'm pretty sure I remember that the DD cooler it had didn't have adequate VRAM cooling, and judging by the symptoms they showed, I'm fairly sure that was the issue.",buildapc,2025-11-17 20:02:27,2
AMD,npe5b4w,"Quite a lot actually, and it's pretty easy to overclock any AMD card these days through Adrenalin. Just go to Performance -> Tuning  First check that Smart Access Memory is on - if it isn't, you're already leaving a lot of performance on the table for no good reason. Google how to turn this on if you need help with it, it's fairly easy but normally requires going into BIOS.  Second, if you're trying to do the bare minimum you can literally just click on ""overclock GPU"" and let the software do its own work. This will typically be a very mild overclock, much less than you could do manually, but it's dead easy and usually very stable.  If you want to get every drop of performance out of your PC, it's still very very easy to overclock via Adrenalin. You could do it in about 30 minutes even if you have no idea what you're doing, [just follow this guide](https://www.youtube.com/watch?v=SnFmMGj9isw)",buildapc,2025-11-17 22:14:30,3
AMD,npfbxje,"It is very anti-consumer.  If you buy a used graphics card which a lot of people do, you hope the GPU doesn't die on you down the road because you are going to have to pay another fee.  With other brands, even if the GPU have a problem down the line and you bought it used, you have the assurance that it is covered by the manufacturer warranty.",buildapc,2025-11-18 02:21:46,0
AMD,npflxi3,That's a GPU from 12 years ago. The standards are significantly better today across the board for both AMD and Nvidia.,buildapc,2025-11-18 03:21:00,1
AMD,npcnhpk,What i meant was the drivers and dlss.,buildapc,2025-11-17 17:46:55,2
AMD,npd792b,Pulse vs gigabbyte gaming?,buildapc,2025-11-17 19:22:25,1
AMD,npduckm,that could be a contributing factor too.,buildapc,2025-11-17 21:19:02,1
AMD,npg27hv,Not necessarily disagreeing but as a business they're well known for the quality of their product and not their post sales support. I think we're used to companies that are huge and can afford these insanely generous policies for post sales but in reality a lot of the world lacks even a decent return policy (Asia being a prime example). It's a pretty clear example of businesses doing what's good for their bottom line (quality product and supporting the consumer that directly purchases from them) and not being incentivized to do the whole 9 yards for someone they generate zero revenue from. From a neutral perspective it's hard to blame them for acting in their own best interests. ¯\\__(ツ\)_\_/¯,buildapc,2025-11-18 05:15:37,1
AMD,npcjsp5,Well you're just a delight! I hope you're successfully able to clean out the shit from your cheerios!,buildapc,2025-11-17 17:28:35,19
AMD,npcqi5u,Drivers are the same regardless of who makes the card.  DLSS is amazing.  Love it on my 4080S.,buildapc,2025-11-17 18:01:16,4
AMD,npcqpt0,Drivers are all the same in that they don't change per brand.    You get all Geforce drivers straight from Nvidia and all Radeon drivers straight from AMD regardless of who made the card.  Brand of the card makes no difference for drivers.   Typically in the past Nvidia DLSS has been considered to be much better than AMD FSR but as of FSR4 people say it's so good now that the gap between DLSS and FSR has gotten very narrow and most people won't notice any difference in visual quality.,buildapc,2025-11-17 18:02:18,1
AMD,npd7bgk,Pulse for sure imo,buildapc,2025-11-17 19:22:45,1
AMD,npcqmr2,So i can skips gigabytes trash app and just with amd adrenaline then?,buildapc,2025-11-17 18:01:53,3
AMD,npd7foq,Thanks :),buildapc,2025-11-17 19:23:20,2
AMD,npcqwi7,Exactly:),buildapc,2025-11-17 18:03:12,3
AMD,npcqu1p,Yup.,buildapc,2025-11-17 18:02:52,0
AMD,npao3av,"Option 1 is better, but it can be hard to argue that it's $500 better unless the game you play really benefits from X3D over a 9700X.",buildapc,2025-11-17 10:32:02,16
AMD,npap558,"If you are planning on playing 4k some times or even have a 4k monitor, I would go with 9700x, but I would swap the card with a 9070x or a 5070 ti. You could also go lower with the CPU and get yourself a 9600x and invest the difference in your GPU.",buildapc,2025-11-17 10:42:41,3
AMD,npaw0km,Is there a 9600x/7600 + 9070xt/5070ti option ? Don't bother with the 8core non X3D parts imo,buildapc,2025-11-17 11:46:31,2
AMD,npapcm4,Is this a pre-built? Which country?,buildapc,2025-11-17 10:44:44,1
AMD,npbcmt5,I would recommend you wait another 2 days as proshop has cheaper parts and haven't even started their own Black week deals yet.,buildapc,2025-11-17 13:44:05,1
AMD,npbijqg,Get option 1. Imo worth it,buildapc,2025-11-17 14:18:46,1
AMD,npao8fp,"I mostly play cpu based games like, Rust, CS2, League of Legends, and most of the competitive games",buildapc,2025-11-17 10:33:31,1
AMD,nparifq,"Im planning on getting a oled display, not sure if i ill invest in the 4k monitor as i dont want to spend too much",buildapc,2025-11-17 11:05:34,2
AMD,npaxlll,No there is no option like that unfortunately. And buying all of the parts separate would be more expensive...,buildapc,2025-11-17 11:59:41,1
AMD,npards0,Komplett Epic Gaming A255 RGB is a pre built while the other one is a pack pre made package but the pc you have to build yourself.,buildapc,2025-11-17 11:04:21,1
AMD,npaofd6,"If you have a 240+ Hz monitor then the 7800X3D could make sense.   At the same time for under $400 you could buy the cheaper build, upgrade to a 7800X3D, and then on top of that you can sell the 9700X and recoup probably another $200+.",buildapc,2025-11-17 10:35:28,4
AMD,npdvc1w,IIRC Rust does benefit pretty heavily from 3D v-cache.,buildapc,2025-11-17 21:23:55,1
AMD,nparvr1,"Well you have witten yourself:  >I'm mainly using the PC for gaming (1440p and some 4K)  and also remember, upgrading the monitor has the most noticeable change in your system.",buildapc,2025-11-17 11:09:06,1
AMD,npb17ae,"An almost identical to Option 2 exist on komplett, similar black week prize, but you can edit and customize the parts there. There you can swap 9700X for 7600 or 9600x, which makes the prebuilt cheaper by nearly 1200Nok:  [Komplett-PC Epic Gaming a215 RGB](https://www.komplett.no/product/1321655/gaming/gaming-pc/gaming-pc-stasjonaer/komplett-pc-epic-gaming-a215-rgb?queryid=b9b614a2ef96f6a62d4cc43aaee79de5&sort=None)",buildapc,2025-11-17 12:28:14,1
AMD,npb5q5v,"Do this instead: [PCPartPicker Part List](https://se.pcpartpicker.com/list/VKhFzP)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core OEM/Tray Processor](https://se.pcpartpicker.com/product/CzZWGX/amd-ryzen-7-7800x3d-42-ghz-8-core-oemtray-processor-100-000000910) | kr3889.00 @ Proshop  **CPU Cooler** | [Thermalright Assassin Spirit 120 EVO 68.9 CFM CPU Cooler](https://se.pcpartpicker.com/product/zrP8TW/thermalright-assassin-spirit-120-evo-689-cfm-cpu-cooler-as120-evo) | kr299.00 @ Webhallen  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://se.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | kr1515.00 @ Amazon Sweden  **Memory** | [TEAMGROUP T-Force Delta RGB 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://se.pcpartpicker.com/product/2JLFf7/teamgroup-t-force-delta-rgb-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-ff3d532g6000hc30dc01) | kr1754.38 @ Amazon Sweden  **Storage** | [Kingston Fury Renegade 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://se.pcpartpicker.com/product/FyZ9TW/kingston-fury-renegade-2-tb-m2-2280-nvme-solid-state-drive-sfyrd2000g) | kr1699.00 @ Webhallen  **Video Card** | [Gigabyte GAMING OC Radeon RX 9070 XT 16 GB Video Card](https://se.pcpartpicker.com/product/h7RnTW/gigabyte-gaming-oc-radeon-rx-9070-xt-16-gb-video-card-gv-r9070xtgaming-oc-16gd) | kr6990.00 @ Webhallen  **Case** | [Montech AIR 903 MAX ATX Mid Tower Case](https://se.pcpartpicker.com/product/2MwmP6/montech-air-903-max-atx-mid-tower-case-air-903-max-b) | kr775.57 @ Amazon Sweden  **Power Supply** | [Enermax REVOLUTION III 750 W 80+ Gold Certified Fully Modular ATX Power Supply](https://se.pcpartpicker.com/product/WcFCmG/enermax-revolution-iii-750-w-80-gold-certified-fully-modular-atx-power-supply-erv750g-ahg-mac) | kr925.00 @ Amazon Sweden   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **kr17846.95**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-17 14:00 CET+0100 |",buildapc,2025-11-17 13:00:44,2
AMD,npaowrb,"Thats very good advice, i was just think which one do you think is more future proof. I dont really ever upgrade my pc i just sell it and buy a new one. I dont know if that changes anything",buildapc,2025-11-17 10:40:23,2
AMD,npask52,"Yeah thats true, i edited it now. So upgrading the monitor would be most noticeable? I have now a 144 hz 1080p 1ms TN Acer Predator display. thinking of upgrading to a OLED display with 240hz  0.03 ms. Or something similar",buildapc,2025-11-17 11:15:27,1
AMD,npb38vt,Im more wondering if the 9070xt and 7800x3d is worth to pay 5500 NOK more than for the 5070 and 9700x. Like is the performance bump worth the money or would i not really see a big difference anyways,buildapc,2025-11-17 12:43:38,2
AMD,npcdpk5,Its 25% tax if he lives in Norway and orders from Sweden,buildapc,2025-11-17 16:58:29,1
AMD,npbh25a,"Upgrading to OLED would be a huge and very satisfying upgrade compared to GPU/CPU. Even if you do not change any component in your system but monitor. I am not sure if your current monitor supports VRR but if it does't, you will be very happy how VRR makes your gameplay smooth and of course nothing beats the color and quailty of a good QD OLED",buildapc,2025-11-17 14:10:06,1
AMD,npblydp,In cpu limited games (only a few out there) the 7800x3d will shine. But in most games there will not be a cpu difference. A 9070xt is about the speed of a 5070ti around 10ish % faster than a normal 5070.  It seems like a lot more to pay for 10% difference BUT other small differences like memory speed or hard drive speed could make it 20-30% difference. Hard to say.,buildapc,2025-11-17 14:38:02,1
AMD,npbzz7y,Why are you considering the 9700x over the 7600 or 9600x for over 1200nok more ?,buildapc,2025-11-17 15:51:04,1
AMD,npce9qb,"But OP hasn't mentioned which country they're from, I assumed they're from Sweden by searching the prebuilt availability.",buildapc,2025-11-17 17:01:13,1
AMD,npbh5sy,"My Advice to you, do not overspend but also do not cheap out on the monitor",buildapc,2025-11-17 14:10:41,1
AMD,npcgej6,"Its just because the package is already pre sold with that, i cant change the components :(",buildapc,2025-11-17 17:11:51,1
AMD,npcg7vs,"Yeah i should have specified im from norway, there really arent any good build option that are cheaper than the ones prebuilt here",buildapc,2025-11-17 17:10:56,1
AMD,npdm0xo,https://www.reddit.com/r/buildapc/s/uzTlsX7tay   You literally can,buildapc,2025-11-17 20:36:46,1
AMD,npcvmpv,"Closet I can come is this:  [PCPartPicker Part List](https://no.pcpartpicker.com/list/j3sVJn)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core OEM/Tray Processor](https://no.pcpartpicker.com/product/CzZWGX/amd-ryzen-7-7800x3d-42-ghz-8-core-oemtray-processor-100-000000910) | kr4089.00 @ Proshop  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://no.pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | kr599.00 @ Komplett  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://no.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | kr1767.00 @ Proshop  **Memory** | [G.Skill Flare X5 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://no.pcpartpicker.com/product/LBstt6/gskill-flare-x5-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-f5-6000j3038f16gx2-fx5) | kr2790.00 @ Komplett  **Storage** | [KIOXIA EXCERIA PRO 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://no.pcpartpicker.com/product/7GC48d/kioxia-exceria-pro-2-tb-m2-2280-nvme-solid-state-drive-lse10z002tg8) | kr1890.00 @ Proshop  **Video Card** | [PowerColor Reaper Radeon RX 9070 XT 16 GB Video Card](https://no.pcpartpicker.com/product/8ZJBD3/powercolor-reaper-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-16g-a) | kr7990.00 @ Proshop  **Case** | [Montech X5 ATX Mid Tower Case](https://no.pcpartpicker.com/product/q88Pxr/montech-x5-atx-mid-tower-case-x5-b) | kr914.00 @ Proshop  **Power Supply** | [Enermax REVOLUTION III 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://no.pcpartpicker.com/product/6QgZxr/enermax-revolution-iii-850-w-80-gold-certified-fully-modular-atx-power-supply-erv850g-ahg-mac) | kr1202.00 @ Proshop   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **kr21241.00**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-17 19:25 CET+0100 |",buildapc,2025-11-17 18:25:39,1
AMD,nn7ja3o,That's an issue with Asrock motherboards.   Do some research to see if the specific programs you use use more than 20 threads/10 cores.,buildapc,2025-11-05 09:07:32,4
AMD,nn7j91i,Where did you hear those issues on ryzen 9 processors wtf. Are you confusing it with the 13th and 14th gen i9's running at too high voltages and killing themselves.,buildapc,2025-11-05 09:07:14,3
AMD,nn7jadh,"there isn't a melting issue like that. any of those cpus will be infinitely better than your current one. in fact, if you have managed to survive with the old one, i don't think you *necessarily* need to chase the high end.",buildapc,2025-11-05 09:07:37,1
AMD,nn7t2it,9950x honestly If u are looking for no compromise great gaming and work experience then the top end 16 core beast is the best answer.,buildapc,2025-11-05 10:44:12,1
AMD,nn7jegi,And that was with zen 5 x3d processors not ryzen 9 processors.,buildapc,2025-11-05 09:08:48,1
AMD,nn7jj5p,I wouldn't trust Asrock motherboards still.,buildapc,2025-11-05 09:10:10,3
AMD,nn7kwn5,"Zen 5 x3d and ryzen 9 are not mutually exclusive, because Ryzen 9900x3d and 9950x3d are both.  Most burn cases happen to be with the Ryzen 7 x3d CPUs, but those are probably tens of times more popular (than Ryzen 9 x3d)",buildapc,2025-11-05 09:24:16,2
AMD,nn7k6vm,"Yea, but mainly due to upgradability concerns with future x3d's. But also it could kill non x3d cpus as well LMAO.  https://www.reddit.com/r/Amd/s/wkqrJdd1Hs",buildapc,2025-11-05 09:16:52,1
AMD,nn7l0dd,I did not imply otherwise. I simply stated that it is not the ryzen 9 part that is the problem but the x3d part.,buildapc,2025-11-05 09:25:20,2
AMD,nn7lb6t,"Well, who knows, neither ASRock nor AMD have given us a good answer why especially ASRock motherboards seem to kill an abnormal amount of amd processors, most of which are the x3d parts (though ""regular"" ones have also died)...",buildapc,2025-11-05 09:28:22,3
AMD,nn7lgoe,IIRC Asus motherboard once had this issue but it was fixed pretty fast.,buildapc,2025-11-05 09:29:56,1
AMD,notcv5g,"https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html  the 9070xt is $600 now at times and makes much more sense than the 7900xtx as it's newer, has much better RT, and has access to FSR4 etc, while being only marginally slower in raster",buildapc,2025-11-14 14:40:51,1
AMD,notd7i6,"I'd get the 9070XT instead if you are looking for an upgrade. They are pretty close in performance, and a 9070XT is cheaper, more efficient and should be fine at that res until the next gen 80 series cards are released.  You can expect around a 30% increase, seems like a steep price to pay for that amount. It's lucky you hate money. :P",buildapc,2025-11-14 14:42:43,1
AMD,notdokp,"Not worried about FSR, only thing that catches my eye is the RT performance.",buildapc,2025-11-14 14:45:16,1
AMD,notdtmr,Well ital be \~40% off after selling the 6900xt.,buildapc,2025-11-14 14:46:01,1
AMD,np0hpz6,Way faster cores plus 2 extra cores (4 threads)  You're probably going to need a better CPU cooler,buildapc,2025-11-15 18:06:07,45
AMD,np0gs1u,Hell yeah  Signed: 2600 owner. It's a piece of shite (today).,buildapc,2025-11-15 18:01:20,12
AMD,np0f1xe,"[It's game dependent](https://www.youtube.com/watch?v=obz7JCIhe3o).  You'll probably get 20% more FPS.  But as games get more CPU dependent in the future you'll see a bigger gain.  I recommend it, even if it gets you another year that's not bad.",buildapc,2025-11-15 17:52:33,9
AMD,np0ee5u,yesssssss,buildapc,2025-11-15 17:49:11,5
AMD,np0fpmq,Even a 5600x would be a worthwhile update.,buildapc,2025-11-15 17:55:55,4
AMD,np0hbwp,"I upgraded from a 2700 to a 5800x3d for €200 a couple of months ago, I am very pleased with the upgrade",buildapc,2025-11-15 18:04:08,5
AMD,np0llxc,Went from 2600 to 5800xt few weeks ago.  Wanted to pro long my AM4 Platform for a few more years.  Its worth it if you don't want to spend money on new am5 parts.  Update your bios and you're good to go,buildapc,2025-11-15 18:25:56,3
AMD,np138of,Get a 5700x to maximize value,buildapc,2025-11-15 19:57:47,3
AMD,np0f2ew,Absolutely Yes.  Either 5800Or 5800xt whichever is cheaper.,buildapc,2025-11-15 17:52:37,3
AMD,np0kv52,"Yes, I upgraded from a 1600x to 5600x about a year and a half ago, best upgrade for my more CPU heavy games.",buildapc,2025-11-15 18:22:09,2
AMD,np0n4w6,It will be an absolutely killer upgrade. We're talking +50% improvement in single threaded performance and 8C/16T total. Should feel like a totally new computer.,buildapc,2025-11-15 18:33:34,2
AMD,np0uaq1,Yes,buildapc,2025-11-15 19:09:41,2
AMD,np12e5v,"I don't know about your market, but where I live, 5700X3D and 5800X3D prices skyrocketed on the used market since AMD stopped making them. It's still a meaningful upgrade for a lot of AM4 users, especially from a Ryzen 2000 series, but I kind of would't want sinking in that kind of money into a dead platform.",buildapc,2025-11-15 19:53:08,2
AMD,np1j4o6,"Dude, that upgrade is likely a 20-40% performance uplift in everything. Plus, with DDR5 being what it is right now, staying on your existing platform isn't the worst idea.",buildapc,2025-11-15 21:26:20,2
AMD,np1jt9z,Short answer: yes.,buildapc,2025-11-15 21:30:05,2
AMD,np1pjmq,"As others have said, absolutely yes. It’s still a very solid cpu that can still be reasonably paired with solid modern mid range options. It also offers A LOT for the money for am4 users.  It’s a solid cpu up to the 9070 or 5070 level for gaming",buildapc,2025-11-15 22:01:35,2
AMD,np1rhwg,"Went from a 2600 to 5700x 2 years ago and i'm very happy with it, don't plan on upgrading anytime soon. So yes",buildapc,2025-11-15 22:12:47,2
AMD,np1vsor,"I upgraded my 2600 to 5600. Big difference. x3d would be massive, but 5800x is great for your case.",buildapc,2025-11-15 22:37:49,2
AMD,np2be6k,"The 3000 series (Zen 2) is when AMD first truly competed with Intel, and in some use cases surpassed them; meaning the 2600 (Zen 1+) is probably worse than the equivalent Intel cpu from that time.  With the 5000s series (Zen 3) AMD cemented their lead, particularly in efficiency, and the 5000 series holds up even today.  So a 5800 will be a huge improvement over 2600, even 5700 or 5600s will be well worth it!  It's a no brainer upgrade to me if you have the money",buildapc,2025-11-16 00:11:50,2
AMD,np32zbg,"I’m running that cpu with a 7900xtx, it definitely bottlenecks a bit but it’s still a beast for most titles",buildapc,2025-11-16 02:58:32,2
AMD,np1yd3s,"You may have already purchased it, but it's worth asking the question.  What are you planning to use it for?  If budget is an issue and you're not doing any tasks that Increasing your core count on your processor would make any Real difference. You could realistically just get a 5600x.  Don't get me wrong, the 5800x Is a great processor. But if your functioning on a budget, especially if you're just playing games , you really don't need it.",buildapc,2025-11-15 22:52:52,1
AMD,np47rq1,Is a used 5700x3d an option?,buildapc,2025-11-16 08:28:32,1
AMD,npdjqy4,"Yes, that’s the way to go",buildapc,2025-11-17 20:25:09,1
AMD,npe05pn,Iam in the exact same situation as you. Have you decided yet? I really need/want an X3D because they seem to help pretty much with 7d2d. I don’t see myself getting a 5800X3D because they just got so expensive so iam playing with the thought of getting a full am5 upgrade with the Black Friday sales.,buildapc,2025-11-17 21:48:04,1
AMD,np0i5ik,"i have an Thermalright Peerless Assassin 120 SE, you think its good?",buildapc,2025-11-15 18:08:18,20
AMD,np0hdwc,"yeah i made this pc back in 2018 (i think) and it waas good paired with my beloved rx580 but i started to noticed how many games are getting cpu heavy, i recently bought a rx7800xt and it was just a matter of time before i switched my cpu too, i was playing Where Winds Meet earlier and the drops where getting too noticable now :( i guess ill buy it then",buildapc,2025-11-15 18:04:25,4
AMD,np0jozd,"Probably a $10-20 difference right now, absolutely no way to not get the 5800xt.  Edit: 5800xt might have gone up a bit in price, but the 5700x is still about the same price as the 5600x, so take the extra cores.",buildapc,2025-11-15 18:16:08,4
AMD,np0hyfl,yeah i saw the x3d series are the best one for tha am4 now but they are like 400 bucks here  :(,buildapc,2025-11-15 18:07:19,5
AMD,np0ieql,"I got the 5800xt a few months back, it's pretty nice. Not sure if the prices are still in that like $145 range though. I only upgraded from the 5600 so it wasn't a super massive upgrade or anything, but I decided it would probably add a couple more years on the back end of this system.",buildapc,2025-11-15 18:09:36,0
AMD,np0nisc,"i knew i was holding back my gpu performance for too long, good to hear :)",buildapc,2025-11-15 18:35:27,1
AMD,np22io1,Mostly gaming and coding but the difference in price between the 5800x and the 5600x is only 10bucks so I thought might as well get the 5800x,buildapc,2025-11-15 23:17:49,2
AMD,np4llw8,from what ive seen mostly are 370euro and above (even used ones) the cheapest ones are from AliExpress (200euro) but dont know if they are really reliable for these kind of items,buildapc,2025-11-16 10:49:29,1
AMD,npegsh9,"I already bought it, it will come tomorrow! I don’t really want to change my other components for the am5 switch, i don’t really see a point of doing so tbh, the games i play run pretty well i wanted to get a new cpu only because i knew i was bottlenecking my PC hard with the gpu, it makes more sense to me to go am5 for a new brand PC imo",buildapc,2025-11-17 23:17:56,1
AMD,np0jihl,"Yeah, it's more than enough. I have the 5800xt and basically the cheapest single fan tower cooler I could find.",buildapc,2025-11-15 18:15:14,13
AMD,np1jvoj,I did 1600 to 5700x3d. It should last me stil am6 with a gpu upgrade as well,buildapc,2025-11-15 21:30:27,3
AMD,np0jodm,Perfect,buildapc,2025-11-15 18:16:04,2
AMD,np19ijy,"CPU bottlenecks can be frustrating, especially with newer titles   The 5800X should give you a noticeable boost, especially in CPU-heavy games. just make sure your cooling solution can handle the extra heat.",buildapc,2025-11-15 20:32:48,3
AMD,np0t3pf,"I live in the Netherlands and they were also crazy expensive but found mine on vinted for 200, maybe I got very lucky.",buildapc,2025-11-15 19:03:30,2
AMD,np24ye3,In that case I would go with it too.,buildapc,2025-11-15 23:32:34,1
AMD,np4pnfl,Damn that's way too much. Go the 5800x then,buildapc,2025-11-16 11:29:41,2
AMD,np5749p,"https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/32.html  https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/37.html  Which one is the better buy pends on the price, but in pure performance the 5070 Ti is slightly better in raster, notably better when RT is enabled",buildapc,2025-11-16 13:47:42,17
AMD,np56o4w,"\>I have heard that the 9070 XT outperforms the 5070 Ti for a smaller price  You heard incorrect. On average 5070Ti is faster. Very slightly in pure raster, notably in RT and hugely in PT.  For all intents and purposes 5070Ti is a better card. If prices are comparable or not too far apart - 5070Ti is a better pick. If 9070XT is notably cheaper and you don't need 5070Ti extra features - 9070XT is a better pick.",buildapc,2025-11-16 13:44:45,69
AMD,np6hift,"If price is the same, I’m always taking the 5070Ti.  The feature set is superior.",buildapc,2025-11-16 18:03:20,12
AMD,np6yxds,"5070 ti is superior in most aspects, the 9070 xt in price",buildapc,2025-11-16 19:28:19,7
AMD,np5tyv2,"To the casual gamer they are both pretty solid cards without DLSS or FSR upscaling enabled It's hard to tell the difference honestly they both do Amazing @ 1080p, Awesome @1440p, Decent @ 4K but the 5070TI wins slightly due to the DLSS quality over FSR4 quality IMHO   * I say if they are both around the same price go for the 5070TI but if it's more than $75 price difference and/or MSRP price the 9070XT is the better price for performance option",buildapc,2025-11-16 16:00:45,6
AMD,np5jq0t,"It depends on the game, 9070 XT outperforms the 5070Ti in the new Call of Duty and Battlefield. That said, the 5070Ti wins in a lot of games too. You'll probably be happy with either, if they were priced the same, the 5070Ti would be the better choice.",buildapc,2025-11-16 15:05:07,8
AMD,np740dt,"I feel ye OP, in this thread, people are claiming the 5070 Ti is better, but I've constantly seen threads where people will claim the 9070 Ti has more power, but if you prefer Nvidia's technology (DLSS, etc, etc), you should go for the 5070 Ti instead (and also if the price makes sense)  Makes one wonder...",buildapc,2025-11-16 19:54:11,1
AMD,np8e0yq,"The differences other have pointed out, while true, are pretty marginal outside of raytracing.  The 5070ti is *slightly* better in performance and the 9070xt does beat it in some games.  AMD FSR 4 is better than DLSS3 but slightly worse than DLSS4 but does best dlss4 in an area or two.  @1440p FSR4 is like DLSS 3.5.  @4k FSR4 is like DLSS 3.8.  You'd be happy with either.  So while the 5070ti is slightly better, its in my subjective opinion ~$50 better, not $150.  That exact amount will vary depending on what games you play and what features you care about.",buildapc,2025-11-16 23:59:29,1
AMD,np8ivds,"9070xt’s upscaling and framegen are great when they work, but the AMD software is glitchy and difficult to manage. Yes, optiscaler exists, and that also is difficult to manage.",buildapc,2025-11-17 00:27:40,1
AMD,np99wd2,They're basically close to identical performance in raster and RT (+/- 5%) but 5070ti gets a noticeable advantage with PT. But that wasn't what tipped me to a 5070ti. The bigger issue with a 9070xt right now is worse software support. Much fewer games support FSR4 compared to DLSS3/4.,buildapc,2025-11-17 03:10:22,1
AMD,np9ionk,"If budget is not much of a constraint go for 5070Ti, its overall better card.",buildapc,2025-11-17 04:07:22,1
AMD,npdjupp,Same dilemma here too. All my life I had Nvidia GPUs and now I'm between a Nitro+ 9070XT and MSI Gaming 5070ti. Bot premium models but at about 220$ price difference. Does that 200$ makes a difference for me personally? Nope. Do I want to give Nvidia that money when they offer me 5%-10% over AMD? Nope. Dunno what to do,buildapc,2025-11-17 20:25:41,1
AMD,np6ce9q,"The Nvidia drivers have been somewhat neglected recently, which might affect stability. It's my understanding that AMD software is better by comparison.I  had to return my 5070ti because of multiple daily black screen crashes. Just my 5 cents, as it's swaying me towards AMD for my next card.  Plus burning power connectors, etc, which doesn't help with consumer confidence",buildapc,2025-11-16 17:36:56,0
AMD,np7f4fv,"5070 ti. I asked myself the same question after my 7800 xt shat the bed. As of the past few AMD driver updates, a lot of people, including me, have had *massive* stability issues and bugs. Went with exact same 5070 ti and am beyond happy with the performance so far. DLSS4 support seem to be more readily available at the moment compared to FSR4 right now if you think you need it, but that might very well change down the line.",buildapc,2025-11-16 20:50:45,0
AMD,np81ows,"With both at MSRP, I would pick the 5070 Ti over the 9070 XT.  Upscaling, ray-tracing, and frame-gen are going to be the new standards in rendering video game graphics going forward. Nvidia is superior in all 3 techniques.",buildapc,2025-11-16 22:49:04,0
AMD,np57joi,"One little caveat to the otherwise excellent points,  There is something to be said for full amd builds. All the more if you wanna avoid windows and plan running Linux distros as your main os.   Fedora (or nobara if you are into gaming) loves amd and it runs incredibly smooth for me.   Plus if you can use some of that money for other upgrades like a better ram or cpu or cooling solution you could well get more performance out of the 9070 even though as a standalone it’s still outperformed slightly by the 5070ti.",buildapc,2025-11-16 13:50:28,22
AMD,np7zegh,Didn't hardware unboxed show the 9070xt is faster in raster after driver updates over a 16 game avg?   iirc the 5070ti only bumps ahead with RT enabled.,buildapc,2025-11-16 22:36:59,4
AMD,np6s4xv,[https://www.techpowerup.com/review/battlefield-6-performance-benchmark/5.html](https://www.techpowerup.com/review/battlefield-6-performance-benchmark/5.html)  In bf6 the 5070 ti is faster and in the new arc raiders for example the 5070 ti is a whopping 30 percent faster at 4k   [https://en.gamegpu.com/mmorpg-/-%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD-%D0%B8%D0%B3%D1%80%D1%8B/Arc-Raiders-beta-test-gpu-cpu](https://en.gamegpu.com/mmorpg-/-%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD-%D0%B8%D0%B3%D1%80%D1%8B/Arc-Raiders-beta-test-gpu-cpu),buildapc,2025-11-16 18:55:03,7
AMD,np9a9fw,Black myth wukong nvidia is king,buildapc,2025-11-17 03:12:37,1
AMD,np7qua3,It’s a better card than the 5070 for sure. I’m wondering where you are seeing the 9070 xt is better than the 5070 ti?,buildapc,2025-11-16 21:51:25,1
AMD,np6of9r,Latest drivers are perfectly fine,buildapc,2025-11-16 18:37:05,5
AMD,np8lfm7,You just assume this person is the 3 of 100 not using windows?,buildapc,2025-11-17 00:42:01,2
AMD,np802yv,"They have shown it once in small sample test. No wide scale test confirms that 9070XT is faster in raster, not even their own retest later on.",buildapc,2025-11-16 22:40:30,9
AMD,np8ii8w,Lmao that’s still going around.,buildapc,2025-11-17 00:25:36,0
AMD,np9lstr,"Indeed, if that is a game you care about, Nvidia has a clear advantage.",buildapc,2025-11-17 04:29:13,1
AMD,np8f988,"Afraid that's not correct - you might want to google around, it's a relatively well known issue",buildapc,2025-11-17 00:06:56,-2
AMD,np9zcfu,I assumed nothing. Just added to the discussion of what to consider when comparing the cards.,buildapc,2025-11-17 06:19:24,6
AMD,np9m9kg,"Sadly I love my souls games. I got a 6700xt atm, but I think I may upgrade next gen. Let's hope we get some juicy options from all competitors.",buildapc,2025-11-17 04:32:07,1
AMD,np9stlo,Lmfao,buildapc,2025-11-17 05:22:22,0
AMD,np3d97s,Always go for more ram so long as it's not insanely high $ more ram will help with caching.,pcmasterrace,2025-11-16 04:06:25,2
AMD,np4dh80,"For music production it would be better to have more RAM, so at least 32GB would be recommended.",pcmasterrace,2025-11-16 09:26:18,1
AMD,np02z7l,"How much is it, btw i noticed i have the same case on my 3rd pc",pcmasterrace,2025-11-15 16:48:56,1
AMD,np0348p,https://preview.redd.it/kab717lbag1g1.jpeg?width=2252&format=pjpg&auto=webp&s=8554067ebd70d677fd1f67a0f4585f5edb852fb9  I got the same case on my 3rd pc (:,pcmasterrace,2025-11-15 16:49:40,1
AMD,np1623c,I paid 1300 for it,pcmasterrace,2025-11-15 20:13:24,1
AMD,np6r8w6,> ICE build  Crazy choice in this political climate lol,pcmasterrace,2025-11-16 18:50:43,1
AMD,np3gsu5,"definitely upgrade your gpu, but doing that you may have to upgrade your psu too",pcmasterrace,2025-11-16 04:31:52,1
AMD,np3pum3,9060xt is very popular,pcmasterrace,2025-11-16 05:42:09,1
AMD,np3u01j,9060 XT 16gb,pcmasterrace,2025-11-16 06:16:20,1
AMD,np4fq5e,>but some of the newer games I wanna play have been giving me issues.   What games are you talking about? Are they CPU or GPU heavy? What resolution do you play games at?  Also are you using any monitoring tool? MSI Afterburner or HWiNFO? Can you check if the GPU and VRAM limits are being hit?,pcmasterrace,2025-11-16 09:49:34,1
AMD,np4ur6g,"> What games are you talking about?   It's mostly games like cyberpunk, but I can get most of them running on their lowest settings. I couldn't get Deadloop to get it's VRAM usage low enough, but it still ran fine even with that.   This is also in large part me trying to prepare for the steam frame, since it seems optimized for PCVR, even though it *can* do standalone play.   I'm not using any monitors besides task manager right now, I'll take a look next time I get on the game",pcmasterrace,2025-11-16 12:16:01,2
AMD,np4wabm,"Those games should run at decent framerate on a Ryzen 5500, you have enough RAM, so the main issue should be the GPU, everything else should work well.  What is the budget you have for the upgrades? And are the games installed on a SSD or HDD?",pcmasterrace,2025-11-16 12:29:02,1
AMD,np4yhhv,"Two SSDs, a 1 & a 2 terabyte Samsung drive. Not at my computer right now to check the models, but they're both pretty modern drives. The 2 Terabyte I added on last year, it was a Christmas gift.   As for budget, it depends on the value I'm getting, but 100% below 700$, 500$ is pushing it.   The other commenters are mentioning the 9060xt, I'll take a look at that one",pcmasterrace,2025-11-16 12:46:53,1
AMD,np4z6b7,"Ahh ok, that's excellent. Any SSD, SATA or NVMe would be perfect. Some games have a hard time running from a HDD, that's why I asked, but having any SSD for games is perfect.  Do you have a max budget for the upgrade? because depending on the GPU you go with, you might have to change the 550W PSU as well.",pcmasterrace,2025-11-16 12:52:13,1
AMD,np5050l,"I'm not against upgrading the PSU since it's gonna be something I'll end up needing to do long term, and a PSU upgrade will probably last me a lot longer than any other part, but I don't wanna spend much more than $100 on a new PSU if I can get away with it. With black Friday coming up though, I'll probably be just fine. Any suggestions for PSU and GPU while I have you?",pcmasterrace,2025-11-16 12:59:36,2
AMD,np53h4o,"For less than $100 you can get the [Montech Century II 850W ATX 3.1](https://talospc.com/product/montech-century-ii-atx-3-1-850w-psu/), that is probably THE best PSU in that price range, A- Tier, so good high'ish-end PSU, and very affordable for what it is (ATX 3.1 850W). Everything else in that price range is either 650W or 750W.  A few others in the $100-120 range are the [MSI A850GL PCIE5](https://talospc.com/product/msi-mag-a850gl-pcie5-850w-psu/), [be quiet! Pure Power 12 M 850W](https://talospc.com/product/be-quiet-pure-power-12-m-atx-3-0-850w-psu/), [Corsair RM850e ATX 3.1](https://talospc.com/product/corsair-rm850e-2025-atx-3-1-850w-psu/) and [NZXT C850 Gold ATX 3.1](https://talospc.com/product/nzxt-c850-gold-atx-3-1-black-850w-psu/). On sale they should be in the $110-115 range.  For GPU, [at least something in the 5060 Ti or 9060 XT range of performance would be a really nice upgrade over a 2060](https://youtu.be/-LAH5vh-Cpg?t=507), from the [TechPowerUp relative performance charts](https://www.techpowerup.com/gpu-specs/geforce-rtx-2060.c3310) they should be a bit over 2x the performance of 2060.  You can get a [Radeon RX 9060 XT 16GB ](https://talospc.com/product-category/hardware/gpu/amd-radeon/rx-9000-series/rx-9060-xt-16gb/?orderby=price)around $350-380, and a [GeForce RTX 5060 Ti 16GB ](https://talospc.com/product-category/hardware/gpu/nvidia-geforce/rtx-5000-series/rtx-5060-ti-16gb/?orderby=price)around $420-450. Those are the first two models (new) worth looking into.  * BTW, both GPUs have a similar TDP to the RTX 2060 (160-180W), so they should work with the current PSU without issues. * So i would only upgrade PSU if you are going for something that uses more power.  If you have a higher budget, literally anything up to an $1000 5080 would work well on a 850W PSU, but a cheaper GPU + CPU upgrade would make more sense.  If you are buying a used GPU, at least 10GB would be nice, avoid the 8GB ones for AAA games, especially if you are playing at 1440p. If you are doing that, you can use the TechPowerUp charts to see what would be a nice upgrade, compared to buying new.",pcmasterrace,2025-11-16 13:23:22,1
AMD,np5cdhd,"That's really good for that PSU, the next teir is $20 more and 1050W, do I have much reason to get that much power if I never plan on getting a meta PC? Is the power consumption of modern parts trending upwards quickly enough it justify it?",pcmasterrace,2025-11-16 14:21:04,2
AMD,np5e24m,"Probably not. The 16-pin cable is limited to 600W, and you need a 5090 to draw that much  A 5060Ti is 2x the performance of a 2060, and draws about the same power.  A 9800X3D is very power efficient, way less power than something like a 9950X or 14900K, and way better gaming performance.   For gaming, you won't need that much power. For productivity, you might if you go with flagship hardware",pcmasterrace,2025-11-16 14:31:31,2
AMD,np5j04q,"productivity? I'm doing gamedev on Godot as a hobby, does that change the question?   Edit: also, I'm planning on switching from windows 11 to a Linux distro (probably SteamOS) when I make the upgrade, which may also affect the answer.   Edit 2: looking at the models, it looks like the Reaper or the Challenger are my best bet for price and performance. The Reaper is $20 cheaper than the Challenger, but the Challenger has 80 MHz more. Is .25c per MHz good enough value to take the Challenger over the reaper?",pcmasterrace,2025-11-16 15:00:56,2
AMD,nonbxoe,"Temperature issues can be fixed. If you're still happy with how it performs, theres no point in upgrading anything  What are the temperatures on the gpu?",pcmasterrace,2025-11-13 15:44:07,2
AMD,noncx9a,My 6800 was always a bit hot until I undervolted it. AncientGameplays on YouTube has plenty of guides how to undervolt AMD. If heat is the only concern I’d start there.,pcmasterrace,2025-11-13 15:48:55,1
AMD,nonie68,It was going up above 70 degrees c. Not critical but did make the rig feel hot.,pcmasterrace,2025-11-13 16:15:27,-1
AMD,nonjfy7,70 is in really good territory lol. Unless it’s ripping closer to 90°C you shouldn’t be worried about a thing.,pcmasterrace,2025-11-13 16:20:37,1
AMD,nonlc01,70 is after I’ve turned graphics down though. Dunno what they were before other than ‘ow that’s hot’,pcmasterrace,2025-11-13 16:29:53,1
AMD,nonlu45,Well don’t touch it lol. Even if it was at 80° that’s still entirely fine.,pcmasterrace,2025-11-13 16:32:22,1
AMD,nonmk9q,I mean yeah it’s not damaging but as this is only power was simulator it shouldn’t be taxing it this much,pcmasterrace,2025-11-13 16:35:55,1
AMD,nonn9ls,"That’s irrelevant, the game can still use resources whether you think so or not- you don’t have any issues",pcmasterrace,2025-11-13 16:39:22,1
AMD,nonoy76,"Good to know, I’ll put off the upgrade for now",pcmasterrace,2025-11-13 16:47:33,1
AMD,npa1fw7,I don't understand why people are still benchmarking with shadow and rise of the tomb raider... I mean shadow is 8 years old game. Rise was 2015 game..   Does it even support direct storage and rebar,pcmasterrace,2025-11-17 06:38:58,1
AMD,npbe7ts,"In my case, i choose games that i have some access (Gamepass/EA Access + Giveway from GOG, Epic, Microsoft, Ubisoft, etc) AND that have built-in benchmark tools, like Rise and Shadow, as they could run automatically with the same preset while i could do something on my phone, notebook or Nintendo Switch. So i not ""waste time"" looking at the benchmark while my computer was doing it.  I like to test older games, because i have a huge backlog of games (one of the reasons to choose the RTX 5060, as it has the performance of an 2080TI, 2080Super and 3070 that was a monsters back int time) and that, for example, made me remove Intel Arc boards from any option, as they only look for newer games. I didn't post here, but many older games i can run 4k ultra with 60fps+ with that setup, like Need For Speed Hot Pursuit Remastered, Skyrim, Metro Redux and other games. Playing 4k 60fps with everything on with that board for me is a huge win.  And, for me, that matters too as i only played until now the Tomb Raider 2013 on Xbox and want to play both games, so looking the performance in both scenarios it's important for me.  Shadow of Tomb Raider, although it's 8 years old, as you can see, with everything on the RTX 5060 struggle to run it, they have RT Shadows implemented, and they have support for FSR, DLSS and XeSS. A great choice for benchmarking what you want to check...   Same board, I went from a scenario (5600GT @ PCI-E 3.0) with sub 60fps average (55) with 95% at 41fps and 34fps min to a scenario (5700X @ PCI-E 4.0) with 78fps average with 95% at 58fps and minimun of 55.The average of the 5600GT is the minimum of the 5700X (not from the raw force but just for upgrading the PCI-E). Both 1440p native and DLSS. This leave room to run it at 4k with RT and everything on.  I put the Rise of Tomb Raider too because was installed when i benchmark my GT1030 vs the onboard Vega7 from the 5600GT, and for fun i tested it too to see the gains from the 1030 to the 5060 lol.",pcmasterrace,2025-11-17 13:53:23,1
AMD,np9vcdy,What was your budget? I'm not a fan of mid and lower asrock boards and I feel like you could do better on the GPU a bit,pcmasterrace,2025-11-17 05:43:48,1
AMD,npa8g4g,The 6600 is still pretty solid in comp titles tbh still getting 140 fps 1080p in performance on the new cod and 120 in battlefield but the biggest thing is what games does op want to play,pcmasterrace,2025-11-17 07:47:24,2
AMD,np9wae7,"I had around 4200 in cash (Brazilian currency). I bought most of it in the heat of the moment because of Black Friday, but I haven't bought the GPU yet. I'm very undecided. Do you have any recommendations? I got a bit discouraged reading some things about ""if it's your first computer, go straight to AM5"".",pcmasterrace,2025-11-17 05:51:57,1
AMD,npb9zka,"I'm not a big fan of competitive games. I think I'd enjoy playing something like Red Dead Redemption or even Age of Empires, and some RPGs too.  But for the first one, I didn't want anything grandiose. Just something that could run some things.",pcmasterrace,2025-11-17 13:28:03,2
AMD,npcezkg,You can definitely run a lot on a 6600 still you may need to fudge the card a bit on some settings for new titles but my wife had a 5600 with a 6600 and had no issues with any game until I got her an ultra wide then I upgraded her to a 7800xt,pcmasterrace,2025-11-17 17:04:47,2
AMD,npcgh54,"Thank you, I was really worried about this! Could you recommend a processor? I was thinking about a 5500X3D but I don't think my motherboard will come with the update to support it. Or can I just keep it at 5600?",pcmasterrace,2025-11-17 17:12:13,1
AMD,nnhn3q6,if u plug the output into the gpu then the pc cannot 'switch' to ur igpu,pcmasterrace,2025-11-06 21:30:02,1
AMD,nnhsv4i,then how did my gpu disable itself and was able to display graphics even tho the gpu itself was disabled?,pcmasterrace,2025-11-06 21:58:10,1
AMD,np7y920,"Basically anything should physically fit with that case design, but what PSU is in there? Do you have a budget in mind?",pcmasterrace,2025-11-16 22:30:53,1
AMD,np7z7wl,Don’t know the brand but it’s a 600 watt psu and preferably something 500$ or under,pcmasterrace,2025-11-16 22:36:02,1
AMD,np868c8,"Then the only thing that makes sense I would say is the 9060 xt 16gb. There are plenty of models around $360 or so. That prebuilt config is wild lol. They advertise a pcie 4.0 SSD, but the CPU can only do pcie 3.0, so it's running slow. This also makes the 6500 xt run [notably slower than it should because of the shitty bandwidth.](https://www.techpowerup.com/review/amd-radeon-rx-6500-xt-pci-express-scaling/28.html)",pcmasterrace,2025-11-16 23:14:28,1
AMD,np8ek25,"I’m so sorry I forgot to mention that I need hardware encoding for vr, does that have it?",pcmasterrace,2025-11-17 00:02:38,1
AMD,nns2pvc,5600x is loads faster. Skip the 3600.,pcmasterrace,2025-11-08 15:37:41,1
AMD,nns31kl,I don’t rll care about loading times or what do u mean I mean I’m on a sata ssd on an i5 6500 wich is pretty slow and it loads fast for me,pcmasterrace,2025-11-08 15:39:27,1
AMD,nns46fx,I mean it's a lot faster in general. Like 20-30%.,pcmasterrace,2025-11-08 15:45:33,1
AMD,nns4bxu,Okay but what do u say to the 3600x I rll don’t want to spend a lot of money,pcmasterrace,2025-11-08 15:46:23,1
AMD,nns5rye,"Isn't a 5600x pretty cheap, too?",pcmasterrace,2025-11-08 15:54:00,1
AMD,nns6qow,90-100€ is pretty expensive for me the 3600 is 40-50€,pcmasterrace,2025-11-08 15:59:04,1
AMD,nns9ct4,5600x is well worth the money over a 3600.    the 3600 already struggle in a lot of newer title of games. specially fast competitive shooters,pcmasterrace,2025-11-08 16:12:52,2
AMD,nns7ddy,Looks good would be better full black but looks good anyways  Enjoy,pcmasterrace,2025-11-08 16:02:23,2
AMD,nnrjfl5,The black and white look so incredible ❤️❤️   Literally edging to this with my bros,pcmasterrace,2025-11-08 13:46:41,1
AMD,np0g5zy,Woah that sloped design caught me off guard. Very nice build!,pcmasterrace,2025-11-15 17:58:13,13
AMD,np0dwsf,"That's not an EVO RGB 😅.   Good build, may it serve you well!",pcmasterrace,2025-11-15 17:46:44,9
AMD,np09ola,"I was gonna say wow who made that case it's clean AF... LL lol should have known, practically the only ones who know where to put a vertical GPU.",pcmasterrace,2025-11-15 17:24:17,5
AMD,np0h2h8,that case is the vector v100. i finished up a birthday build for a cousin using that case last night,pcmasterrace,2025-11-15 18:02:49,4
AMD,np0i3m7,EDIT: it is the Lian Li V100R case!,pcmasterrace,2025-11-15 18:08:02,4
AMD,np19t8h,I dont like rgb but this one feels right.,pcmasterrace,2025-11-15 20:34:27,2
AMD,np1kenr,Looks incredible! Love the RGB setup!,pcmasterrace,2025-11-15 21:33:19,2
AMD,np22zlx,I like your build it is very cool! :),pcmasterrace,2025-11-15 23:20:40,2
AMD,np26v2c,I strive to have such clean cable management 😭 looks great!,pcmasterrace,2025-11-15 23:44:12,2
AMD,np0ohyj,"Your rig is Star Citizen ready young space man.  Come fly with us, and your PC will really showcase what it can do.",pcmasterrace,2025-11-15 18:40:14,2
AMD,np0721o,Cool bro,pcmasterrace,2025-11-15 17:10:24,1
AMD,np0ha4t,is steel series good gpu ?,pcmasterrace,2025-11-15 18:03:53,1
AMD,np215nf,What vertical gpu kit is that?,pcmasterrace,2025-11-15 23:09:36,1
AMD,np21jgt,friends don't let friends buy lian li fans,pcmasterrace,2025-11-15 23:11:53,1
AMD,np2n5gh,Nyccc,pcmasterrace,2025-11-16 01:22:01,1
AMD,np32n2w,That’s a tidy build minimal cables showing good job it’s a good feeling when you build it and turn it on and it works lol the hardest part now if keeping it tidy I built mine a few months ago now and I never realised how much dust gets in there always trying to keep dust out of there,pcmasterrace,2025-11-16 02:56:26,1
AMD,np32ubr,That’s a tidy build minimal cables showing good job the hardest part now if keeping it tidy I built mine a few months ago now and I never realised how much dust gets in there,pcmasterrace,2025-11-16 02:57:41,1
AMD,np58jig,Woahh,pcmasterrace,2025-11-16 13:56:50,1
AMD,np0ued7,Love the little Hangyodon in there! Nice looking build.,pcmasterrace,2025-11-15 19:10:15,1
AMD,np0gcc0,Right! I liked how unique it was. And such a great price! Thanks!,pcmasterrace,2025-11-15 17:59:06,8
AMD,np2cd3w,Its the V100!  I have one and I love the power buttons on the side,pcmasterrace,2025-11-16 00:17:37,2
AMD,np2brmq,"Pedro in the wild, AGAIN!  anyways, thanks for the subreddit man, I hope I never get banned (again)",pcmasterrace,2025-11-16 00:14:04,2
AMD,np32x9x,Yes yall are right my bad!,pcmasterrace,2025-11-16 02:58:11,1
AMD,np0hyho,You’re right! I’ll edit. And that’s so nice of you!,pcmasterrace,2025-11-15 18:07:19,3
AMD,np33dls,Thank you 🤭,pcmasterrace,2025-11-16 03:01:05,2
AMD,np2f5qq,"steelseries is a peripherals company, they do not make gpus",pcmasterrace,2025-11-16 00:33:57,1
AMD,np23a4w,Coolermaster. Highly recommend at least for this case (which is the lian li v100R I made a typo). I bought and returned 2 other popular ones on the market.,pcmasterrace,2025-11-15 23:22:27,1
AMD,np2p5pk,Put a tiny Tony Hawk figurine sk8ing down that ramp.,pcmasterrace,2025-11-16 01:34:01,6
AMD,np2ujb2,coolermaster doesnt help. i need model name. i have a small case that doesnt fit any ive seen but this one looks like it might fit,pcmasterrace,2025-11-16 02:06:02,1
AMD,np33bme,Vertical GPU Card Holder Kit V3 White it’s the first hit on Amazon,pcmasterrace,2025-11-16 03:00:43,1
AMD,np3cjvj,mind attaching a bunch of photos around the bottom of it. i think i looked at that but it wouldnt work in my case,pcmasterrace,2025-11-16 04:01:28,1
AMD,np7ohsy,"That is a pretty beefy gaming machine. 7800X3D is the cpu it does almost all the tasks the computer does, 9070XT is AMD's flagship GPU, does the video related tasks, 32GB of memory is the faster side memory that gets wiped when the computer loses power (volatile memory), 2000+ GB SSD is the memory that does not get wiped after losing power.",pcmasterrace,2025-11-16 21:39:08,1
AMD,np7q3ie,Paul’s hardware  Geekawhat  Pc builder  Pc centric,pcmasterrace,2025-11-16 21:47:30,1
AMD,np71qx9,What is your budget,pcmasterrace,2025-11-16 19:42:45,1
AMD,np74vuh,im currently saving money but prob around 300 to 500 euros,pcmasterrace,2025-11-16 19:58:29,1
AMD,np76pmj,It was kinda tricky but you can upgrade your cpu to the next gen ryzen 7 5700x and then for the gpu u can upgrade to the b580 then get the exact same kit of ram and finish it off with the gigabyte p750gm power supply and then u basically have a 1080 p demolisher and a pc that can play 1440p itll cost 550 euros,pcmasterrace,2025-11-16 20:07:35,1
AMD,nofwx9n,"Will Zen 6 be the final generation for AM5? In that case, I will finally upgrade from a 5900x",AMD,2025-11-12 11:53:23,72
AMD,noiq7fh,Yay buzzwords!,AMD,2025-11-12 20:51:20,10
AMD,nog7wx8,"i was kinda dissapointed, they basically didnt give the smallest hint about next gen gaming GPUs  and Zen 6 also, they didnt tell anything basically",AMD,2025-11-12 13:12:05,34
AMD,nohu9mx,I want more raw performance i dont give a shit about ai man🤦🏽,AMD,2025-11-12 18:13:46,11
AMD,nofvvna,"Heh, I mean what else do you expect? AI generation and ray tracing is clearly the way to go",AMD,2025-11-12 11:44:58,21
AMD,nofutzc,Link doesnt work,AMD,2025-11-12 11:36:15,2
AMD,noi4sgm,Tensor cores…? On a Radeon GPU?  I think this kind of mistakes are not reasonable on specialized press.  What a POS.,AMD,2025-11-12 19:04:01,1
AMD,nokmhjb,I would honestly be happy if AMD dumped all efforts to push ray tracing and focused on GPUs that ran more efficiently.,AMD,2025-11-13 03:18:06,0
AMD,nofz1kd,Zen7 is rumoured to be on am5 and it kind of makes sense considering timelines for ddr6 ram.,AMD,2025-11-12 12:09:58,70
AMD,nog5iwh,Looking like Zen 7 will still use ddr5,AMD,2025-11-12 12:56:38,9
AMD,nogaj3k,"There’s a rumour that Zen7 might come to AM5. It’s just a rumour but it indicates that AMD is still deciding.  I think part of it comes from Zen7 & DDR6 release dates and the uncertainty of DDR6 prices and spec at launch.  DDR5 launch spec were slower than DDR4 because of similar or just a little bitter speeds but loose timings. And DDR6 might come with a similar problem as well.  I’m totally in for Zen7 being released for both AM5 & AM6, not just to make AM5 last longer but to give consumers the option, especially that going AM6 will be expensive for early adopters making Zen7 adoption slow if it was AM6 exclusive.  As for AM6, I really really hope it comes with extra PCIe lanes, like 4-8 lanes, I would hope for 12-16 but it not realistic unless they limit those 12-16 lanes to high end chipsets only (the E series) and can target workstation/server grade motherboards too. Currently AM5 high end, server & workstation grade motherboards are boring because of this, and the only way to get more PCIe lanes is to jump to Threadripper which is a very big jump considering how much more expensive it became compared to first generation Threadripper.",AMD,2025-11-12 13:28:18,5
AMD,noi3amm,"Nice! Sitting on the 5900x myself, tempted a few times but reading about the 9000 burns and stuff - no thanks I'll keep this beast.",AMD,2025-11-12 18:56:42,2
AMD,noh7bcm,I think i'm getting on Zen 6 and was about to get a x870 board in advance but it looks like Zen 6 will have a new chipset and that's probably actually going to be impactful on this processor. I won't bank on Zen 7 being on AM5.,AMD,2025-11-12 16:22:19,2
AMD,nofxatw,Most likely.,AMD,2025-11-12 11:56:24,1
AMD,nohtlik,"Considering zen 6 is most likely coming in 2026 from their roadmaps, they probably are still only in the early stages of zen 7 (at least the desktop variants which while the core dies are the same or similar, the IO die is completely different). I'd wager they have an idea of which platform they would like to stay on, but it could change within the next year or so before they send tape outs to TSMC. That being said, I think they'll stick with AM5 for zen 7 while zen 8 (or whatever they end up calling the architecture) will be the switch to DDR6. DDR6 is supposedly arriving in 2027 which doesn't leave much time for a whole new platform to be developed for 2028/2029, even if AMD started in 2026.  That's also not including potential delays for the DDR6 spec.",AMD,2025-11-12 18:10:34,1
AMD,np2ssrw,Seriously. Unless they magically show they can match what NVIDIA is doing its just that.,AMD,2025-11-16 01:55:46,2
AMD,nostdfx,"Brute forcing everything doesn't make sense. There are areas where you want to use compressions, approximations etc. Ai can help with these. The important part is to use it where it makes sense.",AMD,2025-11-14 12:45:43,11
AMD,noh5wr0,For developers who want to pay less to develop shitty games.  You can take AI frame generation and the performance hit of raytracing and shove it.,AMD,2025-11-12 16:15:28,7
AMD,np2svuj,Thats funny because that's not what this sub was saying for the last 4 years. They were saying Ray Tracing isnt worth it and a dumb endevor shilled by NVIDIA.,AMD,2025-11-16 01:56:16,1
AMD,nogg7ch,AI generation so they name them after monsters? interesting,AMD,2025-11-12 14:01:30,1
AMD,nogqoh7,"They use matrix cores, also the new hardware features were disclosed by AMD and Sony already in a joint video. They will now have hardware BVH traversal on RDNA4’s successor in the form of “radiance cores”. ML continues to improve with neural arrays. Plus there’s a new compression scheme to improve memory performance they are referring to as universal compression.",AMD,2025-11-12 14:59:42,16
AMD,nohevqo,"Pasting as both [a link](https://videocardz.com/newz/amd-lists-next-gen-ai-raytracing-for-future-radeon-gpus-confirms-ryzen-gorgon-and-zen6-medusa) and plain, unformatted text in case that makes it easier:  `https://videocardz.com/newz/amd-lists-next-gen-ai-raytracing-for-future-radeon-gpus-confirms-ryzen-gorgon-and-zen6-medusa`",AMD,2025-11-12 16:59:02,2
AMD,nofzxew,Link works fine for me.,AMD,2025-11-12 12:16:45,0
AMD,norn4tq,Nvidia did not invent tensors.,AMD,2025-11-14 06:17:33,5
AMD,noif553,Matrix-cores.,AMD,2025-11-12 19:55:09,6
AMD,nokx5md,those will be over 3k cost and req between 500 to 800 watts.  fyi you can vm multi cards in 1 card. nvidia does the same.,AMD,2025-11-13 04:29:13,5
AMD,nofzm7m,Might be they release a AM5+ with PCIe6 and more USB4. I also think AM6 will be when DDR6 is ready.,AMD,2025-11-12 12:14:21,33
AMD,nohfsey,"There's a lot of options for keeping performance on DDR5 for a while.  They could implement some kind of consumer standard for MRDIMMs.  They could add another stack of V-Cache.  Heck, we might even see some improvements via 3D DRAM if that becomes a thing in the next year or two.",AMD,2025-11-12 17:03:32,10
AMD,nog9zt8,And probably still not be able to get anywhere near the frequencies DDR5 supports,AMD,2025-11-12 13:25:02,0
AMD,nol3hlm,"Which DDR generation it supports will be dependent on the IO-die, not the CCD.  AMD could make a Zen 2 chip with DDR5 support if they wanted to, or a Zen 5 chip with DDR4.",AMD,2025-11-13 05:17:23,0
AMD,nojogw4,A rumor from an unreliable source is not indication of anything.,AMD,2025-11-12 23:55:03,5
AMD,nojpwuo,"Why not, ddr6 won't come before 2029, zen7 is 2028 most likely",AMD,2025-11-13 00:03:34,1
AMD,noxfkf8,"Yeah, AI upscaling like FSR4 and dlss is one of the big actually useful use cases for AI that I encounter in my day to day. FSR4 is good enough that I'm happy to play with 50-60% resolution in most games which is an insane performance boost. And imho it looks better than the blur from cheap AA so you get even more free performance from not needing AA and the image looks better.  And apparently there's a bunch more stuff you can upscale with ai, I saw a demo where it upscaled indidual textures and my understanding is you can do similar things with shadows and lightmaps.  I feel like 5 years from now modern rendering is going to be some crazy black magic with AI upscaling all over the place that allows you to cheat and only have to render like 20% of the stuff you'd have to do with normal rendering.",AMD,2025-11-15 04:21:59,3
AMD,nostiek,"Thats valid, they just keep pushing ai this ai that, and i have yet to see any real benefit in my opinion.",AMD,2025-11-14 12:46:39,1
AMD,noi3sod,Ty!,AMD,2025-11-12 18:59:09,1
AMD,nog4xn9,"Doesn’t work for me neither. Had issues with a different link today as well, and I never really do. Maybe it’s a Reddit app issue on mobile 🤷‍♂️",AMD,2025-11-12 12:52:34,0
AMD,noisgw3,">I also think AM6 will be when DDR6 is ready.  I agree. That'd line up with how AMD's been doing things since the AM2 days.  >Might be they release a AM5+ with PCIe6 and more USB4  Given what they've been doing since Ryzen 3000 series, i think i'd be more likely that they do it as a chipset thing, a la 500 series chipsets supporting PCIe 4.0 on AM4. You could use Zen 5000 series chips on 300 series chipsets with the appropiate BIOS version, in some cases, you'd also lose support for older parts, and the newer CPU may become limited to the PCIe version of the Motherboard (3000 or newer CPUs being limited to PCIe 3.0 on 400 or older boards)   AM5 may go a simillar route, the upgrade path is one of the things that are attractive about AMD so I don't think they'll revise the socket until DDR6 is a thing, and they move onto AM6. Before Ryzen, there was some compatibility between regular AM2 and AM3 sockets/CPUs and the + versions, but it was a bit of a mess. I don't think AMD would want to go back to that.   The 800 and 600 series chipsets are confusing enough already. B850 supports PCIe 5 while B840 doesn't and B650E does   What i could see happening if Zen 7 comes out before DDR6: 900 Series chipset with PCIe 6 and more USB 4 lanes (, as long as the CPU supports it) still on the AM5 socket.",AMD,2025-11-12 21:02:46,10
AMD,nog02at,That is a possibility yeah,AMD,2025-11-12 12:17:47,9
AMD,noi5b4d,Zen 6 already will have some memory controller changes as 2-DIMM motherboards got revisions earlier this year to be able to boot it.,AMD,2025-11-12 19:06:38,5
AMD,nohkl6c,"I can see 9000-10000 mhz ram being a sweet spot for zen7, that would probably be enough of a performance increase two gens from now.",AMD,2025-11-12 17:27:10,1
AMD,noh845y,We'll see. A lot can happen,AMD,2025-11-12 16:26:13,6
AMD,noh3c21,"Sounds fine to me. Unless AMD is showing significant issues that are a cause of ram bandwidth in relation to their counterparts, I see no reason why they need to be using max ddr5 frequencies. If they're still getting the performance, saves everybody who builds a system money not having to buy the top specs stuff",AMD,2025-11-12 16:02:53,6
AMD,np3thp2,"AM5 can already get above [10,000 MT/s](https://valid.x86.fr/7z1sg6).",AMD,2025-11-16 06:11:55,2
AMD,nojpoye,What? Zen4 and 5 can already do it...,AMD,2025-11-13 00:02:16,2
AMD,nonpb0f,"Yea no shit.  What makes Zen 7 an AM5 chip is the release date. DDR6 won't be ready until 2029. Zen 7 will be released before then, and AMD is not going to make another socket just to change it a year later",AMD,2025-11-13 16:49:19,1
AMD,nol9igt,"If multiple sources leaked the same rumour, then there's some base to this rumour, again I already mentioned it in my previous reply that it's still a rumour, but indicates that AMD is still deciding Zen7 things.",AMD,2025-11-13 06:07:48,3
AMD,nojpstt,It's not just that.,AMD,2025-11-13 00:02:54,1
AMD,nojq39o,You're woefully misinformed if you think 6000Mt/s the highest DDR5 can go,AMD,2025-11-13 00:04:39,2
AMD,noo0w0u,"Can you show me this rumor having more than one actual source.  So not just two places reporting based on the same rumor?    And even then, it's still not a sign that a rumor has any base.  Plenty of rumormongers play off each other's rumors for clout, even when there's no genuine source for the information.",AMD,2025-11-13 17:46:16,2
AMD,noo99ms,"Maybe not, but that line of thinking still really bothers me. lol  It's wild how much people dont realize how easily manipulated they are if they think some 'rumor' is indication of something being true.    We desperately need critical thinking lessons in schools growing up.  And not just one, but new and refresher lessons in every couple grades at least.  It's a skill and it needs to be reinforced and practiced.",AMD,2025-11-13 18:26:33,2
AMD,nojqca8,"You are the misinformed one, as they can do 8000 ram (or even more for apus).",AMD,2025-11-13 00:06:07,3
AMD,noomux1,"There is critical thinking behind it, no worries. Otherwise I agree.",AMD,2025-11-13 19:32:34,2
AMD,nojqh85,"If you think 8000 is the most DDR5 can do  you have no idea what you're talking about  AMD hasn't been able to deliver competitive IO since before Bulldozer, haters gonna block because they're incapable of debate",AMD,2025-11-13 00:06:55,2
AMD,nooqtp2,"Wasn't talking about you, but the original poster who said that a 'rumor' is somehow proof that AMD is deliberating on it.",AMD,2025-11-13 19:52:34,4
AMD,nojqx68,"It's in the upper range, not counting extreme OC which is still done with zen4/5 apus. So however you put it, you are the one who has no idea what he is talking about. Also I know that you are just trolling so you are not worth my time.",AMD,2025-11-13 00:09:32,4
AMD,npcrv1z,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-11-17 18:07:46,1
AMD,npct2d1,"Buy them now, I guess, before RAM starts getting hoarded like gold.",AMD,2025-11-17 18:13:28,278
AMD,npcx381,My 6800XT has lasted me 5 years already. With careful maintenance and undervolt it has to last another 5 I guess. But I'm winding down on gaming anyway since life is getting so busy.,AMD,2025-11-17 18:32:40,88
AMD,npcsqmh,Never underestimate AMD's remarkable ability to miss a perfect opportunity to gain market share and not excite the community.,AMD,2025-11-17 18:11:56,111
AMD,npcvqk7,Just as I'm looking at a 9060XT for a Steam Machine build lol. Between this and how much DDR5 shot up I'm timing this great so far.,AMD,2025-11-17 18:26:10,14
AMD,npcshbv,Looks like I ain't upgrading my PC anytime soon,AMD,2025-11-17 18:10:43,21
AMD,npd0207,"This is due to RAM prices going up. Hardware Unboxed did a video on the RAM/VRAM situation, AMD GPU’s will either get a $50 or $100 increase.",AMD,2025-11-17 18:46:59,13
AMD,npctwou,Great thing about being a gamer is you can play and get played GG,AMD,2025-11-17 18:17:29,10
AMD,npdcybm,Bought my 9060 XT 16GB in July for 336€ in Germany. Lucky me,AMD,2025-11-17 19:50:52,5
AMD,npec7m9,"Wow this hobby has been so ***fun*** these last few years.    So exciting, definitely not exhausting or disappointing.",AMD,2025-11-17 22:51:46,4
AMD,npcuyxj,"they're fucking us one way or another, the whole hardware space is a mess, console, PC, software, everything is fucked up.   I'm sticking with good and old games that I haven't played yet.   Even if I get the money to buy high end parts, this feels disrespectful.",AMD,2025-11-17 18:22:29,12
AMD,npd5prq,I'll check out black friday stuff but if not then my 5700xt will have to hold strong lol. It's struggling a bit these days but I really don't mind lowering settings to get high frame rates so whatever I guess. I'm not paying insane prices for a graphics upgrade.,AMD,2025-11-17 19:14:41,3
AMD,npd8nmm,"The way VRAM prices are going up, it's going to be true for all GPU.",AMD,2025-11-17 19:29:28,3
AMD,npdavgl,You getting a really nice package with 9000 series and prices in eu are really good at least. I recommend buying one if you were thinking about it.,AMD,2025-11-17 19:40:33,3
AMD,npdb5kc,Upgraded to 9070xt just in time from a 3080 10gb. Loving it so far and runs great on linux,AMD,2025-11-17 19:41:57,3
AMD,npdhmf6,They have tons of gpus in stock they can't even get rid of.   People are waiting for the prices to go down a bit they're close to MSRP but still not worth it yet.   Let's see what Redstone has in store before making a purchase.   Again this article comes out right before Black Friday and all the sales coming up.   How convenient!,AMD,2025-11-17 20:14:26,3
AMD,npe2nr8,MSRP = Maybe Should Raise Price,AMD,2025-11-17 22:00:37,3
AMD,npevjx2,This hobby gets more and more expensive.,AMD,2025-11-18 00:43:40,3
AMD,npcvm1c,This does not make sense at all. The 9000 series has reached msrp because the stock has become plentiful due to a  slowdown in sales. Increasing prices will slow down sales even more. If this is true then AMD needs to fire some seriously idiotic decision makers.,AMD,2025-11-17 18:25:34,11
AMD,npdboz0,"RAM has already gone to around double the price, NVME drives are up about 40% in some cases and rising. This is not unexpected. It sucks but with the RAM bump we could see it coming.",AMD,2025-11-17 19:44:39,2
AMD,npdc73i,"Well...The industry can only produce so many chips and the AI hardware manufacturers are buying up as much as they can get. The consumer electronics industry is about to get hit hard, especially in the DIY area. Glad I just built my gaming system and don't have to worry about it for a few years....",AMD,2025-11-17 19:47:08,2
AMD,npde0kr,How about they release FSR 4 on RDNA 3 first.,AMD,2025-11-17 19:56:08,2
AMD,npdhiwo,🤌thank goodness I bought my whole new setup before the spike,AMD,2025-11-17 20:13:56,2
AMD,npdarl9,very tempted to get that asrock card it mentions,AMD,2025-11-17 19:40:01,1
AMD,npdmir8,"I had a feeling this would happen. My Arc B580 (yes, I chose poorly) hasn't been working right so I ordered an RX 9060 XT in case I need to RMA it.",AMD,2025-11-17 20:39:15,1
AMD,npdmz15,Looking for a replacement for my 2080 super I thought I would still have a few years but unfortunately it starts to struggle in some games at 1440p.,AMD,2025-11-17 20:41:34,1
AMD,npdnh4f,Talk with your wallet and don't pay them.,AMD,2025-11-17 20:44:07,1
AMD,npdogv7,Built my new rig in September and I feel like I just hit the jackpot lmao. These prices are insane now.,AMD,2025-11-17 20:49:10,1
AMD,npdoqbr,"Bought the most shitty version I could: Gigabyte 9070XT Gaming OC. The shitty gel issue worries me, but I think its better to pay 616 euro rather than 700-750 euro for the better ones.  I'll put PTM 7950 from aliexpress when the time comes.",AMD,2025-11-17 20:50:30,1
AMD,npdzi3u,There's always gonna be a reason (manufactured or not) for the prices to go up.  Good thing our salaries also go u... wait a minute.!,AMD,2025-11-17 21:44:46,1
AMD,npe0on5,"Dude, can we get a break? Jesus Christ. Want a toaster? Silicon. Want to sell hotdogs? Silicon. Want to clean your house? Silicon. Want to clean your ass after taking a shit? Silicon. First it was crypto. Now IoT and AI are fucking the market. Your cars have silicon, your bikes have silicon.... by the end of 2029 we will be eating sand.",AMD,2025-11-17 21:50:40,1
AMD,npe4p0s,"Memory shortage will patially kill part of the market, sadly, and gpus were just the next in line for price increases..",AMD,2025-11-17 22:11:16,1
AMD,npe4zy6,"If that 9070 xt reference design in the picture was available, the upgrade would be worth it.",AMD,2025-11-17 22:12:52,1
AMD,npe7gqd,"if you plannin on buyin radeon this year, i recommend getting the order in at least 5 days before the end of the year",AMD,2025-11-17 22:25:57,1
AMD,npeicmi,Really happy I managed to snag 9070 for MSRP during launch. This card will hopefully serve me for a long ass time.,AMD,2025-11-17 23:27:01,1
AMD,npekrg7,:),AMD,2025-11-17 23:41:20,1
AMD,npf4aae,Sounds like marketing bullshit,AMD,2025-11-18 01:36:22,1
AMD,npf9n2p,That'll be true for all GPU vendors,AMD,2025-11-18 02:08:24,1
AMD,npf9wwo,Got my 9070 XT just before the shit hit the fan. Should be set for the next 5 years.,AMD,2025-11-18 02:10:00,1
AMD,npfq87n,"Man, if it really took this long for the XT to drop to MSRP, I'm glad I got the ""standard"" 9070 at MSRP instead. Not like I \*needed\* the extra... what, 10 FPS on average?",AMD,2025-11-18 03:48:29,1
AMD,npfxlc8,Not enough wafer to go around due to AI,AMD,2025-11-18 04:40:18,1
AMD,npd1b1n,Glad I got mine last week then.,AMD,2025-11-17 18:53:03,1
AMD,npcyybb,Are we sure Ferrari and AMD aren't in some way connected?,AMD,2025-11-17 18:41:36,-1
AMD,npd1sxd,"And this is exactly why I use GeforceNOW. I don't have time to constantly research and buy new hardware, while they keep increasing pricing for less performance.",AMD,2025-11-17 18:55:26,-5
AMD,npdc2fg,I am 100% upgrading to a 5080 now before shit hits the fan with pricing,AMD,2025-11-17 19:46:29,-2
AMD,npcv26b,"basically yea  ram first, then ssds, hdds, gpus  2026/2027 will be more expensive than we want",AMD,2025-11-17 18:22:55,93
AMD,npdeli9,Or hold out until the AI bubble bursts. Nobody knows when that might be though.,AMD,2025-11-17 19:59:05,14
AMD,npdv439,"I just paid more for 32G 6,400 than I did for 64G 6,400 two months ago by $70!",AMD,2025-11-17 21:22:49,6
AMD,npdymbi,Too late for that.  It's absolutely wild - 64 GB server modules cost as much as entire GPUs now.  The price pressure is starting to hit the consumer ram markets hard too now.,AMD,2025-11-17 21:40:24,3
AMD,npdeb7f,"Don't buy them now, right before christmas.  Buy them January",AMD,2025-11-17 19:57:37,-4
AMD,npcyinv,Same! My 6800xt and 5600 are still fire!,AMD,2025-11-17 18:39:31,20
AMD,npdkso0,"5600x / 6800 still going strong 5 years later on 1440p, also don't have time for gaming these days :(",AMD,2025-11-17 20:30:30,6
AMD,npd9vzt,"got one last year for $225 (edit: actually 6700), aiming for a 9070 or better for $200 in 5-6 years",AMD,2025-11-17 19:35:37,3
AMD,npeacnv,My 6800XT and 3900X are holding down the fort,AMD,2025-11-17 22:41:32,2
AMD,npf67sy,"I'm in the exact state of mind. I have an RX 6800 which I bought right as it was released, but haven't been able to game since the last two years. It's not just that life is so much busier, games don't feel fun anymore. I don't know if it's just me or it's generally the case. I might game more on my laptop Ryzen HX 370 as it will force me to choose my games more carefully. May be I'll have some fun again.",AMD,2025-11-18 01:48:02,1
AMD,npdbwa6,Prices going up are because RAM is getting exponentially more expensive. Blame AI and the stupid AI obsession for this. Anything that uses RAM is getting more expensive.,AMD,2025-11-17 19:45:39,28
AMD,npd1im3,"If you thought AMD was going to eat the rising cost of DRAM for us, you are very naive.",AMD,2025-11-17 18:54:04,85
AMD,npcwgpv,"You monkey mind understands what it means when ram pricing going up so much? Its not just amd, anything that has ram is going up in price.",AMD,2025-11-17 18:29:40,41
AMD,npdor4h,Yeah im about to buy a 9060 xt end of this month hopefully things don't scale to quickly till then otherwise my 5700xt will have to last a lil longer lol.,AMD,2025-11-17 20:50:37,5
AMD,npcudy1,Why would you upgrade anyway when you have a 7900XTX ? Your specs will last 4/5 years min.,AMD,2025-11-17 18:19:44,39
AMD,npcubyi,"To be fair, the 9070 XT is really only a lateral upgrade with native FSR 4 for you anyways. It also has 8GB less VRAM.",AMD,2025-11-17 18:19:29,21
AMD,npctxx1,You rather wait until they increase or?..,AMD,2025-11-17 18:17:38,1
AMD,npd3h0v,:),AMD,2025-11-17 19:03:35,-8
AMD,npdhlnt,"So glad I picked up a 9070 XT last month for 550, it's been a fantastic card so far.",AMD,2025-11-17 20:14:20,2
AMD,npdh531,"This is out of their control, the price of ram has gone up 100%. Smartphone makers, GPU, Ram sticks themselves are all going up as a result of a ram shortage.",AMD,2025-11-17 20:12:00,10
AMD,npf9by1,"What isn't working right, I was expecting to get that card",AMD,2025-11-18 02:06:34,1
AMD,npd6h0a,no AMD is with mercedes,AMD,2025-11-17 19:18:29,3
AMD,npd6dhx,"amd said nothing, learn to read the article, not only the title",AMD,2025-11-17 19:18:00,4
AMD,npczunv,Next: thermal paste.  /s,AMD,2025-11-17 18:45:58,40
AMD,npcwrjr,"HDDs barely have any NAND in them, shouldn't be that affected.",AMD,2025-11-17 18:31:07,10
AMD,npdty2u,WW3 around the corner buddy. What do you expect an economy to do?,AMD,2025-11-17 21:17:02,1
AMD,npea2aa,"Just gotta survive the layoffs caused by the economic contagion of the bubble bursting, to actually afford buying things.  Oracle helped ignite a debt-fueled AI race and already rocketed past 500% debt-to-equity ratio back in September: https://am.jpmorgan.com/content/dam/jpm-am-aem/global/en/insights/eye-on-the-market/the-blob-amv.pdf  > Other recent AI news: Oracle’s stock jumped by 25% after being promised $60 billion a year from OpenAI, an amount of money OpenAI doesn’t earn yet, to provide cloud computing facilities that Oracle hasn’t built yet, and which will require 4.5 GW of power (the equivalent of 2.25 Hoover Dams or four nuclear plants), as well as increased borrowing by Oracle whose debt to equity ratio is already 500% compared to 50% for Amazon, 30% for Microsoft and even less at Meta and Google.  > On OpenAI/Oracle and the capital cycle: “There is no way for Oracle to pay for this with cash flow. They must raise equity or debt to fund their ambitions. Until now, the AI infrastructure boom has been almost entirely self-funded by the cash flows of a select few hyperscalers. Oracle has broken the pattern. It is willing to leverage up to hundreds of billions to seize a share. The stable oligopoly is cracking…The implications are profound. Amazon, Microsoft and Google can no longer treat AI infrastructure as a discretionary investment. They must defend their turf. What had been a disciplined, cash- flow-funded race may now turn into a debt-fueled arms race”. Doug O’Laughlin, Fabricated Knowledge, Sept 202  And just this week... https://www.reuters.com/business/oracle-bonds-sell-off-ai-investment-fuels-investor-concerns-2025-11-14/  > Nov 14 (Reuters) - Oracle bonds have taken a hit in recent days following a report that the cloud and artificial intelligence service provider plans to add another $38 billion to its heavy debt load to fund its AI infrastructure, according to analysts and investors. Oracle has invested billions of dollars to build its cloud and AI infrastructure this year. With roughly $104 billion in debt outstanding, including $18 billion in bonds, the company is spending more than it earns from operations as it bets on future profits through contracts with startups such as OpenAI.  ...  > **Michael Burry, the investor whose successful bets against the U.S. housing market in 2008 were recounted in the movie ""The Big Short,""** and who is closing his hedge fund, Scion Asset Management, has argued that these companies are quietly stretching out depreciation schedules to make earnings look smoother as they commit money to AI development.  > Between 2026 and 2028, those accounting choices could understate depreciation by about $176 billion, inflating reported profits across the sector, Burry estimated.  > Michael Field, chief equity strategist for Morningstar in the Netherlands, noted that it is difficult to attach a depreciation number to the economic life of data centers.  > ""(But) it's decreasing all the time and it could be single, low single-digit years very shortly,"" Field said.  > ""It could be three to four years and then something's obsolete, (and) you have to make a hell of a lot of money in that particular time to pay off the infrastructure that went into that site in the first place.""",AMD,2025-11-17 22:39:58,4
AMD,npdvkb0,"If you want a new ssd, definitely buy it now. In January you will pay at least 50% more. It will probably double just like ram.",AMD,2025-11-17 21:25:04,6
AMD,npd4nw3,6950xt and 5900x here.  Also play less since last year.,AMD,2025-11-17 19:09:28,13
AMD,npdenny,Same but I guess I gonna have to leave the 4k TV for a 1440p monitor...,AMD,2025-11-17 19:59:22,1
AMD,npekfpl,Finally had to retire the 5600x because of BF6,AMD,2025-11-17 23:39:24,1
AMD,npdmw2b,I'm still rocking my Ryzen 3700x and my Radeon 5700xt.,AMD,2025-11-17 20:41:08,8
AMD,npe1u47,"u/FunkyRider  Yeah, same here. I'm sticking with my current setup for now, but I grabbed another DDR4 kit since 16 GB is starting to feel insufficient. I also have less time for gaming these days, and the latest releases aren’t worth an upgrade. I’m not even talking about graphics demands. The overall game quality just isn’t good enough to justify a new GPU, even though I love tinkering with new PC parts.",AMD,2025-11-17 21:56:25,2
AMD,npdanzl,Goodluck for that.,AMD,2025-11-17 19:39:30,4
AMD,npeguhx,"Good price! I bought one right after release, grabbed the last unit in the store for something like $850 US at that time (top of mining craze, sold my 5700 for $370 so not too bad a deal). Still worth every penny since I haven't felt the urge to upgrade for such a long time.",AMD,2025-11-17 23:18:16,2
AMD,npfcwvs,But how come a product that’s already made with old production price is the one that raises its price also? Wouldn’t it be the new batch of products that now have to pay a higher price to be produced… I don’t understand that but I guess it’s just corporate greed.,AMD,2025-11-18 02:27:35,2
AMD,npd9e94,in this case it might not be a bad idea on AMD's part. Lose some money now... to print money later.   Do you think Nvidia wasn't losing least a little money when they where sinking billions if not trillions of dollars over the years into AI? Look at them now that investment paid off in spades.   The real question is not is AMD going to eat the rising cost now. Its is AMD going to take a short term hit for long term profits.   I think we all know the answer.,AMD,2025-11-17 19:33:09,-5
AMD,npczy4e,"Last time I checked Nvidia isn't planning on raising prices.  Also, when your MSRP was already a lie, raising a price is a good way to ensure nobody buys your GPUs when your whole gig is ""Nvidia -50 dollars"", except now it will be GPU that's objectively worse for the same money.  And believe it or not, best way to build marketshare is to eat the loss now for long term gain.  Tldr, AMD is giving people one extra reason to buy Nvidia, less than a month after doing that same thing with drivers.  Literally everyone I know who has AMD GPU after all this BS is swearing not to buy from them again for the next upgrade.",AMD,2025-11-17 18:46:27,-9
AMD,npd1nzo,"And that does not excuse the stupidity of a $600 ""launch"" that came around with an immediate sell-out of cards that were re-listed at $660+ within hour or days.  The price of these cards were already ridiculously high. They have been way too high for months. They should be able to weather these increases through the initially ridiculous prices they allowed to happen.",AMD,2025-11-17 18:54:46,0
AMD,npcx1rp,"Ram goes up 10% GPU price goes up 20%  Edit:  My point was that companies use a small rise and charge the customer even more, so we get double fucked.  I wasn’t implying that RAM has only gone up 10%  I’m saying that if it goes up 10% it feels like companies then charge 20% more on their product that has RAM.",AMD,2025-11-17 18:32:28,-6
AMD,npd16gg,How much do you think a VRAM module costs? Even with the price increase it’s nowhere near the chip itself,AMD,2025-11-17 18:52:26,-2
AMD,npdgpgx,"If the price of a component goes up by 100%, then what do you expect? Smartphone manufacturers are also holding back on orders trying to wait this out.",AMD,2025-11-17 20:09:49,2
AMD,npd192q,"i have a 7900xtx and its not even lasting right now lol, i cant even crank up the settings at 1440p",AMD,2025-11-17 18:52:47,-17
AMD,npcvdwb,"Sorry should be more clear, looking to upgrade my second PC running a 2070",AMD,2025-11-17 18:24:28,13
AMD,npfn9nc,"It was mainly an issue with drivers under Bazzite OS where it wouldn't render the UI correctly but a recent update fixed it. Windows it worked fine, although I'm Borderlands 4 I had a bug during cutscenes. I think you'll be okay.",AMD,2025-11-18 03:29:24,1
AMD,npd3hb6,Cable ties,AMD,2025-11-17 19:03:37,16
AMD,npe70wg,"Ironically other components like CPUs should decrease in price, if ram price increases reduces sales of laptops and PCs in general. I think we are in a short term bubble where the fear of RAM prices increasing in the future is creating more demand and scalping right now",AMD,2025-11-17 22:23:36,6
AMD,npdi423,"except WD has literally said prices are increasing   https://www.trendforce.com/news/2025/09/15/news-western-digital-raises-hdd-prices-amid-soaring-demand-shipping-delays-of-up-to-10-weeks/  AI runs on data, its going to impact all sectors related",AMD,2025-11-17 20:16:54,18
AMD,npd4ozi,"No, but the lack of available SSDs to buy will likely compel people to gravitate towards HDDs if they need more storage, causing an exponential spike in demand. It'll just snowball from here.",AMD,2025-11-17 19:09:37,25
AMD,npczt55,But they are still going up,AMD,2025-11-17 18:45:45,12
AMD,npdmwqy,"The manufacturing end shouldn't affect them, the problem is supply and demand, if SSDs get stupid expensive, high speed HDDs are gonna be next in line as both companies and consumers look to those.",AMD,2025-11-17 20:41:14,4
AMD,npdvsen,They are already going up.  I have been watching for my next build and prices of anything with memory has been rising,AMD,2025-11-17 21:26:12,5
AMD,nperm6c,However price of HDD's has been going up for a while now,AMD,2025-11-18 00:21:02,2
AMD,npf6imj,Hey what happens when a competing product raises their prices?  ...,AMD,2025-11-18 01:49:49,1
AMD,npdwtxk,"if that happens, all bets off I guess",AMD,2025-11-17 21:31:24,3
AMD,nperese,People like to hold up Michael Burry as some kind of oracle (pun intended) based on one famously successful prediction he made. but how often has he been wrong in his predictions since 2008? I bet it's a lot.,AMD,2025-11-18 00:19:51,2
AMD,npfbw27,> Michael Burry  Burry literally just liquidated his fund lol.,AMD,2025-11-18 02:21:31,1
AMD,npfgg7h,Literally bought an NVME SSD on Saturday that went up $60 the very next day.    This turbulent guy still thinks its 2017.   EVERY MAN FOR HIMSELF!!!,AMD,2025-11-18 02:48:01,2
AMD,npdxsuj,"What makes you think that? (Sorry, I work a lot, no idea what's going on globally)  I just stick to my ritual of waiting for January to make my christmas present lol  I can buy storage now though",AMD,2025-11-17 21:36:20,-1
AMD,npd70tm,"Growing up is a bitch. Eventually gaming will feel like a chore and you will rather just scroll endlessly. 5800x3d, 64 gb ram, 7900xt and I play my switch in handheld more than PC.   Maybe I’m just projecting.",AMD,2025-11-17 19:21:15,15
AMD,npeghh0,Good to have your memory sorted out before the AI craze pushed price to stratosphere.,AMD,2025-11-17 23:16:08,3
AMD,npdwf9j,There is absolutely no advantage for AMD to buy market share through lower margins. Nobody will buy an AMD card because more people bought them the year before.,AMD,2025-11-17 21:29:22,13
AMD,npdmk55,"There's nothing for AMD to invest into other than ""gamer's good will"". Nvidia has their technologies, while AMD makes consoles but somehow still behind in gaming. I hope collaboration with Sony about VRAM compression will help.",AMD,2025-11-17 20:39:27,2
AMD,npeno29,"AI has absolutely not paid off in spades, shovels, tea spoons, or any volume.  The ouroboros of funding has inflated stock prices benefiting a handful of people, but the industry is still hundreds of billions of dollars in the red.",AMD,2025-11-17 23:58:21,1
AMD,npd5qs8,"There is no logic behind your argument at all. RAM prices have literally increased by almost 100% in the space of a month, do you honestly think Nvidia will not respond with price increases? It's not a matter of if, it's actually a matter of when.",AMD,2025-11-17 19:14:50,17
AMD,npd0rzf,"Last time you CHECKED, nvidia hasn’t ANNOUNCED they’ll have a price increase. Jensen needs a new jacket, and a second for his buddy Daddy Don. You can trust those prices are going up, there just isn’t a publicly facing blunt announcement yet.  Saw the writing on the wall and amassed the components for a new build back in Feb capping it off with a $600 9070xt on launch day.  Just pulled the 64gb ram kit I installed, it’s literally more than double what I paid for it. No company is going to subsidize their cost on a component doubling, on behalf of their customers. That’s not how business works if you want to run a successful one for the long term.",AMD,2025-11-17 18:50:29,18
AMD,npd9om1,The word is Nvidia is delaying the Super cards because memory prices have skyrocketed.,AMD,2025-11-17 19:34:36,7
AMD,npd1cj1,I have a 7900xt and I honestly hate the card. The graphics drivers crash at least once a play session.,AMD,2025-11-17 18:53:15,-6
AMD,npcy7v6,Ram is up like 100%,AMD,2025-11-17 18:38:05,23
AMD,npd8yj2,"If it goes up $50, your $250 GPU becomes $300, a whole tier up",AMD,2025-11-17 19:30:59,5
AMD,npedwt6,"Samsung was also ""rumoured"" to increase the price of soon to be announced s26 phones due to ram price increase",AMD,2025-11-17 23:01:16,1
AMD,npdc8av,What games are you playing? I have the XTX and game at 4K with most games being very high or ultra settings and get plenty of performance.,AMD,2025-11-17 19:47:18,10
AMD,npd2kcv,"It's just poor optimized games. Most games are made on UE5, and the performance penalty between very high and high is not worth it for the visual gains. Usually, it's just shadows being slightly high res that you won't notice unless you stop and zoom in.",AMD,2025-11-17 18:59:08,8
AMD,npcvs8p,"Ahhh, I see! That makes more sense, I just read your flair and assumed you were meaning that PC haha",AMD,2025-11-17 18:26:23,10
AMD,npd0ayb,Y'all upgrading your second PC while I barely bought a lower midrange PC last year.  Where tf y'all work to afford that in this economy.,AMD,2025-11-17 18:48:12,5
AMD,npcwoby,Maybe just stick a 5070 in there and be done with it?,AMD,2025-11-17 18:30:41,1
AMD,npday6h,Lmfao wtf goes to a subreddit of certain category to talk shit?,AMD,2025-11-17 19:40:55,1
AMD,npf3fky,"Can’t wait for that bubble to pop. Fuck AI language and image generation models, they serve to do nothing other than make life more miserable.",AMD,2025-11-18 01:31:07,2
AMD,npe2gsi,"They are going up, but it has little to do with the direct reason the other items are going up (but a similar underlying reason).",AMD,2025-11-17 21:59:38,1
AMD,npeknwr,Well then I’ll just get low speed rock slabs to etch my data into,AMD,2025-11-17 23:40:45,5
AMD,npfcfrq,"> how often has he been wrong  Often enough, evidently.  https://www.reuters.com/sustainability/sustainable-finance-reporting/michael-burry-big-short-fame-deregisters-scion-asset-management-2025-11-13/",AMD,2025-11-18 02:24:45,1
AMD,npe3g9w,"Data centers buy everything up. GPU's, RAM and storage  The recent spike in RAM is because of this, it will only get worse in the future because data centers still keep buying up whatever exists for probably at least next year",AMD,2025-11-17 22:04:45,6
AMD,npedlxn,anything that has dram on it is affected. higher end ssds are examples (higher end ssds use onboard dram as cache over HMB or SLC cache) adding dram is typically done to increase TBW longevity and have a larger cache for file transfers/minimize writes to drive.,AMD,2025-11-17 22:59:33,1
AMD,npdapds,"I feel this, picked up a 9070 xt today because of the black Friday sale, debating on even removing it from the packaging still, at least the price is locked in lol",AMD,2025-11-17 19:39:42,3
AMD,npevap1,This is why I only have cat kids and DINK,AMD,2025-11-18 00:42:14,2
AMD,npeors0,Reality is: PC just isn't fun and is full of distractions,AMD,2025-11-18 00:04:43,1
AMD,npdzfq6,You Gunna sell?,AMD,2025-11-17 21:44:26,0
AMD,npexjtb,"Ask any regular, normal person what AMD is. They won't be able to tell you. If you ask a normal PC gamer, maybe they'll say AMD makes computer chips, maybe like the Ryzen 5 they have. If you ask them what GPU they have, it'll be an RTX card 9 times out of 10. If AMD had the same reputation as NVIDIA for their GPUs, they wouldn't need to care about good value in the same way NVIDIA doesn't.",AMD,2025-11-18 00:55:24,1
AMD,npe3wlr,"AMD is behind because they don't support their products as they should.  ROCm releasing way later for the 90XX series, Redstone still not being officially announced while the card released in March this year. FSR4 doesn't support Vulcan and won't till sometimes next year.  They keep lagging behind in releasing crucial features that NVIDIA has for a good while",AMD,2025-11-17 22:07:08,0
AMD,npd9tp2,"Well so far there are no rumors about them, but there are for AMD, until those appear I stick with my opinion because it's the one that makes the most sense.  AMD and Nvidia can eat the cost for the sake of consumers, or not.",AMD,2025-11-17 19:35:18,-12
AMD,npdb596,Difference between delaying a rumoured product and increasing the price of already existing cards,AMD,2025-11-17 19:41:54,-5
AMD,npd0ng7,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-17 18:49:53,0
AMD,npd6ml6,I also have a 7900xt and have had no issues at all. YMMV I guess.,AMD,2025-11-17 19:19:16,4
AMD,npcyhjc,The ddr5 i got for $80 is like $200+ today. Its crazy.,AMD,2025-11-17 18:39:22,10
AMD,npeved8,"no its like. every game? one big one i've noticed is arc raiders, which most people are saying is really well optimised, while i can't break 60fps while maxed out at 1080p",AMD,2025-11-18 00:42:48,-1
AMD,npcxr3q,I always forget that I have that flair,AMD,2025-11-17 18:35:51,9
AMD,npd0qb8,"Second hand parts goes a long way + regular savings, living in cheaper cities, being single  Almost nothing in my PC is new other than the storage",AMD,2025-11-17 18:50:16,2
AMD,npehswu,"No kidding, they already depleted the entire 2026 production capacity! This is getting ridiculous.",AMD,2025-11-17 23:23:48,1
AMD,npdzh66,What sale?,AMD,2025-11-17 21:44:38,1
AMD,npes9if,This may be my last PC. The little bit of gaming I do desire is more convenient in bed on a handheld.,AMD,2025-11-18 00:24:48,1
AMD,npesav1,Maybe when the steam deck two comes out,AMD,2025-11-18 00:25:01,1
AMD,npddagn,"i’m not one to defend gigacorps but when a product legitimately increases in manufacturing costs by $100 the company is not required to eat that cost. not to mention that even if nvidia doesn’t raise prices on consumers, they’re going to absolutely subsidize it by jacking up prices on their datacenter cards, a luxury AMD doesn’t have as it id a much smaller part of their sales",AMD,2025-11-17 19:52:33,6
AMD,npevrem,"> AMD and Nvidia can eat the cost for the sake of consumers, or not.  ""You're unbearably naive....""  --Ultron",AMD,2025-11-18 00:44:50,2
AMD,npeylwg,"""for the sake of consumers"" in case you forget these are profit making enterprises, they aren't under any obligation to eat the cost just to make you happy",AMD,2025-11-18 01:01:46,1
AMD,npehpg6,NVIDIA literally fucked people over just to save a few bucks on a grand total of 512MB of VRAM in the 970.,AMD,2025-11-17 23:23:14,0
AMD,npdt12r,"The product was all but confirmed.  Instead of keeping prices the same, but giving you more for the same amount as planned... They are now just keeping prices the same and giving nothing extra, leaving you to pay the same price for tech products that are going to be 2+ years old.  Current rumors are that super is getting skipped all together now. This is Nvidia's version of a price increase.",AMD,2025-11-17 21:12:26,2
AMD,npev6gs,Which brand? Mine is a xfx,AMD,2025-11-18 00:41:34,1
AMD,npd2a79,Same thing here. Paid 2000 DKK for a 64gb set in august. That is now 4800 DKK,AMD,2025-11-17 18:57:46,2
AMD,npf7uub,"Somethings definitlely bottlenecking your build, cause even my old RX6800 can easily do 1080p maxed out at 60+ during the previous two arc raider mp betas.",AMD,2025-11-18 01:57:50,1
AMD,npfcy1h,Lmao what? Your card must genuinely be broken my guy.,AMD,2025-11-18 02:27:46,1
AMD,npehxmo,"That was just one manufacturer that sold out it's production for the last couple of years, so it's nothing special",AMD,2025-11-17 23:24:33,2
AMD,npdzwkx,"Canada Computers has the Gigabyte Windforce OC 9070xt for 799.99, msrp for the 9070xt is ~840 cad converted from USD",AMD,2025-11-17 21:46:47,2
AMD,npevgrv,See you in 2 years,AMD,2025-11-18 00:43:10,1
AMD,npdx2bu,NVIDIA didn't even update their msrp in 2021 when their cards where twice as expensive due to mining and people being stuck at home from covid.,AMD,2025-11-17 21:32:36,1
AMD,npewsni,"Sapphire. Been fairly loyal to them when going AMD since my old HD 7970. Since then I've had a R9 280X, an RX 570, a 5600XT, and now that my 1080ti died, I replaced it with a 7900XT.",AMD,2025-11-18 00:50:55,1
AMD,npe077k,They've got the ASRock challenger for $599 USD on Newegg but waiting to pull the trigger when the other variants come down as I don't really like the aesthetics of the card.,AMD,2025-11-17 21:48:16,1
AMD,npevini,lol right I know,AMD,2025-11-18 00:43:28,1
AMD,npee0l8,there’s not really a functional difference between raising MSRP and raising prices without raising MSRP. one is just more public,AMD,2025-11-17 23:01:53,3
AMD,npe30ib,"Yeah, i just didn't want to have to fork out even more money to get a lower spec product for the same price 6 months down the line, especially the way ram prices are going these days.   Im still undecided on whether I want to keep it or not. Especially with UDNA on the way",AMD,2025-11-17 22:02:28,2
AMD,npevf6q,"UNDA is going to be well into 2026 and with the market as it is now, the pricing will likely be a shit show, let alone supply",AMD,2025-11-18 00:42:56,1
AMD,npdq3ld,"Yep, waiting for Zone pro... Want the Linux for my Zone 1... Give me the damned Beta -,-",AMD,2025-11-17 20:57:28,2
AMD,npecrlt,How's the zone experience? Haven't seen much about Zotac's handhelds,AMD,2025-11-17 22:54:52,1
AMD,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,75
AMD,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
AMD,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,75
AMD,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,15
AMD,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,12
AMD,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,16
AMD,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,9
AMD,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,6
AMD,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,8
AMD,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
AMD,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,4
AMD,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,27
AMD,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
AMD,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
AMD,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,7
AMD,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,7
AMD,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,3
AMD,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
AMD,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
AMD,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
AMD,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
AMD,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,3
AMD,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,5
AMD,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,16
AMD,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,2
AMD,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
AMD,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,2
AMD,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
AMD,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,2
AMD,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
AMD,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,4
AMD,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,7
AMD,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
AMD,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
AMD,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,1
AMD,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
AMD,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
AMD,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
AMD,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
AMD,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,1
AMD,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
AMD,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
AMD,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
AMD,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
AMD,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
AMD,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
AMD,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,1
AMD,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
AMD,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
AMD,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
AMD,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
AMD,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
AMD,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
AMD,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
AMD,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
AMD,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
AMD,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
AMD,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
AMD,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
AMD,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
AMD,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
AMD,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
AMD,norpqss,USB-C port still no power? :/,AMD,2025-11-14 06:40:49,1
AMD,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,1
AMD,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
AMD,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
AMD,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
AMD,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
AMD,nosa7uh,Did AI create these new drivers?,AMD,2025-11-14 10:02:49,1
AMD,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
AMD,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
AMD,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
AMD,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,1
AMD,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
AMD,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
AMD,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
AMD,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
AMD,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
AMD,notyosp,"I don't see any information about the freezes, lockups that started with 9.1.   No a fix, but the problem isn't acknowledged either.",AMD,2025-11-14 16:30:15,1
AMD,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
AMD,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
AMD,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
AMD,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
AMD,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
AMD,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
AMD,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
AMD,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
AMD,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
AMD,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
AMD,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
AMD,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
AMD,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
AMD,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
AMD,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
AMD,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
AMD,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
AMD,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
AMD,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
AMD,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
AMD,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
AMD,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
AMD,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
AMD,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
AMD,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,1
AMD,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
AMD,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
AMD,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
AMD,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
AMD,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
AMD,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
AMD,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
AMD,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
AMD,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
AMD,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
AMD,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
AMD,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
AMD,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
AMD,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,1
AMD,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,1
AMD,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
AMD,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
AMD,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
AMD,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,0
AMD,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
AMD,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
AMD,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
AMD,nopiwaq,Any fixing incoming for the eeudumps logs being written every second on ssds? and thus damaging ssds?,AMD,2025-11-13 22:14:03,0
AMD,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
AMD,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
AMD,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
AMD,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-3
AMD,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
AMD,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
AMD,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,20
AMD,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,16
AMD,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,6
AMD,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
AMD,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
AMD,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
AMD,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
AMD,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
AMD,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,27
AMD,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,3
AMD,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,4
AMD,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,2
AMD,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,101
AMD,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,4
AMD,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,6
AMD,nonhggg,"Same here, but oddly enough I use modded 25.9.1 drivers",AMD,2025-11-13 16:10:52,1
AMD,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
AMD,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
AMD,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
AMD,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,9
AMD,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,5
AMD,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
AMD,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
AMD,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,11
AMD,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
AMD,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
AMD,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
AMD,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,7
AMD,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
AMD,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,1
AMD,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,1
AMD,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
AMD,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
AMD,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
AMD,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,4
AMD,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
AMD,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
AMD,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
AMD,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,1
AMD,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
AMD,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
AMD,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
AMD,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
AMD,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
AMD,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
AMD,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
AMD,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
AMD,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
AMD,nope0rx,Okay.,AMD,2025-11-13 21:49:03,0
AMD,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,18
AMD,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,1
AMD,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
AMD,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,5
AMD,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,1
AMD,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
AMD,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
AMD,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,0
AMD,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,4
AMD,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,6
AMD,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,3
AMD,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
AMD,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,3
AMD,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,5
AMD,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
AMD,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
AMD,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
AMD,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
AMD,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
AMD,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,4
AMD,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
AMD,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
AMD,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
AMD,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
AMD,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
AMD,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
AMD,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
AMD,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
AMD,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
AMD,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
AMD,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,2
AMD,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,2
AMD,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
AMD,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,1
AMD,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
AMD,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
AMD,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,1
AMD,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
AMD,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,1
AMD,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
AMD,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
AMD,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
AMD,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,6
AMD,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
AMD,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
AMD,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,3
AMD,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
AMD,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,1
AMD,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
AMD,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
AMD,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
AMD,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
AMD,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,9
AMD,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,5
AMD,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
AMD,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
AMD,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
AMD,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,11
AMD,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,14
AMD,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
AMD,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,17
AMD,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
AMD,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,15
AMD,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
AMD,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,19
AMD,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,14
AMD,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,5
AMD,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,4
AMD,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,6
AMD,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
AMD,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
AMD,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
AMD,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
AMD,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
AMD,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
AMD,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,9
AMD,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
AMD,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,5
AMD,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,4
AMD,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
AMD,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
AMD,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
AMD,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
AMD,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
AMD,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
AMD,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,6
AMD,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
AMD,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
AMD,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
AMD,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,3
AMD,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,3
AMD,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
AMD,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
AMD,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
AMD,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
AMD,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,1
AMD,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,2
AMD,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
AMD,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
AMD,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
AMD,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
AMD,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
AMD,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
AMD,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
AMD,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,7
AMD,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
AMD,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
AMD,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
AMD,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,6
AMD,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
AMD,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
AMD,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,1
AMD,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,6
AMD,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,0
AMD,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,2
AMD,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,0
AMD,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
AMD,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
AMD,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
AMD,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
AMD,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
AMD,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
AMD,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
AMD,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
AMD,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
AMD,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
AMD,noumgxv,"Possible I had not any freezing till 9.1 and rollng back to 8.1 solved the problem.  Yet, I don't use any rare hardware. The rig is formed of most common brands and models. So there is something wrong with the driver.",AMD,2025-11-14 18:28:59,1
AMD,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
AMD,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
AMD,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
AMD,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
AMD,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
AMD,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
AMD,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
AMD,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
AMD,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,9
AMD,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,9
AMD,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
AMD,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
AMD,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,0
AMD,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,42
AMD,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,2
AMD,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,12
AMD,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,4
AMD,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
AMD,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,24
AMD,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
AMD,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
AMD,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
AMD,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
AMD,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
AMD,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
AMD,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
AMD,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
AMD,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
AMD,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
AMD,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
AMD,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
AMD,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
AMD,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,1
AMD,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
AMD,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
AMD,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
AMD,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,3
AMD,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,5
AMD,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,4
AMD,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,3
AMD,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
AMD,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,1
AMD,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
AMD,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
AMD,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
AMD,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
AMD,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,1
AMD,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
AMD,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
AMD,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-2
AMD,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
AMD,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
AMD,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,3
AMD,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
AMD,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
AMD,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
AMD,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
AMD,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,21
AMD,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,5
AMD,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
AMD,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
AMD,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,8
AMD,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
AMD,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,10
AMD,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,5
AMD,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
AMD,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
AMD,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
AMD,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
AMD,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
AMD,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
AMD,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
AMD,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
AMD,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
AMD,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
AMD,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
AMD,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
AMD,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
AMD,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
AMD,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
AMD,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
AMD,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
AMD,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
AMD,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,2
AMD,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,4
AMD,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,1
AMD,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
AMD,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
AMD,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
AMD,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
AMD,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,0
AMD,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
AMD,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
AMD,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,-1
AMD,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,0
AMD,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
AMD,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
AMD,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
AMD,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
AMD,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
AMD,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
AMD,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
AMD,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,1
AMD,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
AMD,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,1
AMD,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,0
AMD,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,1
AMD,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
AMD,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
AMD,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,1
AMD,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
AMD,nnbgr26,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-11-05 22:17:37,1
AMD,nnbikmf,TLDR 8/40 Strix Halo SKU,AMD,2025-11-05 22:27:10,120
AMD,nnbwyzf,"Should be a great SKU when it launches, probably will be at CES 2026. Personally I'm hoping it means more laptop models because only a 14 inch AI focused hp laptop and an Asus tablet isn't great.",AMD,2025-11-05 23:47:06,32
AMD,nnbu1ho,Oh goddamnit I literally just built a 385 Framework Desktop HTPC...,AMD,2025-11-05 23:30:08,12
AMD,nnd4mk7,"The reason strix halo is relevant is it is the only non-enterprise grade, non-mac processor with 4 channel memory. AI prosumers have been nutting over the 395 and this one is effectively the same thing for their purposes (4 channel memory with up to 96 GB for the GPU) so I expect this to be very popular with that community.",AMD,2025-11-06 04:13:13,12
AMD,nncq50o,Ok now put them in a actual laptop,AMD,2025-11-06 02:39:13,21
AMD,nncxreq,I think no RDNA 4 on these APUs makes them pretty much worthless for gaming. A 5060 laptop would be cheaper and better overall for gaming,AMD,2025-11-06 03:26:51,30
AMD,nne95p3,WHO THE FUCK IS NAMING THIS SHIT?,AMD,2025-11-06 10:22:15,4
AMD,nnd9rz4,Now get me a version with Vcache.,AMD,2025-11-06 04:51:01,2
AMD,nnngr7j,Pricing is key here. It needs to be price competitive against an equivalent with a dGPU or there's no reason to even look at it.,AMD,2025-11-07 19:55:31,2
AMD,nndzv1p,Wtf is AI MAX+,AMD,2025-11-06 08:46:51,2
AMD,nne5y8e,"would love to see this in a compact 13"" laptop!",AMD,2025-11-06 09:50:19,1
AMD,nnj4gfz,"If it was really for gaming, they would remove the garbage AI part that no one needs for gaming. And they would make sure the part is sub $300 MSRP.",AMD,2025-11-07 02:32:48,1
AMD,nncue2t,"So you call the Radeon 8060S an iGPU since it is not a discrete card?  Or it something different, not exactly a iGPU since it is not a a GPU on the CPU die... but somehow more like a System on Chip?",AMD,2025-11-06 03:05:16,-3
AMD,nnbkwrr,"Praise the lord. I've been hoping they would drop one of these for some time. Getting one with an X3D CCD would be insane, but likely not possible with the current Strix Halo packaging design.",AMD,2025-11-05 22:39:36,59
AMD,nnbmiev,Honestly would be pretty neat. I’ve seen the full strip halo and it looks nice but for a gaming APU a bit overpowered(?),AMD,2025-11-05 22:48:06,6
AMD,nnq5qv5,Took so long to see strix halo come out...  Hopefully this variant is quicker.,AMD,2025-11-08 06:10:07,2
AMD,nne2qpd,It will take half a year until you see them in actual products. Enjoy your Framework Desktop.,AMD,2025-11-06 09:16:57,7
AMD,nnwdgwa,The performance is good enough for light AI work in a laptop.  This especially with max RAM.,AMD,2025-11-09 07:51:26,2
AMD,nncyhcs,Best they can do is sell to mini PC manufacturers the 99% of strix halo supply,AMD,2025-11-06 03:31:35,22
AMD,nnd1n53,Honestly this. It’s not like for the purpose of gaming there’s anything that will run well on the 8060S but be bottlenecked by 5060’s 8GB vram after DLSS.,AMD,2025-11-06 03:52:36,17
AMD,nnnkwvm,"Well, I guess there is one use, if you're into Linux mobile gaming it is pretty much your only reasonable option. NVIDIA drivers are kind of shitty on Linux. That's why I would consider this option in a mobile platform. It's got a very mature driver stack and AMD is generally plug and play on Linux.   You're usually talking about a single digit performance gap between Windows and Linux on a mature AMD GPU versus solid double digit gap on NVIDIA hardware.",AMD,2025-11-07 20:17:17,1
AMD,nne4o67,"Not really, optiscaler is REALLY good imho. It's definitely breathing new life into RDNA2 and RDNA3",AMD,2025-11-06 09:37:11,0
AMD,nng3xj4,Why would anyone pair a 9950X3D or 9800X3D with a a 4060?!  V-cache makes zero sense for an APU.,AMD,2025-11-06 17:02:45,3
AMD,nne1nvd,"It's strix halo, the plus seems to mean it has the full 8060s iGPU.",AMD,2025-11-06 09:05:32,2
AMD,nnwdu14,Huh?  The use of AI  hardware will transform gaming!,AMD,2025-11-09 07:55:04,1
AMD,nndz0po,"If you take out 8060S it would just become an I/O die without which the CCDs won't be able to function. So it's indeed an iGPU. Besides, aren't pretty much all 'CPUs' and 'APUs' are SoCs anyway these days. The difference lies in how they are being marketed towards potential buyers.",AMD,2025-11-06 08:38:06,5
AMD,nnc6gmj,X3D cpu is an overkill anyways with what’s effectively a 5060 mobile in gaming.,AMD,2025-11-06 00:41:06,39
AMD,nnblkmh,I wish they'd go further and make a 6/40 SKU. Strix halo does better when CPU's aren't eating the TPD.,AMD,2025-11-05 22:43:04,22
AMD,nnt7ioe,"this will cost like 90% of the full one while being purely a gaming chip, I don't see much point there",AMD,2025-11-08 19:11:57,2
AMD,nnc7pqm,"Definitely has too many CPU cores, unless you’re buying it to play Factorio or Cities Skylines lol",AMD,2025-11-06 00:48:21,7
AMD,nnqovb2,It should as the 388 is literally just a 395 with one 8 core chiplet instead of two but it will depend on when OEMs want to release laptops with them ultimately.,AMD,2025-11-08 09:21:19,3
AMD,nnerpe2,…if ever. It will only have uptick if it’s meaningfully more economical for oems to adopt because 16 cores is a good marketing point for the high end config.,AMD,2025-11-06 12:52:47,3
AMD,nnd1fgc,Niche* mini pc manufacturers to be specific with the sole exception of one HP model. Nothing from dell or the main Lenovo brand.,AMD,2025-11-06 03:51:07,15
AMD,nnnlj7q,"This, some people don't seem to realize even the 9070 XT has literally zero difference between a 9700X and a 9800X3D based on HUB's benchmarks.  Tragic note, I have a 9070 XT and a 9800X3D. Had I known I would've just gotten a 5800X3D instead and stayed on AM4.  People base 9800X3D performance based on non-GPU bottlenecked benchmarks like those HUB and Nexus were putting out on launch, which means with a 4090/5090. Incidentally, anything short of a 5080 desktop does not see any discernible benefit on most titles, and even that is miniscule. Less so at 1080p.",AMD,2025-11-07 20:20:35,3
AMD,nncds3x,"I second this, I love the 8060s but it’s not super beefy and the resolutions that it will dominate at still wouldn’t need x3d,",AMD,2025-11-06 01:24:24,12
AMD,nndnwvs,"An x3d has better ipc so it would allow for lower cpu clocks, lower voltage (hence lower power), allowing the gpu to draw more power, and get more performance.",AMD,2025-11-06 06:48:37,10
AMD,nnd20h4,Nuh uh it ain't. Don't dissuade them!,AMD,2025-11-06 03:55:05,1
AMD,nnbr8vz,"It wouldn't be worth it to put a fully unlocked large chiplet with a cut down much smaller chiplet though, it wouldn't make sense as the cost to produce 6 core + 8060s would be almost the same as 8 core + 8060s.",AMD,2025-11-05 23:14:17,20
AMD,nncumvn,"I thought all Paradox games are still mega single-core quatters, no?",AMD,2025-11-06 03:06:48,3
AMD,nncdwc4,"If it comes out in a laptop for about $1000 it wouldn’t matter, would be massive especially if it can come with 64gb of ram",AMD,2025-11-06 01:25:08,2
AMD,nntu04m,"Yeah.  395 devices are so damn expensive, most of what we've seen outside of 2 handhelds are mini-pcs and that ASUS tablet.",AMD,2025-11-08 21:15:26,3
AMD,nnejgzv,No amount of power in the world would make 8060S faster than a 5060. It’s been tested to death even pushed to 140w in the few strix halo desktops.,AMD,2025-11-06 11:53:59,7
AMD,nnd3xrc,Real life buyers would prefer their money to go to meaningful upgrade. And in a laptop it means always prioritising the gpu for the common 2560x1600 resolution.,AMD,2025-11-06 04:08:21,0
AMD,nnbtzh0,It would be a purely gaming focused product. 6c vs 8c is no difference especially at the GPU level of 40CU. Would be good sense to use bin-failed cpu dies to reach price points the market will actually pay.,AMD,2025-11-05 23:29:49,3
AMD,nnhmiol,"IDK. If it's better for gaming like he says, they could literally sell it at the same price.",AMD,2025-11-06 21:27:13,1
AMD,nnd2d3q,"Cities Skylines 1, yes. Cities Skylines 2 can use at least 16 CPU cores though (I believe you do get bottlenecked by single core at some point).",AMD,2025-11-06 03:57:28,3
AMD,nni9an5,"Sure, but that doesnt mean people wouldnt be interested in a part that is more performant and power efficient.",AMD,2025-11-06 23:27:45,1
AMD,nne1pu4,"X3D CPU means that more bandwidth can be reserved to the GPU whithout using any MALL cache, not to mention that the saved power can be allocated to the GPU it would improve the graphics situation as well.  What I'm not sure about is if it's possible on the custom chiplets AMD uses on Strix Halo.",AMD,2025-11-06 09:06:06,3
AMD,nne3m57,TIL I'm fake.,AMD,2025-11-06 09:26:09,1
AMD,nndztkg,"I get that but I think it's far more likely we would see a 6c + 8050s as it just makes no sense for them to make a 6c + 8060s as their 8060s yeilds would be better off going to a Ryzen AI 392 12c + 8060s or 395 16c + 8060s as the CPU yeilds aren't the problem, the iGPU yeilds are.",AMD,2025-11-06 08:46:26,4
AMD,nneun1d,I think it scales up to 64 or 128,AMD,2025-11-06 13:11:11,2
AMD,nnehgiv,"agree, they share ram, so x3d would help more than on desktop in bandwidth saturated scenarios i think.",AMD,2025-11-06 11:37:51,3
AMD,nnej95y,The MALL on strix halo is already off die on another chiplet. The cpu accessing it for the cpu would airway incur an extreme latency penalty. All ryzen L3 cache is on the same CCD as the cpu die. Why do you think MALL was remotely relevant to the cpu?,AMD,2025-11-06 11:52:18,1
AMD,nneivcl,I’m sure you can find at least one user in the whole world that wants a 9955HX3D paired with a 5050.   That doesn’t mean it’s not an extremely unbalanced combo for gaming and the lack of such real world product proves it.,AMD,2025-11-06 11:49:17,1
AMD,nneuf6l,"Strix Halo doesn't use the standard Zen 5 chiplets, it uses a custom version with a better interconnett, that's why the CCDs are right to the side of the IOD instead of far away like on desktop part, so no, there wouldn't be the usual latency penalty if AMD were to set MALL cache as usable by the CPU.",AMD,2025-11-06 13:09:51,1
AMD,nnf4ul9,"Most eSports and MMO games would run just fine on such a GPU while benefiting massively from the extra CPU horsepower. It doesn't exist because the product doesn't exist. Strix Halo as it is, is a niche product and a novelty. If strix halo can exist at all, I wouldn't rule out a future iteration with support for x3d.",AMD,2025-11-06 14:09:45,1
AMD,nnkaff7,Yeah Wow and FFXIV players seek configs like these. Though a 9800X3D is enough.,AMD,2025-11-07 08:02:04,1
AMD,nneybbu,"The CCDs themselves are architecturally the same and only the interconnects are different. And not being as bad as on desktop Ryzen doesn’t mean it’s usable, cross CCX latency is bad enough even on the same die (Zen 2 CCD’s divided L3 cache and literally all of their monolithic APUs come to mind, all have horrid latency across CCX).",AMD,2025-11-06 13:33:00,1
AMD,nnfr0w8,>It doesn't exist because the product doesn't exist.  It doesn't exist because there is no demand. There is absolutely nothing technically infeasible to pair 9955HX3D with 5050.,AMD,2025-11-06 16:01:17,1
AMD,nng0or4,"Well, that's a 16 core CPU though. AFAICT, there's no 8-core x3d SKU for laptops. We're discussing an 8 core strix halo CPU aren't we?",AMD,2025-11-06 16:47:14,1
AMD,npcswkk,"You heard of motherboard, now get ready for mommyboard",AMD,2025-11-17 18:12:42,36
AMD,npd24hf,And for some reason has fewer USB-C ports than my current gen motherboard,AMD,2025-11-17 18:56:59,14
AMD,npdo4kt,Jginyue walked so maxsun could run,AMD,2025-11-17 20:47:25,6
AMD,npdhye2,Shame there are virtually no boards like that for Intel. Is there just a lot more weebs on AMD?,AMD,2025-11-17 20:16:06,3
AMD,npeistq,What about ITX boards? Are they still mommyboard? XD,AMD,2025-11-17 23:29:41,1
AMD,npezzfx,Imagine a bundle deal with MaxSun board and Yeston GPU.,AMD,2025-11-18 01:10:08,1
AMD,npe8l0y,Rather have more USB-A on my desktop,AMD,2025-11-17 22:31:54,2
AMD,npe8j0h,Intel sucks now,AMD,2025-11-17 22:31:35,6
AMD,npe8tpz,Theres more diy gooners on amd for sure,AMD,2025-11-17 22:33:12,2
AMD,npea8s1,Amd is for neckbeards.   Waifu is for neckbeards.    I think that gpu company maybe made a couple weeb Intel GPU.. Yeston?,AMD,2025-11-17 22:40:57,1
AMD,noim8kg,At $269.  Better off with 7600x3d or 7800x3d,AMD,2025-11-12 20:30:58,274
AMD,noj20id,"This is great, more choices, and more stuff to go on sale cheap when it doesn't sell because upgrading is a better choice!",AMD,2025-11-12 21:50:34,26
AMD,nomar8o,Same Price as 7800x3d here in scandinavia. No point in doing it. Got a 14600k for 120 dollar with bf6 included for my girlfriends pc. Once amd throws out proper budget cpus her pc will be an intel,AMD,2025-11-13 12:05:37,9
AMD,norowge,At what point does it make sense to move on from a 5800x3d? Ram prices are so astronomical I feel like I will be chilling here forever. Still doesn’t bottleneck my GPU…,AMD,2025-11-14 06:33:16,7
AMD,noim5nn,"For international people, it's a Microcenter (US exclusive). You'd have to fly there to buy one, have an american friend, or buy 7800X3D in your market",AMD,2025-11-12 20:30:33,25
AMD,notrluz,"You can get a tray version of 7800x3d for cheaper in Australia, seems pointless.",AMD,2025-11-14 15:55:31,2
AMD,nojrjj5,excellent,AMD,2025-11-13 00:13:10,1
AMD,nolvezq,200$ Aliexpress soon.,AMD,2025-11-13 09:44:59,1
AMD,nova8ff,"better a 9600x at 200 euro, same performance",AMD,2025-11-14 20:30:51,1
AMD,nol81na,Got a 7800x3d for less 2 years back lmao.,AMD,2025-11-13 05:55:05,1
AMD,nojuijo,The 5 people who want this must be so happy.,AMD,2025-11-13 00:30:17,-2
AMD,noj2iqu,Will still never beat a 110$ 7800x3d lol.,AMD,2025-11-12 21:53:05,-18
AMD,noinyin,Yeah only $30 cheaper then a 7600X3D seems really weird. Feel like it needed to be around $229 to make sense.,AMD,2025-11-12 20:39:53,120
AMD,nojkewz,i just bought on amazon entirely new Boxed ( not tray ) versions of a 7800X3D for 273€ each make it kinda pointless honestly.  saw the 7800X3D also for 303€ recently.,AMD,2025-11-12 23:31:02,6
AMD,nokfuzs,I just bought a 7800x3d price matched at Best Buy for $279,AMD,2025-11-13 02:37:41,2
AMD,nolcy44,It’s gonna be used to keep 7800x3d prices high.,AMD,2025-11-13 06:38:49,1
AMD,np0i165,"The prices are a bit different in the UK, 7600X3D is £280 (only one reseller as far as I could see), and the 7500X3d is £240, it is a meaningful change.  Though that said I did pick up a 7800X3D for £177 on Tuesday from a UK warehoused Aliexpress seller. Somaybe right now due to Black Friday deals, this doesn't look like the best priced CPU.",AMD,2025-11-15 18:07:41,1
AMD,nosb5nn,"Certainly not right now with these prices.  Wait 6 months to a year and then ask the same question again.  Right now, the AI bubble is screwing everything up, just like crypto did.",AMD,2025-11-14 10:12:06,6
AMD,nouoyq6,£120 5700X3D was my killer deal. Gonna ride that till AM6 hopefully.,AMD,2025-11-14 18:41:16,3
AMD,np22csr,Gonna ride this thing until it dies. At this point the 5800X3D is gonna be the 1080TI of CPUs.,AMD,2025-11-15 23:16:50,1
AMD,np68eg0,I also have this CPU and I don't see any reason to upgrade in forseeable future.,AMD,2025-11-16 17:16:05,1
AMD,npa7wso,Yes it does.  You're snoozing.,AMD,2025-11-17 07:42:03,0
AMD,noisv40,7500X3D is not US exclusive. I can buy it right now in Poland from local shops.,AMD,2025-11-12 21:04:44,44
AMD,noj29sg,Read that as “buy an American friend”,AMD,2025-11-12 21:51:50,4
AMD,nojia4p,"Usually these last second old gen x3d chips are targeted for other countries with poor pc component prices (5500x3d for example, the targeted consumer base was central-south america)",AMD,2025-11-12 23:18:34,2
AMD,np0zywu,You can buy it in the UK. Both Scan and Box have it in stock.,AMD,2025-11-15 19:40:13,1
AMD,noj7q9l,When was that and which retailer?,AMD,2025-11-12 22:20:13,10
AMD,nok75mf,It's ~$375.,AMD,2025-11-13 01:45:26,3
AMD,noj3mt3,If it’s available in places other than just Microcenter then it makes sense imo,AMD,2025-11-12 21:58:41,65
AMD,noix3l8,It's more than 10% cheaper for a less than 10% drop in performance. Seems very fair?,AMD,2025-11-12 21:25:59,36
AMD,nojg117,"I mean, it’s better price to performance than a 7600X3D.   I don’t understand this weird bias people have when analyzing low end parts. The best price to performance is usually near the bottom, “stretching” to get the slightly better component (say, the 9070 vs the 9060XT) makes your PC. **worse** value. You’d do better off upgrading more frequently and getting these low end value options, heck, you’ll often have better performance.",AMD,2025-11-12 23:05:38,14
AMD,np0o3t6,£177 for a 7800X3D is a bangin price.  Are there any left?,AMD,2025-11-15 18:38:18,1
AMD,noitgey,Share it!,AMD,2025-11-12 21:07:44,1
AMD,nojgc9m,"I mean it does make sense when you think about it.  At some point I had a US friend who, for a year, travelled to Europe and back like 3 times a month for work. I can't for the life of me remember which state he lives in, but he's got super low sales tax, and his work would always get him either an eco+ seat or business class, so he had AMPLE amounts of luggage to bring tons of shit with him.  I basically used the guy as a mule (I wasn't the only one and he was fully willing lol) except instead of drugs it was clothing, food and drinks, and PC hardware. That year I saved thousands of euros because I could get cheap shit with little to no tax, no import fee and no VAT.  Oh and also those AMAZING Microcenter deals. When you combine those package deals with very low sales tax, no VAT, no nothing, I got extremely cheap hardware, prices yet unseen on the European continent.",AMD,2025-11-12 23:07:24,8
AMD,nojnhp2,"Check their post history, they got lucky with a weird deal on a 7800x3D a year ago, and they've been riding that high ever since lol.  I still don't get how them getting a good deal though means this is a normal thing for kther people to expect",AMD,2025-11-12 23:49:18,21
AMD,nojg592,never,AMD,2025-11-12 23:06:18,3
AMD,nojopgc,Might be an OEM seller,AMD,2025-11-12 23:56:28,13
AMD,nok59n9,"makes you wonder why it even exists. one of them. that's not meaningful performance difference  also 7500f/7600/7600x/9500f  it's like same thing. any 'real' performance gain is only between 7400f and 9600x, and it's not even that big of a difference.  [https://www.techspot.com/articles-info/3051/bench/Average-p.webp](https://www.techspot.com/articles-info/3051/bench/Average-p.webp) just.. why  i find 9070/9070xt and 5070ti / 5080 also kinda confusing. they're so close in performance, even same memory. why not make meaningfully faster product instead?  5080 is most infuriating here, price difference is ridiculous for card that's essentially 5070TI super. and gap's huge between it and 5090, both performance and price. if you cant afford 5090 and want something better than 5070ti for 4k, you're shit out of luck",AMD,2025-11-13 01:34:07,7
AMD,nok09o7,"Not necessarily the case. In Australia there always seems to be amazing deals for the 9070/9070xt vs the 9060xt which makes it more price to performance (799 for 9070 rn vs 569 for 9060xt 16gb). Besides, with you already paying a fixed cost with some components like the case and ram, losing a bit of value on price to performance doesn't necessarily translate to the total system being less value. Also in the US, the 9060xt 16gb price to performance is pretty much proportional to the 9070xt rn. I find the sweetspot for gpus has usually been in the ""70 tier"".  This is more true for CPUs tbh where increasing price doesn't add much unless you need to push very high frame rates in competitive games. This means people can often get a ""budget option"" like a 7500f and pair it with a midrange gpu like a 5070/5070ti and be fine at 1440p which is the intended resolution for these cards.",AMD,2025-11-13 01:03:57,4
AMD,np8iqmk,"Sadly this one sold out quickly, hopefully once Black Friday gets closer we'll see similar deals again.",AMD,2025-11-17 00:26:54,1
AMD,noj1hd3,"Me too in Italy: [https://www.idealo.it/confronta-prezzi/208412056/amd-ryzen-5-7500x3d-boxed.html](https://www.idealo.it/confronta-prezzi/208412056/amd-ryzen-5-7500x3d-boxed.html) From Next [https://www.nexths.it/Products/details/sku/100001904WOF](https://www.nexths.it/Products/details/sku/100001904WOF) Well, it's expensive for sure, 280 euros right now.",AMD,2025-11-12 21:47:54,17
AMD,noitydg,[https://www.x-kom.pl/p/1418053-procesor-amd-ryzen-5-amd-ryzen-5-7500x3d.html](https://www.x-kom.pl/p/1418053-procesor-amd-ryzen-5-amd-ryzen-5-7500x3d.html)  Share your source saying it's US exclusive because it is clearly wrong.,AMD,2025-11-12 21:10:16,23
AMD,nojprqa,Wow  That is awesome value,AMD,2025-11-13 00:02:43,1
AMD,nok9cew,"9070 and 9070XT has the extremely simple explanation that it's binning, XTs that aren't quite up to par to be confidently sold as XTs just get the undervolt/underclock treatment and get to be base 9070s. Which tbh the base 9070 is high key really good on temps and power consumption while also just outright beating the 5070, just a shame the on paper and in practice price jump to the XT makes it a hard sell.",AMD,2025-11-13 01:58:33,8
AMD,norpt9t,"5080 is a joke, imo. NV more or less did an arch tweak and a power increase on the 4080 and called it a day. Die is literally the same size. It's a perfectly reasonable graphics card but I can't be impressed by a refresh for a grand that is slower than the XTX tune I've been running for over two years. 16GB has been midrange since Vega Frontier in 2017 if we want to be honest.",AMD,2025-11-14 06:41:27,4
AMD,nojmklz,Probably exclusive in the US? Our murican friends have these kind of shit market behaviors.,AMD,2025-11-12 23:43:53,6
AMD,noiw697,Now cmon man we learn something new everyday 😉,AMD,2025-11-12 21:21:23,-25
AMD,nok7mhd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 01:48:16,1
AMD,noti74r,"it's reasonable card but not for the price imo, it's too close in performance to 5070Ti to justify 280$+ (where i live). same with 5070 here, it has the same price as 9070 on sale and with no sale it's like 20 usd cheaper. but ppl will buy nvidia because it's not amd anyway so why care  wouldn't be surprised if they cut down 5080 so hard to upsell 5090. what are you going to do if you want decent 4k card? well, now you have to spend a lot more than necessary for 5090 or eat D.  i also have a theory it's done to abuse xx80 card's 'status' as a premium, high end product. people won't expect it's now xx70ti super in disguise",AMD,2025-11-14 15:08:45,2
AMD,noxp4js,The nly thing keeping all things form being reasonable is always the price,AMD,2025-11-15 05:37:10,1
AMD,noyvb5e,"only thing?  all things?  using absolute statements is, let's say 'risky'. is a restaurant serving food from grain infested with rat shit reasonable at the right price?  you may say it's a absurd example, but so is using 'all / only'  i never know when to use 'a / an / the'",AMD,2025-11-15 12:29:44,1
AMD,np2u5st,Awesome rig! Enjoy them frames!,AMD,2025-11-16 02:03:47,12
AMD,np33v4h,That's not a computer... it's a desk lamp that runs windows.,AMD,2025-11-16 03:04:13,20
AMD,np37jvj,"Welcome to the Red Team. We have cookies (the nice kind, not the annoying internet ones haha) seriously lush rig OP!",AMD,2025-11-16 03:28:10,4
AMD,np2uhds,Enjoy the rig my guy it beautiful and welcome to team red 🫡,AMD,2025-11-16 02:05:43,3
AMD,np3bafx,"Good choice dude. Of course for CPUs (at least for gaming) it’s a no brained to go AMD. For gpus especially the last couple generations 6000 and 7000, the value you get for amd gpus is unmatched by any other. 9000 series also seems to have amazing value. Enjoy it brother",AMD,2025-11-16 03:52:46,2
AMD,np2zeql,That’s graphics card looks so clean!,AMD,2025-11-16 02:36:12,1
AMD,np3212u,Woohoo!!! Welcome!,AMD,2025-11-16 02:52:36,1
AMD,np3mr3e,Looks great! What case is this?,AMD,2025-11-16 05:16:59,1
AMD,np41c6p,TechHardware mod is crying,AMD,2025-11-16 07:24:45,1
AMD,np4nxlr,My eyes...,AMD,2025-11-16 11:12:45,1
AMD,np4t9pr,Me when my electric sand has a different brand name,AMD,2025-11-16 12:02:56,1
AMD,np4thk7,"Just finished my rig, 7800X3D, 9070XT gigabyte gaming OC, 64 gigs 6400mhz memory CL32, 2Gb 990 pro ssd, MSI P850 wifi 7 mobo. I too was Intel for like 14 years. This thing screams. Love the video card for it's ability to be customized and tuned to run smoothly and quiet. Fan curves and UV have it running perfectly. The processor screams, haven't been able to max it out except in benchmarks. Lots of headroom. Why I love AMD, They don't tie ur hands at every turn allowing you to unleash every bit of performance. It just works .",AMD,2025-11-16 12:04:54,1
AMD,np4y2lv,This image reminds me of early 2000s UV reactive build,AMD,2025-11-16 12:43:35,1
AMD,np50ida,Welcome to the club mate :),AMD,2025-11-16 13:02:20,1
AMD,np59gwr,Welcome! Now... Make the switch to Linux as well 😁,AMD,2025-11-16 14:02:47,1
AMD,np62nge,"Yeah, I couldn't believe my temps when I first got a 9800X3D. So happy I went with it.",AMD,2025-11-16 16:46:05,1
AMD,npfzzb1,Cool lights.,AMD,2025-11-18 04:58:21,1
AMD,np362w7,"Welcome, and def be sure to give Linux a try at some point",AMD,2025-11-16 03:18:39,0
AMD,np30r32,Doesn't it make a 4-5 degree difference in temperature? You've seriously reduced the direction of airflow.,AMD,2025-11-16 02:44:39,-4
AMD,np2va6f,why a x870 and not a lower tier chipset?,AMD,2025-11-16 02:10:30,-8
AMD,np6qg7v,Like I bought used 6900xt ref for 380€ a year ago. Insane value for what it rocks,AMD,2025-11-16 18:46:52,1
AMD,np3ca8e,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-16 03:59:35,1
AMD,np4igpx,That guy is always crying xD  Probably the guy who lost 25k grandmother funds,AMD,2025-11-16 10:17:24,1
AMD,np37odk,Couldn't agree more! Seriously best decision ever!,AMD,2025-11-16 03:29:00,2
AMD,np38jys,And that’ll have 0 impact on anything.,AMD,2025-11-16 03:34:44,3
AMD,np46rg3,I have a vertical 5090 and have no temp deviation.  There’s nothing wrong if mounted correctly and airflow / ambient temps matter more.,AMD,2025-11-16 08:18:21,1
AMD,np3h94w,I’m not sure why you got downvoted but I am curious as to why you’d ask?,AMD,2025-11-16 04:35:23,3
AMD,np5a39k,Bro built a whole new PC just for 25% better performance and you're asking why he spent a bit extra on a chipset💀,AMD,2025-11-16 14:06:45,1
AMD,np4whfe,"What I'm trying to say is that the card's tilt is upward, which is problematic for air coming from below. Fan noise generally changes in this position.",AMD,2025-11-16 12:30:41,1
AMD,np4wlzg,"You can also run the 5090 in the oven, no problem :)",AMD,2025-11-16 12:31:42,0
AMD,nnzrwpr,"The 7500f beating the 9500f in CyberPunk is strange to me.         Also the 8400f is such a poor performer, based off the numbering you wouldn't expect it to perform worse than all the 7000s.",AMD,2025-11-09 20:45:26,32
AMD,nnzue77,8400F is based on a mobile die (Phoenix) and has half the L3 cache (16MB vs 32MB),AMD,2025-11-09 20:57:15,44
AMD,no0ct6i,8400f is garbage tier cpu. worse than a 5600x from 5 years ago.,AMD,2025-11-09 22:29:04,21
AMD,no6a7ku,In some early benchmarks when it was released it only matched the 5600X in gaming,AMD,2025-11-10 21:19:09,1
AMD,nnzvv39,"I'm just more surprised by the naming scheme, but they like to do that with mobile chips.",AMD,2025-11-09 21:04:10,14
AMD,no1qeus,"well in Malaysia,   everyone build it for light gaming lol",AMD,2025-11-10 03:21:32,7
AMD,nob23b7,Well it was garbage when it failed to meet expectations of being an APU. But it could not so it became a CPU.,AMD,2025-11-11 16:49:26,1
AMD,no13x2b,"Yeah that was their old convention wasn't it? Mobile chips are +1000 on the numbering.  I recall 4000 series was Zen2 mobile, then 6000 series Zen3 mobile, 8000 series Zen4 mobile. Though there were outliers too like the 5500U which was also Zen2.",AMD,2025-11-10 01:04:06,8
AMD,no5akk1,"AMD naming scheme has been absolutely fucked for a while.  In the 5000 series for example, you have the Ryzen 7 5700. Is this a downclocked 5700X, like the 5600 is to the 5600X? No. The 5700 is a 5700G with no APU. So some people are getting completely deceived into buying a shitty mobile die.",AMD,2025-11-10 18:20:47,8
AMD,no3pclh,"More like misled by builders thinking it's a newer 7500F, when it's worse",AMD,2025-11-10 13:27:58,9
AMD,no2qruf,Another outlier is the 3200G that was Zen+,AMD,2025-11-10 08:24:20,3
AMD,no3qu1f,hahaha well  for money sakes 😅,AMD,2025-11-10 13:36:57,6
AMD,no40f13,"Yes, but in some countries it's about 20% cheaper than 7500f",AMD,2025-11-10 14:31:55,2
AMD,nocaoul,"It's because 8400F tends to cost as much as 5600, and at least a month ago DDR5 prices weren't much higher than DDR4, so 8400F was great for cheap AM5 builds that let you upgrade the CPU later.",AMD,2025-11-11 20:30:43,1
AMD,no3r2hm,"Only about RM100 difference, basically get scammed by builders if they recommend the 8400F",AMD,2025-11-10 13:38:22,5
AMD,no7i4bb,Issue is that in many cases it performs more then 20% worse lol. Not exactly saving more money.,AMD,2025-11-11 01:29:29,2
AMD,no3rzql,hahah kan  anyway spotted another Malaysian redditors Hi-5,AMD,2025-11-10 13:43:49,3
AMD,npfbis0,"Too bad Nvidia has a better algorithm, but the 9070 XT performance is like 25% higher than the 5070 Ti",AMD,2025-11-18 02:19:22,8
AMD,npfm200,"Also it's not the finished product. It's 0.9.0.0 version, so not even a 1.0 release vs Nvidia Ray reconstruction in its 2/3 years of iterations/development. AMD just needs to refine it and get it into as many games as possible maybe starting with their Sony connections i.e spiderman, ratchet and clank ect...",AMD,2025-11-18 03:21:47,10
AMD,nnvhja9,I'd happily shred the code in an industrial shredder,AMD,2025-11-09 03:23:12,24
AMD,nnv2ahx,Ew,AMD,2025-11-09 01:42:58,22
AMD,nnvaoyi,lol. Wow. They are desperate.  I still think it will be a “success” but I played the beta and it is just more of the same,AMD,2025-11-09 02:37:42,12
AMD,nnxc5hq,I never understood how AMD got the partnership with this game.,AMD,2025-11-09 13:16:57,1
AMD,nnvslkr,"i haven't played since black ops 4, but why does this one feel more like an arena shooter like quake 3?",AMD,2025-11-09 04:44:17,2
AMD,np739wi,This isn’t news. It’s speculation by a Tech/Finance bro plugging AMDs numbers into some spreadsheets with heavy amounts of “AI isn’t a bubble” cope.,AMD,2025-11-16 19:50:32,49
AMD,np6weh8,"Doesn't seem to have worked for GPUs, though...",AMD,2025-11-16 19:15:51,9
AMD,np8mczb,"AMD's roadmap have the tendency to assume it will be equal(or sometime even better CAGR) to NVIDIA, which is a bit funny and sad.",AMD,2025-11-17 00:47:16,4
AMD,np7jhfo,"I bet those roadmaps are all digital, and thus not solid at all",AMD,2025-11-16 21:12:50,3
AMD,np8wn0x,> This isn’t news  I wish I read this comment first,AMD,2025-11-17 01:49:21,7
AMD,npak264,Most of this subreddit is basically AMD marketing and its mostly CPU and AI stuff.,AMD,2025-11-17 09:50:24,3
AMD,npf63eo,"Jesus, I can't wait for the AI bubble to pop. I'm so sick of it",AMD,2025-11-18 01:47:19,2
AMD,np7xulu,Even if its a bubble the investments they're making in it now will pay off in the future.,AMD,2025-11-16 22:28:45,-1
AMD,np718hp,Article is not about consumer gamer products,AMD,2025-11-16 19:40:05,16
AMD,np8dwky,Nvidia is a much tougher opponent than Intel.,AMD,2025-11-16 23:58:46,2
AMD,np6x6sy,Yah cuz “AMD gpus don’t need AI/RT cores” we can just use tech like fsr 3 to compete with tech nvidia has and we don’t mention the employees posting on twitter giving a good PR stunt for AMD gpus,AMD,2025-11-16 19:19:48,-20
AMD,npagxvo,Eh its less that NV is oh so good and more that AMD has been shooting themselves in the foot and face over and over again for years non-stop when it comes to their GPU division.  I have no clue why AMD kept doing this but the smell of giant piles of money seems to be at least partially getting the AMD execs to pull their heads out of their butts.  At least where AI and some GPGPU stuff is concerned.  For gaming stuff they're still stepping on rakes with too much frequency to get much market share back any time soon.,AMD,2025-11-17 09:16:49,5
AMD,np8jlu9,"Welcome back to Team Red, why does your cat look so angry?",AMD,2025-11-17 00:31:46,29
AMD,np8u7d5,Why is the cat aura farming me? Nice build too,AMD,2025-11-17 01:34:28,19
AMD,np8n8jd,PREDATOR!,AMD,2025-11-17 00:52:15,5
AMD,np8m3xk,nice did you snag the woot deal?,AMD,2025-11-17 00:45:51,3
AMD,np9l8u4,contact frame😱  I've never heard of it.,AMD,2025-11-17 04:25:16,3
AMD,np8pl24,so clean,AMD,2025-11-17 01:06:01,2
AMD,np8ql6g,This is a sign that Half-Life 3 is coming up! The cat can almost taste it.,AMD,2025-11-17 01:12:07,2
AMD,np8zre3,Any issues with using the AM5 and 4 sticks of RAM?,AMD,2025-11-17 02:08:23,2
AMD,npbt9m4,Cool cat. Looks like a...Thoroughbred^^^^I'mSorry,AMD,2025-11-17 15:17:13,2
AMD,np92lz5,Seriously nice looking build. Great part selection to get it all to vibe together!,AMD,2025-11-17 02:25:23,1
AMD,np93s61,Are you using Ethernet or wifi,AMD,2025-11-17 02:32:31,1
AMD,np95kaz,Sry...this post should be on the Alien: Earth sub 😅,AMD,2025-11-17 02:43:07,1
AMD,np97eyc,"Usually don't find white builds all that appealing, but this one's beautiful! Great suff!",AMD,2025-11-17 02:54:23,1
AMD,np97nmr,What case is that?,AMD,2025-11-17 02:55:56,1
AMD,np9b68w,"Big props for sticking to the all white theme, even on the parts that won't be seen like the PSU. The case looks great with white on white.",AMD,2025-11-17 03:18:25,1
AMD,np9ehjo,Incredible clean build.  Great work.,AMD,2025-11-17 03:39:29,1
AMD,np9em6p,Should use the 1080ti for lossless scaling (or what ever its called),AMD,2025-11-17 03:40:18,1
AMD,np9n7d8,Your RX Furry still looks to be in good condition!,AMD,2025-11-17 04:38:46,1
AMD,npa5o01,"Just saying, nvidia now has a better and modern control panel.",AMD,2025-11-17 07:19:27,1
AMD,npaxutj,I love your cat,AMD,2025-11-17 12:01:47,1
AMD,npbbld2,More cat photos pls xdxd,AMD,2025-11-17 13:37:51,1
AMD,npc853e,Im just waiting for my last few parts to arrive this has me amped.,AMD,2025-11-17 16:31:16,1
AMD,np8m10d,"He's 16, so he's earned the right to be grumpy",AMD,2025-11-17 00:45:23,36
AMD,np9ohla,Because he's had 16 years of practicing it,AMD,2025-11-17 04:48:16,9
AMD,npcejs3,Yep. Although they're mostly domesticated and eat kibble these days.,AMD,2025-11-17 17:02:37,2
AMD,np8mndi,woot?,AMD,2025-11-17 00:48:54,1
AMD,np9ngtc,https://www.thermal-grizzly.com/en/contact-sealing-frame/s-tg-csf,AMD,2025-11-17 04:40:43,1
AMD,np9nl58,I gave up after the second HL2 expansion....,AMD,2025-11-17 04:41:36,2
AMD,np9ntw2,"None, because it's not 4 sticks. It's 2 DDR5 6000 16Gb sticks and 2 Light enhancement fake DD5 sticks corsair makes  https://www.corsair.com/us/en/p/pc-components-accessories/cmhlekit2-d5/vengeance-rgb-ddr5-light-enhancement-kit-black-cmhlekit2",AMD,2025-11-17 04:43:23,1
AMD,np96b4c,"No, and there wasn't on AM4 either, just a whole lot of people who don't know how to tune RAM. (Which is understandable, RAM OC is a giant PITA even for those who enjoy it).  Yes, it's true that with 4 sticks you won't be reaching an unlinked \~4600MT/s on AM4, or \~8200MT/s on AM5.   But assuming your chip's IF is good, maxing out the FCLK with 4 sticks on both AM4 and AM5 is usually achievable with a few key tweaks and some trial and error (twrrd @ 3-4, correct Proc-ODT for your DRAM ICs, correct RTTs for your board, and finding the CLDO VDDP sweet spot of your IMC).  If you know the ins and outs of memory OC on both platforms, you will almost always be able to max out your linked FCLK limit before you run into your board/IMC MCLK limit with 4 sticks, at least assuming you didn't severely lose the IMC lottery (which is also a rarer occurrence than people think).",AMD,2025-11-17 02:47:37,1
AMD,npc60rh,nice.gif,AMD,2025-11-17 16:20:54,1
AMD,np9oce8,"Thanks much, I'm pleased how it turned out. I call this build *""Fr0stC0re""*",AMD,2025-11-17 04:47:11,1
AMD,np9ny2t,"I'm old school, everything in my house it 2.5Gbps wired other then phones and wife's iPad",AMD,2025-11-17 04:44:15,1
AMD,np9luvv,.... Wat?,AMD,2025-11-17 04:29:33,1
AMD,np9ijcx,"Thanks much, I'm real pleased how it turned out with, my planning paid off. I call this build *""Fr0stC0re""*",AMD,2025-11-17 04:06:21,1
AMD,np9lted,Fractal Design Meshify 3 Pro RGB,AMD,2025-11-17 04:29:19,1
AMD,np9lo5q,"Thanks much, I'm pleased how it turned out, and yeah I was no way in hell gonna stick a black PSU in there lol. I call this build *""Fr0stC0re""*",AMD,2025-11-17 04:28:21,2
AMD,np9i6mn,"Thanks much, I'm pleased how it turned out, I call it *""Fr0stC0re""*",AMD,2025-11-17 04:03:50,1
AMD,np9hfrl,Sold it on ebay for someone else to enjoy,AMD,2025-11-17 03:58:46,1
AMD,np9o2x1,"For 16 years old , other this his poor eyebrows, he sure is",AMD,2025-11-17 04:45:14,2
AMD,npc5wp7,Not from what I experienced,AMD,2025-11-17 16:20:21,1
AMD,npc5zc7,I do to ^_^,AMD,2025-11-17 16:20:42,2
AMD,np8oot7,"the 9070xt spectral white recently went on sale on Woot which is an Amazon website. It went for $649, was just wondering if you got that deal. [https://computers.woot.com/offers/powercolor-red-devil-spectral-white-amd-radeon-rx-9070-xt-16gb-3](https://computers.woot.com/offers/powercolor-red-devil-spectral-white-amd-radeon-rx-9070-xt-16gb-3)  edit: my bad, looks like a hellhound?  Love the build, great job.",AMD,2025-11-17 01:00:38,1
AMD,np9ae20,"Genuine question, in what scenario is any of that worth the time for the performance uplift. As opposed to getting a 6000 CL26 or CL28 kit and setting EXPO. I imagine its worth it if you're really looking to max out your RAM capacity for machine learning, or you love tinkering with the system. But for most people, could they see this be worth it for a considerable uplift in performance?",AMD,2025-11-17 03:13:27,2
AMD,np9g9av,"Seems things may have got a bit more complicated since my last build, shit I’m just trying to get 6000 on AM5, it’s 2 2x16 kits.",AMD,2025-11-17 03:50:47,1
AMD,np9nuq5,"None, because it's not 4 sticks. It's 2 DDR5 6000 16Gb sticks and 2 Light enhancement fake DD5 sticks corsair makes  https://www.corsair.com/us/en/p/pc-components-accessories/cmhlekit2-d5/vengeance-rgb-ddr5-light-enhancement-kit-black-cmhlekit2",AMD,2025-11-17 04:43:34,1
AMD,npaasts,My house is way too old for Ethernet sadly,AMD,2025-11-17 08:11:09,1
AMD,npa2ksu,I can imagine it runs real cool in there. Do you ever hear the chipset/VRM fan for that MB under load?,AMD,2025-11-17 06:49:37,1
AMD,np9if7m,"ah , outdated tech for you , a giant leap for someone else",AMD,2025-11-17 04:05:32,1
AMD,np8rxxk,"Its the Hellhound model, but I did have Best Buy match the Microcenter $400 9800x3D price last month",AMD,2025-11-17 01:20:27,1
AMD,npc9qf5,https://www.youtube.com/watch?v=Fr7Bfr-wPYw  https://www.youtube.com/watch?v=aD-4ScpDSo8,AMD,2025-11-17 16:39:05,1
AMD,npft8nx,">in what scenario is any of that worth the time for the performance uplift.  Basically none.  I couldn't tell you how many hours it took to finally dial in my 4x8GB 3866MT/s OC and get it 100% stable, and in the absolute best case scenario it's worth about 6% in 0.1% lows over 3600 XMP, but usually falls in the 1-2% region, and almost all of that comes more from tighter sub-timings than higher frequency.  But if you asked me personally why I seemingly waste hundreds of hours relearning how to tune and stability test each new DDR generation, my honest answer would be that it's just a compulsion and part of a thorough process of elimination.  My OC philosophy is that I want all hardware that I can control to be operating at the peak possible performance, so in the cases where it seems like some process is slower than it should be, or if there's an FPS dip or stutter, I can more confidently conclude it was a result of unoptimized or sloppily written code and not something I could control.  I run a hugely overkill 1000W PSU in my build for the same reason, because in case I ever have a BSOD or crash I know a lack of good clean power is a variable I can immediately eliminate in troubleshooting and move on to other more likely culprits.",AMD,2025-11-18 04:08:51,1
AMD,npcatpc,A Milwaukee drill would beg to disagree,AMD,2025-11-17 16:44:23,3
AMD,npb7wk6,You can still have it wired. My house is over 100 years old.,AMD,2025-11-17 13:14:55,2
AMD,npa94k2,It doesn't have one,AMD,2025-11-17 07:54:07,2
AMD,npf0izt,"If I'm seeing these correctly, strictly for gaming there doesn't seem to be a massive increase in performance going from something like 6000 MT/s CL30 to 8000 MT/s. The change is even less if you have an x3D. Kinda glad I picked up my 32GB of DDR5 6000 CL28 (for $110!) to go with my 7800x3D. Since its A-Die I could probably squeeze a bit more performance out of it, but its not necessary since I do nothing but game on this system.",AMD,2025-11-18 01:13:26,1
AMD,npctdcg,Costs alot and my house is from 1909 I think,AMD,2025-11-17 18:14:55,1
AMD,npdyd9j,If you have cable internet you can wire into the attic and down and through walls yourself.,AMD,2025-11-17 21:39:08,1
AMD,npedhf9,Yeah I know but it's like over 100 ft a least it's a 3 story house,AMD,2025-11-17 22:58:50,1
AMD,npeh7mm,I would do it personally. Worth it. Just put it in rooms where you need it.,AMD,2025-11-17 23:20:21,1
AMD,nolut38,Does disabling it screw up Freesync?,AMD,2025-11-13 09:38:38,333
AMD,nolybwi,"I think more important question is how much data is written. SSDs aren't dumb, they don't actually write data to it's memory banks every time a new byte is written at the end of file. How does size of this output compares to regular usage of SSD by the OS or the game?",AMD,2025-11-13 10:14:14,367
AMD,noltwq9,"I dont have this folder, or in ...system32\\AMD\\eeudumps  W11 pro 6900XT latest 25.10.2 drivers     it is in ""C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_fbce97352464e3d7\\AMD\\EeuDumps\\"", but just 2 files",AMD,2025-11-13 09:29:02,69
AMD,nolzp76,"Did ya also know that chrome uses your os drive as a media cache and you get a disk write of 50+gb just by having 2 1080p streams on for 5hours? They removed the flag that disables that with the reasoning that it's done to ensure the best browsing experience  So how much write is this doing? You're missing the most important detail despite the long post. Btw i have neither that service nor the log files, on an amd system",AMD,2025-11-13 10:27:52,137
AMD,nolzwlo,"Can confirm this happens.   The worst I could get this to go is a little over 1 MByte/s. For comparison, a random access to the page file during my testing was 80 times worse (RAM wasn't even at 40% utilization).  The Samsung 990 Pro 2TB is rated for 1200TBW. You'll need to be drawing windows around for 11.5 days to use 1TBW of that. Or 38 years to reach the rated writes just using this bug.  So while this is absolutely not a good thing, it's not catastrophic in any sense of the word. AMD should still fix this quickly regardless.",AMD,2025-11-13 10:29:54,128
AMD,nom608m,"Just a thought: Are these actually files that are written to disk, or IO mechanisms manifesting as files via drivers (like pipes), in the same vein that on Linux everything-is-a-file, including RAM & memory-mapped devices (but generally only root has access to read & write those paths)?",AMD,2025-11-13 11:26:48,19
AMD,nolzh6z,I also don't have that folder and my freesync works.,AMD,2025-11-13 10:25:42,17
AMD,nolwkfk,"That ""EeuDumps"" folder doesn't exist in my system32\\AMD directory.",AMD,2025-11-13 09:56:46,14
AMD,nom1gnq,">AMD External Events Utility  Yea i don't have that service, where does it come from?  And the latest change to the 2 eeudumps files is from june(I had some weird windows update issues then, maybe related) and the other 4 files changes are from last year when I switched to the amd board/cpu.  Other thing to note that's maybe related, Igpu is disabled in bios and nvidia gpu so is that why?",AMD,2025-11-13 10:45:06,15
AMD,nomg5bl,Are you sure that the Chipset drivers include this process or did you also install the gpu driver for the iGPU.,AMD,2025-11-13 12:45:13,11
AMD,noltgt2,Big SSD doesn't want you to know this,AMD,2025-11-13 09:24:20,42
AMD,nom12cz,It makes a lot of separate writes but the amount written is tiny compared to the usual system writes.,AMD,2025-11-13 10:41:19,17
AMD,nom2po7,"im not sure why its happening to you as i have TUF x670E-plus MB paired with 7800x3d and 4090, i checked and nothing was going on with that folder.  im on latest bios, chipset drivers and win11 update, however, the log files name start with r-gpu, so i guess its something to do with the iGPU, just to have a peace of mind, i disabled the iGPU in bios, so try that and see if it stops instead of the posted workaround.",AMD,2025-11-13 10:56:50,15
AMD,nom0w87,"This contributes nothing to... anything really. The amount of data written is tiny, and surprise-surprise - other processes do the same thing but at way larger scale.",AMD,2025-11-13 10:39:43,46
AMD,nolzkmi,"i have 25 files in C:\Windows\System32\AMD\EEUDumps\,  and they are from 2022 to 2024",AMD,2025-11-13 10:26:38,6
AMD,nop3mfs,"""*Now just think about how much lifespan AMD takes of SSDs globally doing this*""  Ok I thought. Nothing basically.  So how much did you think OP?",AMD,2025-11-13 20:57:18,6
AMD,noy4uey,"But it's not the number of writes that hurts a drive, it's the amount of data written. If that's just a few hundred lines of text per window you move... that ain't doing anything. So why worry about it?  Seems like you're massively blowing this out of proportion. Wait until you see the amounts of data your browser writes to the drive every day!",AMD,2025-11-15 08:06:34,7
AMD,notjdwg,Damn you just discovered log files xD,AMD,2025-11-14 15:14:47,6
AMD,nop295k,"Man this thread and some of the comments here are in a way fearmongering and/or definitely sciolism.  It's definitely a non-issue, log like this got written all the time. It will not hurt your SSD by any mean, SSDs are not fragile like they are 15 years ago. I've using my AMD rig for 6 years with a single SSDs for everything, I managed to write 60TB to it, the lifetime of my SSD is rated for 500TBW, so a mere 12% after 6 years. As one comment mentioned, you have to literally moving windows constantly and nothing else for 10 days to write another 1TB, now take a deep breath and recollect how many seconds you spent resizing a windows in the last 1 hour? Maybe 5 seconds when you move the browser to the second screen.  Already a lot of people are blindly disabling this ""just because"". Who knows what other function that service do (at least FreeSync). And now when FreeSync and something else suddenly break, everyone is like ""oh man AMD sucks, first the terrIBLe wRiTe BUgs, now stuff xyz is broken"".",AMD,2025-11-13 20:50:22,11
AMD,nom6ybo,Does it happen with MPO disabled?,AMD,2025-11-13 11:35:01,5
AMD,nom73pf,"Is this more concerning than any other types of logging? As in, if you look at (in Windows) the event viewer you will see lots of logs. Is this notably more harmful to the SSD than that, or for that matter, how browsers use caches?",AMD,2025-11-13 11:36:17,3
AMD,nom0gwo,I think this is a extra reason to always buy an SSD with dram to lower the writing rate of actually hitting the SSD  so a 1000 writes would just go to Dram until it's time to actually store it to the drive.  I also hope amd fixes it 👍,AMD,2025-11-13 10:35:33,14
AMD,nom06ac,"AMD Board, CPU and GPU (5800x, 7900xt). I have the service, but not the folder.  Based on task manager (e bytes etc. Active), the service doesn't write anything.  Because the folder is called dump, I suspect this is some kind of error reporting, because something is crashing or not working as it should.  Maybe try scraping the events log?  Also nice background work :)",AMD,2025-11-13 10:32:35,7
AMD,nompi8w,"STOP USING YOUR PC,IT WRITES ON SSD! PANIC",AMD,2025-11-13 13:43:41,9
AMD,nom2rwt,"Yep, it's true, I see it increasing in size as soon as I resize any window, even the file explorer window with that folder open. My folder is located in C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_1aafc0a9b0693712\\AMD\\EeuDumps",AMD,2025-11-13 10:57:25,6
AMD,nom3tfn,"Strangely for me on my shitty work PC (Intel CPU and AMD GPU) the tool isn't detecting anything. I can confirm that ""AMD External Events Utility"" service is running. Need to test this later from my home PC.  My shitty work PC specs:   Intel Core i7 9700   AMD Radeon RX 560",AMD,2025-11-13 11:07:08,3
AMD,nonl5m9,"There is a program called ""Process Monitor"" from the Sysinternals set of tools. It can monitor operating system activity on a file or directory. You can set it to file system events and then to the C:\\Windows\\System32\\AMD\\EEUDumps\\ folder. Here my results opening the folder and then the file in Notepad.    https://i.redd.it/w5qwhzwow11g1.gif",AMD,2025-11-13 16:29:01,3
AMD,noof7nj,My Optane drives have accepted the challenge,AMD,2025-11-13 18:54:46,3
AMD,nolz0y6,"I have a Ryzen 7 4800U with Radeon Vega iGPU here at my hands, both EEUDumps folders are less than 100MByte in size and that's a device with no fresh install since end of 2020. This seems to depend on specific configurations and isn't a general driver issue. TBWs are in the expected range for the use of this notebook.",AMD,2025-11-13 10:21:09,4
AMD,noluqqb,"Thanks for reporting this, I suppose we can expect a fix in 20.000 years +/- 10 decades.",AMD,2025-11-13 09:37:57,14
AMD,nolzs7u,Now we just have to wait 100 years for AMD to fix this.,AMD,2025-11-13 10:28:42,9
AMD,nombkbe,"I've always used AMD, some SSDs are over 5 years old and still above 90% health. I don't see the problem.",AMD,2025-11-13 12:11:53,6
AMD,nomgzgg,And this is bad because....?,AMD,2025-11-13 12:50:57,4
AMD,nomfz8m,"FYI, you can see file activity and much more using Process Monitor. https://learn.microsoft.com/en-us/sysinternals/downloads/procmon",AMD,2025-11-13 12:44:03,2
AMD,nomif87,"You know that despite PCs can easily have 16GB and more RAM, OSes still run a service called Virtual RAM where underused programs are moved from RAM to a designated storage drive.  The OS drive will be the target for a multitude of temporary files, web browser cache, virtual RAM, logs, and more that can age a drive and consume access bandwidth.  This is why I always recommend at least 2 storage drives in a PC.  For Windows, the C: drive should focus on speed over capacity for an SSD, because if any drive will be dying from usage, it will be the C: drive.  Meanwhile, the D: drive should focus on capacity before spending more money to upgrade from PCIe Gen 3 to PCIe Gen 4 or higher.  PCIe Gen 3 is fast enough for this storage drive.  Essentially, I expect the C: drive to fail.  I can easily do a clean reinstall of Windows and then reconnect the user folders to the existing D: drive folders.  The only folders I risk losing are the ""appdata"" folders.  Everything else, I can download and replace.  My clean reinstall of Windows is basically deleting all the partitions from the C: drive and continuing.  My general configuration recommendation for cost effectiveness:  C: Drive - 250GB NVMe SSD PCIe Gen 4 speeds at minimum - I normally recommend 500GB due to Windows.   \- Stores Windows, drivers, tiny replaceable programs, temporary data.  Don't store anything you can't replace  D: Drive - 1 to 2TB NVMe SSD, PCIe Gen 3 - Don't pay extra for faster than Gen 3 speeds, you won't see it.   \- Stores major applications and files considered irreplaceable  E: Drive - 4TB and larger HDD - SSDs of 4TB and larger are expensive.     \-  SATA and HDD can handle many games with little issue, and you can migrate unused Steam games to the HDD to save space on the D: Drive.  OS functions are what typically kills the performance for an HDD.  All of your media files like music and videos can easily run from an HDD.  The HDD is still faster than downloading.",AMD,2025-11-13 13:00:25,2
AMD,nomo4ce,"I'd say the writes are negligible. Also, most people have pagefiles in their systems as well, for Windows to swap memory pages in and out of the actual physical memory. And it's in use the entire time so I don't think this is relevant.",AMD,2025-11-13 13:35:34,2
AMD,nopexei,Is it writing terabytes each day or are wasting our time on nothing here?,AMD,2025-11-13 21:53:33,2
AMD,noscopc,"To put things into perspective: I've built my current PC about three years ago running Windows 11 (which I regret). I recently checked the SMART readings and it's about 33TBW which is about 3% of the SSD's lifetime. Which means, at this rate, the SSD would last about 100 years.",AMD,2025-11-14 10:27:11,2
AMD,np0qsvm,Lucklily we dont resize windows all the time.,AMD,2025-11-15 18:51:47,2
AMD,nomkhyd,"Title makes it sound like it's a big deal, but then realized it's not. Moving on with my day.",AMD,2025-11-13 13:13:37,6
AMD,nolwoxp,"I don't have that file. Ryzen 5700x and Nvidia card, also latest chipset driver, maybe it come from another piece of AMD software?",AMD,2025-11-13 09:58:01,4
AMD,nonq6gm,Wait till OP finds out how windows event works.,AMD,2025-11-13 16:53:35,3
AMD,nootalz,Wait till this guy hears about the insane amount of rewrite windows does in a day ...,AMD,2025-11-13 20:04:53,4
AMD,nov1uoq,The stupidity in his thread is equal to the same proportions as 5G killing people.,AMD,2025-11-14 19:46:41,3
AMD,nosdvoj,"So the solution is simple. Don't turn on your PC, if you don't want it to write data to the SSD. :)",AMD,2025-11-14 10:38:50,3
AMD,noo1lw2,"What is this, baby's first time opening task manager? I'm not saying this is good form, but too verbose logging is not exactly rare. But the data rates in the grand scheme of things will be totally insignificant, especially when compared to hogs like browsers. And there are multiple levels of caches anyway.",AMD,2025-11-13 17:49:48,4
AMD,nomgnj1,"I've tried your program, and it doesn't log anything. I've also looked at the folder, and the first file was modified in Feb 2023, and the last modification was done in July 2024. I have the same service running (6900XT with 5600X).",AMD,2025-11-13 12:48:41,2
AMD,nomwmlo,So how much life time is being wasted? Care to show me your math? Because it sounds blown out of proportion.,AMD,2025-11-13 14:24:30,2
AMD,nonup7g,"Bro just discovered logging and no, this won't kill your SSD more than expected",AMD,2025-11-13 17:15:48,2
AMD,nom8udc,"Interesting, I'm curious to see what happens if I just set that log file to read only. I'll have to try that later today.",AMD,2025-11-13 11:50:33,1
AMD,nom9ys5,Don't have this folder and on 25.10.2. Not seeing this behavior with that service and no writes either.,AMD,2025-11-13 11:59:23,1
AMD,nomr04l,wonder if Mesa has this issue on Linux. Probably not.,AMD,2025-11-13 13:52:16,1
AMD,nomskjr,Is this why my activity LED never stops blinking?!,AMD,2025-11-13 14:01:23,1
AMD,nomsod1,"I hate that drivers are just black boxes we're at the mercy of if we want to use our devices.  I wish they would not just open source them, but document their entire structure and behavior.",AMD,2025-11-13 14:02:02,1
AMD,nomsrl9,"not found folder name, maybe cause i use radeon slimmer?",AMD,2025-11-13 14:02:35,1
AMD,non5zds,My EeuDumps folder is empty,AMD,2025-11-13 15:14:21,1
AMD,nonpcfi,"|| || |**FILE\_ACTION\_MODIFIED** 0x00000003|The file was modified. This can be a change in the time stamp or attributes.FILE\_ACTION\_MODIFIED0x00000003	The file was modified. This can be a change in the time stamp or attributes.|  Are you sure it's not just updating timestamps? That shouldn't be writing directly to the disk. afaik it should update the MFT and that will be written all in one go (like running`sync` on Linux).  The file seems to be in binary. There is plain text, can see my brower executable. Also there is a crapton of `Navi` mentions. tail gives me this interesting driver (am on Arch Linux rn btw): `\SystemRoot\System32\DriverStore\FileRepository\u0420422.inf_amd64_cb23ea54e356fea3\B420106\amdkmdag.sys` followed by what I think is the driver build date or some other timestamp: `2510271353-25.20.21.01-251013`The date seems to be a earlier by 2 days than the latest driver, but the build number is kinda wrong. AMD, are you confused :)  Probably same stuff as in amdgpu but, interesting observations about the mentioned driver (via `llvm-strings`, tl;dr dumps text from compiled files; won't do decompilation or other analysis):  * HOLY SHIT THERE IS SO MUCH TELEMETRY IN THAT DRIVER. not spyware, hwinfo reads the exact same telemetry, the rest is hardware debugging texts. ***c h i l l***. * btw what's this file referring to `C:\DCSlog.log`? llvm-strings shows that it should be used by the driver. * Freesync is handled here, but stopping the service will not stop the driver. On Windows, services can be used to pull data from drivers. ""AMD External Events Utility"" probably just pulls the data from the driver. You can't access kernel mode with a simple file like that. Then what it does with the collected data is another question. No PII (Personally Identifiable Information) is recorded, no need for tinfoil hats. * AMD wtf is `TencentKVM`? * `AmdFineWine-KmUd.sys`  hehehe * `TODO: Write KMD Event Provider description`. \*aughhh.wav\* * `CWDDEPM_EVENT_OC_FUSE_BLOWN` middle\_finger\_gesture.jpg * Lower the pitchforks, AMD supports RDNA 1 and 2. They are *very* referenced. * Hi Jen from AMD! * Why does this string format seems familiar `%08x-%04x-%04x-%02x%02x%02x%02x%02x%02x%02x%02x`?  Latest driver 25.10.DID THEY JUST RELEASE A DRIVER???? I'M STILL WASTING TIME ON THIS ONE >:( (RemindMe! 1 hour ""boot Windows and update the driver, but check current version""), Ryzen 5600X, RX 9070.  I have one file from yesterday. Telemetry is disabled from the AMD app. Performance metrics are enabled tho.  No idea wtf it's recording. Then again, I'm just seeing what llvm-strings thinks are text strings in the driver the log refers to. Could be obfuscated, could not even be from this driver. All I can say for now is - it's probably updating the timestamp or it's logging DXGI communication. Literally 0 (zero) information from the referenced driver on what's actually being logged.  btw: reddit has tables? neat",AMD,2025-11-13 16:49:30,1
AMD,nonpj77,">**FILE\_ACTION\_MODIFIED** 0x00000003 The file was modified. This can be a change in the time stamp or attributes.  Are you sure it's not just updating timestamps? That shouldn't be writing directly to the disk. afaik it should update the MFT and that will be written all in one go (like running`sync` on Linux).  The file seems to be in binary. There is plain text, can see my brower executable. Also there is a crapton of `Navi` mentions. tail gives me this interesting driver (am on Arch Linux rn btw): `\SystemRoot\System32\DriverStore\FileRepository\u0420422.inf_amd64_cb23ea54e356fea3\B420106\amdkmdag.sys` followed by what I think is the driver build date or some other timestamp: `2510271353-25.20.21.01-251013`The date seems to be a earlier by 2 days than the latest driver, but the build number is kinda wrong. AMD, are you confused :)  Probably same stuff as in amdgpu but, interesting observations about the mentioned driver (via `llvm-strings`, tl;dr dumps text from compiled files; won't do decompilation or other analysis):  * HOLY SHIT THERE IS SO MUCH TELEMETRY IN THAT DRIVER. not spyware, hwinfo reads the exact same telemetry, the rest is hardware debugging texts. ***c h i l l***. * btw what's this file referring to `C:\DCSlog.log`? llvm-strings shows that it should be used by the driver. * Freesync is handled here, but stopping the service will not stop the driver. On Windows, services can be used to pull data from drivers. ""AMD External Events Utility"" probably just pulls the data from the driver. You can't access kernel mode with a simple file like that. Then what it does with the collected data is another question. No PII (Personally Identifiable Information) is recorded, no need for tinfoil hats. * AMD wtf is `TencentKVM`? * `AmdFineWine-KmUd.sys`  hehehe * `TODO: Write KMD Event Provider description`. \*aughhh.wav\* * `CWDDEPM_EVENT_OC_FUSE_BLOWN` middle\_finger\_gesture.jpg * Lower the pitchforks, AMD supports RDNA 1 and 2. They are *very* referenced. * Hi Jen from AMD! * Why does this string format seems familiar `%08x-%04x-%04x-%02x%02x%02x%02x%02x%02x%02x%02x`?  Latest driver 25.10.DID THEY JUST RELEASE A DRIVER???? I'M STILL WASTING TIME ON THIS ONE >:( (RemindMe! 1 hour ""boot Windows and update the driver, but check current version""), Ryzen 5600X, RX 9070.  I have one file from yesterday. Telemetry is disabled from the AMD app. Performance metrics are enabled tho.  No idea wtf it's recording. Then again, I'm just seeing what llvm-strings thinks are text strings in the driver the log refers to. Could be obfuscated, could not even be from this driver. All I can say for now is - it's probably updating the timestamp or it's logging DXGI communication. Literally 0 (zero) information from the referenced driver on what's actually being logged.",AMD,2025-11-13 16:50:26,1
AMD,nonpqmg,does this affect the linux kernel too or just windows?,AMD,2025-11-13 16:51:27,1
AMD,nonx803,I just want adrenaline control panel to stop opening every time I open file explorer or right click anything,AMD,2025-11-13 17:28:16,1
AMD,nooffmj,"I checked my system and the files were from 26 January and were only 1.5 MB or a little less than 500 KB in size, with less than 10 files in total. I deleted them, as I don't see them being used.",AMD,2025-11-13 18:55:50,1
AMD,nook98z,Is this only for radeon cards,AMD,2025-11-13 19:19:37,1
AMD,nooli30,my 7840u windows 10 laptop doesnt have the eeudumps folder.   freesync is enabled.  i use driver from [lenovo.com](http://lenovo.com) which the latest is adrenalin 24.10.,AMD,2025-11-13 19:25:48,1
AMD,nooltm8,Why not to simply prohibit anyone to write into that directory?,AMD,2025-11-13 19:27:24,1
AMD,nooqm3y,on windows 11 I got it in C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_1ab435de34589\\AMD\\Eeudumps,AMD,2025-11-13 19:51:31,1
AMD,noos6zj,Not seeing this folder on my system with my similar setup.  Are your integrated graphics enabled though?  I have mine disabled and wonder if it might be related to that.,AMD,2025-11-13 19:59:19,1
AMD,noq5upv,"Speculating about the technical side of this approach, the disk writes aren’t what worry me most. What concerns me is if this gets hooked into the resize/move Windows APIs and ends up blocking on I/O or flooding PCIe interrupts/DPCs every time those APIs are called by the user or any program. I can see that becoming problematic in the same way bad drivers misbehave and screw up latency. Potentially dropping frames, messing with audio, causing stuttering, jitter spikes, etc.",AMD,2025-11-14 00:24:02,1
AMD,nor700p,if you use everything ( the software  of voidtools) you see all the eedumps writes...i just checked now  so what s the fix? i need freesync ahah,AMD,2025-11-14 04:10:27,1
AMD,norkuip,"Is this a ""hidden"" Windows file? Which version of Windows is this? Even turning off hidden items, I can't find EEUDumps at all. Everything Search comes up empty, too. There's no ""AMD External Events Utility"" service, either. I'm using Win10 Pro, fwiw",AMD,2025-11-14 05:57:30,1
AMD,norqo1v,"Disabling the Event Viewer should be fine on IGP, right? Using the IGP on my 9800X3D for a second monitor.  Speaking of, anything else I can disable on the AMD side?",AMD,2025-11-14 06:49:11,1
AMD,nosqgwm,I don't have that folder at all. Maybe try cleaning out the driver fully with DDU and reinstalling?,AMD,2025-11-14 12:25:11,1
AMD,nosvgmr,Is this something that only affects you if you have a freesync capable monitor? All of the files in that path have 1year+ lastWriteTime for me:        Directory: C:\Windows\System32\AMD\EeuDumps     Mode                 LastWriteTime         Length Name     ----                 -------------         ------ ----     -a---          2024-07-01    15:39            512 multimediatelemetrymeta.txt     -a---          2024-06-15    02:44         975747 R-gpu-0-g6-c200-2024-6-14-21-34-45-735.log     -a---          2024-06-20    23:47        1048749 R-gpu-0-g6-c200-2024-6-20-16-34-23-819.log     -a---          2024-06-21    01:22         361176 R-gpu-0-g6-c200-2024-6-20-23-47-35-260.log     -a---          2024-06-29    23:50        1048813 R-gpu-0-g6-c200-2024-6-29-18-28-28-451.log     -a---          2024-06-30    00:52         272337 R-gpu-0-g6-c200-2024-6-29-23-50-0-553.log     -a---          2024-06-30    03:07        1048801 R-gpu-0-g6-c200-2024-6-30-0-54-7-239.log     -a---          2024-06-30    12:31        1048768 R-gpu-0-g6-c200-2024-6-30-11-52-6-775.log     -a---          2024-06-30    12:39         105511 R-gpu-0-g6-c200-2024-6-30-12-31-54-757.log     -a---          2024-06-30    17:09        1048689 R-gpu-0-g6-c200-2024-6-30-12-41-31-181.log     -a---          2024-06-30    17:52         902708 R-gpu-0-g6-c200-2024-6-30-17-9-40-458.log     -a---          2024-06-30    03:25         136005 R-gpu-0-g6-c200-2024-6-30-3-7-37-26.log     -a---          2024-07-03    03:05         271352 R-gpu-0-g6-c200-2024-7-3-2-51-28-147.log,AMD,2025-11-14 12:59:34,1
AMD,notnlb9,"Well, 9800X3D, 9070XT, B650 Tomahawk, Windows 10 Pro, and i DONT have this foler, you mention (C:\\Windows\\System32\\AMD\\EeuDumps), only folders inside AMD are ANR, amdkmpfd, amdfendr, amdafd.",AMD,2025-11-14 15:35:51,1
AMD,nou9lqt,"Can someone translate the useless technobabble in the link to the ""fix"" that doesn't actually contain a fix?",AMD,2025-11-14 17:25:00,1
AMD,nox4y0z,"For me it's also located somewhere else.  C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_a45773f484fe1fd0\\AMD\\EeuDumps  Funnily enough the file basically writes itself. If you have the logfile open in notepad++ and reload the content via hotkey, a popup will occur that tasks if you want to really reload the file and the switch from the notepad window to the popup and back will create entries in the file,...  Also the logfile is filled with weird special characters and even control characters.   Seems like a debugging option that made it to prod. AMD please fix.",AMD,2025-11-15 03:07:17,1
AMD,noyr0aq,I have to ask OP - do you have adrenalin software installed for iGPU? Solution from workaround doesn't work (tried command prompt and power shell),AMD,2025-11-15 11:53:37,1
AMD,np1rb1p,"Do you need the iGPU? With a single CCD chip like yours you honestly don't even need to install the chipset drivers for anything.   The service in question is related to the graphics driver IIRC, I don't install that on my nvidia machine since I don't use the iGPU.",AMD,2025-11-15 22:11:41,1
AMD,np25log,I have this folder but the files in it are tiny and haven't been modified in over a year. External Events service is running and FreeSync works.  Gigabyte B650 Gaming X AX  Ryzen 7 9800X3D  Radeon RX 7800 XT  AMD Software PRO 25.Q3.1 (32.0.21032.24)  Windows 10 Education 22H2 Build 19045.6456,AMD,2025-11-15 23:36:29,1
AMD,np4e8x2,"There is a solution , they can make logging file every 5 sec or less., First cache into RAM then move to SSD after like 5 sec.  Or I have OSFMount which makes part of RAM as Drive , AMD can do this.",AMD,2025-11-16 09:34:23,1
AMD,np5t2hu,Whatever. Reddit loves fear monger post.  FUD.,AMD,2025-11-16 15:56:02,1
AMD,np7jpkk,`C:\Windows\System32\AMD\EEUDumps\`  FWIW the directory does not exist on my PC and neither does the AMD parent directory.  Neither AMD or EEUDumps are showing as hidden directories either using dir /ah.     Running a Ryzen 3700 - RTX 3060 Ti.  `¯\_(ツ)_/¯`,AMD,2025-11-16 21:14:00,1
AMD,np88grx,"""Fun fact: I run an NVIDIA graphics card, but an AMD processor, and the chipset drivers apparently also include this service.""  NVIDIA+AMD and I do not have this problem.",AMD,2025-11-16 23:27:01,1
AMD,npbscho,"Also, here is my investigation regarding this service: https://www.reddit.com/r/radeon/comments/1oum697/attention_radeon_users_cpu_spikes_and_freezes/ I posted it on Reddit in the Radeon community.",AMD,2025-11-17 15:12:27,1
AMD,npcb3am,"Como solucionar:  Buscar las carpetas  \\AMD\\EeuDumps  en ambas carpetas ir a propiedades cambiar propietario en seguridad avanzado   luego cambiar permisos de TODOS a cambio total  luego abrir simbolo del sistema  eliminar carpeta EeuDumps:  rmdir /s /q ""C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_1da2893e2ffb6838\\AMD\\EeuDumps""  y creal enlace a nulo:  mklink /D ""C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_1da2893e2ffb6838\\AMD\\EeuDumps"" NUL:",AMD,2025-11-17 16:45:40,1
AMD,npe0qlp,Does this happen with older drivers as well?,AMD,2025-11-17 21:50:56,1
AMD,npe2es7,Doesn't it take like 35 years of daily writes to have a big impact? Lol. Either way u/amd_vik is this something the team can review?,AMD,2025-11-17 21:59:21,1
AMD,npegsim,"I wonder if this process relates to the AFMF bug I encounter a couple of  times that tanks FPS as you move the mouse.  Should try to replicate that, it happened to me by tabbing out of  POE or Subnautica on for hours and going back to play.   (I never saw this being discussed in r/amd so I assume its very specific to my system)",AMD,2025-11-17 23:17:56,1
AMD,noltql9,"thanks for sharing, with current nand prices affording a new drive actually has actually become a task. Wonder why this isn’t disabled from amd",AMD,2025-11-13 09:27:12,1
AMD,nouy5mw,"Fix For windows users  --  Locate where yours is running from  Mine is with latest chipset installer installed and located into this folder      C:\Windows\System32\DriverStore\FileRepository\amdfendr.inf_amd64_fbce97352464e3d7\AMD\EeuDumps   --  If you are unsure of where you have EeuDumps folder located, search for it and make sure you take the one with latest activity written to files, its possible you have multiple folders but is no longer used or written to because you updated drivers at some point, simply ignore the old dated ones.   --  Grab [gsudo](https://github.com/gerardog/gsudo) **This is needed to get highest possible access to overrule the file while its being live and writing.**  I took the portable zip file and extracted the x64 folder to C drive, now open cmd from this folder or dir to it  remember to run cmd from admin      type gsudo -n --ti  this will open a new window with gsudo running as trusted installer.  then run pin point it to whatever folder you have it in, like the one I described above which was my folder with igpu enabled.    --  Now for the fix I use is this   --      rmdir /s /q C:\Windows\System32\DriverStore\FileRepository\amdfendr.inf_amd64_fbce97352464e3d7\AMD\EeuDumps && mklink /d C:\Windows\System32\DriverStore\FileRepository\amdfendr.inf_amd64_fbce97352464e3d7\AMD\EeuDumps nul:  --  When you are done, delete gsudo again, and you are done.",AMD,2025-11-14 19:27:34,1
AMD,np1lj7m,"This doesn't shorten the lifespan of SSDs...if it was, we would have known by now, especially if it's been going on for a long time. My AM4 system has been active for 6+ years now and still using the same 2tb nvme since day 1. It's still getting the same speeds as the day I put it together.",AMD,2025-11-15 21:39:28,1
AMD,nosswu0,"the workaround doesnt work for me, i get access denied, im running cmd as admin of course and i took ownership of the files and folder",AMD,2025-11-14 12:42:32,0
AMD,nosufmi,Thanks for the heads up.  Doesn't seem to be happening here. As I move/resize windows I don't see any SSD activity.  Also the EeuDumps location in my machine is different.  I just disabled the AMD External Events service just in case. I use a 4090 as main display so that is not an issue. The iGPU still works fine (case screen).,AMD,2025-11-14 12:52:52,0
AMD,nozmvne,Thanks. I think this needs to be fixed ASAP by devs.,AMD,2025-11-15 15:24:07,0
AMD,np2apbe,"Can this be correlated with MPO In windows?  ""MPO support allows graphics hardware to compose multiple layers of content into a single image that it can then display on a screen. ""  I have weird freezes In game PUBG while using scopes on gun. I had this bug on Nvidia which i fixed by disabling MPO In registry.  Now on 9070xt I have also random freezes when using ADS (scope poping on screen, so it's like second window on game if somebody from reddit didn't lied)",AMD,2025-11-16 00:07:36,0
AMD,nosdyxa,"AMD gpu drivers have always been terrible but fanbois keep saying that ""you are holding it wrong"" or ""i have no issues"".",AMD,2025-11-14 10:39:42,-1
AMD,nomtx8n,"Uff that might explain my nvme wear.  My wife and I have 2 identical systems up to the graphic cards like literally identical except the gpu she got nvidia we even play mostly the same games.  My nvme got 83% health left hers 92%.  I was also wondering why my nvme was allways slightly warmer than my wife's ( same nvme same 3rd party cooler on it , same motherboards same bios and stuff )",AMD,2025-11-13 14:09:19,-4
AMD,noo1k53,OMG that's why my new NVME go to 97%  so fast. AMD you own me a new NVME,AMD,2025-11-13 17:49:33,-2
AMD,nomeamx,"AMD and absolutely dogshit drivers, name a more reliable duo?",AMD,2025-11-13 12:32:10,-14
AMD,nomqkkd,"u/lelldorianx, think you guys need to look into this.   >It has been noticed and posted [multiple](https://techestigate.com/amd-ati-drivers-filling-up-space-on-system-disk-in-eeudumps-folder/) [times](https://github.com/GSDragoon/RadeonSoftwareSlimmer/discussions/140) by [different](https://old.reddit.com/r/AMDHelp/comments/1brnh5c/constant_writes_to_windowssystem32amdeeudumps/) [people](https://pc-help.cnews.cz/viewtopic.php?f=46&t=226514), that AMD's drivers constantly writes to logfiles in `C:\Windows\System32\AMD\EEUDumps\`. Yesterday I found out, just how bad it really is. I have recorded a video: https://www.youtube.com/watch?v=Zw1yN0eq5zw  In the video, I have a program running that records every change to the hard drive (using [ReadDirectoryChangesW](https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-readdirectorychangesw)) Every new line in the console window is a modification of the file. Notice how they completely stop when I disable the ""AMD External Events Utility"" service.   Every time you move a window or resize a window on your desktop, those files get modified multiple (hundreds!) of times.  The writes seem to be caused by the service ""AMD External Events Utility"", disabling it immediately stops the writes. Apparently this service handles things like [FreeSync](https://old.reddit.com/r/Amd/comments/8bvcfo/do_not_disable_amd_external_events_utility/) which has been confirmed by an AMD engineer.  There might be other oocasions, where the files are written to, but these are the most egregious situations I could find.  Nowadays, of course, most people will use SSDs, especially for their system drives. SSDs have a limited amount of bytes written before they inevitably degrade and die. Now just think about how much lifespan AMD takes of SSDs globally doing this. In my research I found that this has been an ongoing issue for multiple years.  Fun fact: I run an NVIDIA graphics card, but an AMD processor, and the chipset drivers apparently also include this service.  Testing for yourself: I've created a simple tool to watch for writes to that Folder. You can download it from my [GitHub](https://github.com/tGecko/AMD-EeuDumps)   I have kept the code simple and GitHub compiles it automatically, so it's guaranteed that the .exe represents the code in `AMD-EeuDumps.c` and can easily be verified. Your Browser and Windows will probably complain about the file, simply because it's an unknown .exe. Alternatively, compile the exe yourself from the source code using `cl.exe /O2 /Fe:AMD-EeuDumps.exe AMD-EeuDumps.c`  Workaround: I applied [this](https://github.com/GSDragoon/RadeonSoftwareSlimmer/discussions/140#discussion-7792873) workaround, redirecting writes to the whole folder to nul: so they never touch the disk. It seems to be working fine for me, but apply it at your own risk.  EDIT: Had a mistake in this post, changed the path to the correct one `C:\Windows\System32\AMD\EEUDumps\`  EDIT 2: My relevant specs:   Asus TUF Gaming B650-E Wifi   AMD 7800X3D   NVIDIA 4070 Super   AMD Chipset software 7.06.02.123",AMD,2025-11-13 13:49:48,-4
AMD,non6prd,Remember guys to click and subscribe....  I mean it's not a bad advertising tactic to try to bump channel numbers up.  Such folders do not exist.  Using driver package 25.9.1 on a 9800 X3D + 7900 XTX.  Different drives place logs in different places and it doesn't matter because its tiny.,AMD,2025-11-13 15:18:05,-6
AMD,non7z47,"""why are nvidia cards so expensive?""",AMD,2025-11-13 15:24:26,-4
AMD,nom0sea,and thats why you stay away from amd,AMD,2025-11-13 10:38:40,-29
AMD,nom1cgi,This is the real question that requires an answer.,AMD,2025-11-13 10:44:00,153
AMD,nomihl6,"It screws up more than that. You'll get no external events, which includes hot plugging peripherals to the GPU, monitors w/ compatibility profiles, certain user settings, and more.    Just so you know, this 'bug' isn't actually a bug, and it's a normal part of driver logging, and it would take **decades** for it to write to disk to the point it would cause issues. The write rates are too low to worry about.    Don't want extra writes on your SSD? Don't use Windows, because pretty much every other driver does about the same.",AMD,2025-11-13 13:00:50,286
AMD,nomk9tn,"I can answer that. It's comically negligible. Your Windows Defender will write more data to disk in a day than this will in weeks.   It's not a secret that Windows has a write 'problem' overall, because compared to Linux, it practically never lets the drive idle.    However, even with that in mind, it's still nothing a modern SSD couldn't handle, hence the quotations.",AMD,2025-11-13 13:12:14,219
AMD,nom1fu0,"According to the next few responses, about the expected range of TBWs. And nothing to concern about in the next 2 decades of using whatever current SSD in your system.   I also came to the comment section to say the same as you. How much is actually being written and is it enough to kill an SSD in less than 15 years.",AMD,2025-11-13 10:44:52,113
AMD,non1lkn,"SSDs can be dumb. Games, for example, have murderer SSDs due to bugs in the games themselves.",AMD,2025-11-13 14:51:40,10
AMD,nomk802,"Yes, there is a cache.  Yes, it will probably reduce the actual write operations slightly. No it won’t magically make this not egregious.   When those appends are written, the entire 4kB is read off disk, the 100B appended and then the 4kB chunk is written to back to disk.    If you didn’t align your filesystem to disk sectors, this could be even worse.  If you’re using 8kB block size - or 512b mode on the SSD it will also have amplification implications.",AMD,2025-11-13 13:11:55,11
AMD,nom39ce,same for me but I'm on W10 and 25.8.1 driver,AMD,2025-11-13 11:01:56,9
AMD,nosuqm9,Don't do this. It's a none issue that looks massive to people that may not know about it without anyone actually understanding how PCs work.  Just leave it.,AMD,2025-11-14 12:54:54,4
AMD,nolufpe,"For me it's also only 1 file, but it gets written to constantly. It might be that the path differs under different circumstances.",AMD,2025-11-13 09:34:42,19
AMD,nomo5x4,"This was the path for me (W11 Education, 5800X3D, 9070XT, 25.10.2 drivers): C:\Windows\System32\drivers\DriverData\AMD\MmdDumps",AMD,2025-11-13 13:35:49,2
AMD,nook9oe,Big Flash wants your SSD to fail faster,AMD,2025-11-13 19:19:40,14
AMD,noma3w3,Good thing i use firefox? Or is it the same?,AMD,2025-11-13 12:00:32,22
AMD,nor27qn,"the biggest shame is they do not have a simple way of moving the cache into another drive.   We had that in many software back before 2010, where you can install the software to the drive you picked and have the cache on the drive you picked.   Software like Chrome, Sportify make it difficult to change it. They always seems want to be on OS drive, I do not understand why.",AMD,2025-11-14 03:38:44,4
AMD,noo5nnm,Have you found a workaround for the media cache using the hard drive/SSD?,AMD,2025-11-13 18:09:30,0
AMD,nooxq7v,It'd probably take even longer due to write caching and write coalescing.,AMD,2025-11-13 20:27:19,8
AMD,nom0t26,"Thanks for your input, very valuable.  I agree it's definitely not a catastrophe, but it's something that's been going on for years. I'm hoping this gets enough attention for them to fix it.",AMD,2025-11-13 10:38:51,17
AMD,noofqpd,So it could essentially never hurt my p5800x and p5801x Optane drives 😂😂😂,AMD,2025-11-13 18:57:19,2
AMD,nomyubo,"Not an issue in any sense. Logging events is and has always been an important part of being able to triage and fix issues.   This is why the reporting tools exist. You file a report and the tools will provide the logs around the time the event happened.   It's equivalent to never deleting text messages on your phone. The data being used is so insignificant that it would take decades of daily use, 1k+ messages a day, for a problem to eventually happen.   But there's a non trivial number of people who don't delete text messages and have amassed hundreds of thousands of messages and it doesn't matter, because it uses so little space.",AMD,2025-11-13 14:36:57,2
AMD,nos07l7,"Windows does not utilize the concept of everything is a file, it has made far worse os design decisions lol.",AMD,2025-11-14 08:20:49,1
AMD,nom66xu,"Yes, I just did another test and I can watch the file grow  https://www.youtube.com/watch?v=hga32IiH8lk",AMD,2025-11-13 11:28:26,0
AMD,noq2ljh,"Here there were some files but all were from 2024 or older. Doesn't look like a folder that is normally used, much like ""dump"" files or folders tend to be.",AMD,2025-11-14 00:05:14,3
AMD,nopqagg,This is an important question.,AMD,2025-11-13 22:53:54,3
AMD,nomolpo,I do that anyway to free up ram.,AMD,2025-11-13 13:38:23,3
AMD,nom2uqb,"Thanks for the hint disabling the iGPU. I had it disabled, but recently updated my bios and might've forgotten to disable it again.",AMD,2025-11-13 10:58:09,3
AMD,nomquo2,"The iGPU thing seems plausible.  I have a 7800X3D as well and changed my build a few months ago, along with a Windows reinstall, and that's exactly when the file was last changed for me.  I'm pretty sure I booted a few times with default settings, before changing the BIOS and disabling the iGPU.",AMD,2025-11-13 13:51:25,2
AMD,nomp3pb,Where in the bios do you disable the onboard GPU? I must be blind.,AMD,2025-11-13 13:41:19,1
AMD,nomouos,Well it does. SSD/M2 only have a certain amount or read/writes however miniscule it is it all adds up.,AMD,2025-11-13 13:39:50,-9
AMD,nomlqt2,I do not have that folder.,AMD,2025-11-13 13:21:17,1
AMD,noyri32,"+ replay recording, browser data, game updates, system updates, system telemetry, driver telemetry, system temp files and logs.... I have no idea, why if something is depended of AMD, but do things like other software since years (!) is automatically viral drama. Is this suppose to make AMD price on the stock marked drop?",AMD,2025-11-15 11:57:59,3
AMD,npbb4u2,"I also have logs in `C:\Windows\System32\DriverStore\FileRepository\amdfendr.inf_*\AMD\EeuDumps`. I accidentally noticed this in May 2025, and since then I've been disabling the `AMD External Events Utility` service after every driver update. I have an RX 6600, Win10.",AMD,2025-11-17 13:35:04,1
AMD,nonlq5r,"Thinking about that, it should be possible to monitor the service and see which registry keys or other resources it reads. Maybe it gets the log level from the registry or a config file where one could adjust to less logging?",AMD,2025-11-13 16:31:50,2
AMD,noqbesv,https://i.redd.it/fl2pm6c6f41g1.gif  Very weird. What are these Notifychangedirectory operation when I resize/move window?,AMD,2025-11-14 00:56:25,1
AMD,nophpf0,P4800X 375gb write endurance go brrr,AMD,2025-11-13 22:07:48,2
AMD,nolz51y,"The size doesn't grow for mine either, it just gets overwritten constantly.",AMD,2025-11-13 10:22:17,7
AMD,noo3qyh,Amd never fix some bugs I reported on 2020 lol,AMD,2025-11-13 18:00:12,3
AMD,nom82i8,"By that time, it's actually possible that one SSD has broken due to this issue.",AMD,2025-11-13 11:44:17,10
AMD,nomp0nf,Because it isn't. Other processors do the same and ssd's are quite smart to not always access their memory bank.,AMD,2025-11-13 13:40:50,5
AMD,nonremu,"Do people actually assign D to an SSD? Like A and B for floppies, D will forever be reserved for an optical drive to me, even in a computer without one.",AMD,2025-11-13 16:59:33,2
AMD,nolx7ge,I have a feeling that this is due to igpu being enabled for him,AMD,2025-11-13 10:03:09,6
AMD,nolx76r,"You have a Nvidia GPU, you won't have AMD GPU drivers. Your AMD CPU also lacks a iGPU.",AMD,2025-11-13 10:03:05,-4
AMD,nose3ud,trolling much?,AMD,2025-11-14 10:40:59,-2
AMD,noo2jqn,"Fun fact, I wrote the task manager in the video myself.",AMD,2025-11-13 17:54:22,6
AMD,nopv0um,"No, Windows in general loves accessing drives all the time.",AMD,2025-11-13 23:20:43,3
AMD,nonphfg,I will be messaging you in 1 hour on [**2025-11-13 17:49:30 UTC**](http://www.wolframalpha.com/input/?i=2025-11-13%2017:49:30%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/1ovw0qf/amd_drivers_performing_hundreds_of_ssd_writes/nonpcfi/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F1ovw0qf%2Famd_drivers_performing_hundreds_of_ssd_writes%2Fnonpcfi%2F%5D%0A%0ARemindMe%21%202025-11-13%2017%3A49%3A30%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201ovw0qf)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2025-11-13 16:50:12,1
AMD,noro8ru,I thought I'm the only one on this 😂 Any solution?,AMD,2025-11-14 06:27:24,1
AMD,noul4sa,It's been suggested that C:\Windows\System32\drivers\DriverData\AMD\MmdDumps is the equivalent folder.,AMD,2025-11-14 18:22:26,1
AMD,nos2fgd,Because it won't cause any noticeable degradation to your drive.,AMD,2025-11-14 08:43:31,6
AMD,noy7l3j,"Upvote this, easy and useful, thanks !  Just need to change  amdfendr.inf\_amd64\_\* to your location.",AMD,2025-11-15 08:34:53,0
AMD,nos2m36,It doesn't cause this much degradation. Not even close,AMD,2025-11-14 08:45:22,4
AMD,nomj3qn,nvidia and trash drivers who burn gpus,AMD,2025-11-13 13:04:48,2
AMD,noo11my,"It's a 26 sub channel that has existed since 2006.  It's pretty obvious they are not audience farming, but sharing something they think is worth sharing.  Especially with a linked repo and a pretty thorough write up.",AMD,2025-11-13 17:47:03,4
AMD,nonbt3x,"> I mean it's not a bad advertising tactic to try to bump channel numbers up.  I could not care less about the channel. I am not planning to grow it at all. I use it for random video sharing like this, the video is unlisted so it does not even show up on YouTube organically.  I initally uploaded it to streamable.com but they only keep videos for 2 days.",AMD,2025-11-13 15:43:30,1
AMD,nom4n4c,Yeah and Nvidia installs a node.js server on your computer making it vulnerability,AMD,2025-11-13 11:14:41,10
AMD,nomlzcm,I wonder if nvidia does the same things. Maybe we should research it too. 🤔,AMD,2025-11-13 13:22:43,38
AMD,novmjo6,This\^\^  The windows log writes more than this.,AMD,2025-11-14 21:35:26,3
AMD,nomox33,Wouldn't it be possible to just keep the logs in a queue and flush it every few seconds or something   I feel like there's probably a better way to do this lmao,AMD,2025-11-13 13:40:15,7
AMD,npf0lxj,"I disabled the “AMD External Events Utility” and it hasn’t caused any issues for me, although I’m only using the AMD processor’s iGPU to connect an old secondary monitor without FreeSync. I don’t game on that monitor.",AMD,2025-11-18 01:13:56,1
AMD,np1kx7t,"Disagree. Logging is ok, excessive logging is not and only needed mostly during debug/troubleshooting sessions. There's no reason to log ""every pixel move"" during window resize or log any normal window events. If you need this for dumps in case something goes wrong, then the driver is in a bad shape as such basic functions should have been ironed out aeons ago and should not need tracing nor cause issues. If you want to log such things then do a ram-backed circular buffer or smth. Also give us an option to disable most of this stuff (with a proper disclaimer that not all info will be gathered in case of an issue). This is just lazy. Dedicated stable branch built on years of optimizations my @$$ (with a dozen unacknowledged issues that wont get fixed).",AMD,2025-11-15 21:36:08,-1
AMD,nomw3bw,"Windows *shocks* me with how frequently it spins up my hard drive. I’m only storing game directories on there, so I’m not sure what it’s doing. I nearly forgot what my HDD even sounded like after using Linux for a few months.",AMD,2025-11-13 14:21:30,62
AMD,noo1ihj,"modern windows will use every IOP a HDD has.   The good news for SSDs is that they have THOUSANDS more IOPS. Sometimes tens of thousands, some SSDs hit over 1M IOPS.",AMD,2025-11-13 17:49:20,10
AMD,nomm8t4,"Hey, don't ruin AMD hate train with logic like that. /S",AMD,2025-11-13 13:24:19,30
AMD,nomjzzv,"Depends on type and write capacity too, some do much better than others. I've owned ssds that long (15 years) and used them without issue, DYOR applies. Took a punt this time, NM790 TLC 4tb and will add another in future, should go the distance, 2 years no issue.",AMD,2025-11-13 13:10:32,8
AMD,np2vjjb,SSDs have gotten better. Nobody worries about SSDs failing due to writes if they are modern.,AMD,2025-11-16 02:12:05,2
AMD,noo0v8w,"> When those appends are written, the entire 4kB is read off **cache**, the 100B appended and then the 4kB chunk is written to back to **cache**.  And then after enough of those to fill up the block, it's written back to the disk once.",AMD,2025-11-13 17:46:10,3
AMD,noojdqh,Could you share some of your computer's history?  What version of Windows? Was it updated to Windows 11 from Windows 10? Was it update from Windows 7 to Windows 10? Etc.?  I think this is an issue specific to a very particular set of circumstances.,AMD,2025-11-13 19:15:16,3
AMD,nomaft8,It's safe to assume anything that caches is writing to disk.,AMD,2025-11-13 12:03:10,46
AMD,nop66sb,You can disable disk caching entirely with a couple changes in about:config.,AMD,2025-11-13 21:10:12,5
AMD,nomdg6y,I often leave 5-10 480p Twitch streams open overnight in Firefox and I don't see anywhere close to that figure (around 13GB for 5hrs assuming it doesn't stop writing at some point as I only tested for a few mins).,AMD,2025-11-13 12:26:00,6
AMD,np93y46,"[https://bugzilla.mozilla.org/show\_bug.cgi?id=1304389](https://bugzilla.mozilla.org/show_bug.cgi?id=1304389)   [https://discourse.mozilla.org/t/regarding-firefox-and-heavy-disk-usage/106293](https://discourse.mozilla.org/t/regarding-firefox-and-heavy-disk-usage/106293)  [https://bugzilla.mozilla.org/show\_bug.cgi?id=1763978](https://bugzilla.mozilla.org/show_bug.cgi?id=1763978)  If anything, Firefox suffer the same fate or even worse. Chrome somewhat can disable the cache with --disk-cache-dir=/dev/null but I don't think it's really worth it.",AMD,2025-11-17 02:33:33,1
AMD,nou38mr,"Physically move the folder, then symlink it back to where it thinks it should be?",AMD,2025-11-14 16:52:53,1
AMD,nom3z7u,"It’s not just “not a catastrophe,” it’s not a problem at all really.  This level of logging is entirely normal and there are 25 other programs doing the same thing constantly on your system.",AMD,2025-11-13 11:08:37,39
AMD,nomrmai,"This isn’t an issue. The disk is _constantly_ being written to by applications. Most modern QLC SSDs have about 150TB write endurance. A file like this writing 1MB/s would take nearly 4 years and 9 months, and that would mean you would have to be moving windows the entire time. Let’s say you were moving windows for 2 hours, which I still consider excessive, it would take 47 years.",AMD,2025-11-13 13:55:49,8
AMD,npefy2u,"Without putting this in context of windows default logging, it is not something I would put any weight on.  Unless it is doing something stupid like hogging a resource needlessly for writing (cue Asus ACPI recent bug), there are way worse performance hogs to worry about (cue Asus ACPI recent bug again).",AMD,2025-11-17 23:12:59,1
AMD,not1ihj,"I never said Windows did, but rather that AMD may have implemented this as a part of their cross-platform driver suite, to unify other aspects.",AMD,2025-11-14 13:36:55,1
AMD,nom6lzn,"That doesn't test my suggestion, you'd need to read the path from another OS to see if it's dynamically added to the filesystem via AMD drivers, or truely there when Windows is shut down. A dual/live-boot Linux image could do this.",AMD,2025-11-13 11:32:03,15
AMD,nomq7b1,"each MB brand bios is different, just google it specifying your MB model, just beware, once its disabled, you will no longer be able to boot without a discrete graphics card unless u reset the bios.",AMD,2025-11-13 13:47:41,3
AMD,nompp70,Sure but this will not degrade the lifetime of the SSD in any meaningful way. If you've installed one AAA game through steam you've already written as much data as decaded of non-stop window moving.,AMD,2025-11-13 13:44:49,20
AMD,nomzcs3,"Motor vehicles have a certain amount of mileage you can use it for.   This is the equivalent of driving 10ft at a time. Sure it all adds up, but it's not going to matter because that drive to work everyday for a few miles is so much more wear and tear",AMD,2025-11-13 14:39:45,5
AMD,noxxm00,"Then people should stop using windows. The second you install it, it writes and reads from your drive without you doing anything constantly.  Your cpu only has so many power on/off cycles, should i not use my pc?   There is always a trade off between using and not using your components. Silicon wears with use. This use for the ssd is so miniscule that it doesn't even matter in a decade.",AMD,2025-11-15 06:54:49,2
AMD,novh9d5,"This is Explorer watching the folder for changes because it has it open, and wants to show you if anything changes. Just close most windows before watching events so that you get less noise",AMD,2025-11-14 21:08:05,2
AMD,nolze3k,"TBW is in the expected ranges, so it seems to be a non-issue. At least on my end.",AMD,2025-11-13 10:24:50,4
AMD,nos29h9,"And OP will be there ""I told you so""",AMD,2025-11-14 08:41:49,3
AMD,nonxkjd,"In Windows/DOS fashion, A and B are reserved for floppy drives.  C is reserved for the hard drive.  There really isn't a true convention after C.  You just use the next letter.  I find it more sensible to set the first letters to hard drives and then the next drives are optical drives.  Apparently, network drives start from Z and go descend.  If I can, I would rather reserve the optical drives to A and B.",AMD,2025-11-13 17:30:00,2
AMD,nolxdcx,"Read dude, the guy said, ""Fun fact: I run an NVIDIA graphics card, but an AMD processor, and the chipset drivers apparently also include this service."" imply that the chipset driver could have this. Otherwise, I wouldn't bother to comment",AMD,2025-11-13 10:04:47,17
AMD,notguvx,"i haven't found one, and it has to be a setting somewhere because most of my friends have AMD gpu's as well and they don't have the same issue",AMD,2025-11-14 15:01:49,1
AMD,nouphqh,"I FOUND IT!!!  https://pcforum.amd.com/s/question/0D5KZ0000122gW20AI/when-you-rightclick-on-the-desktop-the-amd-adrenalin-application-opens-how-fix-this  i had to go to the last option to get one to work. i just changed the name of the RadeonSoftware application in program files / AMD / CNEXT / CNEXT from radeonsoftware to radeonbullshit.  still works from the system tray icon when i need to, no longer opens just from right clicking the desktop. such relief!!!",AMD,2025-11-14 18:43:53,1
AMD,nouoz5x,"C:\\Windows\\System32\\DriverStore\\FileRepository\\amdfendr.inf\_amd64\_a45773f484fe1fd0\\AMD\\EeuDumps  That was my location. They're here, but i dont see it as a problem. You really would have to stop using windows, as it log things too, use temp files for everything, and dont use internet browser :)  Also Nvidia uses telemetry too.",AMD,2025-11-14 18:41:20,3
AMD,nov7hw1,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-11-14 20:16:15,1
AMD,nosgmw6,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-14 11:04:32,0
AMD,nom6m5o,"so they both suck, great",AMD,2025-11-13 11:32:06,-4
AMD,nommi12,"I don't even have to check to tell you that yes, they do.   Logging is very important to develop good drivers. People want good drivers right?",AMD,2025-11-13 13:25:52,88
AMD,novhye7,"nVidia did a similar thing (often rewrote file `nvtopps.db3`) in 2021  ([1](https://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/460415/what-is-nvtoppsdb3/),  [2](https://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/464453/nvtoppsdb3-is-being-constantly-overwritten/)),  but that was fixed quite fast after the issue got known.",AMD,2025-11-14 21:11:44,7
AMD,nowkib8,Nvidia has already done this yes [https://learn.microsoft.com/en-us/answers/questions/3823165/constant-disk-writes-by-nvidia-process](https://learn.microsoft.com/en-us/answers/questions/3823165/constant-disk-writes-by-nvidia-process),AMD,2025-11-15 00:54:49,8
AMD,noniycn,"I’ve seen Nvidia telemetry do crazy amounts of writes, like 10+ gigabytes in a few hours. (I keep a close eye on how much data is being written to my SSDs.) It might have been a bug, but I had to disable telemetry",AMD,2025-11-13 16:18:13,11
AMD,non1bat,"Sure, which scenario do you prefer more?    A. Keeping everything in memory and hearing people complain about high memory usage.   B. Periodically writing to a file cached on disk, without any performance overhead.",AMD,2025-11-13 14:50:12,12
AMD,noprktb,"You don't want to lose the lasts logs that happened just before a driver crash, so all logs must be immediately written to disk.",AMD,2025-11-13 23:01:00,2
AMD,non1xp2,"Funny you say that. Just the other day I mounted my long-term backup drive, and I was like: the hell is that?!   For a split second, I thought the bearings on one of the fans were giving up, heh.",AMD,2025-11-13 14:53:25,27
AMD,nonrzpk,"Same, I had to modify my external hard drive enclosure to stop its  standby function, since Windows kept waking it up constantly for no reason and there's a 20s delay every time..",AMD,2025-11-13 17:02:28,10
AMD,nonlvz4,"If it's not your main drive, see if Windows is using it as a pagefile. I think that can cause it to spin up frequently.",AMD,2025-11-13 16:32:38,6
AMD,nooxztq,Huh… I was curious why my hdd was always spinning when it wasn’t being used.,AMD,2025-11-13 20:28:40,0
AMD,npahixi,For QLC it can matter unfortunately.  I've had 4 QLC SSD's die at work (they came with the pre built PC's we use) and we weren't even doing anything all that crazy.    We only buy TLC SSD's now.  I only use TLC at home too.,AMD,2025-11-17 09:23:12,1
AMD,noo39aa,"Not true at all.  Especially not true if you’re doing a synchronous write.    The cache is holding operations to make the disk appear artificially faster.  It will not sit and hold a 4kB chunk open all day until it fills.  It will write and rewrite that chunk thousands of times.  If the SSD doesn’t have a supercap in it, it’s even more aggressive about flushing that cache to disk.  You’d have to drive pretty deep into what the controller is doing AND what windows is doing at the filesystem level AND what type of write command AMD is issuing to that file - but I assure you multiple writes per second are happening on a disk no matter what.",AMD,2025-11-13 17:57:50,6
AMD,nook4f9,"About a 4 weeks old 25H2 install. Very basic.       gcim Win32_OperatingSystem | select Version, InstallDate     Version    InstallDate     -------    -----------     10.0.26200 10/14/2025 7:34:42 AM",AMD,2025-11-13 19:18:57,1
AMD,nopn7ie,What would be the downsides of disabling the disk caching?,AMD,2025-11-13 22:37:09,7
AMD,nomoy6u,Mind if I ask why you open 5-10 480p streams open overnight?,AMD,2025-11-13 13:40:25,13
AMD,npayfmr,Why wouldn’t you set their resolution to 160p?,AMD,2025-11-17 12:06:36,1
AMD,nomayoi,"I know of no program that writes logs at a rate of 1MB/s over an extended period of time (i.e. longer than 1-2 seconds) without that being a bug.  Do you know how much text 1MB is? I'm a software developer and I have access to a few systems that just run fine, but they log a bunch of stuff regardless. Do you know how big a log file is at the end of the day? Usually around 700KB at most.   So a program that thinks it needs to log a total of roughly 250-300 Din A4 pages of data every second very obviously either isn't behaving correctly or is badly written.",AMD,2025-11-13 12:07:13,23
AMD,noohmxw,I don’t know about you but I don’t like the sound of degradation while gaining nothing. I just got this SSD this year.,AMD,2025-11-13 19:06:37,5
AMD,nomjkft,"How often does anyone look at the logs? For most people, never? And not a ""we need this data in case something goes wrong"", because even when something goes wrong, I'm pretty sure those logs still aren't used most of the time.",AMD,2025-11-13 13:07:47,2
AMD,noojkq7,"At this speed? Can you give some examples of that, because I don't know of any such logs in my computer, except when I get some big error on a high speed task of the software I'm coding.",AMD,2025-11-13 19:16:13,1
AMD,nomgjof,Other apps doing it too doesnt make it a valid behavior we should ignore. By that you promote the accumulation of little issues. That behavior is only relevant to troubleshooting a particular issue once in a fullmoon. The accumulation of unnecessary degradation of our Hardware due to multiple applications enabling such granular logging by default could easily be avoided by making it a default-off switch.,AMD,2025-11-13 12:47:57,-1
AMD,notbw3z,That would directly violate multiple things in Windows and would necessitate implementing a deeper bridge layer to make this work. Also the Windows Driver and the Linux/Unix\* Driver are related but separate codebases because Windows device drivers have some fundamental differences to how every other platform does device drivers.,AMD,2025-11-14 14:35:31,1
AMD,nopbdvn,The 5700x doesn't have an igpu,AMD,2025-11-13 21:36:00,1
AMD,np053iq,the amd chipset driver installs the igpu driver (which contains this service) *only* if you have a ryzen with igpu.,AMD,2025-11-15 17:00:04,1
AMD,nolxjbt,True that. But OP implies it happens when resizing a window. I doubt any non graphic device could trigger that behavior.,AMD,2025-11-13 10:06:24,0
AMD,nonlg96,It should be possible to turn the log level down for a user.,AMD,2025-11-13 16:30:28,16
AMD,noo0rio,"I agree that logging is useful, but meaningful and configurable one is even better. Sounds like too much logging in this case, no?",AMD,2025-11-13 17:45:40,11
AMD,noo7sea,"Can't they use ram for this? And save after 5 minutes or so, not every frame",AMD,2025-11-13 18:19:37,7
AMD,nop0ctk,"Sure, I bet it's the same thing.  Just to be clear, I'm not trying to imply that they are better, quite the opposite. That's why I think it would be fair to check this on nvidia (or intel) side too, specially considering that OP has an nvidia gpu.",AMD,2025-11-13 20:40:45,1
AMD,nomoyoq,I'm sometimes not sure about AMD users. They've been defending them pretty hard when they were legit terrible not too long ago.,AMD,2025-11-13 13:40:30,-14
AMD,notfj8x,Uuhhh....HOW? Just how would those Dev's actually \*get\* those files from my PC? That's what I want to know.,AMD,2025-11-14 14:54:57,0
AMD,nomw6rj,You mean they still haven't harvested enough driver logs to fix CP2077 crashes? Damn.,AMD,2025-11-13 14:22:01,-15
AMD,np15oci,"Yeah, it's important to remember these things when there's such a huge effort to bad-mouth AMD all the time. Thanks for sharing this.",AMD,2025-11-15 20:11:16,0
AMD,non4v3n,"A log file *really* shouldn't use that much space, but even if it needs to, can't you just flush to disk after a certain threshold has been reached? It's clearly not doing that here at least  Plus, there's definitely a significant performance overhead of constantly writing to disk like that if you're on a slow HDD (not to mention how it almost certainly uses up the same amount of RAM as you'd get by caching it anyway)",AMD,2025-11-13 15:08:37,14
AMD,noon4un,"It's impossible to do B without doing A. Or do you not know what ""periodically"" means?",AMD,2025-11-13 19:33:58,3
AMD,npec9d1,"Let's not weep over the 1mb of spilled ram, shall we?    By the way, without weighting on the merits of the OP, the described behavior is not _periodical_, it's _triggered_ at every move. To do it periodically you would need a data queue in memory, like the comment you (ineptly) answered  had proposed.",AMD,2025-11-17 22:52:01,1
AMD,nosfa0o,i want to lose them  thats why i have put this folder on a ramdisk via symlink,AMD,2025-11-14 10:51:54,0
AMD,nookvow,Local account or Microsoft account login?,AMD,2025-11-13 19:22:44,1
AMD,now4juu,"Maybe higher RAM usage when the browser is open? I really haven't noticed any downsides in my usage. Cookies and cache are two separate things, so the other commenter's reply is not relevant. Sessions and logins are still saved.",AMD,2025-11-14 23:14:47,1
AMD,npef1r2,"Caches exists to avoid some other ressource usage (like available ram, or costly computation, or time incurred by transmission delays).  Disk cache is most often to avoid   RAM usage.    So while you could expect more ram used, the OS transparently spills ram to disk for more vital processes. But the heuristics it uses to spill probably are less reliable than the application, so it's possible you get more writes to ram _and_ disk by disabling firefox cache, depending on how right/wrong the OS guesses given the amount of available ram. It's one of those things the OS does automagically for applications.",AMD,2025-11-17 23:07:51,1
AMD,nou3ghp,"Not op but I know your browser writes your cookies to disk, if cookies never get written there's no way of staying logged in or preserving sessions on most websites.",AMD,2025-11-14 16:53:58,0
AMD,nomp5f6,Mostly Twitch drops but some channels I like to collect channel points.,AMD,2025-11-13 13:41:35,1
AMD,noqxt85,I Paid For 100% Of Computer I Will Use !00% Meme,AMD,2025-11-14 03:10:42,1
AMD,npazl07,"Because when I'm awake and running them in the background and just want to check what's going on in the stream, you need 480+ for it to be watchable. Setting lower just before bed is too annoying since I'll also need to set it again when I wake up.  That and it doesn't really make any difference on resources, maybe if I had something crazy like 20+.",AMD,2025-11-17 12:15:45,1
AMD,nonj5eu,"well this isn't writing logs over an extended period of time. You're not constantly resizing windows or dragging them around all over the place right? You do it maybe once every few minutes, if that. Maybe once on app startup, maybe once to change between fullscreen and windowed in a video.",AMD,2025-11-13 16:19:10,19
AMD,nopss1x,"It should definitely be fixed, don’t get me wrong.  It’s just not harming your disk or something to panic about.",AMD,2025-11-13 23:07:50,1
AMD,noprv1h,"You should uninstall your operating system then, because that’s what it does nonstop.  I get that they should clean up their logging and not be so verbose, but I just think y’all should stop being so scared of something that is absolutely _normal computer operation_ that SSDs are designed to handle thousands of times over without issue.",AMD,2025-11-13 23:02:36,11
AMD,nopsfbp,"“At this speed” at what speed? Text lines zooming by in a file watcher mean nothing. Computers do this all day every day, it’s not that big a deal. The writes are buffered anyway so it’s not like your SSD is suddenly collecting millions of individual writes. It’s just dropping a couple megabytes on the disk every few seconds, no big deal.  Should they fix it? Of course. But don’t panic.",AMD,2025-11-13 23:05:48,0
AMD,nolxpx5,"Doesn't mean, the package doesn't install the driver that I wouldn't need, beside, he said ""Freesync"" which open to even G-sync monitor as long as it's compatible.",AMD,2025-11-13 10:08:12,1
AMD,noocz8l,"\> I agree that logging is useful, but meaningful and configurable one is even better.    You already can disable logging data.      \> Sounds like too much logging in this case, no?   No. Logging window data while it's being moved is not excessive, it's just how DWM works.",AMD,2025-11-13 18:44:04,13
AMD,nophehy,"Probably the *most* ""interesting"" logs would be just before a system crash, so there's always a trade off of losing those.",AMD,2025-11-13 22:06:11,34
AMD,nood9gg,"No, because RAM is volatile, which means when it loses power (like when your PC crashes, or reboots), all data is lost.",AMD,2025-11-13 18:45:24,21
AMD,nomqjs6,"Frankly, I don't care about any of that. I'm a software developer, not a politician.",AMD,2025-11-13 13:49:40,23
AMD,np2vfhb,Huge effort? Its one dude making a post who is surely an AMD fan. Why spin it like its some conspiracy theory against AMD when 95% of all the shit talk is about NVIDIA?,AMD,2025-11-16 02:11:23,2
AMD,nondacj,"It already does that. Moreover, my response was more tongue-in-cheek, because keeping logs in memory would be absolutely disastrous.  Remember, all memory is dumped the moment your system reboots, so you need disk cache to keep the log files in case of driver timeouts/blue screens.",AMD,2025-11-13 15:50:41,8
AMD,notuvb3,That totally defeats the purpose of logs.,AMD,2025-11-14 16:11:22,3
AMD,nooysyu,Local account,AMD,2025-11-13 20:32:50,1
AMD,non550k,Jesus,AMD,2025-11-13 15:10:03,24
AMD,nor921m,"Just use TwitchDropsMiner instead, it just requests the stream metadata every few seconds and Twitch thinks youre watching. Does up to 199 streams simultaneously.",AMD,2025-11-14 04:24:56,2
AMD,noo8crt,"Dude, you could help the channels by saving electricity and donating that money",AMD,2025-11-13 18:22:18,1
AMD,np9fyzx,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-17 03:48:53,1
AMD,npb36bx,Seems like a pretty huge waste of bandwidth but you do you.,AMD,2025-11-17 12:43:06,1
AMD,nopxm9v,"At the speed mentioned, did you even read what was being talked about? No, it's not normal to have a log file writing at that rate for the entire time your computer is running. Assuming you have no examples. And it's not terrible, won't destroy any SSD, but not ""entirely normal"" at all.",AMD,2025-11-13 23:35:53,1
AMD,noly4pb,"A driver can be installed, but not used. Freesync isn't ""open"" to Gsync monitors. It works with Nvidia GPUs in the form of Gsync compatible/Adaptive Sync.  Again, my belief is that it might happen with a Radeon card or iGPU. OP has a 7800X3D with a iGPU, so it might install that component through the chipset, whereas you have a 5700X without a iGPU.  Just my opinion, you don't have to agree, naturally.",AMD,2025-11-13 10:12:16,2
AMD,nolz10u,"> beside, he said ""Freesync"" which open to even G-sync monitor as long as it's compatible  That's a myth. Freesync is an AMD proprietary technology which is closed-source. Most of the software for it is inside of the radeon graphics driver and it will only run on radeon hardware.  Module-less gsync for desktop monitors is built on VESA adaptive sync. Freesync also is, but they're entirely independant mountains of closed-source and trademarked bullshit on top of that. A bug like this which affects one will not affect the other.",AMD,2025-11-13 10:21:11,-1
AMD,nozct4t,"> No. Logging window data while it's being moved is not excessive, it's just how DWM works.  Why on earth would it be necessary to write that to the disk though? Why not only in RAM?",AMD,2025-11-15 14:26:15,2
AMD,noq6zs1,"logging windows being resized is absolutely not something that is going to make a big differnece and getting that data from EVERY user is monumental overreach and completely and utterly unnecessary for developing a driver. Most people, like me, don't want any logs that go out to a third party so it's literally a waste and completely unwanted. But find me an AMD driver person who says the logs on how I resize my windows is helping them develop better drivers and I'll give you a million bucks.",AMD,2025-11-14 00:30:36,5
AMD,noomvwu,try reading the whole comment. they could use ram. it's what we do for our software.,AMD,2025-11-13 19:32:43,-7
AMD,notdc3p,"Oh c'mon, having an opinion about a company does not make you a politician.   Source: Am also a software dev with opinions about tech companies",AMD,2025-11-14 14:43:24,1
AMD,np3hve1,I'm not talking about OP specifically and there is no reason for you to get butthurt about my comment.,AMD,2025-11-16 04:40:04,0
AMD,notyde3,i have never read em in 10 years,AMD,2025-11-14 16:28:42,1
AMD,noo386a,I sometimes wonder how much energy and bandwidth is wasted on stuff like this globally,AMD,2025-11-13 17:57:40,25
AMD,norge3d,"Cool but it's not really that hard leaving a few tabs open overnight... It uses basically no extra power and I usually leave PC on anyway, I have unlimited bandwidth and I'm not paranoid about a few GBs of disk writes when my main SSD still has 97% life after 2 years (my 2nd drive which was my old main is still 88% after 14 years)...  Just curious though, do you still get people gifting you subs with that?",AMD,2025-11-14 05:20:24,1
AMD,nooi5o5,"with 5 480p streams open but browser minimized, my GPU is reporting 5w, 2w more than idle... A year's worth probably wouldn't be a single sub...  EDIT: and that's probably from the other programs/tabs I have open, I don't think Firefox does much video decoding and zero rendering when it's minimized.",AMD,2025-11-13 19:09:10,3
AMD,npba2yl,My ISP is an unlimited bandwidth plan and even when I'm awake it's not enough to be impactful (4 streams is on average is 1% of my internet speed with spikes to 2%)...,AMD,2025-11-17 13:28:39,1
AMD,noly8u0,You two need to fuck or fight,AMD,2025-11-13 10:13:25,6
AMD,nom18pt,"It's closed source but still a royalty-free open standard, so it's free to use by any company that wants to.",AMD,2025-11-13 10:43:00,1
AMD,np1m2og,"If you wrote logs to RAM, you literally would never have access to the logs that people actually use, the ones when the system crashes because any data in ram will immediately be lost.",AMD,2025-11-15 21:42:23,6
AMD,nozeeze,"Simple, RAM is volatile, SSD is not.",AMD,2025-11-15 14:35:56,2
AMD,noqfqqp,"It's already optional and you can turn it off? Jesus, people...",AMD,2025-11-14 01:22:40,0
AMD,nopqiwo,"If a software stores its logs in RAM and the program crashes, those logs are never written to disk and are lost. And those logs from just before a crash are commonly the most important ones.  Not a good idea use RAM to store logs.",AMD,2025-11-13 22:55:11,17
AMD,noop3tc,"Cached files can be appended to, and do not truncate on power loss. Using RAM to store long-term data, such as logs, sounds stupid.",AMD,2025-11-13 19:43:56,10
AMD,nou1lxi,"Team red, green, or blue, it matters to me not. If the product is good for my use case, that's all that matters.",AMD,2025-11-14 16:44:48,1
AMD,nopmlwd,"Also in video games, so many games fully stress hardware just pausing in the menus.",AMD,2025-11-13 22:33:58,6
AMD,nosjqbc,No idea sorry.,AMD,2025-11-14 11:32:23,1
AMD,npbaszg,"I realize everyone has different priorities so you will likely find this silly but I’m not talking about your resources, I’m talking about resources in general. You’re wasting bandwidth (ISP’s bandwidth) on something you aren’t watching. Again though, you do you.",AMD,2025-11-17 13:33:04,1
AMD,nolyng8,"Nope, I refuse to, just here to report back that with my current PC config, I don't have such issue for them to rule out stuff. I don't have any other further interest.",AMD,2025-11-13 10:17:24,1
AMD,nom1rrc,"For monitors yes, for GPU's no. There is no path for Nvidia or Intel to run Freesync on their hardware because it's closed source code within the radeon graphics driver.  They both had to build their own equivelant implementation on top of VESA adaptive sync, which is free and open. Those implementations have entirely seperate codebases.",AMD,2025-11-13 10:48:01,1
AMD,np41ech,And please tell me the use case when you would need to read those logs? Why cannot you enable debug logging for a short while for that singular case you came up with?,AMD,2025-11-16 07:25:20,-1
AMD,nozevab,Are you claiming DWM needs that logging data after a computer restart and it cannot be on volatile media?,AMD,2025-11-15 14:38:36,2
AMD,noqgg6z,"yeah, that doesn't mean you can ignore good practices. you don't need to log every window resize, ever, for any reason.",AMD,2025-11-14 01:26:58,10
AMD,nou3q0h,It’s literally best practice to batch writes and many SSD controllers will already be doing this in the background.   The person doesn’t suggest that the logs would only be stored in ram.,AMD,2025-11-14 16:55:17,3
AMD,noq0ak2,"I genuinely had a conspiracy theory that some games utilize bitcoin mining when you've paused, because it makes no sense that a paused game uses more GPU than a running one  EDIT: Since this is getting downvoted - I have some experience in gamedev. There are several easy methods of optimizing pause menus. One popular such method was simply taking a screenshot and showing that screenshot in the background of pause menu, meaning you don't need to continuously render the game with extra effects like blur. Depending on the engine, you can also easily limit the game's framerate to 1 while maintaining framerate for UI and mouse. Fixing this is really easy, hence my conspiracy (it was a joke) theory.",AMD,2025-11-13 23:51:39,2
AMD,npbec15,"If the ISP cared enough they wouldn't offer unlimited plans. I'm just using what I'm paying for and besides, it's nothing compared to people who torrent shit. Same thing with Twitch, they shouldn't allow points/drops to be gained when you're not active on the tab but they do. The reality is that Twitch benefit from it by playing more ads (from the average people who watches multiple, I block ads) so they don't have any incentive to stop it.  The only thing that bothers me is that channels will often raid and then I get a sub gifted for that channel that could have been given to someone who regularly watches that channel but I can't turn off auto-joining raids because for the game I want drops, it often goes to a channel that also has drops and the game I mainly care about wants you to watch ~20hrs of streams per week for max rewards.",AMD,2025-11-17 13:54:04,1
AMD,np46xdy,"I don't know what you're talking about but logs from crashes is what people read. That's the most widely used scenario for logs. The point is, if any logs are being stored in memory, they're a waste for the one time people actually use them. I read logs all the time if I'm having a problem with my PC..and as others pointed out, you could disable this logging. I don't know why the OP even bothered with the post because this is extremely minor. It's not affecting anything whatsoever. Even things like MS Defender is doing 50 times more writes than this ever would. It's a non issue.",AMD,2025-11-16 08:20:03,3
AMD,np4fg2u,"We’re literally shooting electricity at a rock at almost light speed and making it calculate more mathematical solutions per second than you will in your entire lifetime. There are times when things will simply catastrophically fail when we shoot electricity at a dinosaur stuck in a box, and when that does happen we want to know about it. If we would know beforehand that things might break, then we obviously would fix it beforehand.",AMD,2025-11-16 09:46:43,1
AMD,nozgo93,"No. DWM is Microsoft's API, and AMD uses it to log window data. Logs must be kept on non-volatile storage so they're not lost during a crash.",AMD,2025-11-15 14:49:19,6
AMD,nne2ib7,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-11-06 09:14:30,1
AMD,nnep8qd,At this point the entire stack will have 3D V-Cache. Can't wait for 7700X3D,AMD,2025-11-06 12:36:20,101
AMD,nne731q,They wanna release everything but 9600x3d,AMD,2025-11-06 10:01:41,75
AMD,nnf8rbq,"Anyone playing Battlefield 6 with any of the X3D’s?  I have a 5900x with 7900XTX, and I feel like I want more performance. Maybe my CPU is holding me back?  And I’m running FSR on Balanced. Overkill graphics but Reflections is Off.  4K 144hz, borderless, vsync off.  Frames are around 100-115 but dips to 80-90 when it gets crazy.  If I got the 9800X3D, is the performance uplift worth it?",AMD,2025-11-06 14:30:50,10
AMD,nnex67m,"Athlon x3d, anyone?",AMD,2025-11-06 13:26:18,13
AMD,nnjcyk2,Personally im waiting for the AMD 3600X3D,AMD,2025-11-07 03:25:37,5
AMD,nng3zht,But this a microcenter exclusive? What about Asia and Europe?,AMD,2025-11-06 17:03:01,2
AMD,nnjbhen,Not bad for a prebuilt people are going to buy at Walmart,AMD,2025-11-07 03:16:13,2
AMD,nni34s1,9600x3d hurry up I want one,AMD,2025-11-06 22:52:33,1
AMD,nntmw7m,So it's time to add better coolers to your boxed CPUs? https://youtu.be/z0jQZxH7NgM?si=oHoJF-786DawNJ0r,AMD,2025-11-08 20:35:28,1
AMD,nneq25u,7100X3D when???,AMD,2025-11-06 12:41:54,51
AMD,nnj1fey,Or a 7400X3D. Let's just give 3D cache to everything.,AMD,2025-11-07 02:14:38,7
AMD,nnfrg41,It's going to quickly become the norm.,AMD,2025-11-06 16:03:17,2
AMD,nnkawel,Hopefully they come out with X4D soon,AMD,2025-11-07 08:06:47,2
AMD,nnekksq,"It'll be the last thing they do because in most games it'll be neck and neck with the 9800 as most games these days barely scale across 8 threads much less 8 cores. Why sell a $350 9600x3d when the 9800x3d is selling like hotcakes at $480?  I expect a few months before Zen 6 you might see it, but not before.",AMD,2025-11-06 12:02:26,55
AMD,nngbzic,"Hey! I can actually answer (anecdotally) this directly. I've played A LOT of BF6 on my old rig (Ryzen 7 5800XT & 2080 Ti/5070 Ti) and when I switched to the 5070 Ti, my performance was just a bit better, but it was only sitting around 60-90fps on large modes. I turned on the simple in game performance metrics, and I saw that while my GPU fps was nearly 200, my CPU was the one limiting my actual displayed framerate.   Two weekends ago, I did a brand-new build with a 9800X3D and 48GB 6000 CL30 and my FPS was unlocked to 170-200fps @ 1440p on large modes (conquest/breakthrough) without any drops. I don't know how so many people are doing well on AM4, when I have over a 100%+ performance increase when I did an AM5 build. I just assume there was something off about using a B350 board with a 5800XT.",AMD,2025-11-06 17:41:55,20
AMD,nnfuafq,This is [not my experience](https://www.techpowerup.com/review/battlefield-6-performance-benchmark/3.html#:~:text=Test%20System) and I'm only trying to extrapolate [the data from the 9070xt benchmark in BF6 with FSR on balanced](https://www.techpowerup.com/review/battlefield-6-performance-benchmark/6.html) to 7900xtx (and [7900xtx is head to head with 9070xt](https://www.techpowerup.com/review/battlefield-6-performance-benchmark/5.html)) so I'm saying yes - you will get more FPS on average (not too much) but you will 100% get better 0.1% and 1% lows which will result in smoother gameplay and less noticeable fps dips.     Is the upgrade worth it? It's up to you. If the only game you plan to play for the next N years is BF6 and you want the best performance then surely go for 9800x3d,AMD,2025-11-06 16:16:37,5
AMD,nni9res,My system with 9800X3D has significantly higher minimum framerate than my other system with 5800X3D in BF6. I'm sure the difference between 5900x and something like 7600X3D would be even greater.  9800X3D with 9070XT @ 3440x1440 is locked at my 175 FPS framerate limiter in BF6 (I play with a mix of mostly High/Medium settings instead of all Overkill),AMD,2025-11-06 23:30:26,3
AMD,nngui95,I had a 5900x and 7900xtx before I replaced it with a 5700x3d and got a decent performance bump. But then I got the 9800x3d and now I'm sitting at max frames of 165fps at 1440p (my monitors refresh rate) and it doesn't break a sweat lol.  Edit: This is on BF6.,AMD,2025-11-06 19:09:13,3
AMD,nngjfl6,I have a 3080ti with a 7800x3d. I play on 4k/144 with dlss4 on quality. Most graphics settings are maxed out save a few on high or so. I lock my fps at 105 and it is pretty much pegged there with 90-100% gpu usage. 7900xtx should be quite a bit faster so I think ur cpu is holding back a bit.,AMD,2025-11-06 18:17:07,2
AMD,nnhya9l,"5700x3d with 3080ti, graphic almost max out, at 1440p im getting like 9x-100 fps",AMD,2025-11-06 22:26:15,2
AMD,nnkuzjq,Turn on FPS stats in game. Check what is lower cpu fps or gpu fps. What ever stat is lower is your bottleneck and you will see big improvements with upgrading that.,AMD,2025-11-07 11:26:52,1
AMD,nnqbs49,"5800x3d 5070ti I run 5120x1440 (pixel count is closer to 4k than 1440p) native w/o DLSS, a mix of medium/high at a steady 110-116. The user.cfg file fix is what helped me as my cpu was being absolutely hammered for seemingly nothing.",AMD,2025-11-08 07:07:40,1
AMD,nnfkb03,cooled passively without a heatsink. the cache does all the work.,AMD,2025-11-06 15:29:14,26
AMD,nnl73gg,AthlonX3D would 🔥,AMD,2025-11-07 12:56:35,11
AMD,nnj5839,5800x3dv2 PLEAAAAASE,AMD,2025-11-07 02:37:29,5
AMD,nneov8o,"It probably mostly depends upon yield rates, if there's a pile of 6-of-8-cores-working X3D Zen5 dies piling up from 9800X3D production.",AMD,2025-11-06 12:33:42,21
AMD,nni3d66,Yup my regular 9600x plays nearly the same as the 9800x3d when tuned. It's fast as hell.,AMD,2025-11-06 22:53:49,2
AMD,noeycxp,"You joke but if the rumors of them doing work on PIM's* for e-cores pan out for Zen7 then this might sorta pan out!  *PIM= processor in memory, old idea but usually implemented as a accelerator and not a general purpose CPU.",AMD,2025-11-12 06:18:12,1
AMD,nnr72ge,"SempronX3D.  No multi threading, no iGPU.  Just a single core and an ocean of cache",AMD,2025-11-08 12:20:02,5
AMD,nnje81a,I really do feel like I won the lottery upgrading to 5800x3d from my 2600. I've since got a 9070 xt and I reckon this PC will serve me well for 10+ years before I finally replace my am4 build.,AMD,2025-11-07 03:33:42,11
AMD,nnf3wuq,This is the real answer AMD is waiting for the supply of the defective 9800x3d cpus with 6 cores to build up,AMD,2025-11-06 14:04:29,24
AMD,nnj72lp,*Really*   :),AMD,2025-11-07 02:48:29,3
AMD,nofmrpp,"von neumann bottleneck, eh? our brains certainly are memory first and don't require much cooling, though i suppose technically they are liquid cooled.  PiM's cookies are delicious.",AMD,2025-11-12 10:21:40,2
AMD,nnry0bv,🥵🥵🥵,AMD,2025-11-08 15:11:54,3
AMD,nnn3a6l,"I had the same build but managed to snag a brand new AM5 motherboard and a 7600X for cheap on Amazon. Sold the 5800x3d and my AM4 board and ram and turned a small profit. The 5800x3d used prices are crazy. Set me up to upgrade to the next AM5 x3d chip in a few years, if it's even needed.",AMD,2025-11-07 18:47:37,1
AMD,nnfimy2,And also defective 9900x3ds,AMD,2025-11-06 15:21:08,9
AMD,nnh2s8q,I fully expect we're gonna see both a 9700X3D which is just gonna be binned 9800X3Ds clocked down for stability and a 9600X3D that's the 9800X3D binned with two cores out of spec so make it a hexacore but keep the clock speeds. I imagine they've just been collecting the not quite up to spec 9800X3Ds this whole time.,AMD,2025-11-06 19:49:49,4
AMD,nnftrvi,Which are already defective 9950x3ds lol,AMD,2025-11-06 16:14:11,16
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,48
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,10
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,9
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,7
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,-1
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-14
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,5
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,5
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,2
AMD,noojlcp,">The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.  I fully agree with what you said and this makes me wonder what in the genuine fuck was going through Lenovo's head when pricing their Thinkpad line of laptops. They're all wildly, and I mean WILDLY overpriced for what they offer, speaking as objectively as possible.  For the average Joe there's still no beating getting a refurbished Snapdragon X Elite laptop with 16 or 32GB or RAM at a €800 pricepoint. Nothing comes close in battery life, and these laptops come with decent OLED.",Intel,2025-11-13 19:16:18,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,4
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,1
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,0
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,26
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,17
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,1
AMD,np8gg6z,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,1
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,1
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,4
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,2
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,3
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-10
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,1
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,18
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,6
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,10
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,4
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,1
AMD,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,1
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,82
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,30
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,3
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,34
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,29
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,5
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,54
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,10
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,2
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,2
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,2
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,2
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,14
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,21
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,2
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,1
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,4
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-7
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,5
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,3
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,5
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,20
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,7
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,10
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,4
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,6
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,6
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,4
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,14
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,3
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,20
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,15
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,4
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,6
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,7
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,3
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,0
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-11
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-10
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,41
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,11
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,4
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-9
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,18
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-7
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,3
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,6
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,10
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,6
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,1
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,0
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-3
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,12
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,5
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-2
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,11
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,6
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,4
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,54
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,46
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,4
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,7
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,15
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,3
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,-1
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-3
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-5
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-8
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,20
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,9
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,12
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,2
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,4
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,7
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,6
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,14
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,10
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,9
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,4
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,6
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,14
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,-2
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-2
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,5
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,8
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,1
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,46
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,16
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,36
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,10
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,5
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,2
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,33
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-1
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,4
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,7
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,33
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,65
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,27
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-1
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-5
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,5
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,9
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,5
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,4
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,9
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,7
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,2
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,6
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,15
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,5
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,6
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,64
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,79
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,7
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,10
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,8
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,8
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,nd42fep,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Intel,2025-09-08 16:53:22,1
AMD,ndg3xrf,I will follow all!,Intel,2025-09-10 13:49:20,1
AMD,n9j60kj,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-8
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,11
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-5
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,4
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,6
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,4
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,5
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,7
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,4
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,19
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because it’s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,22
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Intel,2025-08-19 13:20:05,4
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,3
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-5
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,7
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,5
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,6
AMD,n9ks0ao,It’s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,4
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Intel,2025-08-19 13:34:10,4
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-4
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,-2
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,5
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Intel,2025-08-19 20:05:16,3
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,4
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,4
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,3
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,62
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,35
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,5
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-21
AMD,n6bwcya,It’s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,18
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,12
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,4
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,27
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,21
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,13
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,17
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,7
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-9
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-2
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
AMD,n6klfjc,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Intel,2025-08-02 19:05:15,24
AMD,n6q6qct,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Intel,2025-08-03 17:28:08,2
AMD,n7ht68l,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Intel,2025-08-07 21:45:26,1
AMD,n72229n,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Intel,2025-08-05 14:15:36,1
AMD,n7ewvah,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Intel,2025-08-07 13:19:41,2
AMD,nanq5zn,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Intel,2025-08-25 21:47:41,3
AMD,n7m0sli,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Intel,2025-08-08 14:57:07,1
AMD,n6m90qi,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Intel,2025-08-03 00:48:59,6
AMD,n6nytko,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Intel,2025-08-03 08:55:06,3
AMD,n6l6ttw,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Intel,2025-08-02 21:03:36,4
AMD,n7m8w5n,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Intel,2025-08-08 15:35:34,1
AMD,n8s3s3l,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Intel,2025-08-15 04:54:15,1
AMD,n6orqc9,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Intel,2025-08-03 12:59:19,7
AMD,n6qeres,Thank you :),Intel,2025-08-03 18:07:58,1
AMD,n4v3wzh,Wouldn't the 300 series actually be Arrow Lake Refresh?,Intel,2025-07-24 08:11:30,42
AMD,n4v46fh,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Intel,2025-07-24 08:13:59,45
AMD,n4y92ft,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Intel,2025-07-24 18:56:57,11
AMD,n4w6o9h,This is pat's work,Intel,2025-07-24 13:07:04,11
AMD,n4v6nq7,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Intel,2025-07-24 08:37:59,4
AMD,n4vlaga,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Intel,2025-07-24 10:49:31,7
AMD,n4w07wg,Is Intel finally making a comeback with their cpus? I hope so,Intel,2025-07-24 12:30:48,12
AMD,n4vi19u,Large L3 cache without reducing latency will be fun to watch.,Intel,2025-07-24 10:22:42,19
AMD,n4v3ue7,the specs sure do shift a lot..,Intel,2025-07-24 08:10:48,2
AMD,n4v8k6w,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Intel,2025-07-24 08:55:55,5
AMD,n4z4yed,it's just a bunch of cores glued together - intel circa 2016 probably,Intel,2025-07-24 21:27:20,2
AMD,n5798px,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Intel,2025-07-26 02:39:20,2
AMD,n4w3arv,Bout time,Intel,2025-07-24 12:48:32,1
AMD,n4w52bq,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Intel,2025-07-24 12:58:18,1
AMD,n510yns,Will it be available in fall of this year or 26Q1?,Intel,2025-07-25 04:05:49,1
AMD,n525u2m,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Intel,2025-07-25 10:03:56,1
AMD,n54i8vo,Noob question:  Is this their 16th gen chips?,Intel,2025-07-25 17:44:39,1
AMD,n5c2bcs,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Intel,2025-07-26 21:56:10,1
AMD,n5r7teb,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Intel,2025-07-29 07:47:01,1
AMD,n5rui8a,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Intel,2025-07-29 11:14:18,1
AMD,n4yqz30,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Intel,2025-07-24 20:20:50,1
AMD,n55s05h,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Intel,2025-07-25 21:27:44,1
AMD,n4xrnev,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Intel,2025-07-24 17:35:44,0
AMD,n4vu505,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Intel,2025-07-24 11:52:38,0
AMD,n4wlmkp,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Intel,2025-07-24 14:23:50,0
AMD,n4vi1cf,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Intel,2025-07-24 10:22:43,-12
AMD,n50egti,"""E-Cores"",  Ewww, Gross.",Intel,2025-07-25 01:40:31,-3
AMD,n4y77tz,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Intel,2025-07-24 18:47:57,0
AMD,n4vx5x8,NVL will definitely be the 400 series. PTL is 300.,Intel,2025-07-24 12:12:08,14
AMD,n4vqabe,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Intel,2025-07-24 11:26:19,26
AMD,n4x2p0n,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Intel,2025-07-24 15:43:01,8
AMD,n4vnjk4,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Intel,2025-07-24 11:06:28,42
AMD,n4vbjjr,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Intel,2025-07-24 09:24:22,8
AMD,n4vqnzd,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Intel,2025-07-24 11:29:01,13
AMD,n4xnlen,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Intel,2025-07-24 17:17:52,2
AMD,n4y6q9e,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Intel,2025-07-24 18:45:35,2
AMD,n9a1zq5,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Intel,2025-08-18 01:51:09,1
AMD,n54f312,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Intel,2025-07-25 17:30:13,0
AMD,n50iof9,More like *despite* him.,Intel,2025-07-25 02:05:32,-5
AMD,n4wx0ud,All the SKUs rumored so far are BLLC,Intel,2025-07-24 15:17:14,1
AMD,n4w5o0o,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Intel,2025-07-24 13:01:33,10
AMD,n4vwjon,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Intel,2025-07-24 12:08:14,-2
AMD,n51lyb6,Why not 8 P cores with HT + even more cache and no e cores at all,Intel,2025-07-25 06:56:21,-1
AMD,n4wel6l,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Intel,2025-07-24 13:49:07,14
AMD,n4w5tga,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Intel,2025-07-24 13:02:22,4
AMD,n4wwsgj,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Intel,2025-07-24 15:16:10,2
AMD,n8gsgik,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Intel,2025-08-13 13:37:20,2
AMD,n4xic5y,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Intel,2025-07-24 16:54:39,0
AMD,n51u6hg,Nova lake? More like 26Q4.,Intel,2025-07-25 08:13:29,1
AMD,n5272v5,Only Pantherlake for mobile,Intel,2025-07-25 10:15:01,1
AMD,n52fjpx,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Intel,2025-07-25 11:22:11,2
AMD,n4yb3g9,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Intel,2025-07-24 19:06:33,1
AMD,n4vwh5f,It is really happening. The only question is when? Or can they release it on the next year without delay?,Intel,2025-07-24 12:07:46,1
AMD,n4wxgvv,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Intel,2025-07-24 15:19:15,3
AMD,n4wm6y8,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Intel,2025-07-24 14:26:35,1
AMD,n514twx,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Intel,2025-07-25 04:33:51,1
AMD,n5153vl,"E cores are the future, P cores days are numbered.",Intel,2025-07-25 04:35:54,5
AMD,n50iukn,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Intel,2025-07-25 02:06:36,2
AMD,n514kx3,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Intel,2025-07-25 04:32:00,1
AMD,n65knix,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Intel,2025-07-31 12:35:52,1
AMD,n4vw5ub,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Intel,2025-07-24 12:05:46,8
AMD,n4vrhxo,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Intel,2025-07-24 11:34:53,20
AMD,n588bec,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Intel,2025-07-26 07:21:16,2
AMD,n4xxr8n,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Intel,2025-07-24 18:03:15,4
AMD,n50iwkc,The former ceo,Intel,2025-07-25 02:06:56,3
AMD,n4xy0ww,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Intel,2025-07-24 18:04:31,3
AMD,n4x3wyl,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Intel,2025-07-24 15:48:35,9
AMD,n4wb0qc,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Intel,2025-07-24 13:30:18,12
AMD,n4xgvaw,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Intel,2025-07-24 16:48:02,7
AMD,n4vzlh2,No different to 12-14th gen then.,Intel,2025-07-24 12:27:02,-2
AMD,n4wathc,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Intel,2025-07-24 13:29:14,-1
AMD,n51m5t6,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Intel,2025-07-25 06:58:12,3
AMD,n5267k5,E core is 30% faster than hyper threading,Intel,2025-07-25 10:07:14,2
AMD,n9ruz42,HT is worse than E cores,Intel,2025-08-20 20:15:01,2
AMD,n4wi0su,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Intel,2025-07-24 14:06:24,5
AMD,n5m42rc,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Intel,2025-07-28 14:26:44,1
AMD,n4xxluj,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Intel,2025-07-24 18:02:33,4
AMD,n50irzt,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Intel,2025-07-25 02:06:09,3
AMD,n526u6b,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Intel,2025-07-25 10:12:51,1
AMD,n514fo0,ARM chips regularly do this sometimes on a yearly basis.,Intel,2025-07-25 04:30:55,1
AMD,n5maqpy,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Intel,2025-07-28 14:59:12,3
AMD,n6ijxrr,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Intel,2025-08-02 12:13:51,1
AMD,n4yg12q,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Intel,2025-07-24 19:29:54,-2
AMD,n4wewa9,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Intel,2025-07-24 13:50:43,2
AMD,n4x5x1h,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Intel,2025-07-24 15:57:43,0
AMD,n5m9vma,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Intel,2025-07-28 14:55:00,1
AMD,n5awbw7,"Lmao, some of my games still runs on e cores",Intel,2025-07-26 18:06:33,1
AMD,n50x83s,People are still disabling e cores for more performance.,Intel,2025-07-25 03:39:29,1
AMD,n525z7c,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Intel,2025-07-25 10:05:10,1
AMD,n5m4vc2,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Intel,2025-07-28 14:30:40,2
AMD,n511bh4,fym no different they're 50% faster,Intel,2025-07-25 04:08:20,3
AMD,n5m5ymk,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Intel,2025-07-28 14:36:05,3
AMD,n528glp,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Intel,2025-07-25 10:27:04,1
AMD,n4wm3t2,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Intel,2025-07-24 14:26:09,6
AMD,n4wln0q,"As a advice, Nova Lake will further increase memory latency.",Intel,2025-07-24 14:23:54,-3
AMD,n50qnfg,In which gen iteration?,Intel,2025-07-25 02:55:31,0
AMD,n526q2r,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Intel,2025-07-25 10:11:49,1
AMD,n5m8bv4,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Intel,2025-07-28 14:47:35,1
AMD,n4xhpq9,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Intel,2025-07-24 16:51:53,1
AMD,n4xw2bt,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Intel,2025-07-24 17:55:25,10
AMD,n4zgeej,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Intel,2025-07-24 22:26:02,7
AMD,n52evsy,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Intel,2025-07-25 11:17:22,4
AMD,n588e12,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Intel,2025-07-26 07:21:58,2
AMD,n5266cc,They don't pay attention,Intel,2025-07-25 10:06:55,2
AMD,n5m6uax,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Intel,2025-07-28 14:40:21,1
AMD,n5dzs6f,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Intel,2025-07-27 05:47:15,1
AMD,n51sxrb,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Intel,2025-07-25 08:01:41,3
AMD,n50ijg8,How do you think they're doing 2 compute tiles if the memory controller is on one?,Intel,2025-07-25 02:04:42,2
AMD,n4wml1q,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Intel,2025-07-24 14:28:27,4
AMD,n5m2xjb,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Intel,2025-07-28 14:21:04,2
AMD,n528fgg,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Intel,2025-07-25 10:26:47,3
AMD,n5m9sth,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Intel,2025-07-28 14:54:37,1
AMD,n4xx7gl,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Intel,2025-07-24 18:00:40,2
AMD,n51508q,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Intel,2025-07-25 04:35:11,1
AMD,n526cpd,That didn't stop Intel with N3B,Intel,2025-07-25 10:08:30,0
AMD,n55ndnc,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Intel,2025-07-25 21:04:09,-1
AMD,n5qxo88,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Intel,2025-07-29 06:12:03,1
AMD,n80drlt,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Intel,2025-08-10 21:49:03,1
AMD,n4yjbs7,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Intel,2025-07-24 19:45:28,0
AMD,n55qjrv,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Intel,2025-07-25 21:20:18,6
AMD,n4z1sm2,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Intel,2025-07-24 21:11:47,2
AMD,n4z2fz2,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Intel,2025-07-24 21:14:56,1
AMD,n526jxa,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Intel,2025-07-25 10:10:18,1
AMD,n4znijc,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Intel,2025-07-24 23:05:11,0
AMD,n53d0lp,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Intel,2025-07-25 14:33:18,0
AMD,n3haicz,Will it also introduce Lunar Lake successor?,Intel,2025-07-16 17:36:05,16
AMD,n3htfns,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Intel,2025-07-16 19:02:16,10
AMD,n3hw0vs,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Intel,2025-07-16 19:14:21,5
AMD,n3h5hhc,Is anyone left?,Intel,2025-07-16 17:13:41,22
AMD,n3h8963,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Intel,2025-07-16 17:26:01,8
AMD,n3hcp5u,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Intel,2025-07-16 17:45:45,6
AMD,n3jhgvd,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Intel,2025-07-16 23:53:49,1
AMD,n3k7s38,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Intel,2025-07-17 02:31:14,1
AMD,n3mdvwq,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Intel,2025-07-17 12:44:29,1
AMD,n4c03u2,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Intel,2025-07-21 12:54:03,1
AMD,n3huqus,That's Panther Lake in a few months,Intel,2025-07-16 19:08:25,13
AMD,n3ik298,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Intel,2025-07-16 21:05:21,11
AMD,n3lcumc,"No, the memory config wouldn't work.",Intel,2025-07-17 07:51:04,2
AMD,n3i46cl,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Intel,2025-07-16 19:53:15,7
AMD,n3h7xps,No not really.  It's pretty f'n bleak atm.,Intel,2025-07-16 17:24:37,18
AMD,n3h9kp1,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Intel,2025-07-16 17:31:54,6
AMD,n3l4fgi,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Intel,2025-07-17 06:35:04,-1
AMD,n3irdqf,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Intel,2025-07-16 21:39:27,5
AMD,n3hywlr,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Intel,2025-07-16 19:27:56,6
AMD,n3k42qb,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Intel,2025-07-17 02:08:16,7
AMD,n3k6ewf,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Intel,2025-07-17 02:22:43,3
AMD,n3jnkuo,PTL's not really a LNL successor.,Intel,2025-07-17 00:29:37,1
AMD,n48vx1p,"Yeah, it was just to prove a point that ARM is overrated.",Intel,2025-07-20 22:58:27,4
AMD,n3k5a4s,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Intel,2025-07-17 02:15:39,2
AMD,n4sfvrm,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Intel,2025-07-23 21:35:03,2
AMD,n3h8tf7,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Intel,2025-07-16 17:28:32,13
AMD,n3kgv3j,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Intel,2025-07-17 03:30:53,5
AMD,n3jgnpe,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Intel,2025-07-16 23:49:16,5
AMD,n3hafqh,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Intel,2025-07-16 17:35:45,2
AMD,ngib62p,Source?,Intel,2025-09-27 17:03:09,2
AMD,n3jfseq,"The BOM is lower, so the question is where the markup is coming from.",Intel,2025-07-16 23:44:28,2
AMD,n3k5vvd,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Intel,2025-07-17 02:19:23,2
AMD,n3k8ay7,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Intel,2025-07-17 02:34:31,2
AMD,n3khaz8,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Intel,2025-07-17 03:33:57,1
AMD,n3k63l5,>still cheaper,Intel,2025-07-17 02:20:44,1
AMD,n3k776n,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Intel,2025-07-17 02:27:36,11
AMD,n3ugqr7,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Intel,2025-07-18 16:50:56,1
AMD,n3l78dq,It already has 32MB infinity cache.,Intel,2025-07-17 06:59:18,2
AMD,n3k81ho,The 7840HS is cheaper because it is older.,Intel,2025-07-17 02:32:52,5
AMD,n3lcowt,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Intel,2025-07-17 07:49:34,2
AMD,n3ut4ta,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Intel,2025-07-18 17:47:58,1
AMD,n3kh6ij,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Intel,2025-07-17 03:33:05,-1
AMD,n3levrg,I will see the performance envelope of PTL U and decide should dump my LNL or not,Intel,2025-07-17 08:10:07,4
AMD,n3uubsu,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Intel,2025-07-18 17:53:36,1
AMD,n3ki3cu,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Intel,2025-07-17 03:39:32,5
AMD,n3l7fsl,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Intel,2025-07-17 07:01:04,1
AMD,n3laz95,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Intel,2025-07-17 07:33:37,1
AMD,n3l7mu1,I think I didn't say anything that deviates from what you just said.,Intel,2025-07-17 07:02:48,1
AMD,n3lach0,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Intel,2025-07-17 07:27:39,1
AMD,n1d5sl1,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Intel,2025-07-04 20:26:29,20
AMD,n1d44wc,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Intel,2025-07-04 20:17:30,14
AMD,n1hmxgn,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Intel,2025-07-05 16:10:42,4
AMD,n1je3rl,"who needs quality control, what can go wrong?",Intel,2025-07-05 21:53:07,3
AMD,n4zj8x9,Gigabyte is trash,Intel,2025-07-24 22:41:41,1
AMD,n1j4fq9,The Elon Musk method,Intel,2025-07-05 20:58:12,4
AMD,n2m6czi,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Intel,2025-07-11 20:40:54,1
AMD,n25f2x8,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Intel,2025-07-09 10:28:21,1
AMD,mzzaf4q,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Intel,2025-06-27 00:15:57,64
AMD,n023uge,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Intel,2025-06-27 13:03:00,10
AMD,mzzyzii,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Intel,2025-06-27 02:44:24,13
AMD,mzz7wta,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Intel,2025-06-27 00:01:01,17
AMD,n029b76,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Intel,2025-06-27 13:33:26,3
AMD,mzz8z0y,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Intel,2025-06-27 00:07:18,9
AMD,n07rvul,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Intel,2025-06-28 09:38:27,3
AMD,n0tqugl,Intel should be ahead of the curve on things not looking to compete on previously created tech,Intel,2025-07-01 20:43:31,1
AMD,n0x6eyy,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Intel,2025-07-02 10:56:54,1
AMD,n5g03d8,"Lol, requires a new socket. Intel is such trash.",Intel,2025-07-27 15:10:56,1
AMD,n05c074,Intel will simply always be better than amd,Intel,2025-06-27 22:40:39,1
AMD,mzzj3f4,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Intel,2025-06-27 01:08:00,-8
AMD,n00a0en,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Intel,2025-06-27 03:59:29,21
AMD,n01ajgx,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Intel,2025-06-27 09:25:00,10
AMD,n03naj2,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Intel,2025-06-27 17:33:15,6
AMD,n022v6j,4070 ti won’t cut it man - upgrade!,Intel,2025-06-27 12:57:19,-2
AMD,n067n87,Broadwell could have been so interesting had it planned out.,Intel,2025-06-28 01:50:48,4
AMD,n0254vg,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Intel,2025-06-27 13:10:22,6
AMD,n06s4h5,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Intel,2025-06-28 04:12:23,6
AMD,n028uwl,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Intel,2025-06-27 13:30:56,5
AMD,n014nai,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Intel,2025-06-27 08:27:08,11
AMD,n00xlat,"Adamantaium was on the interposer, did they change plans?",Intel,2025-06-27 07:17:41,3
AMD,n02fp39,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Intel,2025-06-27 14:06:40,6
AMD,n0r9clc,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Intel,2025-07-01 13:40:00,1
AMD,n027hjt,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Intel,2025-06-27 13:23:25,-1
AMD,mzzxdtg,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Intel,2025-06-27 02:34:12,17
AMD,n05blhb,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Intel,2025-06-27 22:38:16,5
AMD,n01isnc,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Intel,2025-06-27 10:39:30,15
AMD,n0421zg,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Intel,2025-06-27 18:42:47,10
AMD,n5exp59,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Intel,2025-07-27 11:11:38,1
AMD,n835hud,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Intel,2025-08-11 10:24:29,1
AMD,mzzqcpb,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Intel,2025-06-27 01:51:25,20
AMD,mzzxpgo,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Intel,2025-06-27 02:36:14,8
AMD,mzztuv0,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Intel,2025-06-27 02:12:29,8
AMD,n014s2d,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Intel,2025-06-27 08:28:28,3
AMD,n0r948t,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Intel,2025-07-01 13:38:44,1
AMD,n03y3vh,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Intel,2025-06-27 18:23:40,6
AMD,n0798ym,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Intel,2025-06-28 06:37:39,4
AMD,n01j4io,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Intel,2025-06-27 10:42:13,8
AMD,n01g781,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Intel,2025-06-27 10:17:10,6
AMD,n01fnuq,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Intel,2025-06-27 10:12:22,9
AMD,nj5zwte,Zen6 is the last AMD CPU using its socket anyway,Intel,2025-10-12 20:57:17,1
AMD,n02ibwo,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Intel,2025-06-27 14:19:43,7
AMD,n045qi7,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Intel,2025-06-27 19:00:29,2
AMD,n3fijqh,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Intel,2025-07-16 11:51:08,1
AMD,n0ap7lk,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Intel,2025-06-28 20:15:41,12
AMD,n02j14s,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Intel,2025-06-27 14:23:07,12
AMD,n04xv5b,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Intel,2025-06-27 21:22:28,5
AMD,n0429de,"How was ""system snappiness"" measured?",Intel,2025-06-27 18:43:46,10
AMD,n01uxxc,No.,Intel,2025-06-27 12:08:29,12
AMD,n08zo5w,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Intel,2025-06-28 14:51:50,2
AMD,n02l2ku,So? Major upgrade for everything else,Intel,2025-06-27 14:33:03,2
AMD,n0x0cru,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Intel,2025-07-02 10:05:39,0
AMD,n046538,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Intel,2025-06-27 19:02:27,8
AMD,n043af1,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Intel,2025-06-27 18:48:47,-2
AMD,n5fdqig,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Intel,2025-07-27 13:07:23,0
AMD,n84bzza,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Intel,2025-08-11 14:53:13,1
AMD,n1f1onn,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Intel,2025-07-05 03:53:59,1
AMD,n0jysi1,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Intel,2025-06-30 10:29:27,1
AMD,n02dy44,You can very simply get 9800x3D to 5.4 with little effort,Intel,2025-06-27 13:57:44,6
AMD,nj6nf3i,"No, they will do zen7 too",Intel,2025-10-12 23:12:44,1
AMD,n0qy2r3,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Intel,2025-07-01 12:35:15,6
AMD,nc1pr0f,"7800x3d here and never had these issues, came from intel",Intel,2025-09-02 17:25:21,1
AMD,n02jgm4,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Intel,2025-06-27 14:25:11,-3
AMD,n01v9di,Lets just ignore the whitepaper WD and Intel did about this.,Intel,2025-06-27 12:10:35,4
AMD,n5fmwbs,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Intel,2025-07-27 14:00:49,1
AMD,n006ga4,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Intel,2025-06-27 03:34:22,11
AMD,n03645p,On which benchmark(s) / metrics?,Intel,2025-06-27 16:12:47,4
AMD,n1jzzta,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Intel,2025-07-06 00:05:28,1
AMD,n0ksjbi,"In theory, yes. For packaging reasons and market segmentation, probably not.",Intel,2025-06-30 13:51:44,2
AMD,n02kkbn,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Intel,2025-06-27 14:30:33,9
AMD,n0226kr,where is this whitepaper,Intel,2025-06-27 12:53:19,6
AMD,n02akpt,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Intel,2025-06-27 13:40:12,6
AMD,n5g6rxb,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Intel,2025-07-27 15:44:33,1
AMD,n7xvp9r,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Intel,2025-08-10 13:55:29,1
AMD,n023hsg,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Intel,2025-06-27 13:00:57,4
AMD,n5gbwrf,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Intel,2025-07-27 16:09:50,1
AMD,n007dg1,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Intel,2025-06-27 03:40:49,6
AMD,n03knav,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-06-27 17:21:00,2
AMD,n046z2m,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Intel,2025-06-27 19:06:33,3
AMD,n7xwq6a,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Intel,2025-08-10 14:01:29,2
AMD,n5ghx4j,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Intel,2025-07-27 16:38:58,1
AMD,n13yjw6,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Intel,2025-07-03 11:37:55,2
AMD,n5gkgn2,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Intel,2025-07-27 16:51:17,1
AMD,n0081vw,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Intel,2025-06-27 03:45:34,11
AMD,n049nak,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Intel,2025-06-27 19:19:40,2
AMD,n5gmhbv,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Intel,2025-07-27 17:01:05,0
