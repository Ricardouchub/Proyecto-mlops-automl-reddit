brand,comment_id,text,subreddit,created_utc,score
Intel,nt4yxvd,Spoiler. Yes is a new variant but it is call B70 arc pro series  Not the B770 we want,hardware,2025-12-09 16:45:21,69
Intel,nt4tav4,"Performance wise, how what nvidia series card will it compare to?",hardware,2025-12-09 16:18:42,46
Intel,nt5cr57,"I'm sure B770 will launch as a side-effect if G31, but the real reason is for a low cost (relative) 32GB pro card to target AI. (Called B70?)  If Intel can offer 32GB VRAM on the pro card for $1000, it would be hugely popular within it's specific niche. And it's most important job is to get it into the hands of AI hobbyists to build out software support OneAPI in the open source AI community.  G31 isn't going to revolutionize the gaming market. It won't revolutionize Intel's finances. But it's goal is going to be high-VRAM at a low cost to try and build out alternatives to CUDA.   It's just a shame that it couldn't have come at a worse time from a cost-structure POV",hardware,2025-12-09 17:52:34,19
Intel,nt5yxu4,Curious to see if they can actually compete this time.,hardware,2025-12-09 19:48:10,4
Intel,nt5l18u,What niche is this going to fill? Battlemage was already kinda iffy performance wise when the B580 launched. It really had to compete with its increased VRAM and low price. Performance is going to be aged even more now. I imagine it’ll lose to a 5060ti or at best a 5070. Obviously price is king but I don’t see this generation being performance competitive.,hardware,2025-12-09 18:38:36,7
Intel,nt54keg,"As always too late - it's hard to compete when launching mid-late AMD/nvidia generation. Also curious how CPU driver head will reflect on much more powerful card, because it's still not quite resolved based of B580 tests.  Something like RX 9070 has very good pricing, especially in Europe, then RTX 5070 is very popular too. They would need to price it under 500€, likely closer to 450€ to have decent appeal, and I don't know if that's possible.",hardware,2025-12-09 17:12:31,14
Intel,nt6gntg,"Oh, Intel, the *timing*...",hardware,2025-12-09 21:15:07,9
Intel,nt5ehpm,"On TSMC 5nm with gddr6 memory, theoretically this card could have launched in 2020-2021, not 2026.",hardware,2025-12-09 18:00:41,12
Intel,nt5zccm,Wondering when we'd start hearing CES stuff.,hardware,2025-12-09 19:50:08,3
Intel,nt60atj,"I might be interested in buying one of these, if it's priced right.",hardware,2025-12-09 19:54:48,3
Intel,nt72tyx,Celestial or maybe even Druid was supposed to be out in 2025 at one point.   https://www.techpowerup.com/292208/intel-targeting-2024-for-ultra-enthusiast-arc-celestial-gpus,hardware,2025-12-09 23:08:26,3
Intel,nt76anv,"Maxsun edition: 32gb memory, $1299 via Hydratech reseller.",hardware,2025-12-09 23:28:02,1
Intel,nt7rnnk,With how insane RAM shit has gone. These stock oils were bought long long ago. It won’t be in stock like… at all. Ever.,hardware,2025-12-10 01:31:41,1
Intel,nt67wn7,For me the sweet spot would be 5070ti performance with 32gb vram,hardware,2025-12-09 20:32:36,0
Intel,nt4x7de,Id sell my 4070ti super and move to this if it releases,hardware,2025-12-09 16:37:10,-3
Intel,nt5jopc,"Watch it end up being AI-focused, without graphics output, and part of the Flex family.",hardware,2025-12-09 18:30:22,29
Intel,nt548cp,"4070, 4070 Super at best (5070).",hardware,2025-12-09 17:10:53,32
Intel,nt51qpx,"b580 looks to use around 170W, this new card has a TDP of 300W so assuming the same efficiency and it uses the full 300W it'll be up to 1.76x the speed putting it between 4070 and 5070.",hardware,2025-12-09 16:58:45,41
Intel,nt5kmm9,"1.6x the cores, but only 1.33x the memory bus with maybe 5% faster RAM is like 1.4x the bandwidth.   So I'd guess 1.5x the B580 at best.  Which puts it below an RTX 4070.",hardware,2025-12-09 18:36:05,16
Intel,nt51h9g,This must be the “4080 performance” RedGamingTech has been mentioning since 2020 /s,hardware,2025-12-09 16:57:30,26
Intel,nt5re3f,"1.6 times the number of cores, 1.5 times the power draw compared to the B580. Let's say 1.4 to 1.6 times faster than the B580. That puts it at approximately the 5060ti to 4070 level.",hardware,2025-12-09 19:10:38,5
Intel,nt4w7jt,Nobody knows yet  I guess between 5060 ti and 5070,hardware,2025-12-09 16:32:28,7
Intel,nt4wjku,vaguely 4070-5070 ish,hardware,2025-12-09 16:34:03,7
Intel,nt4w774,"we're not sure yet, its theorised to be around 5070-5070ti or 9070/xt but if intel does what they do with the b580 it'll completely destroy them on value, let's hope the ram issue doesn't kill what was shaping up to be the best gpu in recent time. the b580 is delightful.  intel 140t/v and b580 gained like a cumulative 20% perf boost this year just from drivers alone",hardware,2025-12-09 16:32:25,13
Intel,nt5sudk,People are saying \~4070 at a lower cost. Intel GPUs are still space inefficient compared to Nvidia (so is AMD) so them dumping a lot of power into the chip is probably the best way for them to keep costs down.,hardware,2025-12-09 19:17:54,2
Intel,nt6pgo0,"Intel has so far shown to match Nvidia regarding RT performance per dollar but not ""real"" performance per watt against AMD.  As their last flagship was kinda matched into AMD 5700XT and Nvidia RTX 2070, but like 5 years too late.",hardware,2025-12-09 21:58:05,1
Intel,nt64887,5060 Ti 16gb to 4070 Super range.,hardware,2025-12-09 20:14:25,-1
Intel,nt77j95,> If Intel can offer 32GB VRAM on the pro card for $1000  If you can even find the card.  It’s bad enough that I just went with an AMD R9700 instead of waiting for the B60 or its 48GB dual variant to show up.,hardware,2025-12-09 23:35:12,2
Intel,nt5yrz0,"Realistically? None. This is a long-term play for Intel. They know the real money's in datacenters, but the ramp up to get there now is a decade+ with how complex modern GPUs are. They're just putting a few consumer products out there to establish the product line along the way.",hardware,2025-12-09 19:47:21,10
Intel,nt63i3a,It'll be priced in the 5060ti range but offer better performance. AMD doesn't offer a product at that price.,hardware,2025-12-09 20:10:48,4
Intel,nt5buas,"> because it's still not quite resolved based of B580 tests.  And it probably never will be, since a large part of it is architectural.  AMD never solved the GCN single thread bottleneck in DX11 either.",hardware,2025-12-09 17:48:15,9
Intel,nt62byj,"Maybe, if it comes out soon it will be a better value than both potentially.",hardware,2025-12-09 20:04:53,1
Intel,nt723yf,"Rumors were that it wasn't gonna come out. Something changed in the market to make Intel resume releasing G31.   And I'm convinced that's the fact that B70 could disrupt the market *(the market being the relatively small but growing home AI market - don't underestimate the impact this market can have on software stack support in professional environments)  As for the gaming side of things, I guess we'll find out if the driver overhead is a fixed amount or one that increases with the card's performance.",hardware,2025-12-09 23:04:25,1
Intel,nt6o51r,"Where are likely a year and a half to the next Nvidia consumer cards releasing.  Assuming things like memory shortage goes away at that time, or else they might even Delay it even more.",hardware,2025-12-09 21:51:42,3
Intel,nt5l5hr,Nvidia released RTX 30 Series in 2020 on a variant of Samsung's 10nm.,hardware,2025-12-09 18:39:19,16
Intel,nt5rrk0,All about price points.,hardware,2025-12-09 19:12:30,7
Intel,nt5rvki,"Aside from the architecture not existing back then, the tech is a lot cheaper now than it was then. Not using bleeding edge hardware is a good thing for a budget card.",hardware,2025-12-09 19:13:03,7
Intel,nt6hko4,"No way in hell it'll be anywhere near that with only 32 Xe cores, it's looking like 5060Ti at best, and only if they can get it to scale well",hardware,2025-12-09 21:19:29,8
Intel,nt54o1l,"You’ll end up with the same performance on the games Intel optimized the drivers for, worse performance on everything else, and no DLSS.",hardware,2025-12-09 17:13:01,15
Intel,nt5jyff,Why? You already have a superior GPU.,hardware,2025-12-09 18:32:04,11
Intel,nt6j62a,The 4070Ti Super is literally a beast of a card. It's between the >RX 9070 and <5070Ti in rasterization and a cutdown of 4080/S,hardware,2025-12-09 21:27:17,3
Intel,nt6udn3,That'd be a weird thing to show off at CES.,hardware,2025-12-09 22:23:06,17
Intel,nt6yn9m,Flex was killed.,hardware,2025-12-09 22:45:44,4
Intel,nt76xro,"That’s the relatively unobtainable Arc Pro B60 24gb and its unicorn tier 48gb dual model, save for the lack of Flex branding.",hardware,2025-12-09 23:31:45,5
Intel,nt5vhx9,"that would be an excellent sweet spot for sales. more than most anyone needs to play games, but not excessive.",hardware,2025-12-09 19:31:03,22
Intel,nt6ys6d,It's not going to be linear with power. Core count and especially memory bandwidth are not scaling to that degree.,hardware,2025-12-09 22:46:26,12
Intel,nt5j65s,Haven't seen them on my youtube feed in a long time.  I prefer to get all my fake rumors from MLID.,hardware,2025-12-09 18:27:13,22
Intel,nt5tpr7,That was a rumored XE3 card. Not sure if it'll come.,hardware,2025-12-09 19:22:13,1
Intel,nt4xwpg,The rumor when B580 was coming out was that this should be a 4070 competitor. Don't tell people this will be anywhere close to a 5070ti lol; that'll undermine any chance of Intel having success with this card real quick.,hardware,2025-12-09 16:40:29,35
Intel,nt56xdk,\>5070ti or 9070/xt   Absolutely and utterly out of the question,hardware,2025-12-09 17:24:00,19
Intel,nt6n2l0,They can probably just make the money back for launching the larger die from cranking out 32GB professional cards.     Wouldn't be surprised if what they launch for consumers is a slightly cut down version.,hardware,2025-12-09 21:46:29,3
Intel,nt7518c,"Because ""that price"" is iGPU range.",hardware,2025-12-09 23:20:51,-4
Intel,nt74uu2,Intel will also end up being a year and a half out of date shipping another non-competitive entry level enterprise card.,hardware,2025-12-09 23:19:50,7
Intel,nt5y2ri,"Fuck, it's already been half a decade.",hardware,2025-12-09 19:43:53,10
Intel,nt601c1,And apple launched m1 mac on tsmc 5nm in 2020,hardware,2025-12-09 19:53:31,0
Intel,nt72jrg,"My guess is that'll outperform 5060ti at higher resolutions, but trade blows and maybe even be slightly slower at lower resolution.",hardware,2025-12-09 23:06:52,1
Intel,nt60ifh,If this can match the 4070ti super (>5070) in any gpu bottlenecked game that would be a huge success. Probably impossible given that this only has 32 Xe cores,hardware,2025-12-09 19:55:51,2
Intel,nt79avi,Exactly why I think it's more likely than them releasing a B770.,hardware,2025-12-09 23:45:27,4
Intel,nt7osyw,Been waiting for the B60 to be commonly available here and it’s more rare than the 5090 FE,hardware,2025-12-10 01:14:24,2
Intel,nt6xcls,"Yeah, depending on the price it could be the perfect 1440p card.",hardware,2025-12-09 22:38:49,4
Intel,nt504pj,"b770 has shown up in benchmarks with 16gb gddr6 and double the CUs so that's why i gave a range, it means it is going to fall between those probably with lower spec being realistic and higher optimistic.  itd possibly be better for the consumer if it was 5070 spec and 16gb since it would be a killer affordable 1440p card with potential to do entry 4k and the community needs value right now.  if it competes with high mid range it will probably cost a ton. if they can do this card under 500 its going to be insane value. my wife and i think it'll be 500-600(msrp) but of course scalpers will do their thing",hardware,2025-12-09 16:51:03,7
Intel,nt5vmku,Really all they need to do is provide a better card than the 9060xt and the 5060ti. If it's $370 it will crush the competition.,hardware,2025-12-09 19:31:43,1
Intel,nt67ips,That's because only Apple  has early access to TSMC most advanced node.,hardware,2025-12-09 20:30:40,7
Intel,nt7tire,I’d bet that equivalent generation Quadros or Radeon Pro cards are far easier to find at any price much less near MSRP or even introductory MSRP.,hardware,2025-12-10 01:42:53,1
Intel,nt748ra,"I mean honestly a 5070 is fully playable even at 4k,",hardware,2025-12-09 23:16:22,3
Intel,nt7bmqw,I keep wondering if Apple will get everything together to actually make a gaming alternative.  They are throwing enough die space at it seemingly.,hardware,2025-12-09 23:58:49,1
Intel,nt7qqo0,"The total worldwide gaming market is $400 billion, about equal to Apple's yearly revenue, and only 4x Apple's yearly profit.  They will invest the minimum in the one gaming cash cow they have (iPhone) and ignore the rest, as usual.",hardware,2025-12-10 01:26:07,3
Intel,nt8b5te,"They make excellent gaming devices, that's more on developers avoiding them for whatever reasons. The closed shop nature doesn't help either, they really should keep the closed stuff as a premium brand and release a new brand of more open arm hardware. They could probably make a killer handheld at a minimum.",hardware,2025-12-10 03:28:44,1
Intel,nt8f1uv,Apple is a large player in the *total* gaming market through iOS,hardware,2025-12-10 03:53:57,1
Intel,nt8c8ss,I was thinking of proton on macOS that correctly uses metal.  Maybe that could happen?  Granted it adds x86 compatibility to the mix but it’s not like they did not already address that….,hardware,2025-12-10 03:35:43,1
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,7
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,10
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,10
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,11
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,8
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,3
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,1
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,1
Intel,nskpbhm,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",hardware,2025-12-06 10:33:27,72
Intel,nskzrhs,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",hardware,2025-12-06 12:12:49,134
Intel,nsl4bqs,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",hardware,2025-12-06 12:50:37,43
Intel,nspebmq,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,hardware,2025-12-07 03:40:56,7
Intel,nsm5bxh,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",hardware,2025-12-06 16:32:46,8
Intel,nszc9eq,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",hardware,2025-12-08 18:55:08,1
Intel,nt0zpd3,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,hardware,2025-12-09 00:04:12,1
Intel,nsnsg7n,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,hardware,2025-12-06 21:49:06,-6
Intel,nskuu1l,Wait isn't xess a lower resolution per quality setting?,hardware,2025-12-06 11:27:34,34
Intel,nsuy6fa,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",hardware,2025-12-08 00:59:57,1
Intel,nslxyj3,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,hardware,2025-12-06 15:53:05,31
Intel,nspes5s,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",hardware,2025-12-07 03:43:50,5
Intel,nsl2hua,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,hardware,2025-12-06 12:36:00,8
Intel,nt09q01,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,hardware,2025-12-08 21:40:56,1
Intel,nslaf1q,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",hardware,2025-12-06 13:33:32,-10
Intel,nsl9m9r,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",hardware,2025-12-06 13:28:13,-8
Intel,nsmb10n,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",hardware,2025-12-06 17:02:45,-7
Intel,nslqts2,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,hardware,2025-12-06 15:13:39,25
Intel,nspdyuf,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",hardware,2025-12-07 03:38:38,1
Intel,nsn70s5,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",hardware,2025-12-06 19:49:40,11
Intel,nsmn51w,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",hardware,2025-12-06 18:06:54,7
Intel,nsl1d2d,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",hardware,2025-12-06 12:26:39,38
Intel,nskx602,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",hardware,2025-12-06 11:49:24,1
Intel,nsxjyy3,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,hardware,2025-12-08 13:22:59,1
Intel,nsxk5xm,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,hardware,2025-12-08 13:24:15,2
Intel,nt0aiwr,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",hardware,2025-12-08 21:44:54,1
Intel,nsl44tr,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",hardware,2025-12-06 12:49:07,76
Intel,nslgt6n,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",hardware,2025-12-06 14:14:46,17
Intel,nssphno,Gpus are way more expensive to produce,hardware,2025-12-07 18:04:53,1
Intel,nsxk9lk,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",hardware,2025-12-08 13:24:53,1
Intel,nslrykc,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,hardware,2025-12-06 15:20:04,0
Intel,nsridpx,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",hardware,2025-12-07 14:17:24,0
Intel,nsmyw1o,"It does, but also, inflation has been extremely bad for 5 years.",hardware,2025-12-06 19:06:53,-1
Intel,nslef0x,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",hardware,2025-12-06 13:59:32,34
Intel,nsmzdz9,TSMC inflation is FAR higher than CPI. You are half right,hardware,2025-12-06 19:09:24,11
Intel,nsxnqe7,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",hardware,2025-12-08 13:46:23,2
Intel,nslsb0f,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,hardware,2025-12-06 15:22:04,12
Intel,nsuys01,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",hardware,2025-12-08 01:03:40,1
Intel,nsmjt4j,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,hardware,2025-12-06 17:50:02,0
Intel,nsokkai,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,hardware,2025-12-07 00:31:34,10
Intel,nsxo5ua,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,hardware,2025-12-08 13:48:58,1
Intel,nsq49gv,And that only in terms of native resolution and does not mean equal final image quality.,hardware,2025-12-07 06:54:10,5
Intel,nsljs69,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,hardware,2025-12-06 14:32:33,20
Intel,nt3tjxg,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,hardware,2025-12-09 13:05:09,1
Intel,nsq2h5v,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",hardware,2025-12-07 06:37:50,7
Intel,nslou4e,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",hardware,2025-12-06 15:02:28,-11
Intel,nsm6adc,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",hardware,2025-12-06 16:37:49,-10
Intel,nstfa7l,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",hardware,2025-12-07 20:08:33,-6
Intel,nsm3fs2,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",hardware,2025-12-06 16:22:46,17
Intel,nslu7ja,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,hardware,2025-12-06 15:32:39,-9
Intel,nsluf98,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,hardware,2025-12-06 15:33:51,17
Intel,nslugvd,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,hardware,2025-12-06 15:34:05,2
Intel,nsnx0y4,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,hardware,2025-12-06 22:14:23,4
Intel,nspdbpk,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",hardware,2025-12-07 03:34:19,4
Intel,nsxjt6i,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,hardware,2025-12-08 13:21:58,5
Intel,nsqehxx,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",hardware,2025-12-07 08:33:16,10
Intel,nsm3au4,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",hardware,2025-12-06 16:22:03,37
Intel,nsp8t2z,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,hardware,2025-12-07 03:04:36,3
Intel,nsm6g3n,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",hardware,2025-12-06 16:38:39,4
Intel,nsmzr6o,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",hardware,2025-12-06 19:11:17,2
Intel,nsn5d71,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",hardware,2025-12-06 19:40:49,9
Intel,nsmgtnv,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",hardware,2025-12-06 17:34:05,7
Intel,nsmsf6b,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",hardware,2025-12-06 18:33:48,1
Intel,nsxkhp8,it does not matter how close to the top GPU is. its a completely useless comparison.,hardware,2025-12-08 13:26:19,2
Intel,nsm74h5,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,hardware,2025-12-06 16:42:14,1
Intel,nslzd1j,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",hardware,2025-12-06 16:00:40,4
Intel,nsxodfi,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",hardware,2025-12-08 13:50:12,1
Intel,nsm6ry8,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,hardware,2025-12-06 16:40:22,3
Intel,nsxkob8,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",hardware,2025-12-08 13:27:28,1
Intel,nsmnevw,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",hardware,2025-12-06 18:08:19,-10
Intel,nsxl3k9,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",hardware,2025-12-08 13:30:09,2
Intel,nsxl8jt,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,hardware,2025-12-08 13:31:02,1
Intel,nsmwsat,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",hardware,2025-12-06 18:56:07,-1
Intel,nsoze8r,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",hardware,2025-12-07 02:04:41,3
Intel,nsxmt3k,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",hardware,2025-12-08 13:40:46,1
Intel,nsxn5gg,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",hardware,2025-12-08 13:42:53,1
Intel,nsm8g0h,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,hardware,2025-12-06 16:49:09,5
Intel,nslzlxy,"Fair point, but isn't lower and competitive prices good for us?",hardware,2025-12-06 16:02:01,1
Intel,nsmqlcg,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",hardware,2025-12-06 18:24:26,17
Intel,nspgewd,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",hardware,2025-12-07 03:54:39,6
Intel,nsxlsss,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,hardware,2025-12-08 13:34:33,1
Intel,nsxop63,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",hardware,2025-12-08 13:52:10,1
Intel,nsmvfxi,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",hardware,2025-12-06 18:49:17,-1
Intel,nsxm00p,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",hardware,2025-12-08 13:35:48,1
Intel,nsrkwkw,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",hardware,2025-12-07 14:32:28,1
Intel,nsncxlc,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",hardware,2025-12-06 20:22:05,5
Intel,nsyfiin,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",hardware,2025-12-08 16:15:19,1
Intel,nss9lt6,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",hardware,2025-12-07 16:44:57,2
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,52
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,70
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,7
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,5
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,npgcjdt,This would still no where be close to M4/M5 in single core and GPU,hardware,2025-11-18 06:45:37,1
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-5
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,45
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,14
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,29
Intel,notfij9,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,9
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,5
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,0
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,16
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,11
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,4
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,9
Intel,nouozrl,I’m assuming the caveat to posts like this is “running a version of windows/linux”,hardware,2025-11-14 18:41:25,11
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,22
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,2
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,11
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-8
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,14
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,7
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,11
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,7
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,20
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,16
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,10
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,63
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,46
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,6
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-6
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-2
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-6
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,51
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,14
Intel,nnntnit,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,3
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,23
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,10
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,11
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-2
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-4
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,4
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,22
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,11
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,12
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,9
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,1
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-8
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,27
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,5
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,12
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,6
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,11
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,17
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,19
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-7
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,13
Intel,nnh18hs,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,12
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,7
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,11
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,7
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,7
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,8
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,13
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,5
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-3
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-4
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,6
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,7
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,2
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,14
Intel,nngrskq,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,13
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,12
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,8
Intel,nnhryps,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,15
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,12
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,4
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,12
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,7
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,12
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,11
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,13
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,3
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,16
Intel,nnguuaj,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,7
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-7
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,3
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then …,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,6
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,5
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,9
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,-2
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,8
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,8
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,4
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,10
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,8
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,12
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,8
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,4
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,5
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,0
Intel,nnhohos,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,8
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,11
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,7
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,4
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,3
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,14
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,6
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,5
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,9
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,2
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,4
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-4
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,10
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,2
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,6
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,7
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,9
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,4
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,6
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,29
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,60
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,15
Intel,nmn2kpy,Fine wine,hardware,2025-11-02 02:36:42,1
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-21
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,13
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,25
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,5
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,30
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,12
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,5
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,19
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,15
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,3
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,2
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,6
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,30
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,1
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-3
Intel,nis0ezd,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,16
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,17
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,5
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,37
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,29
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,14
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,33
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,10
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,10
Intel,nii5519,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,hardware,2025-10-08 22:44:00,8
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,9
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,3
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,12
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,17
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,5
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,8
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,3
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,5
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,9
Intel,niixhey,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",hardware,2025-10-09 01:35:25,72
Intel,nij1t7l,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,hardware,2025-10-09 02:02:14,34
Intel,nijk1dw,that's insane vram density,hardware,2025-10-09 04:05:01,11
Intel,nijb3xq,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",hardware,2025-10-09 02:59:53,17
Intel,niiwt1u,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 01:31:12,2
Intel,nimo4dq,Is there a fork of chrome that runs on gpus,hardware,2025-10-09 17:24:22,2
Intel,nindfe4,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",hardware,2025-10-09 19:31:05,2
Intel,nikln6u,but is that faster than a single 5090?,hardware,2025-10-09 10:10:09,2
Intel,nik40r8,Is this enough VRAM for modern gaming?,hardware,2025-10-09 07:06:00,-4
Intel,nil0agd,Nvidia: ill commit s------e,hardware,2025-10-09 12:07:51,-8
Intel,nil8gj0,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",hardware,2025-10-09 12:59:34,19
Intel,no4mb2m,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",hardware,2025-11-10 16:22:54,1
Intel,nikh8oe,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",hardware,2025-10-09 09:25:55,-14
Intel,nij5zhc,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",hardware,2025-10-09 02:27:37,39
Intel,niml0xn,I don't think servers are supposed to stay idle for long.,hardware,2025-10-09 17:09:18,2
Intel,niqkqmc,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",hardware,2025-10-10 07:57:46,2
Intel,nijeto3,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,hardware,2025-10-09 03:25:25,42
Intel,nilc2tl,Could that make it very cost effective for any particular use cases?,hardware,2025-10-09 13:21:04,6
Intel,nim2kjx,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",hardware,2025-10-09 15:38:17,4
Intel,nij8lcp,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,hardware,2025-10-09 02:43:33,31
Intel,nijhbrl,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",hardware,2025-10-09 03:44:18,6
Intel,nilfpel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",hardware,2025-10-09 13:41:48,4
Intel,nin9mev,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",hardware,2025-10-09 19:11:33,3
Intel,nijt662,At least its a human hallucination and not AI hallucination.,hardware,2025-10-09 05:21:24,16
Intel,nik407y,Can also be bad translation.,hardware,2025-10-09 07:05:52,2
Intel,nijj2tk,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",hardware,2025-10-09 03:57:40,16
Intel,nilgnxj,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",hardware,2025-10-09 13:47:09,2
Intel,nili8e7,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",hardware,2025-10-09 13:55:46,3
Intel,niokcbz,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,hardware,2025-10-09 23:22:33,12
Intel,nilzeoj,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",hardware,2025-10-09 15:22:43,27
Intel,nime2oj,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",hardware,2025-10-09 16:35:12,19
Intel,nilf97b,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 13:39:16,1
Intel,nim7yeq,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,hardware,2025-10-09 16:04:46,-5
Intel,nilnsw1,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",hardware,2025-10-09 14:24:56,-25
Intel,nionxww,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,hardware,2025-10-09 23:43:45,4
Intel,nimlaz8,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",hardware,2025-10-09 17:10:41,4
Intel,nimhv2z,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,hardware,2025-10-09 16:53:51,5
Intel,nimkx2o,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",hardware,2025-10-09 17:08:47,21
Intel,nin877l,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,hardware,2025-10-09 19:04:21,7
Intel,nir42up,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,hardware,2025-10-10 11:05:57,5
Intel,nirvt3d,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,hardware,2025-10-10 14:00:42,2
Intel,nixqbsp,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",hardware,2025-10-11 13:25:20,1
Intel,nimkw5p,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,hardware,2025-10-09 17:08:39,6
Intel,nilrz1f,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",hardware,2025-10-09 14:45:58,22
Intel,nilwkhh,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",hardware,2025-10-09 15:08:45,10
Intel,nioq01t,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,hardware,2025-10-09 23:56:01,5
Intel,nirvexf,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",hardware,2025-10-10 13:58:40,5
Intel,nimlpyk,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",hardware,2025-10-09 17:12:42,4
Intel,nimm20x,Celestial was based on Xe3p.,hardware,2025-10-09 17:14:19,6
Intel,niokiba,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",hardware,2025-10-09 23:23:33,1
Intel,niy2dcz,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,hardware,2025-10-11 14:38:03,3
Intel,nimmzhl,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,hardware,2025-10-09 17:18:48,13
Intel,nirvxip,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",hardware,2025-10-10 14:01:21,3
Intel,nin3vrr,That's not what my colleague's say,hardware,2025-10-09 18:42:31,2
Intel,nilz2fy,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,hardware,2025-10-09 15:21:03,26
Intel,nim3npl,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",hardware,2025-10-09 15:43:35,-14
Intel,ninrarq,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",hardware,2025-10-09 20:40:49,-6
Intel,niy1tvt,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",hardware,2025-10-11 14:35:03,1
Intel,nj09dos,Xe3p is a significant architectural advancement says Tom Petersen.,hardware,2025-10-11 21:46:07,2
Intel,nim48wq,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",hardware,2025-10-09 15:46:27,-14
Intel,nim6va1,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",hardware,2025-10-09 15:59:21,8
Intel,nimks6h,"So much buzzwords, yet it sounds like a stroke.  You need help.",hardware,2025-10-09 17:08:07,6
Intel,nip4uap,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",hardware,2025-10-10 01:23:47,4
Intel,nim4t3o,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",hardware,2025-10-09 15:49:13,13
Intel,nino52j,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",hardware,2025-10-09 20:24:55,-2
Intel,nirwlsr,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,hardware,2025-10-10 14:04:53,1
Intel,ninor5p,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",hardware,2025-10-09 20:27:56,-9
Intel,ninre0y,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",hardware,2025-10-09 20:41:15,7
Intel,nixpud1,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,hardware,2025-10-11 13:22:13,1
Intel,ninu5gl,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",hardware,2025-10-09 20:55:03,-1
Intel,nio14ia,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",hardware,2025-10-09 21:31:45,6
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,32
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,36
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,12
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,8
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,6
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,3
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,2
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,42
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,7
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,9
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,4
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-7
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,9
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,7
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,5
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,6
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,11
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,8
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,36
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,77
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,12
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,12
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,35
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,9
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,17
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,8
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,31
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,6
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,11
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,4
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-21
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,5
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,21
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,15
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,8
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,3
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,4
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,3
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,3
Intel,nfbo7bs,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,hardware,2025-09-20 22:33:22,139
Intel,nfb2xr8,Far more than I expected them to come out at. Damn.,hardware,2025-09-20 20:35:14,21
Intel,nfarr9a,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",hardware,2025-09-20 19:36:31,60
Intel,nfbz1kb,I’m getting one when it releases in Australia,hardware,2025-09-20 23:37:15,4
Intel,nfj2y23,Thats great and all but when will there be stock? (Canada),hardware,2025-09-22 02:00:50,2
Intel,ng8k9a2,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,hardware,2025-09-26 02:15:59,1
Intel,ngid0f0,I'm disappointed.  My order was canceled,hardware,2025-09-27 17:12:34,1
Intel,nfaolmx,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-20 19:19:41,1
Intel,nfbxe5d,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",hardware,2025-09-20 23:27:31,133
Intel,nfc739o,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,hardware,2025-09-21 00:26:10,17
Intel,nfateyp,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",hardware,2025-09-20 19:45:24,203
Intel,nfb479l,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",hardware,2025-09-20 20:41:48,80
Intel,nfb166b,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",hardware,2025-09-20 20:26:03,37
Intel,nfb3pdb,The 3090 does not have ECC on it's VRAM nor certified drivers,hardware,2025-09-20 20:39:13,18
Intel,nfc4ak9,Totally not worth it.,hardware,2025-09-21 00:08:54,-1
Intel,nfb6hdv,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,hardware,2025-09-20 20:53:44,-9
Intel,nfd2k0s,"I'm more looking forward to the B50, but obviously local pricing is everything.",hardware,2025-09-21 03:53:22,7
Intel,njp8t1o,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",hardware,2025-10-15 22:34:30,1
Intel,nfbgzuw,"It has SR-IOV, certified drivers and other professional features...",hardware,2025-09-20 21:51:30,23
Intel,nfc15d7,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",hardware,2025-09-20 23:49:42,7
Intel,nfdv26x,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",hardware,2025-09-21 08:04:03,15
Intel,nfbzlhb,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,hardware,2025-09-20 23:40:29,-11
Intel,nfeqn5s,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",hardware,2025-09-21 12:41:54,5
Intel,nfauwnn,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",hardware,2025-09-20 19:53:22,21
Intel,nfau9ib,Do not underestimate the lack of CUDA.,hardware,2025-09-20 19:49:57,32
Intel,nfdmqe2,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",hardware,2025-09-21 06:45:03,2
Intel,nfbbfys,A used 3090 is only $100 more.,hardware,2025-09-20 21:20:29,1
Intel,nfauhs7,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",hardware,2025-09-20 19:51:10,-16
Intel,nfc1012,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,hardware,2025-09-20 23:48:50,-3
Intel,nfb5jdp,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,hardware,2025-09-20 20:48:47,30
Intel,nfc0fwm,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",hardware,2025-09-20 23:45:29,19
Intel,nfbovre,Atlas 300i duo,hardware,2025-09-20 22:37:31,-4
Intel,nfb510h,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",hardware,2025-09-20 20:46:06,28
Intel,nfb3ro9,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",hardware,2025-09-20 20:39:33,8
Intel,nfc234i,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,hardware,2025-09-20 23:55:21,1
Intel,njpmmsp,Yeah it was very scummy imo,hardware,2025-10-15 23:55:13,1
Intel,nfcbz1s,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",hardware,2025-09-21 00:56:35,1
Intel,nfc72xn,It was meant to be a joke. Not so funny I guess.,hardware,2025-09-21 00:26:06,6
Intel,nfek88y,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",hardware,2025-09-21 11:57:29,7
Intel,nfsgdep,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",hardware,2025-09-23 16:15:20,2
Intel,nfd2naj,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,hardware,2025-09-21 03:54:03,11
Intel,nfc0nds,Why does this matter?,hardware,2025-09-20 23:46:45,19
Intel,nfv9260,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",hardware,2025-09-24 00:45:10,2
Intel,nfb63fj,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,hardware,2025-09-20 20:51:42,40
Intel,nfbbr35,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",hardware,2025-09-20 21:22:13,9
Intel,nfbedl2,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",hardware,2025-09-20 21:36:51,8
Intel,nfbifxk,Used market is freaking insane. It is better to grab new or open box.,hardware,2025-09-20 21:59:41,5
Intel,nfcx3qi,Over here used 3090s are sold for 500-600€.,hardware,2025-09-21 03:13:43,1
Intel,nfbbgrb,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",hardware,2025-09-20 21:20:36,30
Intel,nfhd7ci,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,hardware,2025-09-21 20:19:33,1
Intel,nfb5o6f,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,hardware,2025-09-20 20:49:29,22
Intel,nfayuhx,the super gpus are not expected to release soon?,hardware,2025-09-20 20:14:00,8
Intel,nfcl0od,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",hardware,2025-09-21 01:53:39,14
Intel,nfcm0px,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",hardware,2025-09-21 02:00:02,11
Intel,nfc9w3x,Used 4070 Supers dont sell for nearly $700+ on the low dude.,hardware,2025-09-21 00:43:40,4
Intel,nfco0nb,Yes running business of used cards is how its done.....,hardware,2025-09-21 02:12:43,3
Intel,nfdimrg,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,hardware,2025-09-21 06:07:29,9
Intel,nfbr125,Typically run at 250W though to be fair.,hardware,2025-09-20 22:50:29,5
Intel,nfc1wom,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",hardware,2025-09-20 23:54:16,-7
Intel,nfbgadn,not on the Vram like professional cards,hardware,2025-09-20 21:47:32,5
Intel,nfdjdro,Q4.,hardware,2025-09-21 06:14:16,2
Intel,nfdm9xu,I thought it was funny ¯\_(ツ)_/¯,hardware,2025-09-21 06:40:53,3
Intel,nfjxn7s,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,hardware,2025-09-22 05:47:02,3
Intel,nfddcxx,"Yeah, apparently a year’s worth of it",hardware,2025-09-21 05:20:13,-5
Intel,nfcew77,Because they're super late to the party.,hardware,2025-09-21 01:14:55,-8
Intel,nfb6fud,"Wow, 800-900USD after 5 years is more than I would have expected.",hardware,2025-09-20 20:53:31,15
Intel,nfdc18j,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",hardware,2025-09-21 05:08:46,4
Intel,nfc1e0n,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,hardware,2025-09-20 23:51:10,-6
Intel,nfc1j2x,For what exactly do they need vram without cuda ?,hardware,2025-09-20 23:52:00,-5
Intel,nfbq925,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",hardware,2025-09-20 22:45:52,-11
Intel,nfksagq,Gamers are not niche. They are 20 billion a year business.,hardware,2025-09-22 10:46:43,-1
Intel,nfbohl3,"False, they’re releasing in december or jan. So about 3 months from now",hardware,2025-09-20 22:35:06,4
Intel,nfbxm75,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",hardware,2025-09-20 23:28:51,12
Intel,nfbksrd,"The 3090 Ti did, but the standard 3090 did not.",hardware,2025-09-20 22:13:21,8
Intel,nfjz0lo,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",hardware,2025-09-22 05:59:38,3
Intel,nfdem0c,"If intel felt they could have released this safely earlier, they would have",hardware,2025-09-21 05:31:12,7
Intel,nfb6vit,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,hardware,2025-09-20 20:55:46,22
Intel,nfc975k,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",hardware,2025-09-21 00:39:26,1
Intel,nfg0b8g,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,hardware,2025-09-21 16:40:24,3
Intel,nfcmei6,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,hardware,2025-09-21 02:02:24,7
Intel,nfc3jog,"rendering, working on big BIM / CAD models, medical imaging,...",hardware,2025-09-21 00:04:18,21
Intel,nfc3y1b,CUDA isnt the only backend used by AI frameworks,hardware,2025-09-21 00:06:45,23
Intel,nfde0a9,"I use opencl for doing gpgpu simulations, this card would be great for it",hardware,2025-09-21 05:25:54,2
Intel,nfbqi8f,You mean your 0 profile history because you have it private?,hardware,2025-09-20 22:47:22,7
Intel,nfd62l7,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",hardware,2025-09-21 04:20:02,0
Intel,nfbqms5,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,hardware,2025-09-20 22:48:07,-1
Intel,nfksqka,Spoken like a true gamer.,hardware,2025-09-22 10:50:25,1
Intel,nfbt6h6,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,hardware,2025-09-20 23:03:03,2
Intel,nfcz630,You still get about 85% performance compared to stock settings.,hardware,2025-09-21 03:28:33,2
Intel,nfdr09z,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",hardware,2025-09-21 07:24:57,-3
Intel,nfbvai4,What are they doing that's making them money? Or are theu selling the compute somehow?,hardware,2025-09-20 23:15:17,17
Intel,nfc1jx5,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",hardware,2025-09-20 23:52:08,-7
Intel,nfc9f6k,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",hardware,2025-09-21 00:40:47,1
Intel,nfkntvr,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",hardware,2025-09-22 10:07:27,4
Intel,nfcsv0e,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",hardware,2025-09-21 02:44:26,3
Intel,nfdll3m,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,hardware,2025-09-21 06:34:32,1
Intel,nfks66x,It is the only functional one.,hardware,2025-09-22 10:45:44,1
Intel,nfbqlfx,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",hardware,2025-09-20 22:47:54,-6
Intel,nfd6ez2,0 people are using autodesk with an intel arc gpu lmao,hardware,2025-09-21 04:22:47,-1
Intel,nfbriv2,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",hardware,2025-09-20 22:53:25,0
Intel,nfkvz03,"Oh no, im part of 60% of world populatioin. how terrible.",hardware,2025-09-22 11:15:48,0
Intel,nfd2pwi,Which is 36% higher performance per watt.       ... to be fair.,hardware,2025-09-21 03:54:35,4
Intel,nfdsvbs,The reason they didn't is because it would have been worse to release it earlier,hardware,2025-09-21 07:43:03,0
Intel,nfbw0b9,Software Development,hardware,2025-09-20 23:19:26,-4
Intel,nfc44n9,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,hardware,2025-09-21 00:07:53,8
Intel,nfcbku1,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:54:09,-1
Intel,nfcbqrf,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:55:09,-4
Intel,nfkwldk,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",hardware,2025-09-22 11:20:33,1
Intel,nfcv5fj,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",hardware,2025-09-21 03:00:04,8
Intel,nfeidh5,Which ones?,hardware,2025-09-21 11:43:23,1
Intel,nfcjfv3,Private profile = Complete troll.  You're not an exception to this rule.,hardware,2025-09-21 01:43:42,12
Intel,nfbs5c3,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,hardware,2025-09-20 22:57:03,-3
Intel,nfdu2bk,"Right man, my point is that it shouldn’t have been",hardware,2025-09-21 07:54:31,-1
Intel,nfbxo4k,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,hardware,2025-09-20 23:29:11,14
Intel,nfcdfer,"Hell, one of them was just the cooler without the actual GPU.",hardware,2025-09-21 01:05:34,8
Intel,nfcde1y,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",hardware,2025-09-21 01:05:20,0
Intel,nfgcxg6,for example every local image and video generation system I've seen.,hardware,2025-09-21 17:38:32,3
Intel,nfbsbxa,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",hardware,2025-09-20 22:58:06,2
Intel,nfc0g5t,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",hardware,2025-09-20 23:45:32,-1
Intel,nfducqd,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,hardware,2025-09-21 07:57:16,3
Intel,nfd4nt4,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",hardware,2025-09-21 04:09:11,4
Intel,nfh4t9j,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,hardware,2025-09-21 19:41:40,1
Intel,nfd0war,And the one directly under that was the actual PCB...,hardware,2025-09-21 03:41:08,6
Intel,nfgecr2,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,hardware,2025-09-21 17:44:40,0
Intel,nfksgbg,Propaganda is 90% of chinas economic output though.,hardware,2025-09-22 10:48:04,0
Intel,ne6ahg8,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",hardware,2025-09-14 14:46:52,181
Intel,ne5xw93,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,hardware,2025-09-14 13:39:18,227
Intel,ne6669f,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",hardware,2025-09-14 14:24:15,63
Intel,ne680ly,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",hardware,2025-09-14 14:34:03,55
Intel,ne6hcg6,L1 techs had a great feature on these.,hardware,2025-09-14 15:21:17,20
Intel,ne62jka,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",hardware,2025-09-14 14:04:39,32
Intel,ne8mbnv,1:4 ratio of FP64 performance is a pleasant surprise,hardware,2025-09-14 21:19:44,10
Intel,ne6anee,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",hardware,2025-09-14 14:47:44,16
Intel,ne65b1l,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",hardware,2025-09-14 14:19:36,17
Intel,ne6rl68,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",hardware,2025-09-14 16:11:13,5
Intel,ne5wmh6,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-14 13:32:02,-2
Intel,ne6nm7f,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,hardware,2025-09-14 15:52:11,-21
Intel,ne7zdif,what's up my Neweggâs,hardware,2025-09-14 19:32:23,93
Intel,ne95tlq,My favorite game  Fallout Neweggas,hardware,2025-09-14 23:05:48,25
Intel,ne9ba6q,It also sounds like an ikea lamp name or something,hardware,2025-09-14 23:35:36,7
Intel,ne7p3l1,Geriau nei Bilas Gatesas,hardware,2025-09-14 18:44:05,3
Intel,ne6o1us,I gotta spell Neweggâs?!?,hardware,2025-09-14 15:54:14,1
Intel,ne6355p,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",hardware,2025-09-14 14:07:57,73
Intel,ne7314f,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",hardware,2025-09-14 17:04:11,5
Intel,ne8v3zo,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,hardware,2025-09-14 22:05:58,15
Intel,ne6kilx,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",hardware,2025-09-14 15:37:09,-72
Intel,ne7d8mq,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,hardware,2025-09-14 17:49:31,21
Intel,ne6eqfv,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",hardware,2025-09-14 15:08:14,-8
Intel,ne70sx3,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",hardware,2025-09-14 16:54:03,3
Intel,nealgik,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",hardware,2025-09-15 04:31:20,9
Intel,ne6c0of,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",hardware,2025-09-14 14:54:44,44
Intel,ne6dpqm,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",hardware,2025-09-14 15:03:08,20
Intel,ne6yi39,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,hardware,2025-09-14 16:43:33,11
Intel,nebc25u,ECC memory.,hardware,2025-09-15 08:46:16,1
Intel,neb16s5,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",hardware,2025-09-15 06:53:42,4
Intel,neaqhtz,Oh when they get enough enterprise customers they will definitely charge licensing fees,hardware,2025-09-15 05:14:05,4
Intel,ne70g18,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,hardware,2025-09-14 16:52:26,18
Intel,ne878lu,Lmao,hardware,2025-09-14 20:08:06,10
Intel,nebbyvw,Komentarą skaitai per tapšnoklį ar vaizduoklį?,hardware,2025-09-15 08:45:18,1
Intel,ne66mwj,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,hardware,2025-09-14 14:26:44,74
Intel,ne6xv28,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",hardware,2025-09-14 16:40:31,5
Intel,ne8a75b,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",hardware,2025-09-14 20:21:25,3
Intel,ne95ql6,Vulcan does fine with inferencing.,hardware,2025-09-14 23:05:20,3
Intel,nfekr88,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",hardware,2025-09-21 12:01:22,1
Intel,neavc5w,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,hardware,2025-09-15 05:57:49,3
Intel,ne8ur5d,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",hardware,2025-09-14 22:03:59,19
Intel,neprgo9,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",hardware,2025-09-17 14:56:51,0
Intel,ne6jp74,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,hardware,2025-09-14 15:33:02,37
Intel,ne8fips,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,hardware,2025-09-14 20:46:12,14
Intel,ne8eldw,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",hardware,2025-09-14 20:41:52,11
Intel,ne93icy,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",hardware,2025-09-14 22:52:49,3
Intel,nealnf3,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",hardware,2025-09-15 04:32:57,12
Intel,ne6dmlw,I recognize those as words...,hardware,2025-09-14 15:02:42,7
Intel,ne7cbc2,I think it was obvious I was being facetious.,hardware,2025-09-14 17:45:29,-2
Intel,ne6qvax,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",hardware,2025-09-14 16:07:47,21
Intel,ne6p0f3,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,hardware,2025-09-14 15:58:50,2
Intel,ne8kqkn,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",hardware,2025-09-14 21:11:39,13
Intel,ne95wnj,Have you tried GPU paravirtualization?,hardware,2025-09-14 23:06:16,1
Intel,nebgla3,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",hardware,2025-09-15 09:33:31,3
Intel,ne8r5hr,They need GPU IP for two things: client and AI. Anything else is expendable.,hardware,2025-09-14 21:44:37,4
Intel,ne9x6oh,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",hardware,2025-09-15 01:47:23,7
Intel,ne6f8e1,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",hardware,2025-09-14 15:10:44,35
Intel,neanu9y,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,hardware,2025-09-15 04:51:08,12
Intel,ne6s65g,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,hardware,2025-09-14 16:13:57,7
Intel,neaqr14,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",hardware,2025-09-15 05:16:19,2
Intel,neb03p4,Pathetic 1:64 ratio of FP64 flops,hardware,2025-09-15 06:43:15,2
Intel,neb28v9,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,hardware,2025-09-15 07:03:44,2
Intel,ne705od,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",hardware,2025-09-14 16:51:06,15
Intel,nea091d,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",hardware,2025-09-15 02:05:50,3
Intel,nebghm1,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",hardware,2025-09-15 09:32:29,3
Intel,necmv9o,my guy sr-iov is a type of gpu paravirtualization.,hardware,2025-09-15 14:23:28,1
Intel,nenukwd,The Asrock b60s are $599,hardware,2025-09-17 06:30:55,1
Intel,ne90ue7,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,hardware,2025-09-14 22:37:50,3
Intel,ne8u0oh,"One use case for ECC, is when the data is critical and can’t be lost.",hardware,2025-09-14 21:59:59,9
Intel,nea1lfo,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,hardware,2025-09-15 02:14:04,12
Intel,nebc61d,ECC should be in literally all memory.,hardware,2025-09-15 08:47:24,1
Intel,ne70ox2,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",hardware,2025-09-14 16:53:32,-10
Intel,ne6xml5,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,hardware,2025-09-14 16:39:25,7
Intel,nee4dd7,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",hardware,2025-09-15 18:42:34,2
Intel,neozcix,"Where can you get them? And are they for sale yet, or pre-orders, or...?",hardware,2025-09-17 12:27:00,1
Intel,ne9vlfz,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",hardware,2025-09-15 01:38:00,8
Intel,ne9xfff,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",hardware,2025-09-15 01:48:50,5
Intel,neav264,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,hardware,2025-09-15 05:55:13,2
Intel,neaiu6t,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",hardware,2025-09-15 04:10:17,11
Intel,ne78lg0,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",hardware,2025-09-14 17:29:09,14
Intel,neea3nk,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",hardware,2025-09-15 19:10:25,6
Intel,ne79g56,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,hardware,2025-09-14 17:32:53,8
Intel,ne79riz,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",hardware,2025-09-14 17:34:16,-13
Intel,ne8go27,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",hardware,2025-09-14 20:51:40,10
Intel,ne92tmq,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",hardware,2025-09-14 22:49:00,-7
Intel,nd4rusz,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",hardware,2025-09-08 18:55:24,85
Intel,nd5dm3n,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,hardware,2025-09-08 20:41:18,24
Intel,nd8r4il,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",hardware,2025-09-09 10:54:51,10
Intel,nd57awu,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,hardware,2025-09-08 20:11:04,27
Intel,nd5z0zb,Too late for me I already went with a 9060xt but hell I had dreamt of it!,hardware,2025-09-08 22:32:44,4
Intel,nd4ofdy,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",hardware,2025-09-08 18:38:32,2
Intel,nd5mkse,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,hardware,2025-09-08 21:25:37,4
Intel,nd8g10o,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,hardware,2025-09-09 09:13:47,2
Intel,nd4ety2,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-08 17:51:42,1
Intel,ndaptcd,Depending on the price I might give it a shot.,hardware,2025-09-09 17:16:31,1
Intel,nd7euko,Love it going to get one if I can scratch some money together,hardware,2025-09-09 03:38:15,1
Intel,nd5tii5,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,hardware,2025-09-08 22:02:05,-5
Intel,nd6wdjt,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",hardware,2025-09-09 01:46:51,0
Intel,nd4lt4f,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,hardware,2025-09-08 18:25:36,-11
Intel,nd5ec1f,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",hardware,2025-09-08 20:44:45,-9
Intel,nd50mkd,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,hardware,2025-09-08 19:38:23,14
Intel,nd7icfg,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",hardware,2025-09-09 04:02:28,-3
Intel,nd91aqv,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",hardware,2025-09-09 12:07:11,6
Intel,nd5nmhj,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",hardware,2025-09-08 21:31:01,20
Intel,nd50bqm,I have a B580 and the driver seems pretty stable to me at this point.,hardware,2025-09-08 19:36:56,37
Intel,nd4xpjv,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",hardware,2025-09-08 19:24:08,10
Intel,nd5kt0f,my b580 has been stable,hardware,2025-09-08 21:16:33,8
Intel,nd7ucd7,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",hardware,2025-09-09 05:39:29,2
Intel,nd86r6j,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",hardware,2025-09-09 07:37:52,2
Intel,nd7ihdv,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,hardware,2025-09-09 04:03:27,3
Intel,nd4pva0,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,hardware,2025-09-08 18:45:36,14
Intel,nd4qkrx,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",hardware,2025-09-08 18:49:07,20
Intel,nd57ffs,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,hardware,2025-09-08 20:11:40,39
Intel,nd5dyvw,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",hardware,2025-09-08 20:43:00,1
Intel,nd63tmr,Where do you find such information?,hardware,2025-09-08 23:00:34,16
Intel,nd648nd,Source: I made it the fk up,hardware,2025-09-08 23:03:03,5
Intel,nd55qwm,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",hardware,2025-09-08 20:03:30,18
Intel,nd4z315,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",hardware,2025-09-08 19:30:52,11
Intel,nd7nnb6,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,hardware,2025-09-09 04:42:33,42
Intel,nd6d7u0,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",hardware,2025-09-08 23:55:26,15
Intel,nd6ck88,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",hardware,2025-09-08 23:51:40,9
Intel,nd5oln7,Try Mechwarrior 5: Clans on high and say there are no problems again.,hardware,2025-09-08 21:36:04,-13
Intel,nd6bnfg,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",hardware,2025-09-08 23:46:18,14
Intel,nd89f2e,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,hardware,2025-09-09 08:05:10,11
Intel,nd6cup4,"It doesn't even run, it crashes left and right on an Arc.",hardware,2025-09-08 23:53:20,-4
Intel,nd6drxa,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",hardware,2025-09-08 23:58:41,5
Intel,nd6en4h,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",hardware,2025-09-09 00:03:40,-1
Intel,nsbtjff,"Make sure you have resizable bar turned on inside your bios and hopefully you got a pretty solid CPU. Other than that, this is one of the better gpus for 1080p gaming.",buildapc,2025-12-04 22:48:16,1
Intel,nsbu3av,"My current cpu is 5 5600g but I'll upgrade it next year probably, for now it's not that bad ofa bottleneck I hope. I will turn resizeable bar as soon as my new mobo comes, I got the Asus Rog Strix B550-A gaming. Also my monitor is 1440p but from what I've seen, this gpu is perfect for that resolution.",buildapc,2025-12-04 22:51:21,1
Intel,nsh5q2i,no need to be very solid. an entry level cpu suffices  i'm using 225f and most of the time during gaming the gpu is 100% and the cpu is 33%,buildapc,2025-12-05 19:35:15,1
Intel,nssf66t,I sold a 3080 for $260. I think you can get a 3070ti for that price.,buildapc,2025-12-07 17:13:36,4
Intel,nssh9h6,"Your current GPU is the $100 option.  To double your performance you'll need something on par with RTX 4060 or RX7600 or RTX2080 or RX6650XT - all out of your reach.  RTX2060 Super will give you 50% uplift for $200.  ARC is still hit and miss, but A580 is around the same as 2060 Super. RX6600 is the same category.",buildapc,2025-12-07 17:24:22,2
Intel,nst9vao,"You will be best off navigating the used market. $100 is a pretty tough number to work with that would likely limit you to sidegrades or very small increases - closer to $200 will be able to get you a really nice upgrade, and there are a few decent choices in-between.  I'd be looking at a 3060 Ti personally - they're a really good used option that can be found around the $175-200 range right now. If you can find a good deal, the 6600XT is another solid choice to consider, though it'd be a smaller uplift and they're not much cheaper than the 3060 Ti from what I've been seeing.",buildapc,2025-12-07 19:41:58,2
Intel,nstcjxx,"If you're rocking 4 sticks of ram, sell 32bg of ram then buy a better GPU. It sounds troll but with current ram prices this might be the best option to increase your budget  32 gb total is more than enough nowadays",buildapc,2025-12-07 19:55:06,2
Intel,nssh6t6,Id say look for a new b570 or a used 2070 super.,buildapc,2025-12-07 17:24:00,1
Intel,nstpl7h,"honestly, why not give the older 1080 Ti a try. it may not have the fancy features but it does go raster for raster with the 4060.  just basing this off of benchmarks from Passmark, the score for the 590 is **9315** and the score for the 1080 Ti is **18607**.   (The 1080 is in the $150 range currently)",buildapc,2025-12-07 20:59:40,1
Intel,nstv4ga,"Gaming @ 1080p around $100-200, shoot for an AMD RX 5070xt from ebay. Buddy got one for $69 a few weeks ago. Just depends on how lucky you get.",buildapc,2025-12-07 21:27:02,1
Intel,nsta9ii,Someone got a good deal. I'm mainly seeing 3080 10 GB models for around $300 atm - really nice used option right now for sure.,buildapc,2025-12-07 19:43:54,1
Intel,nstmu0w,He can get a 2070 Super for $200. Has to negotiate. I got a full 3080 PC for $615 with a 10700k.,buildapc,2025-12-07 20:45:55,1
Intel,nt0irw2,"That's ingenious, actually.",buildapc,2025-12-08 22:27:39,1
Intel,nssa7pd,"The rx9060xt 16gb would be fine for those games at 1440p.  Having the 7800x3d would be fine for getting a GPU upgrade in 3-6:years.  Something like a r7-7700 and rx9070xt would be better right now, but the additional gaming headroom of the 7800x3d may be nice later with a stronger GPU.",buildapc,2025-12-07 16:48:06,13
Intel,nssaf0e,"Completely irrelevant, but there’s a 9070xt on sale for 560 at micro center if there’s one near you",buildapc,2025-12-07 16:49:08,6
Intel,nssbsel,"I would recommend a 5070 or 9070 if you can find them around the $500 price range, those will give you much better performance.",buildapc,2025-12-07 16:56:12,5
Intel,nssbsf8,"If you haven’t gotten the 7800x3d yet, I’d highly recommend something like a 9600x or 9700x (7600x and 7700x also work) and going with a 9070xt or 5070ti. Pairing an x3d with a “mid range” GPU like a 5060ti or 9060xt isn’t the best way to spread your money",buildapc,2025-12-07 16:56:12,2
Intel,nssdng9,"If between the two, the rtx 5060ti slightly better than rx 9060xt in almost all games. Not that much big different. If u have the money go for rtx5060ti. I have rx 9060xt for me its more than enough.  Edit forgot to mention the rtx have better ray tracing",buildapc,2025-12-07 17:05:51,1
Intel,nssixq9,"Just make sure of getting the 16GB version.   Look for those specific games benchmarks in YouTube, as for example, ""ARC Raiders 9060XT 16GB vs 5060Ti 16GB""",buildapc,2025-12-07 17:32:51,1
Intel,nstfjwx,The 5060 ti is 40 percent faster than the 9060 xt in arc raiders(very heavily nvidia favoured title). E33 doesn't have fsr 4 native only dlss 4 so yeah I think the 5060 ti is the easy choice here.  In most games they are pretty equal though with about a 4-6 percent average lead for the 5060 ti,buildapc,2025-12-07 20:09:52,1
Intel,nstg6ir,"I have a 5070ti (Gigabyte) and a 7900XTX (Sapphire Pulse). IMHO, the 5070 has a better pack of features than the 7900. The performance is similar, with a tiny variation in specific games. NVIDIA app has a ton of customization and QoL features that tip the scale for me.   Now, regarding GPUxCPU... Always split more of your budget towards a video card than you do towards CPU. As you climb resolution or graphic configurations, the performance is heavily tethered down to what your GPU can handle.   There's a great chart by Tom's Hardware in the [following link](https://www.tomshardware.com/pc-components/gpus/cpu-vs-gpu-upgrade-benchmarks-testing) that'll be a good representation of what I'm saying.   You can never go wrong with a GPU upgrade when in doubt.   PS: Wait a little for RAM chips to get cheaper. When the AI bubble bursts, hopefully the chips will plummet xD.",buildapc,2025-12-07 20:13:02,1
Intel,nstpebr,"I would save a bit on the CPU here (7600(X)/9600(X)) and go for the bigger GPU here. But it depends what games you plan on playing in the future, graphics heavy go bigger GPU, competitive heavy go CPU.",buildapc,2025-12-07 20:58:43,1
Intel,nssly55,I would recommend 5070ti,buildapc,2025-12-07 17:47:39,-1
Intel,nssfq4z,"I got the 5700x3d on sale when they were clearing stock, fits the rx9070xt perfectly for 1440p",buildapc,2025-12-07 17:16:28,1
Intel,nssexua,With 16 GB? I'm not sure if that's possible to find this cheap,buildapc,2025-12-07 17:12:23,3
Intel,nssljs0,"Thanks! I found a 5070ti for around $850 and I'm considering pairing it with the 9600x, seems like the benchmarks are pretty good",buildapc,2025-12-07 17:45:41,2
Intel,nssmfx8,"Heck of a chip, but unfortunately a 5700x3d used costs about what a 7800x3d costs new these days.",buildapc,2025-12-07 17:50:03,2
Intel,nssfvo0,"If you want 16GB, get the RX 9070, the 5070 has only 12GB.",buildapc,2025-12-07 17:17:15,2
Intel,nsso476,"Yeah you’ll get more mileage out of that combo than the previous one. CPUs contribute less than GPUs and even less the higher your resolution is. At 1440p or 4k, you’re going to be mostly reliant on the GPU so your cpu doesn’t make as much of a difference",buildapc,2025-12-07 17:58:13,1
Intel,nssp4u3,Yeah you had to get it during the cleareance sale last october. I upgraded from a 3600x for 150 dollars,buildapc,2025-12-07 18:03:10,3
Intel,nsso4r9,"[https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N507TGAMING-OC-16GD/dp/B0DTRC7782?th=1](https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N507TGAMING-OC-16GD/dp/B0DTRC7782?th=1)  Am I missing something, I'm pretty sure this is 16 GB",buildapc,2025-12-07 17:58:18,1
Intel,nstdfoy,"In 1440p, an x3d GPU will give up to 20% more FPS compared to a 9700x.  It's only in 4K where they're almost dead even, and most people aren't playing at 4k.",buildapc,2025-12-07 19:59:27,1
Intel,nst1kpb,Amazing upgrade!  That 5700x3d will last ya 3-5 years easy,buildapc,2025-12-07 19:02:07,2
Intel,nssp5ek,That says $800 for me so that’s def not in your range,buildapc,2025-12-07 18:03:15,1
Intel,nsukviq,"That's the TI version. RTX 5070, RX 9070, RTX 5070 ti, and RX 9070 XT are all 4 different cards.",buildapc,2025-12-07 23:43:51,1
Intel,nssqumu,"Yea that's true, but if I'm downgrading the CPU a bit I might choose this",buildapc,2025-12-07 18:11:22,1
Intel,nssr5lr,"You can do what I did and go for the ryzen 7 7700x and pair it with the 5070ti, my limit was actually my monitor as it doesn’t go above 1440",buildapc,2025-12-07 18:12:46,1
Intel,nrxqzab,"If you have any plans for a future GPU upgrade, swap the PSU for MSI Mag 850w.  But overall, I don't see a problem with this build. It's solid for 1080p+60FPS gaming.",buildapc,2025-12-02 19:29:01,5
Intel,ns115tb,"Overpaying for cpu cooler unless you care about keeping it for potential upgrade but if that was the case you would likely get a better motherboard. The money spent towards case and fans would be better spent towards a case with more upgradability and built in fans like the montech xr or montech 903 airmax. If you don't care too much about upgrading to a higher end gpu in a similar tier to the 9070, you could change out the psu to like an Adata XPG pylon 550w which is actually similar quality but alot cheaper.",buildapc,2025-12-03 07:06:10,2
Intel,nrxw7hj,Nothing wrong with build,buildapc,2025-12-02 19:54:31,1
Intel,nry2msj,"Just posting this comment for future builders: wait until you have enough money to buy everything at the same time. Why?  * So you don’t get stuck with a single component that is hard to find or is having a price spike, leaving you with an unfinishable build * you won’t be able to return first set of components to the store you bought them from, if something didn’t work. Most stores have 14-30 day return policies. Returning to the store is a lot easier than doing a warranty claim with the part maker.  * You can’t test that everything works together until you can fully assemble the PC. See previous point about return policies.",buildapc,2025-12-02 20:25:56,1
Intel,nry63xt,"All looks good. Except be quiets arent really good case fans. Get thermalright cl14s or something else, anything really. If u wanna get a better psu and save somewhere else, just get one cl14 and maybe a cheaper thermalright cooler. I bought a used thermalright gf1 750w for 55 eur, so u can look into that as well.",buildapc,2025-12-02 20:42:57,1
Intel,nry945n,What is the total allotted budget? And location?,buildapc,2025-12-02 20:57:28,1
Intel,ns222e8,The thermalright royal pretor is as expensive on aliexpress (here in Belgium last i checked) but waaay better at cooling.  I'm using it myself on a 7800x3d and my idle is 30°c and gaming it goes 55°c. I haven't done a stress test yet though. (might boot one up rn)  I would also recommend you not to skimp on the psu but everyone has their reasons and preferences.,buildapc,2025-12-03 12:41:34,1
Intel,ns2d2jv,"Above build is okay.  If your budget allows you to change the motherboard to a B650, then do it",buildapc,2025-12-03 13:49:37,1
Intel,ns4ll27,i'd recommend using intel 225f with b860 mobo. no need to buy an extra cpu cooler as the 225f comes with one. overall that'd be cheaper than what you planned and intel platforms are more stable and reliable,buildapc,2025-12-03 20:25:17,1
Intel,nrxpzum,"First build I’ve ever seen on this subreddit with an intel gpu  Looks pretty good to me, I would change out the PSU, you should be going for gold rated or higher",buildapc,2025-12-02 19:24:13,0
Intel,nrxtcga,"Dawg, I was already gonna use the 550w version of that PSU, but switched over bc the 650w one was gonna let me upgrade the GPU down the line. My total wattage is like 380 lmao",buildapc,2025-12-02 19:40:39,0
Intel,ns3k7iu,"Actually, what would be a better mobo?",buildapc,2025-12-03 17:26:22,1
Intel,ns2c5vd,"800€. I went over that, but I can just about manage with this. I'm in Lithuania",buildapc,2025-12-03 13:44:25,1
Intel,ns2cy5l,"Well, the PSU is ok, right? Like, it works according to reviews",buildapc,2025-12-03 13:48:56,1
Intel,nrxydcc,"I think I was clear when I said ""IF"" you want to upgrade.  Upgrade isn't only from B580 to something like 5060 Ti. It could be a 5070Ti or 5080 too.",buildapc,2025-12-02 20:05:02,4
Intel,ns4vz5r,It's quite a bit more but also a board with excellent power delivery and the feature set and io is improved. It's the b650 eagle ax for 50 more euros.,buildapc,2025-12-03 21:15:18,2
Intel,ns2cvbd,I cant. Due to no option for Lithuania in pcpricetracker.,buildapc,2025-12-03 13:48:29,1
Intel,ns2f7oq,"Thinking about it. It really depends on you.  Are you upgrading next year, go for the 850w one. Also if it is not a big upgrade. You want some headroom and also keep that battery at it's sweet 50% mark.  If you are upgrading waay later like 2-3 years. Just get this one and save up for a good psu  Edit: your psu has most important safety features but is nothing to write home about. With a warranty of 5 years. So you should be fine.  It has everything you need but constraints you in wattage and only one 4x4 connector. (if you plan on upgrading your motherboard for a better cpu)",buildapc,2025-12-03 14:01:55,2
Intel,nryqgkt,Look at you contributing to the post,buildapc,2025-12-02 22:21:27,1
Intel,ns21ejb,Not the way I woukd word it but he is right. If you buy a good psu it will last you multiple builds. :),buildapc,2025-12-03 12:36:57,1
Intel,nrxyyf1,"Yeah, but if I wanted to upgrade my GPU, I probably wouldn't immediately jump to the latest and greatest",buildapc,2025-12-02 20:07:56,1
Intel,ns2ja44,"True, but what is the ""sweet 50% mark""?",buildapc,2025-12-03 14:24:45,1
Intel,ns3hrz4,more of a contribution than saying “go for gold bro”,buildapc,2025-12-03 17:14:40,0
Intel,nrxzj78,"Well sorry that I'm not your childhood friend and I don't know your preferences by default.  Because there people here in this sub who upgrade from 1070 to 5080. And you know it's indeed ""POSSIBLE"" to jump from a low end card to the latest and greatest.",buildapc,2025-12-02 20:10:44,1
Intel,ns3jksk,At 50% capacity the psu will run most efficiently and also stay optimal longer,buildapc,2025-12-03 17:23:19,1
Intel,nrxzo99,"Well, sorry ig",buildapc,2025-12-02 20:11:24,0
Intel,nrxeoei,"If GPU isn't close to 100% you are CPU limited. The only sizable upgrade would be a ryzen x3d CPU (the i9s aren't much faster than the 13700k), which means new motherboard and ram.   I would honestly stick with what you have. If you want to improve performance a bit you could try overclocking the CPU and ram.",buildapc,2025-12-02 18:30:10,2
Intel,nrxew34,"How does 80% usage point to your GPU needing to be upgraded? The only worthwhile jump for you would be either an X3D chip CPU which is not cheap as you'd have to swap mobo and ram, or going to like a 5070 TI which is only I think like a 30% performance jump for like $750 at best?",buildapc,2025-12-02 18:31:11,1
Intel,nrxfng8,"Looks pretty good as is, you are cpu limited though. Running at 1080p? Only thing I would suggest (which is free) is ensuring XMP is enabled and your ram is running st full speed. Other than that, you could try disabling e-cores and/or overclock the P-cores.  Also would you really benefit from more than 160-200fps?",buildapc,2025-12-02 18:34:47,1
Intel,nrxi1oh,"Leave it as is  Tweak your RAM for a bit more negligible performance gains  If you want more out of your current GPU, you'd want to get on the AM5 platform   If you want to ""bottleneck"" more so with a faster GPU, you'll have to upgrade your power supply as well",buildapc,2025-12-02 18:46:05,1
Intel,nryumdm,"I have a 9800X3D and get the same numbers in BF6. I run it at 1080p low.   There's really no point in upgrading. BF6, Arc raiders, Warzone are all CPU-bound games. But they all run very well.   In order for you to get higher than the 160-200fps that you're currently getting in those games, you'll need to either use DLSS and/or Frame Gen (it looks really ugly and fuzzy using DLSS at 1080p); or spend over $1200 upgrading CPU and GPU for what would amount to barely a 40fps increase - if any. DDR5 prices are through the roof and with me being on DDR5, I'm telling you that moving to DDR5 alone is NOT going to give you a performance improvement because your limitation is not memory bandwidth as I already stated I get similar numbers.   Just keep what you have. You got a great rig. No reason to needlessly upgrade your rig when the improvement isn't going to give you what you're looking for. And thats even after selling your old parts.  Overclocking might give you a very tiny bump in performance - but in order to have a stable and reliable overclock, on GPU, CPU & RAM... its incredibly tedious work that takes weeks of testing every single change - for what amounts to usually less than a 10% gain. CPU & GPU already overclock themselves due to algorithmic boost clocks that are based on temperature headroom and the load of the instruction set as it is.",buildapc,2025-12-02 22:43:11,1
Intel,nrzjiz6,Get a cheap AMD second GPU and run lossless scaling 👍 you might need a bigger PSU but overall this is a cheap upgrade path for 2x dos since you'd have to upgrade to a beefier GPU or new mobo+CPU to get gains here.,buildapc,2025-12-03 01:03:13,1
Intel,nrxkne9,"Your CPU is fine – a 13700KF isn’t holding you back in Warzone/BF. The weak link right now is the RAM. You’ve got 32GB of DDR4-3200, which is pretty slow for that chip. Swapping to a 32GB kit of DDR4-3600 CL16 is cheap and will help 1% lows and overall smoothness.  On the GPU side: even though you “only” see ~80% usage, it can still be the limiter. GPU usage is an average – it can spike to 99% in microbursts, and as long as GPU frame time is higher than CPU frame time, the GPU is what’s capping FPS. In practice, if the GPU is consistently loaded more than the CPU in a game, you’re effectively GPU-bound.  If you want more FPS right now, a 4070 Ti Super or 4080 Super is the logical upgrade. If you’re not in a rush, you could skip a mid-cycle upgrade and wait for a 50-series card (5070/5080/5090) once prices settle – that’ll be a bigger jump per dollar than going from a 4070 to another 40-series card today.  Also, the HYTE Y40 looks great but can be a bit tight on GPU airflow. Adding a couple of bottom intake fans blowing at the GPU is a cheap tweak that helps temps and lets the card hold higher boost clocks.  TL;DR: – Keep the 13700KF – Upgrade RAM to 32GB DDR4-3600 CL16 – You’re still effectively GPU-limited even at ~80% usage – Options: 4070 Ti Super / 4080 Super now or wait and jump to a 50-series (5070/5080/5090) – Add bottom intake fans for better GPU airflow",buildapc,2025-12-02 18:58:20,0
Intel,nrxl0aa,He's not CPU limited at 40% usage.,buildapc,2025-12-02 19:00:02,-1
Intel,nrxzu24,"Game's don't use many cores, so CPU usage stays low even when you are cpu limited",buildapc,2025-12-02 20:12:12,2
Intel,nrytacp,"40% utilization on OP's 13700KF can mean there are 4-6 cores that are at 100% utilization.   Most games barely use 6 cores so a CPU that has 16 cores like OPs CPU, a CPU doesn't have to hit 100% utilization in order to be CPU-limited.   Many games behave this way because they are primarily CPU-bound and there really isn't anything you can do about it.   OP upgrading to a 13900k or 14900 MIGHT help, but the performance improvement is going to be so low that it will never actually justify the cost of upgrading.",buildapc,2025-12-02 22:36:09,1
Intel,nrr741z,r/buildapcforme,buildapc,2025-12-01 18:57:32,1
Intel,npohn56,Excellent bang for buck with drivers getting better and better.,buildapc,2025-11-19 15:15:47,5
Intel,npoiebw,They're good if it's a decent bit cheaper than the 9060 xt.  You also need a fast CPU since the b580 scales with how good/bad your CPU is.,buildapc,2025-11-19 15:19:44,2
Intel,npohrqg,"Are you buying brand new or used? If you are considering new, I would suggest going used instead as you can get a better card for the same amount of money as a new B580.",buildapc,2025-11-19 15:16:27,1
Intel,npoivsa,I mean it's alright. It's got 12GB of VRAM which is cool I guess but XeSS is not that great compared to DLSS 4 and FSR 4 these days and the performance is slower than a 5060 or 9060 XT 8GB.,buildapc,2025-11-19 15:22:14,1
Intel,npp92i2,"I have a B580 linked up to a 4K TV. The only quirk I've really noticed is that changes in resolution, refresh rate or HDR on/off will occasionally cause the TV to hard reboot. I'm not sure if this would happen with a regular PC monitor, but this never happened with the previous GTX 750 Ti I had.  Otherwise performance in games is as expected from reviews. I don't play AAA games in 4K with it, but haven't had crashing issues that some people report.",buildapc,2025-11-19 17:32:52,1
Intel,npq0ffm,"with how bad GPU pricing is slated to be due to the memory shortages, I'm starting to consider a B580 so I can at least have a GPU that should be able to play BeamNG in 3440x1440. not keen on having to replace it within less than three years though, but maybe I'm severely overestimating how much GPU I actually need in a realistic gaming scenario. the used market here has been rather dodgy as of late.",buildapc,2025-11-19 19:47:53,1
Intel,npohv53,"I'm happy with my B580. It runs games in 1440p with frame generation, was the best option for this price.  Come visit r/IntelArc to learn more.",buildapc,2025-11-19 15:16:56,1
Intel,nppmr5z,"That's their problem. Intel cards are fine, but AMD cards are priced almost the same and they are more reliable.",buildapc,2025-11-19 18:40:05,2
Intel,npp3dwn,i never buy used hardware. it doesn't feel good,buildapc,2025-11-19 17:04:10,1
Intel,npp3snh,"don't know what op's monitor is. if op is going to connect the pc to a tv, tv usually has upscaling function and we do not need the upscaling algorithm in the gpu",buildapc,2025-11-19 17:06:13,1
Intel,npqr959,"B580 changed everything.... For the six months until AMD released 9060XT.  I hope they keep making GPUs tho. B580 is seriously impressive for the price, but they've got so much catching up to do.",buildapc,2025-11-19 22:03:57,2
Intel,npqdvbi,"Agree, the only high ticket items i buy used are things i can physically fix myself such as cars and bikes",buildapc,2025-11-19 20:56:35,1
Intel,npquxfp,"You are incredibly misinformed. TV upscalers are nothing like DLSS and FSR.  A TV's upscaler really doesn't work all that well in gaming one because they generally don't really make games running at 1080p look like 4K since that's what most TVs are nowadays and this is coming from someone with a 4K Samsung.  Also DLSS and FSR improve performance, reduce motion blur, give better anti aliasing than regular TAA all of which a TV upscaler doesn't do.   Also TV upscalers introduce extra processing lag from the upscaling which DLSS and FSR straight up lower latency because of the performance uplift.",buildapc,2025-11-19 22:23:26,1
Intel,npqw09s,"It's technically slower than even the 9060 XT 8GB which is usually around the same price give or take 20-30 bucks. I wouldn't call it ""seriously impressive for the price"" at all it's only saving grace is having 12GB of VRAM but it's got iffy drivers, cpu overhead and a worse upscaler.  It's not impressive at all. It's decent at best. It's cool that it has 12GB of VRAM and that might matter in a handful of games at 1080p at maxed out settings or matter more in 1440p but it really lacks reasonable performance to play the latest games in 1440p without relying on upscaling which it has been surpassed by in FSR4",buildapc,2025-11-19 22:29:13,0
Intel,npvnz68,"oh thank you so much for the info. i'd look into that. i'm using a samsung 65"" 4k tv and i usually output 1440p from the gpu via hdmi to the tv and let the tv do the upscaling thing. i'd try enabling the upscaling in gpu and output 4k to the tv and see if i could spot the difference",buildapc,2025-11-20 17:46:03,1
Intel,npqx10b,>around the same price give or take 20-30 bucks.  Not the case at all where I live lmao. More like a 100 euro difference between a piece of shit 8gb vram card and the B580.,buildapc,2025-11-19 22:34:44,1
Intel,npqxt47,Cheapest b580s are 250 here. Cheap 9060 XT 8GBs are like 270. The b580 is pointless imo.,buildapc,2025-11-19 22:39:00,1
Intel,npr1khc,"Cheapest B580s are 250 here, the cheapest 9060XT 8gbs are 340. Ergo, B580 is not pointless. Simple stuff.",buildapc,2025-11-19 22:59:42,1
Intel,npud5fc,It's pretty pointless to pay 20 extra less bucks for a card that performs worse.,buildapc,2025-11-20 13:49:51,1
Intel,nssfkvr,"I would do the 5070Ti and use the other money towards a 2k monitor.  A 5080 on 1080 is going to be overkill like crazy, and I think a 5070ti will be plenty fine for 1440.  You're getting close to your monitor being your bottleneck with any upgrades.",buildapc,2025-12-07 17:15:42,17
Intel,nstn28h,9070xt!,buildapc,2025-12-07 20:47:03,4
Intel,nsshdok,Just grab a 5080 for 150. Buy once cry when paying the statement balance.,buildapc,2025-12-07 17:24:58,1
Intel,nssi02w,"Buy the best you can afford, this is the main rule and advice. If you can afford buy 5080. If you're want more budget friendly and more value option - 5070ti.",buildapc,2025-12-07 17:28:08,1
Intel,nssibix,The best time to buy a gpu will always be yesterday I say go for it,buildapc,2025-12-07 17:29:46,1
Intel,nssli62,Like others said 5070 Ti and save the money to go towards a nice OLED. There’s a 0% chance we’re getting a SUPER refresh if consumer memory is this expensive.,buildapc,2025-12-07 17:45:28,1
Intel,nstbq7r,"Even if you had a 4k monitor, it would be difficult to justify a 5080. Usually the 80 series is the most compelling price point, but this generation it isn't. You're paying a lot for very little performance improvement. Get a 5070ti. Especially with your current monitor.",buildapc,2025-12-07 19:51:05,1
Intel,nstmkjx,Is 850watt PSU enough for the 5070ti?,buildapc,2025-12-07 20:44:40,1
Intel,nsu9486,The 5070ti for sure. Then switch to 1440p OLED. exactly what I did but with a 5800x3D,buildapc,2025-12-07 22:37:29,1
Intel,nszebxo,9070xt now might be able to find a 4070ti super used or open box somewhere for cheaper if you want nvidia,buildapc,2025-12-08 19:05:11,1
Intel,nssemnj,"5070 Ti is the better choice. Especially at 1440p, you really don’t need anything more. 5080 is only 15% faster and is 30% more expensive.",buildapc,2025-12-07 17:10:49,1
Intel,nssjoq3,Get the RTX 5080 Ventus.,buildapc,2025-12-07 17:36:33,1
Intel,nssg1fr,"I think the 5070ti is a good choice.  Keep in mind, you'd need to upgrade the PSU if you go higher which would add a little more costs to them.",buildapc,2025-12-07 17:18:05,1
Intel,nsspa6f,Whait a couple more years and then ask the same question again. There will always be next big/good thing. Don't look into future that much if you have the need right now.,buildapc,2025-12-07 18:03:52,1
Intel,nssbudg,"Hell yeah a 5080 will be a really good upgrade !   It is worth it, if you have the money for it.  You will you will.. worst case scenario you crank the graphics. Obviously you will feel the slower CPU and RAM down the line but not tomorrow !   And don't forget to set nvidia boost thing to just ON not on+ boost since you will have a weaker CPU than GPU",buildapc,2025-12-07 16:56:29,-1
Intel,nsseitr,If you can grab the 5080 for retail do that.,buildapc,2025-12-07 17:10:17,0
Intel,nst6k6b,"I would keep it, isn't 2070 good enough?",buildapc,2025-12-07 19:25:44,0
Intel,nssiwyx,I ended up getting the 5070ti for the reasons you mention. After doing research it seemed like 5070ti was great for 1440 and so far I’ve been loving it. I’m getting around 140fps on Arc Raiders.,buildapc,2025-12-07 17:32:45,5
Intel,nsske6u,Yep the 5080 is a bad value compared to the 5070 Ti and is nowhere near the 5090 despite the name.,buildapc,2025-12-07 17:40:02,3
Intel,nsslsyq,"Exactly, not sure what everyone is smoking to justify the 5080 when OP wants a monitor too.",buildapc,2025-12-07 17:46:57,0
Intel,nsshhp4,Is 750w really not enough for 5700x3d and 5080? I was planning on updating the PSU when I update my CPU and RAM when the situation maybe gets a little better.,buildapc,2025-12-07 17:25:32,1
Intel,nsslaca,It’s bad advice to say the 5080 is worth a 28% increase in price for far less than that in performance,buildapc,2025-12-07 17:44:24,-3
Intel,nssootc,"There actually is also the possibility of going 9070 XT when talking value. Cheapest, which is Powercolor's Reaper going for 639€ right now compared to the 5070 TI's 769€ pricetag.",buildapc,2025-12-07 18:00:59,1
Intel,nsslh1c,I'm just going off what nVidia recommends and what google outputs.  The 5700x3D is a 105W CPU and the recommendations probably include 170Ws.  I suspect the 750W would work on a lot of games and crash at random times.  It also might just kill the PSU sooner.  I mainly like the 5070s cause as far as I know they don't suffer from any connectors melting.,buildapc,2025-12-07 17:45:18,1
Intel,nssonvq,Not what I said haha bro can do with he wants he will see a difference in perf. Not talking about price. If he has the money go for it.   My 5090 is bad advice but shit does it ever rip !,buildapc,2025-12-07 18:00:52,-1
Intel,nst885v,I just upgraded to a 9070 XT from a 2070 Super. Couldn’t be happier. It has amazing performance for the price. I use a 4k tv as a monitor and it is tackling 4k games well and 1440p games perfectly.   I was on the fence between the 9070 XT and the 5070 TI but the cost savings for essentially equal performance helped me decide.,buildapc,2025-12-07 19:33:59,3
Intel,nstxnbm,Well yeah that’s another 130€ you could save especially if you’re playing BF6 and CS2,buildapc,2025-12-07 21:39:13,1
Intel,nst5pf0,130€ puts them at equal value once you take nvidias feature set into account in my eyes. Id go 5070ti + 280hz tandem oled,buildapc,2025-12-07 19:21:34,-1
Intel,nsstfsr,Mainly what I'm interested is will the difference in performance be noticeable between 5070TI and 5080 since I'm probably pretty CPU bound with 5700x3d. I might probably sell my GPU and go all out in 2 years.,buildapc,2025-12-07 18:23:31,1
Intel,nsty0sg,Holy fuck buddy bro said he wants a monitor at some point maybe blowing another ~$370 CAD on a graphics card isn’t a good idea,buildapc,2025-12-07 21:41:00,1
Intel,nst6y1r,"What I tend to do is look for YouTube benchmarks of builds I intend to go with. Like ""5700x3d 5070ti vs 5700x3d 5080"" then go through it until I see the resolution I play at. On some games the 5080 is 30% more typically, then on other games it's not so much.",buildapc,2025-12-07 19:27:38,1
Intel,nrnxxoh,"What are your struggling scenarios that you want to upgrade almost the whole system? Games are not that hard to operate, except if you want absolute graphics quality.",buildapc,2025-12-01 05:23:27,1
Intel,nro65vz,"Coffee Lake is old at this point, and there isn't really a good upgrade path. Even the top dog of that era the 9900k is quite slow by modern standards, and will be a bottleneck for most mid to high end GPUs. I guess you can choose to upgrade just the GPU alone and stick with the 9600k. Just know that in CPU limited games, you most likely will see a substantial CPU bottleneck.",buildapc,2025-12-01 06:32:10,1
Intel,nro84qb,"The main thing I am coming across is titles like Monster Hunter: Wilds, Borderlands 4 etc that are both poorly optimised and CPU intensive. In theory my current specs are right on the ""minimum"" specs to play MH:Wilds, but in practice I get barely a stable 30fps at 1080p, even using upscaling and low settings.    I have contemplated just a GPU upgrade (e.g. picking up a 30series GTX), anything newer and I assume I'll end up severely CPU limited. But then I still have a 6 generation old CPU and a few generation old GPU, so probably land in the same situation with the next generation of games. And a 3060 new still costs almost as much as a B580 or RTX5060, so doesn't seem cost effective.",buildapc,2025-12-01 06:50:02,1
Intel,nro8v9a,That's why I'm contemplating a full CPU/mobo/GPU upgrade and keeping the ram/SSD/PSU.    Its just not clear to me what a good GPU match for a 13-14 gen i5 (or equivalent AMD) would be so I can keep my RAM. Or even whether it's a good idea at this point to hold on to my RAM (e.g. should I just wait until I can afford a newer system and go full DDR5?,buildapc,2025-12-01 06:56:46,1
Intel,nroap0h,"There are 2 things we can probaly do:  1. Tweaking this old system to squeeze the most out of it for best performance at stable state while gaming. You won't get significant improvements but in other hand, it is free. Your risk is the temperature and voltage. 2. Generational upgrade that cost us minimal changes with most reusable parts.",buildapc,2025-12-01 07:13:58,1
Intel,nroi2jr,"Well, there's also the problem that your RAM 2666 is pretty slow for DDR4. Also, whether it's AM4 Zen3 or 12/13/14th gen, you're basically investing in a OLD platforms that's not only slower than modern AM5, but also has no future.   Honestly, i'd stick with Coffee lake at this point, maybe see if i can swap to a 8700k for cheap, and go with a low end GPU like a 5060 or B580 and deal with the CPU issues until prices normalize.",buildapc,2025-12-01 08:27:22,1
Intel,nronlp0,"Thanks, that makes sense. I kind of suspected that while my RAM and SSD were compatible with newer systems, but not actually very good anymore (given they weren't exactly cutting edge 7 years ago when I got them) - but it's good to confirm that.   I think I'll look around for cheap/second hand GPU options and otherwise wait around until I can afford a complete system update.",buildapc,2025-12-01 09:25:20,2
Intel,nroqwdo,"No problem. Your SSD is decent, if a bit smaller capacity. I had this exact same model and it's in my dad's pc right now. 500GB is still ok for a system storage drive if u don't have a ton of stuff. Gen 3 speeds are still more than enough for daily use imo. You'll still need a extra SSD for games if you don't have that.   As for the ram, you could see if u can overclock it to 3000 mhz. If you don't mind used, then it's ok to migrate them to a AM4 b450/550 platform and maybe pair it with a Ryzen 5 5600x . You can probably get the pair for around $150-200 used and elevate some CPU performance issues you'll have. Granted, what I said about investing in old platforms with no future still stands, but if the price is right and you really need the increased CPU performance, then it could be justifiable.",buildapc,2025-12-01 09:59:56,1
Intel,noclqcf,"Arc B580 is a good budget card and leagues better then a 3050, just note it will probably struggle on high settings on 1440p. On tomshardware it scored 42.6 FPS on average for 1440p Ultra Rast which included God of War Ragnarök in there game selection.   If you want to play high settings consistently without frame gen at 1440p, I would spend the extra $100 for the 9060XT 16GB. It scored 40% higher then the B580, and cost 40% more. So you're getting same value wise, for price to performance 1440p Ultra.  Also note, Intel GPU's are still new and have the most vary performance with time,  so I would actually imagine the B580 is a bit better then the recorded benchmark I'm refrencing from.    [https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025)",buildapc,2025-11-11 21:27:29,2
Intel,nocm5b2,"The B580 will about 90% faster (assuming no CPU bottleneck). But, if you can afford it, the Radeon RX 9060 XT 16GB is about 160% faster and can usually be found for about $350 in the US.",buildapc,2025-11-11 21:29:38,1
Intel,nocvm6w,"Depends on pricing, the B580 is a decent deal in the UK, you can get it here for £200 and a choice of a £60 game out of four options including BF6 so a no brainer for budget builders. Anymore than £200 or the dollar equivalent id just jump to a 5060 or better yet 9060XT 16gb. There will be some CPU bottleneck, but you can get alot of performance out of itself still.",buildapc,2025-11-11 22:18:42,1
Intel,nsy2217,"Hab den neuen GPU 591.44 Treiber installiert, seit dem hab ich das auch, deswegen bin ich hier drauf gestoßen.",buildapc,2025-12-08 15:07:41,1
Intel,nsm28yq,4k will get like 60+ in some games 1440p 144+ in most,buildapc,2025-12-06 16:16:22,11
Intel,nsm4a2e,"I have a 9070 xt and a 4k screen, I have been on 4k with even my previous 6900 xt. It's 100% worth it for me personally to turn on fsr or lower some settings and be able to enjoy all the pixels.  Almost all my friends prefer 1440p so it's really a personal preference thing.",buildapc,2025-12-06 16:27:13,3
Intel,nsm3ytr,"At 4K you’ll likely need to use FSR for some games to be playable/smooth, which is no problem since FSR 4 is pretty good.   I use FSR 3 for 4K with my 7900 XTX when I need it, but most games run on >100 FPS anyway and games like Overwatch even at 200+ FPS. If you really want Raytracing/Pathtracing you have to choose between FSR or low FPS",buildapc,2025-12-06 16:25:35,2
Intel,nsm6dav,"Depends on your tolerance for 60 fps. If 60 fps is enough then go 4k. A lot of us don't play below 100 gps because it just doesn't look good comparatively, and you can't consistently get over 100 fps at 4k on a 9070xt. I have a 4090 and stay at 1440p. Even a 4090 is not 4k ready if you demand a high refresh experience in triple a.",buildapc,2025-12-06 16:38:15,2
Intel,nsmsh1t,"I have a 7500F + 9070 XT system, and the majority of games I play I get 80-120 fps playing at high settings (max/ultra is never worth it) and quality mode upscaling. With some exceptions like Stalker 2, where it's more like 60-80 fps.  Personally I would go 4K if you were getting a TV, but not if you're getting a monitor. To me the jump from 1440p to 4K on a monitor isn't worth it, but it's 100% worth it on a quality TV.  Also, I wouldn't be worried about having to use FSR. FSR 4 quality mode is as good as native anyway. Even performance mode looks great, especially on an OLED TV or something of similar quality.",buildapc,2025-12-06 18:34:04,2
Intel,nsn9dv9,1440p and don’t look back,buildapc,2025-12-06 20:02:25,2
Intel,nsojc5r,4K is never worth it for gaming.,buildapc,2025-12-07 00:24:30,2
Intel,nsqr8nf,4K is super overrated. 1440p looks amazing and everything will run so much better.,buildapc,2025-12-07 10:40:38,2
Intel,nsm700a,"Just to consider. 1440p 27” has a dpi of 109, 4k 42” 105.",buildapc,2025-12-06 16:41:34,1
Intel,nsmbxo4,"Honestly, I always thought 1440p 27"" was the sweet spot but I've got 32"" 4k and not looked back - and I'm running off a Legion Pro 7 with a 4090M on it.  I think you'll be fine just manage expectations / be sensible with choices you make RE settings etc. For me the 4k real estate for photo editing and just general video content was basically a must have.",buildapc,2025-12-06 17:07:41,1
Intel,nsmdf1z,"I would go in the middle and get a 34"" ultrawide",buildapc,2025-12-06 17:15:43,1
Intel,nso0fk0,"I play 4K with the 7900XT (just slightly lower performance than 9070XT) and with FSR3 I get 140-160fps on Battlefield 6 at High Settings. Souls games will be great at 4K since they are frame locked (Elden ring is locked to 60fps). I play any souls like at 4k max settings with no issues and maintain either 60fps or exceed my 165hz monitor in fps. I’m really not sure what games would ever hold you back as far as gpu goes. Unless you are trying to milk every frame in a competitive shooter, in which case, you can just get a dual mode monitor that can swap to a lower resolution (I have a 165hz @ 4K that toggles to 320hz at 1080p)",buildapc,2025-12-06 22:33:52,1
Intel,nso6t7q,Get an OLED TV like an LG. You're going to hate losing inches.,buildapc,2025-12-06 23:10:44,1
Intel,nt1ar8m,"I got a 4k monitor for 9070XT. Im sending it back. Looks great BUT there are games with no upscaling. Elden ring nightreign, and it will run worse. Monster Hunter wilds, my favourite game, runs like crap sadly even on performance mode.",buildapc,2025-12-09 01:09:04,1
Intel,nsm3brh,"4k you can play a considerable amount of games in 60fps, but some games you will struggle, mainly in Nvidia sponsored games, and RayTracing scenes. In 1440p you'll be able to play almost any game no problem, unless it's an extremely poorly optimized game.  Realistically if you're looking to game in 4k and want an AMD GPU, get the RX 7900 XTX, as it has 24gb VRAM in comparins to the 9070xt's 16gb VRAM, which most likely won't be enough for a decent amount of games in 4K. Also, the 7900 XTX performs extremely close to the RX 9070 XT in terms of performance. I was watching a few benchmarks a little while ago because i was going to upgrade.",buildapc,2025-12-06 16:22:12,1
Intel,nso2zex,"1440p. No brainer.  I don't recommend 4k, even on a 5090. 1440p is where it's at these days. Get a high refresh 1440p OLED.  I run a 9800x3d, 5080, and 64GB of RAM with a 240hz 1440p OLED. It's incredible. I upgraded to high refresh 1440p from 4k 🙂. 4k was a lot more viable when we were all stuck at 60hz.  I don't even use my 85"" 120hz 4k TV for gaming now.",buildapc,2025-12-06 22:48:18,1
Intel,nsm368o,2k is better,buildapc,2025-12-06 16:21:21,0
Intel,nso1hps,4k with only 60fps in some games? It will get at double or triple that in the vast majority unless you are cranking to the max graphics in super demanding games. At 1440p I would be looking at 240hz monitors for sure. Pretty sure I was playing Wukong with the 7900xt at 100fps on second highest graphics settings,buildapc,2025-12-06 22:39:47,0
Intel,nsmvclq,"I also prefer at least 80 fps or more just for the reduced input lag, but 60 actually isn't too bad if you have a VRR capable monitor.  Especially if you use a controller, like on a TV for example.",buildapc,2025-12-06 18:48:50,1
Intel,nsm8j67,"True, but 9070XT gets FSR4. With the similar raw performance but better upscaler, the 9070XT is a marginally better buy (without discussing differences).   Saying that as an XTX owner that plays in 4K",buildapc,2025-12-06 16:49:38,2
Intel,nsmuvi9,">16gb VRAM, which most likely won't be enough for a decent amount of games in 4K.  Why are you commenting if you don't have the knowledge base to make an informed remark?  16GB is still enough for 4K. 16GB is only a limitation when completely maxing out a very limited handful of games with ray tracing, and the 9070 XT is not fast enough to completely max out new AAA games with ray tracing.  Even newer Unreal 5 games like Stalker 2 doesn't use more than ~11GB of VRAM completely maxed out (without ray tracing).  The 9070 XT will also need upscaling in a variety of games, which reduces VRAM usage by a measurable amount.  The only reason to choose a 7900 XTX over the 9070 XT is if you can find it much cheaper, or you're planning on playing heavily modded games with huge textures.",buildapc,2025-12-06 18:46:23,-1
Intel,nsm38tn,1440p ≠ 2K,buildapc,2025-12-06 16:21:45,1
Intel,nsmjdxb,"But for 4k the 7900 XTX is the better choice, specifically because of the extra VRAM. 9070 XT is more future proof, 7900 XTX is a better 4k card. 9070 XT i think has slightly better raw performance.  Future Proof = 9070 XT   Strictly 4k Gaming = RX 7900 XTX   Alternative = wait for RDNA 5 because the price for both 9070 xt and 7900 xtx will probably drop over time",buildapc,2025-12-06 17:47:52,1
Intel,nsn0knu,">16GB is still enough for 4K. 16GB is only a limitation when completely maxing out a very limited handful of games with ray tracing, and the 9070 XT is not fast enough to completely max out new AAA games with ray tracing.  >Even newer Unreal 5 games like Stalker 2 doesn't use more than \~11GB of VRAM completely maxed out (without ray tracing).  You're literally specifying here without RayTracing, obviously without RayTracing 16gb is going to be enough. If you have to specify specific settings you cannot use to try and disprove me, then you're argument is trash. Also, the 9070 XT is CAPABLE of RayTracing, and if someones card is capable, their most likely going to want to play using those settings. But there is a VRAM overload, so people have to compromise. Because the 7900 XTX has 8gb more VRAM, you don't HAVE to compromise and you can max out RayTracing without having to worry about VRAM. You're literally cherry picking settings to try to disprove my point.   Also, what if i wanted to mod using my RX 9070 XT but can't because using 4K resolution with max graphics has used up all my VRAM and i have no more headroom? You said the only reason to choose a 7900 XTX is if you're planning on heavily modding games yet you clearly didn't take into consideration someone may want to mod with an RX 9070 XT as well. The RX 9070 XT is widely considered a premium 2k card, not 4k. And it's not considered 4k because playing with 4k max settings with RayTracing usually exceeds 16gb VRAM, and people LIKE RayTracing. So strictly for 4K, people will opt for the 7900 XTX.  RX 9070 XT however is newer, has FSR 4, more updated drivers. But the RX 7900 XTX has way more VRAM headroom, especially for people who want to use RayTracing in 4K, which the RX 9070 XT won't be capable of in a fair number of titles unless you manually choose the settings yourself, and even then you're leaving yourself no headroom for modding. RX 9070 XT is more future-proof in terms of performance, but RX 7900 XTX is more future-proof strictly for gaming in 4K, as games are only going to get more VRAM demanding.  7900 XTX is only better under specific uses, and this specific use being 4K. And that's SPECIFICALLY because of the extra 8gb VRAM.",buildapc,2025-12-06 19:15:33,-1
Intel,nsm6wpv,It's 2.56k and 4k is actually 3.84k,buildapc,2025-12-06 16:41:05,2
Intel,nsm3txx,"Uhmm ahhh akthuallii 🤓👆  Yes 2k is not 1440p but almost everyone uses it and people understand that by saying 2k, you actually mean 1440p. Fucking smartass.",buildapc,2025-12-06 16:24:51,1
Intel,nsn2rlr,"The 7900 XTX can't max out ray tracing at 4K, so that's a completely moot point. The amount of ray tracing that card can do at acceptable framerates will never make it go above 16GB VRAM.  And yes, I specifically said to get the 7900 XTX if you want to play games with huge texture mods, but you can still play modded games with a 16GB card just fine. I played a heavily modded Skyrim with almost only 4K textures, and it hovered at ~12.5GB  Sure, people like ray tracing, but that doesn't change the fact that neither of those cards can max out ray tracing - and keep acceptable framerates - to the point where it becomes an issue with regards to their VRAM buffer.  The only way you're doing max settings and path tracing is with performance mode upscaling, at which point you're just not maxing out that 16GB VRAM buffer. Case in point: https://youtu.be/8bEo1dgeRn4?si=Z3WwktPwxqDgsHYK  Max settings + path tracing, barely scratches 11GB dedicated.",buildapc,2025-12-06 19:27:06,1
Intel,nsmdnrg,Yeah so FullHD is 1.92K wich is way closer to 2K than 1440p,buildapc,2025-12-06 17:17:02,1
Intel,nsmvl7n,Fragile.,buildapc,2025-12-06 18:50:04,0
Intel,nshroj2,"EDIT: SOLUTION: problem was gpu in bottom slot not top -> switching to top slot solved it!  okay i also have a 9800x3d + 5080 but with 64gb ram but that shouldnt matter and i easily get 200fps in most games on ultra in 1440p  okay a few ideas: 1. can u check if your cpu is boosting or just stuck to the base clock if not check bios 2. the current bios version of msi x870 is known for poor audio, earlier versions are better but i do not know which 3. try slower ram. set it to expo 1 not expo 2 and run it at 5600MT i had a similar problem but on an older system 4. install amd chipset driver since windows updates do not install them 5. are ur pcie lanes limited? is ur gpu slot set to pcie5x4 and not 5x16?  yea these are my ideas hope it helps :/  do the chipset driver update first",buildapc,2025-12-05 21:30:31,38
Intel,nsi7o8k,GPU on top or bottom pcie slot?  All GPU cables plugged in and firmly?  Did you by chance flip the bios 2 mode on the GPU?  Download afterburner and set all settings to default. Then max out the power slider.,buildapc,2025-12-05 22:59:01,29
Intel,nshu6u4,I had the same problem when I upgraded my CPU. Turns out my CPU cooler wasn’t properly screwed down and was unevenly placed causing my CPU to have heat spikes and I was having major stuttering and lag. I was so confused because it was a far better and more expensive CPU. If you have an AIO make sure you removed the sticker or that the pump is working correctly,buildapc,2025-12-05 21:43:56,12
Intel,nsi0hjq,"Turn off expo and try again, if it's stutter free then your ram at expo isn't stable on your particular system",buildapc,2025-12-05 22:18:04,5
Intel,nshytoz,Quick question. Is doing anything in windows also quite laggy or jittery?,buildapc,2025-12-05 22:08:55,3
Intel,nsi5dth,"Out of curiosity, is ReBAR enabled",buildapc,2025-12-05 22:45:45,3
Intel,nshrxyt,"Are you using a riser cable by any chance?  I would also do a DDU pass with no internet plugged in and install the GPU and Audio river manually, maybe Windows update got in the way there - unusual these days but hardly unheard of.  Shoudl also probably install HWINfo64 and check temps and clocks across the board. Maybe you have a single core that is heating up and that doesn't show in averages.",buildapc,2025-12-05 21:31:55,4
Intel,nshs7f3,Some other instances of very poor performance or stuttering were resolved by changing the PCIe setting in the BIOS from Auto to something else.,buildapc,2025-12-05 21:33:19,5
Intel,nshyyvn,From your responds to messages here it sounds like your motherboard is DoA. Or wild guess you missed a standoff or motherboard isn’t aligned correctly on case and that could be causing the audio interference and shorting the motherboard would cause all the other issues,buildapc,2025-12-05 22:09:42,1
Intel,nsi1kpd,I've seen multiple AMD builds act weird until the latest AMD Chipset Drivers are manually installed: [https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-06-02-123.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-06-02-123.html),buildapc,2025-12-05 22:24:10,1
Intel,nsi3498,Change monitor refresh rate to the highest it will go.  Disable v-sync in games.  Reboot and load into BIOS. What is your IDLE temp?,buildapc,2025-12-05 22:32:51,1
Intel,nsi38us,Are you plugged into the GPU or the Mobo? This might sound dumb but your PC might be defaulting to your integrated graphics.,buildapc,2025-12-05 22:33:35,1
Intel,nsi5onc,Do you have a spare drive to test with? Any 3rd party AV? What monitor is being used? Tried GPU in another system?,buildapc,2025-12-05 22:47:28,1
Intel,nsibqiz,Do you have your old PC to test the new GPU in? Provided you have a decent enough PSU to use a 5080 that is.   Before that try reseating the GPU. Pull it out of its socket and reinstall it. I know it's silly but it's actually been a culprit for me years ago after like 5 years of no issues. It's also an easy thing to check.,buildapc,2025-12-05 23:23:31,1
Intel,nsidc1i,Any RGB software running? if so good to disable it for now. I generally check task manager for anything taking over a few percent cpu. My case management software cost me 5% performance drop too,buildapc,2025-12-05 23:33:25,1
Intel,nsidf56,"Are you using dport or hdmi?  If you have upgraded to a 4k monitor, this can have a huge impact on your fps.",buildapc,2025-12-05 23:33:57,1
Intel,nsiezrb,"I have this weird issue with my build. Performance tanks and I hear crackling and static sound for noise. It's a ryzen 7600 with a 7700xt. Here is how I HAVE to start my PC in order for it to work properly. I've tried many many many ways, reinstalls, bios settings, and even going to Linux! This might help you. First, If my PC is off, I have to move my display cable to the integrated slot, then I turn on the PC and let it boot to desktop. After that, I open task manager and watch the graph for my discrete GPU. Obviously the igpu will have all of the fluctuations, but my discrete will stay flat for close to a minute. Then I open steam or YouTube and wait until I see repetitive motions on the graph for the discrete. In my case, I get tiny little mountains back to back. That means I can move the display cable to my discrete and everything works fine.  Any variation on this and it won't work. Give it a shot. Basically hot starting your GPU",buildapc,2025-12-05 23:43:42,1
Intel,nsifig9,"Are you on the latest nvidia driver, Dec 04? It fixes very low performance on 50 series gpus from Win11 KB5066835 update. I'll quote the patch note:   ""Users running R580 branch drivers (58x.xx) or newer may observe lower performance in some games after updating to Windows 11 October 2025 KB5066835 \[5561605\]""",buildapc,2025-12-05 23:46:54,1
Intel,nsim9o6,A friend of mine had this happen. Even rebuilt the whole pc to no avail. Turned out to be a problem with the outlet he was running off of and had to get an electrician to fix it.,buildapc,2025-12-06 00:28:32,1
Intel,nsip945,"If you’re worried about GPU sag, get a support bracket to mount underneath it",buildapc,2025-12-06 00:46:43,1
Intel,nsizv2u,Manual requires close examination and thought about how PCI works.,buildapc,2025-12-06 01:55:09,1
Intel,nsk3fzu,wow never knew this,buildapc,2025-12-06 06:52:40,1
Intel,nshwi6u,You plugged monitor into motherboard hdmi instead of video card hdmi,buildapc,2025-12-05 21:56:16,-1
Intel,nshrfb4,"Reinstall drivers, check again that you didn’t output to integrated graphics make sure your system is registering the 5080 and 9800x3d if it is then you probably have a bad gpu.",buildapc,2025-12-05 21:29:09,0
Intel,nshro1r,"Your parts list seems jumbled a little, what is your PSU?",buildapc,2025-12-05 21:30:27,0
Intel,nshrtgr,Or a bad motherboard,buildapc,2025-12-05 21:31:15,0
Intel,nshrvu7,Could you elaborate on cpu temps and clock speed under stress?,buildapc,2025-12-05 21:31:36,0
Intel,nsi1iqv,This just sounds like a broken PSU or motherboard. I would buy one from a store where you can definitely return it even if used and when you figure out what part is faulty you send that original back for a new one or return it completely and use the new fine one.,buildapc,2025-12-05 22:23:51,0
Intel,nsi6453,"Have u tried going to your wifi and stop ""auto roaming""? This did the fix for me",buildapc,2025-12-05 22:49:57,0
Intel,nskcw4y,"Happy for you that it was such an easy fix.  My post may butthurt some people but its not meant as a judgement or criticism of anyone. its good that people rush to help others this is what reddit was meant to be like. and in fact its awesome that those who aren't the most knowledgeable or capable also jump in as well, that's how we learn by putting our mistakes in public and being corrected.  But just a word of advice, the fact that you say so many eyeballs and noone picked up on your slot placement. Be strategic and do your own research when asking for help on Reddit in future as many will kindly rush forward to help you but how qualified are all the helpers really, X-D And that can take you down wrong paths  Quick check of your motherboard spec form MSI website and you would be able to see for yourself  3x PCI-E x16 slot   PCI\_E1 Gen PCIe 5.0 supports up to x16 (From CPU)   PCI\_E2 Gen PCIe 3.0 supports up to x1 (From Chipset)   PCI\_E3 Gen PCIe 4.0 supports up to x4 (From Chipset)  PCI\_E1 slot   • Supports PCIe 5.0 x16 (For Ryzen™ 9000/ 7000 Series processors)   • Supports PCIe 4.0 x8 (For Ryzen™ 8700/ 8600/ 8400 Series processors)   • Supports PCIe 4.0 x4 (For Ryzen™ 8500/ 8300 Series processor)  But at least everyone reading this thread has learnt something they aren't going to forget now.",buildapc,2025-12-06 08:25:23,0
Intel,nslqzko,"Lil bro put the GPU into PCie 3.0 x1 or 4.0 x4 slot and wonders why it's slow. It's not latency, it's bandwidth.",buildapc,2025-12-06 15:14:35,0
Intel,nshsj94,"1. How would you recommend checking that?   2. I've tried the last two BIOS updates. Is it worth going further back?   3. I have not tried this and will see if that helps thank you!   4. I will double check this as well.    5. I honestly can't remember. It seemed like the cords would only fit into their respective spots but its worth popping open.   Thank you for the feedback, this does give me something to check.",buildapc,2025-12-05 21:35:06,7
Intel,nsic4my,GPU is on the bottom.   GPU cables to the motherboard or the cables into the GPU?  All the other BIOS settings would be default.   I will try afterburner and see how that helps!,buildapc,2025-12-05 23:25:56,13
Intel,nshuq99,"It did feel weird installing the CPU cooler so I will check that as well when I pop it back open. I appreciate the thought to check that as well, I wouldn't have otherwise",buildapc,2025-12-05 21:46:48,3
Intel,nshz3j4,Alt Tabbing between monitors makes it laggy for a moment. What would that mean?,buildapc,2025-12-05 22:10:25,3
Intel,nsigs65,"Hey I have a similar issue and my windows is definitely laggy and jittery. Opening a window or right clicking is visually less smooth but also hard to notice. The mouse movements are also slightly laggy. The biggest offender is I hear a weird synthetic buzzing sound when I do something that involves sound and my FPS in a game stays below 30. Was this similar to yours?? I've been dealing with this for a year, but the work around I have been doing is starting my PC with iGPU plugged in and then moving the plug to my main GPU. This method lets it run perfectly but I have to do it every restart and also can't use sleep for PC unless I move the display cable around",buildapc,2025-12-05 23:54:46,1
Intel,nsi6an8,Is that the BIOS setting?,buildapc,2025-12-05 22:51:00,2
Intel,nshsslo,I am not using a riser cable.   I completed that as well and updated the post to mention that.   I didn't think to check any single core so thank you. I will give that a shot!,buildapc,2025-12-05 21:36:28,3
Intel,nsht2m7,If you can remember what that something else is I can take a look. Another user mentioned the PCIe cables so I would want to check it all if I can.,buildapc,2025-12-05 21:37:57,1
Intel,nsi0at2,Any idea how to definitely check to see if it is DoA? I'm trying to exhaust my troubleshooting options as best I can.,buildapc,2025-12-05 22:17:03,1
Intel,nsi3lca,I gave this a shot as well. Thanks for the link but I don't think it made the difference.,buildapc,2025-12-05 22:35:33,1
Intel,nsi491f,"Just double checked the monitor, it is still running at 144. V sync is disabled in all games. Idle temp is around 40C on the CPU.",buildapc,2025-12-05 22:39:18,1
Intel,nsi4dc4,I am plugged into the GPU. Even went so far as to test disabling the integrated GPU to ensure it is the right one.,buildapc,2025-12-05 22:39:58,2
Intel,nsi6v0p,No spare driver. No 3rd party AV. And I am using an 2460g4 aoc monitor with updated drivers from the manufacturers site.,buildapc,2025-12-05 22:54:16,1
Intel,nsicera,I don't have the old PC to test anything with unfortunately. I will reinstall it as well to check. It feels more and more like it is a physical issue with one of the connections.,buildapc,2025-12-05 23:27:39,1
Intel,nsig2bz,No RGB in the slightest. All black.,buildapc,2025-12-05 23:50:19,1
Intel,nsigc6w,I am using a Dport with a standard 1080 monitor.,buildapc,2025-12-05 23:52:02,1
Intel,nsigp3b,I am currently using the updated driver. I will double check that as well though. Thank you.,buildapc,2025-12-05 23:54:15,1
Intel,nsipn5p,Now that it is all fixed that will be the next step.,buildapc,2025-12-06 00:49:06,1
Intel,nshy7oy,Double and triple checked that it is connected to the GPU and not the integrated graphics.,buildapc,2025-12-05 22:05:32,4
Intel,nshzs1x,That was my first thought too,buildapc,2025-12-05 22:14:10,1
Intel,nshs22y,Is there a preferred way to ensure that the computer is registering the 5080 and 9800x3d?,buildapc,2025-12-05 21:32:32,1
Intel,nshrxex,"I fixed the list a bit more. The PSU is a ""be quiet! Pure Power 13 M 1000 W 80+ Gold""",buildapc,2025-12-05 21:31:51,2
Intel,nsht4k9,God I hope it's not cooked out of the box.,buildapc,2025-12-05 21:38:14,1
Intel,nsht6mv,If you know of a program to test that I can bring the results back.,buildapc,2025-12-05 21:38:32,1
Intel,nshydwp,Cinegraph is running in the background but with that in mind HWinfo shows I am effectively operating at 5230 MHz at a x52 times ratio. I think this is max capacity? Active clock says 5225 MHz is expected. Temp is sitting at 40C.,buildapc,2025-12-05 22:06:29,1
Intel,nsi3g48,That seems to be the general consensus. Between the PCIe inputs for the power and motherboard issues I would enjoy exhausting all other options before needing to rip all those parts and cables out.,buildapc,2025-12-05 22:34:44,1
Intel,nsi6i2r,I an using a hardwired connection. Unless you suspect a wifi setting is interfering?,buildapc,2025-12-05 22:52:11,1
Intel,nshtg3j,"1. run a stresstest, cinebench or so. open taskmanager, if the clock is above > 5ghz ur good 2. could but i do not know and read it a week ago 5. no, this is a software thing in the bios. the slot should be right but could be software limited to 1/4 of its bandwith.",buildapc,2025-12-05 21:39:57,9
Intel,nshv0q0,"some other ideas: check ur rops. download gpu-z and check how many ROPs u have, should be 112, otherwise its defective -> return it could also be hags: Disable Hardware-Accelerated GPU Scheduling (HAGS) - Settings → Display → Graphics Settings → Turn OFF ""Hardware-accelerated GPU scheduling"" → Reboot",buildapc,2025-12-05 21:48:19,2
Intel,nshuqow,"I think it might just be XMP vs Expo 1. XMP is for Intel, Expo is AMD.",buildapc,2025-12-05 21:46:52,0
Intel,nsicmvh,"That's the issue. GPU must always be at the top, nearest to the CPU. The 2nd slot is slow so it affects your GPU performance.  Power cables to the GPU.  Changing to the top slot should solve your problem. Have fun with it",buildapc,2025-12-05 23:29:03,37
Intel,nsig5to,"Bottom port is PCIe 4.0 x 4 and the middle slot is PCIe 3.0 x 1. Top port (closest to cpu) is PCIe 5.0 x 16. That’s the one you want, even if you set or are using a PCIe 3/4 card.",buildapc,2025-12-05 23:50:55,4
Intel,nsievs0,"GPU in top. If you posted a picture, this would have been solved asap",buildapc,2025-12-05 23:43:02,5
Intel,nsig0aa,The general rule is that GPU should be placed in the slot closest to the CPU. I believe it's cause the mobo traces are shorter and it gets priority access to the CPU.,buildapc,2025-12-05 23:49:58,1
Intel,nsi0frq,Had a similar problem a few years back. My computer started having performance spikes and stuttering playing old games that were locked at 144hz before. Turns out my CPU fan was loose. I have no idea how it happened but I reseated the cooler and it good as new.   I also feel like half of these posts are from something silly like forgetting to peel the plastic cover on the heat sink or something… it’s worth checking.,buildapc,2025-12-05 22:17:49,3
Intel,nsi1gyt,You’d see cooling issues in hwinfo if that was the problem.,buildapc,2025-12-05 22:23:35,1
Intel,nsjd2u9,You can use hwinfo to check temps.  You don't need to disassemble anything.  If there's an issue your temps will be 90+,buildapc,2025-12-06 03:21:24,1
Intel,nsi13yb,"For me, I had to change the pxie mode for my x16 slot in the bios from auto to 3.0. I upgraded the cpu and gpu on my sff build. The riser cable I had was 3.0 so I believe that was my issue. For me windows in general was really jittery. Quick question. To rule out your Motherboard. Do you have your old GPU?",buildapc,2025-12-05 22:21:33,2
Intel,nsi42ks,"That could just be the lag from going from a fullscreen application/game to one that's not. Check the app you're tabbing from is it in fullscreen mode? Or windowed fullscreen? If it is in FS, switch to windowed FS or borderless windowed and see what happens when you alt tab.",buildapc,2025-12-05 22:38:16,1
Intel,nsihzf7,For me it was either the bios or the pcie riser I was using for my gpu. I had to change the pciex16 from auto to 3.0 because my cable was a pcie 3.0 riser.,buildapc,2025-12-06 00:02:15,1
Intel,nsiaz2o,Yes. It's called resiseable bar. Very important for new cpus.,buildapc,2025-12-05 23:18:54,1
Intel,nshtwbw,"In the BIOS, your PCIe setting will probably be on Auto. Some people find success with 3.0, others 4.0, and some 5.0. Seems like the Auto is the culprit sometimes. I would start with 5.0 and work your way downwards and see if any version solves it.",buildapc,2025-12-05 21:42:23,3
Intel,nshucil,https://www.reddit.com/r/nvidia/s/65LHOUp1Ej   https://www.reddit.com/r/nvidia/comments/1ifg24e/rtx_5080_founders_edition_pcie_50_issues/,buildapc,2025-12-05 21:44:46,2
Intel,nsi0g7c,For a motherboard? Only way to test is swap the motherboard with a known working motherboard unfortunately.,buildapc,2025-12-05 22:17:53,4
Intel,nsikopk,Darn. Hope you're able to get it resolved. It's an awesome build.,buildapc,2025-12-06 00:18:55,2
Intel,nsk6qji,"May seem a bit obvious, but have you auto detected game settings and are running with everything on ultra-stupidly-high settings?  If your previous computer was using more moderate settings, this would be why your fps was better on the old computer.",buildapc,2025-12-06 07:24:18,1
Intel,nsilbca,Disable windows security active scanning.it could be that,buildapc,2025-12-06 00:22:43,1
Intel,nsipqq7,I put one on the 5070 in the build I just made. Better safe than sorry,buildapc,2025-12-06 00:49:42,1
Intel,nshsw44,Nvidia app has a setting that lets u select which gpu to utilize. I had this issue in warzone where it was only using the igpu on my Intel and not my gpu.,buildapc,2025-12-05 21:37:00,1
Intel,nshvbst,Here's the link: [https://www.hwinfo.com/download/](https://www.hwinfo.com/download/),buildapc,2025-12-05 21:49:58,2
Intel,nshu7ii,"HWinfo is free and will display lots of data about your system. CPU, GPU, GPU memory junction, GPU memory, etc.  I highly recommend it.",buildapc,2025-12-05 21:44:02,1
Intel,nshubu4,HWInfo64,buildapc,2025-12-05 21:44:40,1
Intel,nshuu1b,"https://www.hwinfo.com/  That will give you a great output of temps/speeds.  To stress the system, you can use something as simple as CPUZ(https://www.cpuid.com/softwares/cpu-z.html), which has a CPU stress/benchmark, or something more complicated like OCCT: https://www.ocbase.com/download",buildapc,2025-12-05 21:47:22,1
Intel,nshzsy7,"40C is VERY low for that load. Are you sure you’re checking the right column? I don’t remember if it’s HWinfo or HWMonitor but one of the two will show low, high, and average temp. Could you take a screenshot of the HWInfo panel and upload it to imgur?",buildapc,2025-12-05 22:14:19,1
Intel,nshtrsi,"On point 1, hwinfo64 is a better option.",buildapc,2025-12-05 21:41:42,10
Intel,nshy3td,HAGs is now turned off. ROPs is at 112/336. No dice.,buildapc,2025-12-05 22:04:56,2
Intel,nsi1aqv,XMP is fine on a ryzen system. Many ram sticks have both profiles and/or people have gotten xmp only sticks to save money.,buildapc,2025-12-05 22:22:36,2
Intel,nshv8tz,"there are often different expo levels, one is more aggresive but brings more perfomance expo 2 is often more stable",buildapc,2025-12-05 21:49:32,1
Intel,nsibiup,G.SKILL Flare X5 Series  is specifically marketed for AMD EXPO.,buildapc,2025-12-05 23:22:14,1
Intel,nsinc9f,It absolutely was the case. Everything is working as you'd expect from a rig like this. Thank you so much for the help!,buildapc,2025-12-06 00:35:05,38
Intel,nsig8v1,That would be amazing if it is the case. I will try it immediately.,buildapc,2025-12-05 23:51:26,9
Intel,nsimc2s,"That's an incomplete and misleading statement. The top slot is typically connected directly to the CPU, while lower slots can be connected via the chipset which can reduce performance slightly.  Lower slots can also be different PCIe gens, or have a lower lane count, both of which may or may not restrict performance depending on the GPU used.",buildapc,2025-12-06 00:28:57,1
Intel,nsipkkz,I really appreciate your help solving the the issues with my PC. It seems to be working now and I am very thankful. Is there some way I can express my appreciation?,buildapc,2025-12-06 00:48:39,3
Intel,nsiqt36,"To clarify, the top-most x16 slot with x16 pins, or x8 pins if you are repurposing server or workstation equipment.  Some motherboards have a top slot that is electrically x1 but x16 in length. This is less common nowadays with a m.2 slot taking the space of this PCIe card connection, but for those in the future you can check your motherboard manual or just look into the slot to see if it has gold pins in the whole slot or just the first 1-4 cm.  Again, less common nowadays but the visual inspection is the quickest way to tell.",buildapc,2025-12-06 00:56:22,2
Intel,nsi1chm,If I have to rebuild it and double check every part and piece to get my peace of mind back I will.,buildapc,2025-12-05 22:22:52,2
Intel,nsi38qr,It doesn't seem to be temp issues. It is about 60C while under load. That seems normal.,buildapc,2025-12-05 22:33:34,2
Intel,nsi2y5h,I do not have the old GPU anymore. I am slowly testing and working down the list of PCIe modes in the BIOS though. So fingers crossed that works.,buildapc,2025-12-05 22:31:53,1
Intel,nsi4je5,I will give it a check. I normally find myself alt tabbing between a game that is in full screen into a web browser or desktop Discord.,buildapc,2025-12-05 22:40:54,1
Intel,nsj2itd,"I checked my bios and I don't have those actual options..my options are auto, x8x8, x8x4x4, and x4x4x4x4 which I've never heard of so I'm going to look into it. Yours understandably makes more sense why it didn't work properly",buildapc,2025-12-06 02:12:20,1
Intel,nsj8x58,"I found it in a different section and it works?!!!! Wtf I don't get it. My GPU is 4.0x16 and my mobo is 4.0x16. Set to auto is bad, set to 4.0 is bad, but set to 3.0 and the problem is gone  Thanks a lot you helped me narrow down the problem but now I gotta find out why this is an issue lol",buildapc,2025-12-06 02:53:42,1
Intel,nshwlcs,I will read into this and give it a try. Thank you!,buildapc,2025-12-05 21:56:44,3
Intel,nsi04vd,"Thank you, I changed it to Gen 5 and will see if that helps!",buildapc,2025-12-05 22:16:09,1
Intel,nshuh4v,Using the app I was able to confirm its using the 5080 and 9800x3d and must have been with all the other troubleshooting. :(,buildapc,2025-12-05 21:45:27,1
Intel,nsi2mxi,You are right it seems that was the idle temp. At 5 minutes max load it is just short of 60C on all cores.,buildapc,2025-12-05 22:30:09,1
Intel,nshtzqp,"true but if its just for a quick check shouldnt matter. hwinfo could still give some clues about whats wrong here, good tip",buildapc,2025-12-05 21:42:53,6
Intel,nsi2gx4,any changes perfomance wise?,buildapc,2025-12-05 22:29:13,2
Intel,nsi8hh8,Oohhhh ok! Thanks for clarifying and teaching me something I didn’t know. I was so wrong lol.,buildapc,2025-12-05 23:03:51,1
Intel,nshvqx4,Got it - but my point is he enabled XMP. Not either of the Expos.,buildapc,2025-12-05 21:52:13,1
Intel,nsiol77,Glad I helped. Please update the OP so people who face this issue can solve it easily,buildapc,2025-12-06 00:42:41,15
Intel,nsjd71z,Put an edit in your top post for those that are still trying to help!,buildapc,2025-12-06 03:22:12,5
Intel,nsit9ro,Sweet! Happy for you.,buildapc,2025-12-06 01:11:58,1
Intel,nsk5nu5,Send them your 5080 as a thank you.,buildapc,2025-12-06 07:13:53,1
Intel,nsiaq1j,Agreed,buildapc,2025-12-05 23:17:24,1
Intel,nsi5q6x,In that case that stutter is normal as the display adjusts from focusing entirely on the game to a non-fullscreen focus.,buildapc,2025-12-05 22:47:43,2
Intel,nsi0jop,"Just remember to keep lowering it and if it’s still no good at 3.0, just change it back to Auto.",buildapc,2025-12-05 22:18:24,1
Intel,nsi4ela,"Okay, that makes more sense. 60C suggests to me that you installed your cooler correctly (and you have great airflow). Unfortunately I really think this may be a bad motherboard, especially with the popping and audio distortions you’re hearing.   Since it’s self built, some cables may not be fully seated. Here is the link to the digital English manual for your motherboard: https://download.msi.com/archive/mnu_exe/mb/MAGX870TOMAHAWKWIFI_English.pdf . You want to find the diagram showing what cables go where. I would:   1) specifically pay attention to the 24-pin power cable (a big ass, stiff ass cable that goes from PSU directly to MOBO, plug is labeled ATX_PWR on the diagram). Make sure it’s fully seated. If you want to unplug it and replug it, be careful as it will require some force but you want to be slow with it.   2) look at the two 8-pin power connectors for the CPU. It’s on the top left of the MOBO (labeled CPU_PWR1 and CPU_PWR2 in diagram). This ones a lot easier to plug and unplug. Make sure BOTH are plugged in and fully seated.   3) Confirm that the power connector to the graphics card is fully plugged in. Do that same unplug and replug. Examine the power connector for signs of damage (mainly burning/melted plastic).   4) You may have already done this, but fully reseat your RAM.   5) Make sure the audio cable is fully plugged into the MOBO. This is on the bottom left of your MOBO and is labeled “JAUD1” on the schematic.  IIRC it’s a funky looking plug, haven’t seen anything else like it.    Apologies if you have already done these steps. The only thing it can really be at this point is the MOBO or possibly GPU related, because your processor seems to be doing its job.  EDITED after reading more details.",buildapc,2025-12-05 22:40:09,1
Intel,nshvpcu,Cinegraph is running in the background but with that in mind HWinfo shows I am effectively operating at 5230 MHz at a x52 times ratio. I think this is max capacity? Active clock says 5225 MHz is expected.,buildapc,2025-12-05 21:52:00,3
Intel,nsi3xya,I just did a quick check using Rust as a baseline. Maybe an extra 5 FPS there but still under 40 too consistently.   Audio issues are too sporadic to check manually.,buildapc,2025-12-05 22:37:32,2
Intel,nshw1aa,I know they have different names for the same purpose. Would it be possible I enabled the wrong one?,buildapc,2025-12-05 21:53:45,1
Intel,nsi68aw,That is good to know. Thank you for the help!,buildapc,2025-12-05 22:50:38,1
Intel,nsi65a8,I appreciate the in depth breakdown. For the moment I am trying all the software settings and testing before I start ripping it back open.       During the installation nothing seemed out of place or funny but its been a while since I did a custom build and I can double check that everything works with that.,buildapc,2025-12-05 22:50:09,2
Intel,nshxhay,thats good,buildapc,2025-12-05 22:01:32,3
Intel,nsil42x,Get a 3dmark score and compare to similar system scores in the results,buildapc,2025-12-06 00:21:31,2
Intel,nshx4eq,"Yes - in your original post you stated you enabled XMP. That could be the source of your problem. I would try disabling any RAM BIOS OC/double check and see if you did set it to XMP, Try Expo 1 or 2.",buildapc,2025-12-05 21:59:36,0
Intel,nsi6h7c,"Fair enough. Look back through my comment if/when you do tear it open, I’ve made some edits to be more specific and concise. If it helps, I don’t think you’ll need to remove any parts to get to any of the cables I mentioned. (MAYBE have to uninstall GPU to get to the audio cable, MIGHT have to remove CPU cooler to get to CPU plugs, depends on how beefy it is).  EDIT: Also, if you do end up taking the side panel off, feel free to DM me some pictures of the inside. I can try to help more in depth as I’m available.",buildapc,2025-12-05 22:52:03,1
Intel,nshxv6n,That's good I guess. Thank you for the help,buildapc,2025-12-05 22:03:38,1
Intel,nshxl2r,oh yeas good idea i completely overread this and my brain said same thing,buildapc,2025-12-05 22:02:07,1
Intel,nshzmqb,It seems I mistyped. I did enable EXPO1 not XMD and changed the post to match.,buildapc,2025-12-05 22:13:22,1
Intel,nsi6nd2,They are both beefy....   But thank you again. I will reread them once I pop it open.,buildapc,2025-12-05 22:53:03,2
Intel,nsi2dyh,just means that it isnt the cpu and or mainboard. could be the gpu or the slot or some weird software stuff,buildapc,2025-12-05 22:28:46,2
Intel,nsi05n3,Ok got it. Could be worth trying moving to the more stable Expo 2 or disabling all together… sorry you’re having issues.,buildapc,2025-12-05 22:16:16,1
Intel,nsih9xd,You may want to just shoot a short video or bunch of photos showing where everything is plugged in.   Take it slow and be sure to show everything.,buildapc,2025-12-05 23:57:47,1
Intel,nr9nfwt,More ram because this is a amazing build,buildapc,2025-11-28 20:00:04,1
Intel,nr9p2c4,"Get fast 32GB DDR4 RAM (or even more). The rest of the build is well balanced and upgrade meaning completely new system (except PSU and storage). The GPU still good, but produce less fps in new games compare to 5070Ti (own both - personal experience).",buildapc,2025-11-28 20:09:20,1
Intel,nr9vn4i,RAM 32GB (16 * 2),buildapc,2025-11-28 20:46:35,1
Intel,nsc9tfg,400 CAD for 3080 is good,buildapc,2025-12-05 00:23:08,3
Intel,nscat2a,You should also look into getting a new CPU as well that can help your FPS as well.,buildapc,2025-12-05 00:28:49,1
Intel,nscb0oe,"That's more than I would spend on a used GPU, personally. Who knows how it used and how much life it has left in it? Before I spent that kind of money on a used GPU, I would save a little more for something new from the current generation, like maybe a 5060ti. Not only does it average right about 60fps in pure rasterizarion, without DLSS and frame generation in play, but you would also get the benefit of both of those things, which you will not have with the 3080.  The performance numbers I have were based on a Tom's Hardware review of the 5060ti. I suggest doing your own research, of course.  Good luck!",buildapc,2025-12-05 00:30:02,1
Intel,nsccuc6,"A 3080 no, a 3080ti I would consider it but would also check the 4070 if prices are to your wallet. I own a 7800xt and it's a solid 1440p card. Pair it up with a 5700x and you have a solid combo with vram to spare... RT is... capable.  Also, make sure your psu can handle them.",buildapc,2025-12-05 00:40:32,1
Intel,nscd0jy,"100%, for the right price. Ive been recommending a used 3080 or used 3080 Ti for over a year now. I also recommend a used 6900 XT. You can get one for like 375 on ebay. Insane value imo",buildapc,2025-12-05 00:41:33,1
Intel,nsch652,"I'm playing arc raiders with a 3080 10gb and 5800x, on 1440p monitor. With graphics set to epic, I'm getting 90-100 fps.",buildapc,2025-12-05 01:06:30,1
Intel,nscp1iy,"30 series ain't too old, just try to haggle it down some.",buildapc,2025-12-05 01:53:49,1
Intel,nscytv2,"Seems expensive for a 3080, but it’s still a solid card",buildapc,2025-12-05 02:50:35,1
Intel,nscanmk,Yeah the 3080ti is still a very strong card. A 3080ti 12gb is about 20% stronger than a 5060ti 16gb while being about the same price. Though the 5060ti has more VRAM and will have have driver support further down the road while having access to the new DLSS.   Its a reeaally tough choice. 20% extra FPS is more worth it IMO versus the extra Vram,buildapc,2025-12-05 00:27:57,1
Intel,nscl50f,yeah it is,buildapc,2025-12-05 01:30:53,1
Intel,nscb9i0,"For sure. I can’t afford to upgrade a ton right now, but I figured I could do it piece by piece, starting with my gpu for now yk",buildapc,2025-12-05 00:31:26,3
Intel,nscbl68,Thanks so much!,buildapc,2025-12-05 00:33:19,1
Intel,nscbsj8,I’ll definitely take this into consideration,buildapc,2025-12-05 00:34:30,1
Intel,nscy24r,Wab a 3090ti?,buildapc,2025-12-05 02:46:06,1
Intel,nscf7mj,"It’s not a ti.  So potentially 2GB less memory, fewer CUDA cores, and 5-15% less perf. I still think it’s a deal but might want to amend your post.",buildapc,2025-12-05 00:54:32,3
Intel,nscllir,what board and psu do you have?,buildapc,2025-12-05 01:33:38,2
Intel,nsde7vo,and you get bonus a.i. cores 😅,buildapc,2025-12-05 04:27:35,1
Intel,nsf526y,"Considered it, but too expensive in my country (used market sucks) and it's a power hog. But a beast of a card.",buildapc,2025-12-05 13:24:55,1
Intel,nscyta9,"I can’t remember the exact variant, but I have a b450m matx motherboard. When swapping the gpu I’ll probably be switching out the psu for a 750w or 850w",buildapc,2025-12-05 02:50:30,2
Intel,nsd0uno,"that would be wise.  are you eventually going to AM5 in the near future? if not, i would be looking into a slightly newer cpu for better single thread performance and additional cpu cache for gaming.  pick a few out and price watch them. since they are end of life they might go up in price",buildapc,2025-12-05 03:02:22,2
Intel,nsd764g,Check Amazon returns. They have a few older Am4 overstock chips their trying to clear out of inventory that hasn't sold. You can get a really good deal on a chip rn thats better than your current.,buildapc,2025-12-05 03:41:33,2
Intel,nsd8tb2,Sick! Any specific ones worth checking out?,buildapc,2025-12-05 03:52:08,1
Intel,nsdc3hz,"Only 2 worth keeping an eye on upgrade wise from you 3700x since that chip was such a huge value buy is the 5600x3d, 5700x3d. The ones on Amazon get snapped up really fast but you can find one reliably on ebay. That CPU will give future proofing for a long time and keep you from having to make the switch to Am5 and will work really well with current Gen CPU.    Because honestly if it wasn't for ramageddon the normal choice would be to make the jump to Am5 because of the value to performance on the newer 7800x3d and 9800x3d chips.",buildapc,2025-12-05 04:13:43,2
Intel,nsddwn8,Ramageddon fits rn😅,buildapc,2025-12-05 04:25:33,2
Intel,nsdn30z,"Valid point. I’ll keep this in mind 🙏. Regarding the spike in ram prices tho, wouldn’t it make sense to just buy used? I’ve seen some exceptional prices compared to what they’re at right now",buildapc,2025-12-05 05:32:41,2
Intel,nsxtdp4,"If they could hit close to 5070 performance and price it at $399, they might just go from 1% to 2%!",pcmasterrace,2025-12-08 14:19:35,181
Intel,nsz2pa6,holy shit. these cards are not cancelled?  are they still coming?       theres still hope for Celestial?  https://preview.redd.it/daxvcoxft06g1.jpeg?width=300&format=pjpg&auto=webp&s=8b3dba05fcdcfb82799679baccf7b3c3b05073f6,pcmasterrace,2025-12-08 18:09:07,53
Intel,nsylv71,"Over 50% more power, so it should be over 50% more faster.  So slightly faster than the 5060ti and 9060xt.",pcmasterrace,2025-12-08 16:46:28,73
Intel,nsyyx13,"Spicy.  Nice to see a unit with some power.   Really, their target only needs to be match the 5070/5080/9070XT range.  If they can do that and actually get volume built, they'll sell a ton.   I've other very interesting thing they could do is go heavy on vram, not because it's needed for games but because it's needed for bigger at home AI models.   Drop a 32gb just for fun, or larger and let people run bigger models than can even fit on 4090/5090 cards.  Create a cheap alternative to the pro cards.  Don't even worry about speed.  Just gotta fit model sizes.",pcmasterrace,2025-12-08 17:50:43,15
Intel,nsytl29,Going to be a very interesting card. In the age of expensive GPUs...,pcmasterrace,2025-12-08 17:24:33,2
Intel,nszmp6l,"At that TDP it better be very, very close or better than RX 9070 XT... Ok, fine RX 9070, non XT.",pcmasterrace,2025-12-08 19:46:56,3
Intel,nszfu17,I’d sell my 4070s to support them if they can hit the same amount of performance level. I know sounds redundant as hell but need to have more competition in the gpu space,pcmasterrace,2025-12-08 19:12:37,5
Intel,nt11pio,Hmm for 300watts isn’t that close to 9070xt and 5070TI power usage? Does intel in general use more power? Just from efficiency standpoint and power usage that seems where it may land unless it is power hungry and inefficient compared to competition? Any info is appreciated. Trying to line this up as well 👍.  Either way glad to see it’s finally coming out. I wish they announced it sooner though would have tried to get it over the 9070XT to be honest.,pcmasterrace,2025-12-09 00:15:40,1
Intel,nt1y0r4,I’ll be watching this with interest. I love my b580 and I’ve had 0 issues with it regardless of whatever crap about them gets spouted online. Never had a crash or driver issue related to the card and the driver update have been coming hard and fast bringing improvements   If these are priced right I’ll be upgrading and replacing my wife’s 3060 with my b580,pcmasterrace,2025-12-09 03:26:29,1
Intel,nt2fwaw,Motherfucker I just bought a B580 because I thought this thing was a myth!,pcmasterrace,2025-12-09 05:34:58,1
Intel,nt2m225,"Lotta people hoping for 5070 performance who will be pretty disappointed. I'm not saying this will be 5060Ti level necessarily, but probably best we can hope for is something between 5060Ti and 5070 performance but with 16GB of VRAM, in the $350-$399 range.",pcmasterrace,2025-12-09 06:27:47,1
Intel,nt0t379,Gross.  You guys know higher tdp is bad right? It’s literally a measure of power consumption.,pcmasterrace,2025-12-08 23:25:32,-4
Intel,nsy6tp1,"The B580 is 4060 performance at almost 200 watts, it's probably gonna be 4070 levels.",pcmasterrace,2025-12-08 15:32:13,55
Intel,nsxwwbt,"""5070 Performance!""",pcmasterrace,2025-12-08 14:39:43,13
Intel,nsykf0w,They can make it $150 and it still won't increase market share.,pcmasterrace,2025-12-08 16:39:27,2
Intel,nszb49c,Well if Nvidia pulls a crucial and stops selling consumer cards then Intel has a shot!,pcmasterrace,2025-12-08 18:49:43,1
Intel,nt2v17z,Just from sales standpoint 1%->2% means 100% more sales. Which is allright ?,pcmasterrace,2025-12-09 07:52:18,1
Intel,nsyp1w7,"At 300w TDP it better be, but definitely not at that price.",pcmasterrace,2025-12-08 17:02:03,1
Intel,nt124qw,"Can't wait for first qtr of 2027 to see them!  Honestly quite annoyed with this launch, they are seemingly having issues at this point. Battlemage has been out for a whole year with no news, only leaks of the 700 series cards.",pcmasterrace,2025-12-09 00:18:05,2
Intel,nsz9p4a,"Judging by the late timing maybe they were ""uncancelled"" =p",pcmasterrace,2025-12-08 18:43:00,-2
Intel,nt1fgyw,"That's assuming performance scales linearly with power consumption, I'm not saying it doesn't but that's not really something we can verify without some hard data.",pcmasterrace,2025-12-09 01:37:32,9
Intel,nsz2c2d,Oof I'm hoping around 5070,pcmasterrace,2025-12-08 18:07:20,17
Intel,nszf9nb,I'll take that. The speed bump that comes with fitting everything in vram for AI outclasses raw power by a mile,pcmasterrace,2025-12-08 19:09:49,5
Intel,nt0m9ct,With the price of memory right now dont count on it.,pcmasterrace,2025-12-08 22:46:53,4
Intel,nt0tqie,Honestly if it’s 399 and hitting 5070/9070 performance. I’m buying in a heartbeat,pcmasterrace,2025-12-08 23:29:17,1
Intel,nsznb9w,"Not to ruin your day, but selling your 4070S just to ""support"" them is IMO a stupid decision.  The support you ""provided"" is completely negligible, literally a water droplet in a lake. In exchange, you get a far worse experience in your performance, software, power efficiency and compatibility.  All of that just so that you can feel right in your own PoV lol. Selling it doesnt change the fact that you already gave Nvidia your money.  You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.",pcmasterrace,2025-12-08 19:50:00,3
Intel,nt0u0sp,But with TDP performance goes in pair. NVIDIA nailed it and they are low TDP high performance. Thats why they are 90%. But to get some traction you have to compete in performance much more then in TDP. Thats what most games care for - FPS.,pcmasterrace,2025-12-08 23:30:57,3
Intel,nt2g36j,I doubt you're the only person here who was smart enough to put that together u/flatroundworm.,pcmasterrace,2025-12-09 05:36:33,1
Intel,nsz2eo1,4070 and 5070 are extremely close.,pcmasterrace,2025-12-08 18:07:41,27
Intel,nsy2i8s,"""4090 Performance!""",pcmasterrace,2025-12-08 15:10:03,21
Intel,nt2lrii,"Why not? As others have mentioned, their market share is already successfully increasing. There's no reason to think a new desirable GPU wouldn't continue that trend.",pcmasterrace,2025-12-09 06:25:11,2
Intel,nt1mcld,"Exactly.  If they were smart, they just build what others don't and build something no one can currently compete with.  Build big and take the sales.   There are three challenges.  One, it needs to be good enough to be on par with the 5070/5080/9070XT.  It needs to be a viable alternative, not 30% worse but on par, however they need to do it.  Two, they need to build a feature no other manufacturer has.  This is the vram part.  Go big.  Make it do something no one can compete with for a period of time   Three is the bigger challenge, but it might or might not matter to certain people.  This is cost and efficiency.  If the cost can be competitive to the 5070 and 9070 XT, they will get considerable sales, not like 1%, but as many sales as they can build for.  The second part efficiency.  If they can be highly efficient, they could market a budget option for AI use, not necessarily fastest but efficient for the work load.  This too could promote a significant sales volume just because it costs less at scale.   But, if very little is as achievable, you can always have a feature no one else has, and you can still sell quite well.",pcmasterrace,2025-12-09 02:17:50,0
Intel,nt2jnh8,If it's 32 Xe cores (up from 20 on the B580) that'd likely put it somewhere more like a 6800XT or 5060Ti :(,pcmasterrace,2025-12-09 06:06:49,1
Intel,nszq3p8,It prevents someone else giving Nvidia their money while propping up intel - if more people did it like this mad lad it would make a difference,pcmasterrace,2025-12-08 20:03:48,3
Intel,nt0x34v,"I bought the card used lol , no way I’d pay over msrp . But your statement stands true .",pcmasterrace,2025-12-08 23:49:07,1
Intel,nszzd1u,It’s closer to 4070ti. It’s a 23% improvement over 4070. Putting it a little slower than 4070ti but faster than 4070S.,pcmasterrace,2025-12-08 20:49:50,9
Intel,nsys236,"""4100 Performance!""",pcmasterrace,2025-12-08 17:16:57,2
Intel,nszwbto,"It doesnt change the fact that the 4070S was already sold. Nvidia already got their money from that unit. Selling your 4070S DOESNT RETURN what you already gave to nvidia's pockets lol.  You can think of it as **paying for someone** else to get a 4070S.  That is the thing that people like dude above is missing.  In the end, you just sacrificed your own experience for nothing. Its a ***Lose Win*** situation.  I aint gonna be stupid to sabotage myself just for a temporary feeling of being right and morally superior, a feeling that I did the right thing, because in the end, its inconsequential, and I just fucked myself in the process, and what is waiting for me in the end is just regret lol.  You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  Also, you dont want to mindlessly give Intel your own money. You want them to earn it. Buy Intel GPUs not because you want a 3rd player, but because you know its a good product and helping out a 3rd player in the GPU market is just a nice bonus.  If Intel half assess the B770's pricing and launch, and you still bought it just ""to support the underdog"", you are just incentivizing Intel to continue half assing their approach",pcmasterrace,2025-12-08 20:34:51,-4
Intel,nt00jx3,"Damn I was thinking mobile, those are like 10-15 percent",pcmasterrace,2025-12-08 20:55:42,2
Intel,nt03r22,"Private profile, shilling. Jog on buddy, no one knows what the card will be like.",pcmasterrace,2025-12-08 21:11:29,0
Intel,nt07q12,"You keep repeating the idea that “the 4070S was already sold, so nothing changes,” which tells me you’re stuck in the sunk cost fallacy. Nvidia getting money from the original sale is irrelevant to the next decision. What matters in economics is marginal demand. When someone sells their 4070S and chooses Intel for their next GPU, they remove one potential new Nvidia sale (the buyer of the used card may otherwise have purchased new) and simultaneously create one new Intel sale. Those marginal shifts are exactly how incumbents lose revenue and challengers gain market share. Sunk revenue doesn’t grant Nvidia permanent immunity from switching behavior.  A strong second hand market absolutely hurts Nvidia’s future sales. It lowers the effective cost of GPU ownership, pushes price sensitive buyers into used units instead of new ones, and reduces Nvidia’s pricing power. This is standard industrial organization theory used goods cannibalize new goods. Selling the 4070S does not “give Nvidia money,” it gives someone else a path to avoid paying Nvidia at all. Then replacing that card with Intel means Intel gets fresh revenue while Nvidia gets zero. Combine both actions and the net effect is Nvidia loses one new sale, Intel gains one new sale. That is the literal mechanism by which competition grows.  And pretending that “one person doesn’t matter” ignores how collective marginal actions scale. If even a small percentage of consumers switch from buying new Nvidia cards (~€600 on average) to Intel instead, the foregone revenue becomes massive. For example, 100,000 people doing this is €60 million Nvidia doesn’t get and €60 million Intel gains. That’s not “inconsequential”, that’s enough to change pricing strategy, product roadmaps, and R&D priorities. Markets don’t change because one individual defects they change when many individuals do, and that only happens if people stop rationalizing inaction with the sunk cost excuses you’re using.  Your whole argument hinges on a single transaction worldview, as if each GPU purchase is isolated. But the GPU market is dynamic. Nvidia’s future behavior depends on demand elasticity, competitors’ traction, and consumer switching. Selling your Nvidia card and buying Intel is a strategic choice that weakens incumbent dominance, strengthens pricing competition, and signals that alternatives are viable. Yes, you shouldn’t blindly buy Intel if the product is bad but when Intel is competitive, shifting demand is exactly how you incentivize innovation and better pricing across the board. In other words you don’t “sacrifice yourself,” you just refuse to keep subsidizing the monopoly.  In short, your logic collapses because you ignore marginal effects, second hand market cannibalization, collective action, and basic microeconomics. The 4070S sale is sunk the choice of what you buy next is what actually shapes the market.",pcmasterrace,2025-12-08 21:30:57,0
Intel,nt046ln,"So what does a private profile got to do with it?  Or did you just ignore all my points.     The fact that said the word ""shilling"" is clear indicator you didnt understand what I said.  Did you want to go full Ad Hominem Fallacy by using my own profile against me while not doing anything with my statements LOL?",pcmasterrace,2025-12-08 21:13:36,-1
Intel,nt1j8g4,"Give it time and that guy will switch it up and come back with ""consumers don't matter anymore to Nvidia, only data center"". These corporate bootlickers are very narrow minded.",pcmasterrace,2025-12-09 01:59:51,0
Intel,nt058qt,"The fundamental point you have is that a hypothetical card will be worse than the 4070 super, which is silly to say because we just don't know.",pcmasterrace,2025-12-08 21:18:52,2
Intel,nt06c8v,"Which still doesnt change the fact that itll be less efficienct and have weaker software. The 4070S draws 220W, and has access to a multitude of features at far greater availability and support, that is Nvidia's biggest strength that even AMD stans wont deny.  It still doesnt change the fact that you already gave money to Nvidia.  You could use your own statement against OP and yourself. Why would you sell your GPU when we dont even know how good it is? Lol. You are dumping a good card for an unknown one.  And I already said the following:  >You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  >You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.  Tell me, wheres the shilling here?",pcmasterrace,2025-12-08 21:24:13,2
Intel,nt3bukf,It might run Crisis with the Arc card but not very well with the GeForce.  Crisis has been a very bad experience for the last (I think) half a year on Nvidia cards because Nvidia driver has broken some DX9 compatibility,pcmasterrace,2025-12-09 10:42:31,2
Intel,nt41cwc,The crysis part was more of a meme XD,pcmasterrace,2025-12-09 13:52:31,1
Intel,nt44aic,"Yeah I know, I just find it funny that a 5070 (and any GPU currently supports by Nvidia) can't run a 18 year old game.",pcmasterrace,2025-12-09 14:09:23,1
Intel,nson3ou,"The CPU, RAM, and GPU alone would put you at around $1500(if you’re lucky) if bought separate with the current ram prices.",pcmasterrace,2025-12-07 00:46:54,11
Intel,nsoxki3,"I just bought exactly those components, a 5070TI, 9800X3D, RM850X, 2TB NVMe, AIO and a new chassi.  All of this cost me... 2200$, which means that for 500$ less than I spent you get a motherboard and **32GB DDR5 RAM!!!**  That would be a good deal even before the RAM shortage, now its literally just highway robbery.",pcmasterrace,2025-12-07 01:53:13,8
Intel,nsongpo,Yeah I think that's a solid price. Should be a very solid rig for a very long time.,pcmasterrace,2025-12-07 00:49:07,2
Intel,nson7az,super job well done and you have done phenomal job finding these deal for you and you are lucky bastard and what type work you do,pcmasterrace,2025-12-07 00:47:30,2
Intel,nsoqrn2,Solid build,pcmasterrace,2025-12-07 01:09:59,1
Intel,nsos7un,"With RAM prices, yes, it’s a good deal.",pcmasterrace,2025-12-07 01:19:20,1
Intel,nsoswex,yes its good for the price.,pcmasterrace,2025-12-07 01:23:35,1
Intel,nsovpom,Woah….,pcmasterrace,2025-12-07 01:41:33,1
Intel,nspmhzy,Goddamn. Nicely done.,pcmasterrace,2025-12-07 04:35:18,1
Intel,nsprdli,Oh dude you got a hell of a great deal on that one good sht,pcmasterrace,2025-12-07 05:09:17,1
Intel,nt5mvi8,"That's actually a really good price, if you bought everything separately youd be looking at 2k +",pcmasterrace,2025-12-09 18:48:24,1
Intel,nsom9s0,Why's it matter if ya already bought it?,pcmasterrace,2025-12-07 00:41:49,0
Intel,nsvafwp,"All these ppl are just wanting attention, nobody spending this amount of money than asking did I do good.... You knew. You want attention! Lol",pcmasterrace,2025-12-08 02:14:24,0
Intel,nsor271,"these components are great at these prices, considering how AI is drastically impacting prices of computer components. do be aware that cyberpower will ship those out with other parts that just work, like your motherboard may not be pimped out and be basic, but if you’re not really a power user, you’ll be good with that build",pcmasterrace,2025-12-07 01:11:52,-1
Intel,nsoybli,Feel like its overkill for i need but i felt like i couldnt pass up that deal. It was a little out of budget. I been trying to snag it for 2 weeks it was always sold out in minutes,pcmasterrace,2025-12-07 01:57:55,2
Intel,nsotgj1,"I trade, so alot of data streaming and platforms running at the same time",pcmasterrace,2025-12-07 01:27:10,2
Intel,nsomdcp,Because i can still return it? Til jan 31st,pcmasterrace,2025-12-07 00:42:25,2
Intel,nsou59i,Get more monitor and rams upto 64gb ddr5,pcmasterrace,2025-12-07 01:31:34,-1
Intel,nsou60i,You should stare at it unopened until the 30th and ask again .,pcmasterrace,2025-12-07 01:31:42,-1
Intel,ns788nb,Make sure the BIOS is up to date to support Resizable BAR for the B580,pcmasterrace,2025-12-04 05:34:05,1
Intel,nt5vs42,I would sell the GPU to affoard 16gb of ddr5 RAM💀  Edit: Bakers House,pcmasterrace,2025-12-09 19:32:28,625
Intel,nt5vl7x,That lab in the movie with them crazy lasers.,pcmasterrace,2025-12-09 19:31:31,73
Intel,nt5vpi5,House Beneviento,pcmasterrace,2025-12-09 19:32:06,37
Intel,nt5v152,Baker house,pcmasterrace,2025-12-09 19:28:44,38
Intel,nt6hggw,Never had the chance to play so I'm not sure. I'd love to give it a try though,pcmasterrace,2025-12-09 21:18:55,17
Intel,nt5vz8k,***RCPD is very iconic to me and tight spaces***,pcmasterrace,2025-12-09 19:33:26,24
Intel,nt7ysxv,Re4 Ashley segment💀,pcmasterrace,2025-12-10 02:14:05,5
Intel,nt5wd7t,"Resident Evil 3, the very first nemesis encounter in front if R.P.D. building. I dreaded that location ever since that encounter.",pcmasterrace,2025-12-09 19:35:23,3
Intel,nt5w1vw,"wow my brother would love this, he's been in need of a new rig and loves RE.  I personally haven't played any of the games in this series besides the first few hours of RE:Revelations on 3DS.  The vibe of the cruise ship was creepy enough that I couldn't play too far. Horror games are not for me.",pcmasterrace,2025-12-09 19:33:49,6
Intel,nt5w9nx,Baker house,pcmasterrace,2025-12-09 19:34:53,2
Intel,nt5x8zd,Only one I played was Village. The outside area at the start creeped me out.,pcmasterrace,2025-12-09 19:39:45,2
Intel,nt5ysul,"The starter zone of RE 5, I don't really know it just made me so nervous...",pcmasterrace,2025-12-09 19:47:29,2
Intel,nt612at,"The opening village section in **Resident Evil 4** by far left the biggest impression on me, i would play it over and over on Gamecube i had never seen such smooth third person shooting mechanics and kept i getting my head chain sawed off.",pcmasterrace,2025-12-09 19:58:35,2
Intel,nt7s5rk,Spencer mansion,pcmasterrace,2025-12-10 01:34:45,2
Intel,nt8bflo,COMPTER GAEMS!  I think the swamp in Resident Evil 4 was the creepiest for me. That swamp monster was fear-inducing.,pcmasterrace,2025-12-10 03:30:30,2
Intel,nt5wd54,I think it the Baker house. I just didn't like that place.,pcmasterrace,2025-12-09 19:35:22,2
Intel,nt5wgx4,***Spencer Mansion***,pcmasterrace,2025-12-09 19:35:53,1
Intel,nt5wjdh,For me RCPD,pcmasterrace,2025-12-09 19:36:13,1
Intel,nt5wqwo,I've never played the original RE2 and when I finally played it's remake I was terrified by how tight RCPD's corridors felt,pcmasterrace,2025-12-09 19:37:16,1
Intel,nt5wqzz,RCPD,pcmasterrace,2025-12-09 19:37:17,1
Intel,nt5wso1,RCPD! Played RE2 Remake and absolutely loved it. Was young when the original came out and too scared to play it. Very glad they did a remake and I go to experience it.,pcmasterrace,2025-12-09 19:37:30,1
Intel,nt5xcl6,baker estate most definitely,pcmasterrace,2025-12-09 19:40:15,1
Intel,nt5xhtn,hope so,pcmasterrace,2025-12-09 19:40:58,1
Intel,nt5xhve,the basement in re7,pcmasterrace,2025-12-09 19:40:58,1
Intel,nt5xjue,RCPD,pcmasterrace,2025-12-09 19:41:15,1
Intel,nt5xlma,Spencer Mansion is goated.,pcmasterrace,2025-12-09 19:41:30,1
Intel,nt5xmdf,Racoon City police department,pcmasterrace,2025-12-09 19:41:37,1
Intel,nt5xqcu,I felt like the Baker Ranch was properly scary. That whole family was so unnerving and the house was so spooky.,pcmasterrace,2025-12-09 19:42:10,1
Intel,nt5xr2x,umbrella labs for sure,pcmasterrace,2025-12-09 19:42:16,1
Intel,nt5xr95,The first nemesis encounter in Resident Evil 3. I was a kid back when I played that and it freaked me tf out. Still carrying that trauma.,pcmasterrace,2025-12-09 19:42:18,1
Intel,nt5xsye,Mansion!,pcmasterrace,2025-12-09 19:42:32,1
Intel,nt5xx85,Baker house from biohazard,pcmasterrace,2025-12-09 19:43:07,1
Intel,nt5y2yy,rcpd,pcmasterrace,2025-12-09 19:43:55,1
Intel,nt5y41u,"Resident Evil 1, ps1, spencer mansion hands down.  I still love the game, more than the remake that came out for game cube and later pc",pcmasterrace,2025-12-09 19:44:04,1
Intel,nt5y56p,"House Beneviento, oh yeah",pcmasterrace,2025-12-09 19:44:13,1
Intel,nt5y6ed,Spencer Mansion…the creepy vibes and being scared at every turn,pcmasterrace,2025-12-09 19:44:23,1
Intel,nt5y72f,"RCPD for sure. Imagine working there pre-outbreak with all of the puzzles, hidden doors, and riddles. The architect went all out.",pcmasterrace,2025-12-09 19:44:29,1
Intel,nt5y88s,RCPD,pcmasterrace,2025-12-09 19:44:39,1
Intel,nt5ycft,I was probably most impressed by the Raccoon City Police Department.,pcmasterrace,2025-12-09 19:45:13,1
Intel,nt5yd07,Bake house ftw !,pcmasterrace,2025-12-09 19:45:18,1
Intel,nt5yerz,Baker house,pcmasterrace,2025-12-09 19:45:32,1
Intel,nt5ygpo,Definitely the castle from village,pcmasterrace,2025-12-09 19:45:48,1
Intel,nt5ypns,RCPD,pcmasterrace,2025-12-09 19:47:02,1
Intel,nt5yr9a,RCPD in Dead by Daylight. Been used and abused many times there as a survivor,pcmasterrace,2025-12-09 19:47:15,1
Intel,nt5ytyg,baker house for sure,pcmasterrace,2025-12-09 19:47:38,1
Intel,nt5yzp5,***RCPD***,pcmasterrace,2025-12-09 19:48:24,1
Intel,nt5z0e6,Baker house obvs!!!,pcmasterrace,2025-12-09 19:48:30,1
Intel,nt5z2xl,RCPD,pcmasterrace,2025-12-09 19:48:51,1
Intel,nt5z3og,RCPD when you go past the windows a dozen times. That dozen +1 smash gets me every time.,pcmasterrace,2025-12-09 19:48:57,1
Intel,nt5z4am,Castle Dimitrescu,pcmasterrace,2025-12-09 19:49:01,1
Intel,nt5z56z,Which ever one was on dreamcast. It's the only RE I've owned and it scared the shit out of me as a kid. Traded a friend for that game.,pcmasterrace,2025-12-09 19:49:09,1
Intel,nt5z5t7,The OG Spencer Mansion!,pcmasterrace,2025-12-09 19:49:14,1
Intel,nt5zc7x,Def Baker House,pcmasterrace,2025-12-09 19:50:06,1
Intel,nt5zihf,"Baker house was definitely the creepiest, but RPD is just iconic.",pcmasterrace,2025-12-09 19:50:58,1
Intel,nt5ziza,I haven't played all but for me it always will be the night at village,pcmasterrace,2025-12-09 19:51:02,1
Intel,nt5zj8a,A 5090 would bottleneck my Ryzen 5 3600x so bad at 1080p lol but I don’t care I want it lol.,pcmasterrace,2025-12-09 19:51:04,1
Intel,nt5zl11,Marry Chrysler,pcmasterrace,2025-12-09 19:51:19,1
Intel,nt5zm3d,House Beneviento!!,pcmasterrace,2025-12-09 19:51:27,1
Intel,nt5zm8m,Friggin Baker House man. Friggin SWAMPS BRUH,pcmasterrace,2025-12-09 19:51:28,1
Intel,nt5zmev,Baker house did it for me,pcmasterrace,2025-12-09 19:51:30,1
Intel,nt5znb9,Easily RCPD.,pcmasterrace,2025-12-09 19:51:37,1
Intel,nt5zo67,Spencer Mansion 100%,pcmasterrace,2025-12-09 19:51:44,1
Intel,nt5zp8c,Mansion all the way!!,pcmasterrace,2025-12-09 19:51:54,1
Intel,nt5zqcb,The basement in 7,pcmasterrace,2025-12-09 19:52:02,1
Intel,nt5zqxx,RCPD,pcmasterrace,2025-12-09 19:52:07,1
Intel,nt5zr1f,Baker House RE7,pcmasterrace,2025-12-09 19:52:07,1
Intel,nt5zs92,Castle Dmimitrescu,pcmasterrace,2025-12-09 19:52:17,1
Intel,nt5zvlr,spencer mansion for sure,pcmasterrace,2025-12-09 19:52:44,1
Intel,nt5zwaw,Definitely the baker's estate,pcmasterrace,2025-12-09 19:52:50,1
Intel,nt5zwsm,RCPD,pcmasterrace,2025-12-09 19:52:54,1
Intel,nt5zx5i,RCPD definitely in my case.,pcmasterrace,2025-12-09 19:52:57,1
Intel,nt5zxkc,Baker house for sure,pcmasterrace,2025-12-09 19:53:00,1
Intel,nt5zy7u,RCPD is so iconic it can't be beat imo,pcmasterrace,2025-12-09 19:53:05,1
Intel,nt5zy7z,House Beneviento,pcmasterrace,2025-12-09 19:53:05,1
Intel,nt5zylr,The room with the lasers in the movie.,pcmasterrace,2025-12-09 19:53:08,1
Intel,nt5zze3,Spencer mansion 100%. It still bothers me 🤣,pcmasterrace,2025-12-09 19:53:15,1
Intel,nt5zzpb,Spencer Mansion has been ingrained into my mind forever.,pcmasterrace,2025-12-09 19:53:17,1
Intel,nt601i5,I loved the Spencer Mansion.  It has a great level of tension throughout.,pcmasterrace,2025-12-09 19:53:32,1
Intel,nt602d1,Never forget in biohazard my hand getting cut off in the big house 😭,pcmasterrace,2025-12-09 19:53:39,1
Intel,nt6034u,RCPD is such an obvius answe5 its hard to say anything else,pcmasterrace,2025-12-09 19:53:46,1
Intel,nt603g7,Baker house!,pcmasterrace,2025-12-09 19:53:48,1
Intel,nt6046f,Baker house definitely,pcmasterrace,2025-12-09 19:53:54,1
Intel,nt606ig,RCPD,pcmasterrace,2025-12-09 19:54:13,1
Intel,nt606xx,"RPD Police Station RE3 PS1, when Nemesis came thru the damn WINDOW, shts makes me jump so hard back in the days...",pcmasterrace,2025-12-09 19:54:17,1
Intel,nt607br,RCPD is iconic to me. And also I hated it lol,pcmasterrace,2025-12-09 19:54:20,1
Intel,nt608oo,Definitely baker's house,pcmasterrace,2025-12-09 19:54:31,1
Intel,nt608x3,Spencer Mansion!!!,pcmasterrace,2025-12-09 19:54:33,1
Intel,nt609mt,Baker house was pretty cool,pcmasterrace,2025-12-09 19:54:39,1
Intel,nt60a2c,Rcpd,pcmasterrace,2025-12-09 19:54:42,1
Intel,nt60aex,Have to be the Baker house.,pcmasterrace,2025-12-09 19:54:45,1
Intel,nt60c29,RCPD! Amazing map and so many good scares,pcmasterrace,2025-12-09 19:54:58,1
Intel,nt60cbk,Spencer Mansion!,pcmasterrace,2025-12-09 19:55:01,1
Intel,nt60cdo,Has to be Spencer Mansion,pcmasterrace,2025-12-09 19:55:01,1
Intel,nt60cno,Definitely the baker house by far,pcmasterrace,2025-12-09 19:55:03,1
Intel,nt60com,Baker House for sure,pcmasterrace,2025-12-09 19:55:04,1
Intel,nt60cse,Spencer mansion,pcmasterrace,2025-12-09 19:55:04,1
Intel,nt60d2b,Baker House,pcmasterrace,2025-12-09 19:55:07,1
Intel,nt60dc8,Baker house,pcmasterrace,2025-12-09 19:55:09,1
Intel,nt60dj0,RCPD by a country mile,pcmasterrace,2025-12-09 19:55:11,1
Intel,nt60ebk,Baker House,pcmasterrace,2025-12-09 19:55:17,1
Intel,nt60f8l,Umbrella labs of course is the focal point of all RE,pcmasterrace,2025-12-09 19:55:25,1
Intel,nt60ffq,RCPD when the dogs bash through the windows. That scarred me when I was young.,pcmasterrace,2025-12-09 19:55:26,1
Intel,nt60fjo,"Definitely the Baker Estate, super creepy",pcmasterrace,2025-12-09 19:55:27,1
Intel,nt60grt,Dimitrescus mansion. Beautiful and scary,pcmasterrace,2025-12-09 19:55:37,1
Intel,nt60gyt,"I've only watched a friend play the \~30 years ago, so I guess Raccoon City. Too creep for me to play lol.",pcmasterrace,2025-12-09 19:55:39,1
Intel,nt60h1a,RCPD is so fucking iconic and the remake made it even moodier and frightening!,pcmasterrace,2025-12-09 19:55:40,1
Intel,nt60h5s,Never played RE. But I should would like to be entered for a steam card!,pcmasterrace,2025-12-09 19:55:41,1
Intel,nt60hfw,Definitely Baker House,pcmasterrace,2025-12-09 19:55:43,1
Intel,nt60i1o,Defo baker house,pcmasterrace,2025-12-09 19:55:48,1
Intel,nt60i47,"Definitely spencer mansion, especially with it's atmosphere :))",pcmasterrace,2025-12-09 19:55:48,1
Intel,nt60inz,Baker House,pcmasterrace,2025-12-09 19:55:53,1
Intel,nt60izg,It'll always be the RCPD. Love that location,pcmasterrace,2025-12-09 19:55:55,1
Intel,nt60j10,The Baker house in Resident Evil 7. Playing first person for the first time in the (main) series helped give an air of mystique and intenseness that really set it apart for me.,pcmasterrace,2025-12-09 19:55:55,1
Intel,nt60kyt,Baker house,pcmasterrace,2025-12-09 19:56:12,1
Intel,nt60l0m,"The multi player map on the resident evil thst looked like a giraffe. Maybe 6? Anyways, rented it and it was fun.",pcmasterrace,2025-12-09 19:56:12,1
Intel,nt60mjk,The hive!,pcmasterrace,2025-12-09 19:56:24,1
Intel,nt60nr5,i've never played im just gonna put down Spencer Mansion,pcmasterrace,2025-12-09 19:56:34,1
Intel,nt60ob1,House Beneviento,pcmasterrace,2025-12-09 19:56:39,1
Intel,nt60orh,RCPD,pcmasterrace,2025-12-09 19:56:43,1
Intel,nt60oxh,"For me it was the train on RE Zero. I wish it lasted longer than it did. However, Baker House is a close 2nd.",pcmasterrace,2025-12-09 19:56:44,1
Intel,nt60prn,RCPD,pcmasterrace,2025-12-09 19:56:51,1
Intel,nt60pv1,***Spencer Mansion hands down***,pcmasterrace,2025-12-09 19:56:52,1
Intel,nt60rip,RCPD,pcmasterrace,2025-12-09 19:57:05,1
Intel,nt60rrj,"Spencer Mansion's eerie, maze-like design and creepy atmosphere make it unforgettable. But the Umbrella labs' cold, sterile horror takes the cake for pure dread.",pcmasterrace,2025-12-09 19:57:07,1
Intel,nt60rvh,"has to be hands down, rcpd",pcmasterrace,2025-12-09 19:57:08,1
Intel,nt60sp8,absolutely the Baker household. i LOVE RE7. scariest in the franchise.,pcmasterrace,2025-12-09 19:57:15,1
Intel,nt60t9p,RCPD,pcmasterrace,2025-12-09 19:57:20,1
Intel,nt60uhu,"Resident Evil Zero, The Train. It was my first real exposure to survival horror games and has stuck with me 23 years",pcmasterrace,2025-12-09 19:57:29,1
Intel,nt60upa,"RCPD, you ain’t save even in police station:)",pcmasterrace,2025-12-09 19:57:31,1
Intel,nt60uwt,Raccoon Police Department,pcmasterrace,2025-12-09 19:57:33,1
Intel,nt60uyi,The umbrella labs definitely.,pcmasterrace,2025-12-09 19:57:33,1
Intel,nt60uzh,I like the castle in Re 4,pcmasterrace,2025-12-09 19:57:34,1
Intel,nt60w1r,***Spencer Mansion***,pcmasterrace,2025-12-09 19:57:43,1
Intel,nt60x6i,RPD Headquarters of course! Imagine the staff trying to unlock puzzles just to get to the toilet.,pcmasterrace,2025-12-09 19:57:52,1
Intel,nt60x70,Baker house for sure,pcmasterrace,2025-12-09 19:57:52,1
Intel,nt60xw4,Baker louisiana house,pcmasterrace,2025-12-09 19:57:58,1
Intel,nt60ybp,Spencer Mansion because I was young and it was the first encounter that I had with RE.,pcmasterrace,2025-12-09 19:58:02,1
Intel,nt60yfr,Rcdc,pcmasterrace,2025-12-09 19:58:03,1
Intel,nt60ykq,"I just remember there being a giant crocodile or something in RE2 I believe, this was many moons ago and I was little watching an older friend play",pcmasterrace,2025-12-09 19:58:04,1
Intel,nt60z06,>***Which of Resident Evil’s spooky locations left the biggest impression on you? Spencer Mansion? RCPD? One of many abandoned Umbrella labs?***  Spencer Mansion easily,pcmasterrace,2025-12-09 19:58:07,1
Intel,nt60z1a,The meat locker / Freezer full of regenerators,pcmasterrace,2025-12-09 19:58:07,1
Intel,nt60zan,The Benviento House. Something about dolls doesnt sit right with me,pcmasterrace,2025-12-09 19:58:10,1
Intel,nt60zdl,RCPD without a doubt.  Playing it for the first time back in the day was terrifying.,pcmasterrace,2025-12-09 19:58:10,1
Intel,nt60zlr,Spencer Mansion,pcmasterrace,2025-12-09 19:58:12,1
Intel,nt60zt5,Definitely Baker's house,pcmasterrace,2025-12-09 19:58:14,1
Intel,nt6108g,Baker house,pcmasterrace,2025-12-09 19:58:18,1
Intel,nt610gr,RCPD,pcmasterrace,2025-12-09 19:58:20,1
Intel,nt611y4,Baker house,pcmasterrace,2025-12-09 19:58:32,1
Intel,nt6121g,The Baker house just feels so wrong,pcmasterrace,2025-12-09 19:58:33,1
Intel,nt6125c,Definitely rcpd!,pcmasterrace,2025-12-09 19:58:34,1
Intel,nt6136i,Baker house. Left the biggest impression,pcmasterrace,2025-12-09 19:58:42,1
Intel,nt614j8,The eerie creaking halls of the Spencer Mansion with its dim lighting and unsettling silence set the perfect tone.,pcmasterrace,2025-12-09 19:58:53,1
Intel,nt614kw,"Baker house in VR, 1000%",pcmasterrace,2025-12-09 19:58:53,1
Intel,nt614xg,It will always be the baker house in VR. Had to throw away my pants after my first (and last) time trying it.,pcmasterrace,2025-12-09 19:58:56,1
Intel,nt615xi,"The Island with the freaking Regeneradores, I still have nightmares of them growling and squeaking",pcmasterrace,2025-12-09 19:59:04,1
Intel,nt616a0,RCPD,pcmasterrace,2025-12-09 19:59:07,1
Intel,nt616aw,"I'd have to say Baker house, it felt more claustrophobic and scary than any other place.",pcmasterrace,2025-12-09 19:59:07,1
Intel,nt619hf,I never played RE. Could give it a shot though winkwink nudgenudge,pcmasterrace,2025-12-09 19:59:33,1
Intel,nt61acd,Gotta be the village from resident evil 4,pcmasterrace,2025-12-09 19:59:40,1
Intel,nt61bp5,RCPD but I only ever played the 2 remake. Horror games are not my cup of tea lol,pcmasterrace,2025-12-09 19:59:51,1
Intel,nt61bvi,The first hallway after the dining room in the original RE on PlayStation 1. That cutscene with the first zombie was terrifying to like 10 year old me.,pcmasterrace,2025-12-09 19:59:53,1
Intel,nt61cfa,Baker's house,pcmasterrace,2025-12-09 19:59:57,1
Intel,nt61dcm,Definitely Baker house,pcmasterrace,2025-12-09 20:00:05,1
Intel,nt61dxb,That one staircase where the chainsaw majini appears for the first time in re5.   That guy completely traumatized me lol,pcmasterrace,2025-12-09 20:00:10,1
Intel,nt61e05,RCPD for sure,pcmasterrace,2025-12-09 20:00:11,1
Intel,nt61etm,Baker House. Something about a regular family home just made it even more eerie.,pcmasterrace,2025-12-09 20:00:17,1
Intel,nt61fa8,"Got a bro who has a new rig but is still desperately in need of a new graphics card. Be sweet to hook him up.    Either way, happy gaming this holiday season to all you filthy animals.",pcmasterrace,2025-12-09 20:00:21,1
Intel,nt61gn2,Spencer mansion gave me some terrible night terrors as a child,pcmasterrace,2025-12-09 20:00:32,1
Intel,nt61hbf,RCPD! One of the most iconic locations in gaming,pcmasterrace,2025-12-09 20:00:38,1
Intel,nt61hev,House Beneviento,pcmasterrace,2025-12-09 20:00:39,1
Intel,nt61hgv,I only played the beginning of the original re4 actually,pcmasterrace,2025-12-09 20:00:39,1
Intel,nt61ihd,Definitely RCPD,pcmasterrace,2025-12-09 20:00:48,1
Intel,nt61iz2,Spencer Mansion,pcmasterrace,2025-12-09 20:00:52,1
Intel,nt61j11,"Umbrella Labs, for its distinctly sci-fi theme",pcmasterrace,2025-12-09 20:00:52,1
Intel,nt61jag,RCPD for sure.,pcmasterrace,2025-12-09 20:00:54,1
Intel,nt61kfl,Rcpd,pcmasterrace,2025-12-09 20:01:03,1
Intel,nt61kk6,RCPD definitely!,pcmasterrace,2025-12-09 20:01:04,1
Intel,nt61kul,Rcpd,pcmasterrace,2025-12-09 20:01:07,1
Intel,nt61lil,Probably the mansion from the first game?   I'll never forget those dogs jumping through the window.,pcmasterrace,2025-12-09 20:01:12,1
Intel,nt61luu,Spencer Mansion for sure,pcmasterrace,2025-12-09 20:01:15,1
Intel,nt61oq7,RCPD,pcmasterrace,2025-12-09 20:01:40,1
Intel,nt61ov5,Spencer Mansion,pcmasterrace,2025-12-09 20:01:41,1
Intel,nt61pmw,RE7 and the backwoods of WV. I live near by and the whole atmosphere was nailed. Really drove home the down south horror. Lol,pcmasterrace,2025-12-09 20:01:48,1
Intel,nt61q2u,Spencer Mansion.,pcmasterrace,2025-12-09 20:01:51,1
Intel,nt61rjz,Baker House,pcmasterrace,2025-12-09 20:02:03,1
Intel,nt61sa4,THE hallway in the RE1 Spencer Mansion.,pcmasterrace,2025-12-09 20:02:09,1
Intel,nt61sft,the guesthouse in r7,pcmasterrace,2025-12-09 20:02:10,1
Intel,nt61sip,Baker House goes crazy,pcmasterrace,2025-12-09 20:02:11,1
Intel,nt61sv1,RCPD was a nice experience.,pcmasterrace,2025-12-09 20:02:14,1
Intel,nt61t5j,Baker house!,pcmasterrace,2025-12-09 20:02:16,1
Intel,nt61tra,The RCPD easily fits the bill for me.  There's just something off (obviously on purpose) about the chaos in buildings that were abandoned quickly,pcmasterrace,2025-12-09 20:02:21,1
Intel,nt61u4s,Spencer mansion,pcmasterrace,2025-12-09 20:02:24,1
Intel,nt61uh8,RCPD for sure,pcmasterrace,2025-12-09 20:02:27,1
Intel,nt61vho,"Definitely RCPD, especially the sewer where Hunk escaped from!",pcmasterrace,2025-12-09 20:02:35,1
Intel,nt61vvi,Baker house for sure.,pcmasterrace,2025-12-09 20:02:39,1
Intel,nt61wik,I'm playing RE4 right now. Village is iconic!,pcmasterrace,2025-12-09 20:02:44,1
Intel,nt61wo7,I enjoyed Spencer Mansion!,pcmasterrace,2025-12-09 20:02:45,1
Intel,nt61ybv,RCPD is great.,pcmasterrace,2025-12-09 20:02:59,1
Intel,nt61z2b,Baker house,pcmasterrace,2025-12-09 20:03:05,1
Intel,nt61z2h,RE4 End mansion,pcmasterrace,2025-12-09 20:03:06,1
Intel,nt61z7z,RCPD all the way,pcmasterrace,2025-12-09 20:03:07,1
Intel,nt61zab,Baker House,pcmasterrace,2025-12-09 20:03:07,1
Intel,nt61zcw,Rcpd for sure. Red was the first one I played,pcmasterrace,2025-12-09 20:03:08,1
Intel,nt6204g,I never played RE. But I will with the steam card.,pcmasterrace,2025-12-09 20:03:14,1
Intel,nt62062,"The sewers in RE2, those giant worms always scared the shit out of me",pcmasterrace,2025-12-09 20:03:15,1
Intel,nt62069,Rockfort Prison,pcmasterrace,2025-12-09 20:03:15,1
Intel,nt6216o,Baker estate for sure,pcmasterrace,2025-12-09 20:03:23,1
Intel,nt621k9,I've only played a few Resident Evil games but the RCPD and Racoon City as a whole has to be the first location I think of whenever I hear Resident Evil mentioned.,pcmasterrace,2025-12-09 20:03:26,1
Intel,nt622su,The labs were creepy as heck.  Thanks for the chance :),pcmasterrace,2025-12-09 20:03:37,1
Intel,nt623g7,"100% Baker House, not even my wife liked watching me play in there.",pcmasterrace,2025-12-09 20:03:42,1
Intel,nt6241j,"Resident Evil 7 - The basement. All of Biohazard is spooky, but trudging around in that grungy basement makes my skin crawl every time.",pcmasterrace,2025-12-09 20:03:47,1
Intel,nt624ch,"Never played, too scare",pcmasterrace,2025-12-09 20:03:49,1
Intel,nt624hk,Spencer Mansion,pcmasterrace,2025-12-09 20:03:50,1
Intel,nt624yo,Spencer Mansion,pcmasterrace,2025-12-09 20:03:54,1
Intel,nt6257k,Gotta be the baker estate,pcmasterrace,2025-12-09 20:03:56,1
Intel,nt626a3,Baker house no doubt,pcmasterrace,2025-12-09 20:04:06,1
Intel,nt626uk,RCPD,pcmasterrace,2025-12-09 20:04:11,1
Intel,nt6280g,definitely Baker House,pcmasterrace,2025-12-09 20:04:20,1
Intel,nt62957,"RCPD, it's the place I picture whenever I think of Resident Evil",pcmasterrace,2025-12-09 20:04:29,1
Intel,nt629gw,Hallway of dogs.,pcmasterrace,2025-12-09 20:04:32,1
Intel,nt629jo,"Spencer mansion, the fixed camera really added to the 'what's around the corner' fear.",pcmasterrace,2025-12-09 20:04:33,1
Intel,nt629lw,The Spencer mansion as a 7 year old being shown a zombie at sleepover at a friend’s house. That shit left me scared as hell. I fell in love with Resident Evil that night,pcmasterrace,2025-12-09 20:04:33,1
Intel,nt629ti,rcpd,pcmasterrace,2025-12-09 20:04:35,1
Intel,nt62aeg,Spencer Mansion,pcmasterrace,2025-12-09 20:04:40,1
Intel,nt62agx,"I always really liked the labs under the RPD in RE2, very spooky and isolated down there",pcmasterrace,2025-12-09 20:04:40,1
Intel,nt62bb3,Spencer mansion for me,pcmasterrace,2025-12-09 20:04:48,1
Intel,nt62bk1,RCPD for sure.  The more normal/calm the scene the creepier it is.,pcmasterrace,2025-12-09 20:04:50,1
Intel,nt62css,"Would go with ""Spencer Mansion"" on that one.",pcmasterrace,2025-12-09 20:05:00,1
Intel,nt62dkx,All of it. I'm a wuss and it freaks me out.   Good luck everyone!,pcmasterrace,2025-12-09 20:05:06,1
Intel,nt62egh,Spencer mansion,pcmasterrace,2025-12-09 20:05:14,1
Intel,nt62g8f,Old House from RE7 for sure,pcmasterrace,2025-12-09 20:05:29,1
Intel,nt62gq8,"RE4, when you get to the starting village and the guy says: “Matalooooooo!”",pcmasterrace,2025-12-09 20:05:33,1
Intel,nt62h4e,"Biggest impression has to be RPD. The history of it being a museum before, the recently turned zombies, the music. Nothing can beat it!",pcmasterrace,2025-12-09 20:05:37,1
Intel,nt62h7s,the RE7 initial woods part i played it on PSVR then switched to pc because it was too scary,pcmasterrace,2025-12-09 20:05:38,1
Intel,nt62how,Baker house,pcmasterrace,2025-12-09 20:05:42,1
Intel,nt62ibw,Baker house,pcmasterrace,2025-12-09 20:05:47,1
Intel,nt62ipl,Ooof .... If I had to pick I'd say Baker Estate left me in unease,pcmasterrace,2025-12-09 20:05:50,1
Intel,nt62jhy,Spencer Mansion,pcmasterrace,2025-12-09 20:05:57,1
Intel,nt62jrw,"RCPD is obviously very iconic, but I'd also say House Beneviento left a pretty big impression.",pcmasterrace,2025-12-09 20:05:59,1
Intel,nt62jvu,Honestly has to be RCPD. Was young when the game came out but wanted to either play it or watch my brother play it but since I was too scared it would mostly be him playing and me watching. The past was really great since i got to enjoy many games with my brother even if all I did was watch.,pcmasterrace,2025-12-09 20:06:00,1
Intel,nt62jw6,RCPD,pcmasterrace,2025-12-09 20:06:00,1
Intel,nt62l0j,RCPD for sure!,pcmasterrace,2025-12-09 20:06:09,1
Intel,nt62l36,Banker House,pcmasterrace,2025-12-09 20:06:10,1
Intel,nt62l4d,Baker house for sure,pcmasterrace,2025-12-09 20:06:10,1
Intel,nt62lig,RCPD for sure when I was growing up,pcmasterrace,2025-12-09 20:06:14,1
Intel,nt62ljp,The labs!,pcmasterrace,2025-12-09 20:06:14,1
Intel,nt62lxi,Abandoned unbrella labs 100%,pcmasterrace,2025-12-09 20:06:17,1
Intel,nt62m2h,RCPD in resident evil 2 remake with inmortal mr X around.,pcmasterrace,2025-12-09 20:06:18,1
Intel,nt62m68,Rcpd,pcmasterrace,2025-12-09 20:06:19,1
Intel,nt62m6h,None of them - I haven't played it. But I might if I get a card.,pcmasterrace,2025-12-09 20:06:19,1
Intel,nt62mnb,Baker house,pcmasterrace,2025-12-09 20:06:23,1
Intel,nt62mts,Now I wish I could have played Resident Evil 😭😭,pcmasterrace,2025-12-09 20:06:25,1
Intel,nt62nle,Rcpd lol,pcmasterrace,2025-12-09 20:06:31,1
Intel,nt62nvt,RCPD is/was iconic !,pcmasterrace,2025-12-09 20:06:34,1
Intel,nt62o13,The only RE game I completed so far was RE revelations on the original 3ds xl. Love hated it but was such a great game. Currently playing RE 2 and having a blast with the puzzles and jumps cares for sure!,pcmasterrace,2025-12-09 20:06:35,1
Intel,nt62oqw,"The baker house was incredibly scary to me. A lot of tension from being chased, plus the random molded everywhere really got me tense",pcmasterrace,2025-12-09 20:06:41,1
Intel,nt62p8b,RPD,pcmasterrace,2025-12-09 20:06:45,1
Intel,nt62pf7,Racoon City PD for sure,pcmasterrace,2025-12-09 20:06:46,1
Intel,nt62pn8,RCPD,pcmasterrace,2025-12-09 20:06:48,1
Intel,nt62qrg,Rcpd,pcmasterrace,2025-12-09 20:06:57,1
Intel,nt62qrv,RCPD,pcmasterrace,2025-12-09 20:06:57,1
Intel,nt62r4k,Rcpd,pcmasterrace,2025-12-09 20:07:00,1
Intel,nt62r9d,RCPD. Playing that on N64 creeped the hell out of me.,pcmasterrace,2025-12-09 20:07:02,1
Intel,nt62rih,Baker house,pcmasterrace,2025-12-09 20:07:04,1
Intel,nt62rli,The labs are the most interesting,pcmasterrace,2025-12-09 20:07:04,1
Intel,nt62sty,Awesome thanks,pcmasterrace,2025-12-09 20:07:15,1
Intel,nt62syo,Resident evil 4 like the farm area because I played it in vr and it was freaking scary haven’t gotten past that,pcmasterrace,2025-12-09 20:07:16,1
Intel,nt62th2,Spencer Mansion og,pcmasterrace,2025-12-09 20:07:20,1
Intel,nt62tpz,I would have to say the Beneviento house. Haven't been traumatized by a crying baby like that since the old Silent Hill demo from the PSX days.,pcmasterrace,2025-12-09 20:07:22,1
Intel,nt62twj,Baker Estate. Really messed up place.,pcmasterrace,2025-12-09 20:07:24,1
Intel,nt62u1d,Spencer Mansion 100000%,pcmasterrace,2025-12-09 20:07:25,1
Intel,nt62ucr,Heisenberg's factory,pcmasterrace,2025-12-09 20:07:28,1
Intel,nt62ufi,RCPD for sure,pcmasterrace,2025-12-09 20:07:28,1
Intel,nt62vdm,The cemetery behind Spencer’s mansion was always so spooky to me as a kid I always thought that would be the end game 😭😂 when I did finally get past that puzzle I was shook lol,pcmasterrace,2025-12-09 20:07:36,1
Intel,nt62vmf,Spencer Mansion,pcmasterrace,2025-12-09 20:07:38,1
Intel,nt62vp8,Cool stuff to win,pcmasterrace,2025-12-09 20:07:39,1
Intel,nt62wfu,Spencer Mansion,pcmasterrace,2025-12-09 20:07:45,1
Intel,nt62wkk,"Spencer Mansion, where everything started, where eerie  music creeped me out in the late 90’s",pcmasterrace,2025-12-09 20:07:46,1
Intel,nt62wsh,Spencer mansion,pcmasterrace,2025-12-09 20:07:48,1
Intel,nt62x30,The baker house got something really cool going on,pcmasterrace,2025-12-09 20:07:50,1
Intel,nt62xtf,rcpd back in the day!,pcmasterrace,2025-12-09 20:07:57,1
Intel,nt62y0m,The Bakers House,pcmasterrace,2025-12-09 20:07:59,1
Intel,nt62yr8,The Baker house is so chilling. I can't go back.,pcmasterrace,2025-12-09 20:08:04,1
Intel,nt62ze6,"Definitely Spencer’s mansion, made me go crazy lmao",pcmasterrace,2025-12-09 20:08:10,1
Intel,nt62zon,Baker house for sure,pcmasterrace,2025-12-09 20:08:12,1
Intel,nt62zww,Probably the original police department,pcmasterrace,2025-12-09 20:08:14,1
Intel,nt6309l,RCPD all the way,pcmasterrace,2025-12-09 20:08:17,1
Intel,nt630hy,The Spencer Mansion,pcmasterrace,2025-12-09 20:08:19,1
Intel,nt630t8,Spencer mansion,pcmasterrace,2025-12-09 20:08:22,1
Intel,nt631ck,Baker Estate,pcmasterrace,2025-12-09 20:08:26,1
Intel,nt631id,Baker house,pcmasterrace,2025-12-09 20:08:28,1
Intel,nt631qr,Spencer mansion for sure,pcmasterrace,2025-12-09 20:08:30,1
Intel,nt6328s,RCPD is iconic,pcmasterrace,2025-12-09 20:08:34,1
Intel,nt6329g,Oh dude the baker house man. Spooky Louisiana farm house. I played through that game so many times. Good luck every one 🍀,pcmasterrace,2025-12-09 20:08:34,1
Intel,nt632wy,"RCPD, a perfect picture how chaos can take over order",pcmasterrace,2025-12-09 20:08:39,1
Intel,nt633yu,Raccoon City's Police Station!,pcmasterrace,2025-12-09 20:08:48,1
Intel,nt634he,House Beneviento,pcmasterrace,2025-12-09 20:08:53,1
Intel,nt634ht,RCPD,pcmasterrace,2025-12-09 20:08:53,1
Intel,nt634o8,"Spencer Mansion for sure. It’s what got me started into the series and I’ve continued to play them all ever since. It’s my favorite setting of the series, especially with all the puzzles, the atmosphere, and the music!",pcmasterrace,2025-12-09 20:08:54,1
Intel,nt635rl,"The village in RE4.  Something about it being crazed (and parasitized) PEOPLE, instead of monsters, was deeply unsettling the first time through!",pcmasterrace,2025-12-09 20:09:03,1
Intel,nt6360q,the baker estate was by far the creepiest location in resident evil,pcmasterrace,2025-12-09 20:09:06,1
Intel,nt6364s,RCPD! Way too iconic to not answer with it lol,pcmasterrace,2025-12-09 20:09:06,1
Intel,nt636cx,***le Manoir Spencer***,pcmasterrace,2025-12-09 20:09:08,1
Intel,nt636kc,RCPD,pcmasterrace,2025-12-09 20:09:10,1
Intel,nt636z0,RCPD,pcmasterrace,2025-12-09 20:09:13,1
Intel,nt6374u,"Spencer Mansion, can never forget playing RE for the very first time, iconic game",pcmasterrace,2025-12-09 20:09:15,1
Intel,nt637gh,the first house encounter on Resident Evil 4  where we meet the first Ganados,pcmasterrace,2025-12-09 20:09:17,1
Intel,nt637gv,Baker house,pcmasterrace,2025-12-09 20:09:18,1
Intel,nt638b6,Baker house 100%,pcmasterrace,2025-12-09 20:09:25,1
Intel,nt638cc,Has to be spencer mansion,pcmasterrace,2025-12-09 20:09:25,1
Intel,nt638sq,Spencer mansion,pcmasterrace,2025-12-09 20:09:29,1
Intel,nt639jk,Spencer Mansion,pcmasterrace,2025-12-09 20:09:36,1
Intel,nt639wb,I hate the jump scares in the Spencer Mansion and those tight corners eeeeek,pcmasterrace,2025-12-09 20:09:38,1
Intel,nt63c0x,Spencer mansion left the biggest impression on me,pcmasterrace,2025-12-09 20:09:57,1
Intel,nt63cui,Spencer Mansion definitely,pcmasterrace,2025-12-09 20:10:04,1
Intel,nt63ddk,"Spencer Mansion ... it leaves the biggest impression. It’s claustrophobic, mysterious, full of hidden rooms and creepy silence. It’s also the iconic birthplace of the whole Resident Evil atmosphere!",pcmasterrace,2025-12-09 20:10:08,1
Intel,nt63djp,The first Umbrella lab for sure!,pcmasterrace,2025-12-09 20:10:09,1
Intel,nt63dk4,RCPD is so iconic,pcmasterrace,2025-12-09 20:10:09,1
Intel,nt63dlk,The lab haunts me,pcmasterrace,2025-12-09 20:10:10,1
Intel,nt63dng,rockfort island,pcmasterrace,2025-12-09 20:10:10,1
Intel,nt63eb1,"Guest house 7, it's terrifying",pcmasterrace,2025-12-09 20:10:16,1
Intel,nt63g7g,Going through RCPD as a kid in my room with the lights off is a helluva memory.,pcmasterrace,2025-12-09 20:10:32,1
Intel,nt63gjg,"RCPD as it was a background in loby in colab with PUBG mobile way back, tying two games I like together",pcmasterrace,2025-12-09 20:10:35,1
Intel,nt63gs6,Spencer Mansion,pcmasterrace,2025-12-09 20:10:37,1
Intel,nt63h5a,RCPD,pcmasterrace,2025-12-09 20:10:40,1
Intel,nt63h5k,Definitely baker house! Thanks,pcmasterrace,2025-12-09 20:10:40,1
Intel,nt63hak,Definitely baker house! Epic,pcmasterrace,2025-12-09 20:10:41,1
Intel,nt63ho4,All in Raccoon city not even ambient but sound track too it make me scary when frist played,pcmasterrace,2025-12-09 20:10:44,1
Intel,nt63i46,Baker House,pcmasterrace,2025-12-09 20:10:48,1
Intel,nt63iew,Baker house,pcmasterrace,2025-12-09 20:10:51,1
Intel,nt63ih9,As an old head RCPD to this day still gives me the yikes,pcmasterrace,2025-12-09 20:10:51,1
Intel,nt63ikw,Didnt play it,pcmasterrace,2025-12-09 20:10:52,1
Intel,nt63iv1,Spencer Mansion,pcmasterrace,2025-12-09 20:10:54,1
Intel,nt63jhs,The Baker house for sure,pcmasterrace,2025-12-09 20:10:59,1
Intel,nt63jko,The mansion is the OG RE set piece. I think the first movies did a great job showing it to audiences but I feel like the reveal of the underground lab could have been better. Maybe reveal it at the end and spend more time with the horror aspects of the mansion,pcmasterrace,2025-12-09 20:11:00,1
Intel,nt63js5,It has to be Spencer Mansion!,pcmasterrace,2025-12-09 20:11:02,1
Intel,nt63jvt,Spencer Mansion 💯,pcmasterrace,2025-12-09 20:11:03,1
Intel,nt63k8k,Gimme something good for Christmas,pcmasterrace,2025-12-09 20:11:06,1
Intel,nt63k9d,Definitely rcpd. It's so well designed with all the corridors and all the scares,pcmasterrace,2025-12-09 20:11:06,1
Intel,nt63kdr,RCPD,pcmasterrace,2025-12-09 20:11:07,1
Intel,nt63kkg,* ***Which of Resident Evil’s spooky locations left the biggest impression on you? Spencer Mansion? RCPD? One of many abandoned Umbrella labs?***  RE2 remake's Racoon City PD,pcmasterrace,2025-12-09 20:11:08,1
Intel,nt63knt,the house in 7,pcmasterrace,2025-12-09 20:11:09,1
Intel,nt63kw0,100% Spencer Estate,pcmasterrace,2025-12-09 20:11:11,1
Intel,nt63l50,"Definitely Spencer Mansion, first time I played it as a kid, less than 10 years old, had me screaming due to nightmares. I was disciplined many times haha because I couldn't control my screaming, but the streets of Racoon City also left an impression.",pcmasterrace,2025-12-09 20:11:13,1
Intel,nt63l61,"Havent played an RE game yet, have a few in the backlog",pcmasterrace,2025-12-09 20:11:13,1
Intel,nt63l6h,"Spencer mansion for me for sure, back in the OG RE days.",pcmasterrace,2025-12-09 20:11:13,1
Intel,nt63m3s,would be either the village from RE 4 or the baker's house from RE7 it was RE 4 which pulled me into the series,pcmasterrace,2025-12-09 20:11:21,1
Intel,nt63mcg,Im gonna have the go with RCPD.,pcmasterrace,2025-12-09 20:11:23,1
Intel,nt63mfc,Baker house,pcmasterrace,2025-12-09 20:11:23,1
Intel,nt63mgc,Baker house,pcmasterrace,2025-12-09 20:11:24,1
Intel,nt63nzy,RCPD,pcmasterrace,2025-12-09 20:11:36,1
Intel,nt63o63,Spencer mansion no doubt! Goodluck everyone,pcmasterrace,2025-12-09 20:11:38,1
Intel,nt63pcm,The Mansion from RE1,pcmasterrace,2025-12-09 20:11:48,1
Intel,nt63q0x,"For me, it's the revival of the RE franchise - RE 7's: The Baker house.   You are slow, weak, and Jack is menacingly chasing you around the house. It was terrifying, and hooked me back into the franchise",pcmasterrace,2025-12-09 20:11:53,1
Intel,nt63q2t,Baker house 😱,pcmasterrace,2025-12-09 20:11:53,1
Intel,nt63q2z,Sewers in RE4 with Verdugo.,pcmasterrace,2025-12-09 20:11:53,1
Intel,nt63q51,Spencer mansion from RE1 because of its claustrophobic corridors,pcmasterrace,2025-12-09 20:11:54,1
Intel,nt63qh4,Spencer mansion definitely,pcmasterrace,2025-12-09 20:11:57,1
Intel,nt63qii,SPENSER MANSION,pcmasterrace,2025-12-09 20:11:57,1
Intel,nt63qk5,RCPD,pcmasterrace,2025-12-09 20:11:57,1
Intel,nt63qwm,The lasers 😱,pcmasterrace,2025-12-09 20:12:00,1
Intel,nt63r72,"Nothing tops the iconic mansion, but if I had to choose another location it would be the ship from Revelations.",pcmasterrace,2025-12-09 20:12:02,1
Intel,nt63rmo,Bakers compound scares me while traversing it. Idc if nothing happens once you clear rooms the ambiance has me thinking I’m schizophrenic,pcmasterrace,2025-12-09 20:12:06,1
Intel,nt63ro4,"The Spencer Mansion, easily. It was my first real introduction to Resident Evil’s atmosphere—tight hallways, creepy puzzles, and that constant feeling something could be waiting around the corner. It set the tone for the entire series.",pcmasterrace,2025-12-09 20:12:06,1
Intel,nt63s34,RCPD,pcmasterrace,2025-12-09 20:12:10,1
Intel,nt63sqy,Baker house,pcmasterrace,2025-12-09 20:12:16,1
Intel,nt63tp6,Uroboros Labaratory,pcmasterrace,2025-12-09 20:12:23,1
Intel,nt63tqc,House Beneviento. Not even a competition.,pcmasterrace,2025-12-09 20:12:23,1
Intel,nt63ugy,"I never actually played any resident evil game, but figured I would comment anyways! Thanks for the giveaway!",pcmasterrace,2025-12-09 20:12:30,1
Intel,nt63vez,Baker house for sure!,pcmasterrace,2025-12-09 20:12:38,1
Intel,nt63vno,"I’ve still never played it before, but I want to someday",pcmasterrace,2025-12-09 20:12:40,1
Intel,nt63w83,"Baker House, the family, the environment as a whole",pcmasterrace,2025-12-09 20:12:45,1
Intel,nt63x21,Spencer Mansion,pcmasterrace,2025-12-09 20:12:52,1
Intel,nt63x6m,RCPD,pcmasterrace,2025-12-09 20:12:53,1
Intel,nt63x7c,Oh man in RE4 when you get to the tower lab with the creature that you have to kill with the thermal scope and sniper rifle. That location paired with the task scared me silly as a teenager. It took so many tries to finally get to the boss fight at the top.,pcmasterrace,2025-12-09 20:12:54,1
Intel,nt63xyr,Spencer Mansion,pcmasterrace,2025-12-09 20:13:00,1
Intel,nt63y40,Baker House,pcmasterrace,2025-12-09 20:13:01,1
Intel,nt63yxx,The Spencer Mansion.  Constant creepy dread.,pcmasterrace,2025-12-09 20:13:08,1
Intel,nt63z9k,RE4 on the wii the first jump scare encounter in the shack at the start of the game. Noped out of the series for over a decade,pcmasterrace,2025-12-09 20:13:11,1
Intel,nt63zjz,OG mansion for sure,pcmasterrace,2025-12-09 20:13:13,1
Intel,nt63zld,RCPD,pcmasterrace,2025-12-09 20:13:13,1
Intel,nt63zoc,Baker house,pcmasterrace,2025-12-09 20:13:14,1
Intel,nt640fa,"Great giveaway, good luck everyone 🙏",pcmasterrace,2025-12-09 20:13:20,1
Intel,nt640qz,Gas station mission :-(,pcmasterrace,2025-12-09 20:13:23,1
Intel,nt640tz,Spencer Mansion,pcmasterrace,2025-12-09 20:13:23,1
Intel,nt640x2,"Spencer Mansion, all mansion or ""big"" houses always scares me.",pcmasterrace,2025-12-09 20:13:24,1
Intel,nt6411n,I have never played Resident Evil but I would use the gift card to buy it and play it,pcmasterrace,2025-12-09 20:13:25,1
Intel,nt6415e,Baker house for sure,pcmasterrace,2025-12-09 20:13:26,1
Intel,nt641lt,RCPD,pcmasterrace,2025-12-09 20:13:30,1
Intel,nt641p1,RCPD,pcmasterrace,2025-12-09 20:13:31,1
Intel,nt641x6,Baker house,pcmasterrace,2025-12-09 20:13:32,1
Intel,nt64244,i must say the village in re4,pcmasterrace,2025-12-09 20:13:34,1
Intel,nt6426u,"lady dimitrescu's castle.....man, it was def something else, top to bottom, and almost every section of the castle looks different and feels different",pcmasterrace,2025-12-09 20:13:35,1
Intel,nt642ec,The halls in the mansion. Those damn dogs,pcmasterrace,2025-12-09 20:13:36,1
Intel,nt642w2,"I only played a few minutes of a RE game on the ps3, I couldn't stand the janky gun controls and quit about 15 minutes in, sorry can't name a location.",pcmasterrace,2025-12-09 20:13:40,1
Intel,nt643kb,"It may seems strange but it is the RCPD when i played the first time, i was terrified of that place",pcmasterrace,2025-12-09 20:13:46,1
Intel,nt643uv,RCPD,pcmasterrace,2025-12-09 20:13:48,1
Intel,nt6449q,The Queen Zenobia,pcmasterrace,2025-12-09 20:13:52,1
Intel,nt644ol,Resident evil...I'm in,pcmasterrace,2025-12-09 20:13:55,1
Intel,nt644ss,RCPD,pcmasterrace,2025-12-09 20:13:56,1
Intel,nt6451u,Old labs just freak me out man. Too clinical and yet so horrific.  Thanks for the giveaway guys!,pcmasterrace,2025-12-09 20:13:58,1
Intel,nt645oo,House Beneviento freaked me out the most. Could not get out of there fast enough.,pcmasterrace,2025-12-09 20:14:03,1
Intel,nt646cm,Spencer mansion,pcmasterrace,2025-12-09 20:14:09,1
Intel,nt646fy,Castle Dimitrescu.,pcmasterrace,2025-12-09 20:14:10,1
Intel,nt646jx,RCPD 100%,pcmasterrace,2025-12-09 20:14:11,1
Intel,nt646qv,"RCPD is one of the most classic locations in a video game. You spend a large amount of RE2 there, exploring, unlocking, and fighting. I always loved how things you did with the first character effects the other, and then you find out how certain events even happened, such as the helicopter crash, etc. Brilliantly done, my childhood gaming would not be the same without it.",pcmasterrace,2025-12-09 20:14:12,1
Intel,nt646wr,RCPD,pcmasterrace,2025-12-09 20:14:14,1
Intel,nt646yc,Spencer Manson,pcmasterrace,2025-12-09 20:14:14,1
Intel,nt647d2,RCPD ( I've never played but would love a 5090),pcmasterrace,2025-12-09 20:14:18,1
Intel,nt647de,RCPD!,pcmasterrace,2025-12-09 20:14:18,1
Intel,nt647mu,The Village!,pcmasterrace,2025-12-09 20:14:20,1
Intel,nt6482v,Baker house,pcmasterrace,2025-12-09 20:14:23,1
Intel,nt648fu,"The Raccoon City Police Department in RE2 remake.  It's this uncanny mixture of being a tourist attraction with all the puzzles you can find there, and actual police department that was doing its thing and even ready to welcome Leon before the T-virus leaked and caused the whole city to go awry.  It's this mixture of events coming together to create a very uneasy environment that in its entirety never feels anything normal, just plain eerie.",pcmasterrace,2025-12-09 20:14:26,1
Intel,nt648it,"Spencer Mansion for me, that place just freaked me out the most.",pcmasterrace,2025-12-09 20:14:27,1
Intel,nt648vs,Baker house,pcmasterrace,2025-12-09 20:14:30,1
Intel,nt648yw,"This is a hard question. Resident Evil is by far my favourite Capcom title. I can call out so many ""locations"" that left a impression. So, if I must, everyone who has played the original game will be familiar with the dog scene. Running up the hallway in Spencer's Mansion when the dog crashes through the window behind you. Then when you come running the down the hallway around the corner another crashes through in front of you. As a kid, that scene, or sequence of events, gave me a jump scare each and every time! That is a impression that will last a lifetime. Truly unforgettable.",pcmasterrace,2025-12-09 20:14:31,1
Intel,nt6495z,Rcpd for sure!,pcmasterrace,2025-12-09 20:14:32,1
Intel,nt6497m,The streets after leaving the police station in RE2.,pcmasterrace,2025-12-09 20:14:32,1
Intel,nt649d4,RCPD,pcmasterrace,2025-12-09 20:14:34,1
Intel,nt649sg,RCPD for sure,pcmasterrace,2025-12-09 20:14:37,1
Intel,nt64a2d,Baker House,pcmasterrace,2025-12-09 20:14:39,1
Intel,nt64aaw,That corridor in the mansion. You know the one,pcmasterrace,2025-12-09 20:14:41,1
Intel,nt64b7f,"Heck, the first opening scene of Resident Evil 1 was enough to spook a young me....",pcmasterrace,2025-12-09 20:14:49,1
Intel,nt64bft,Mansion,pcmasterrace,2025-12-09 20:14:51,1
Intel,nt64boq,Oh house beneviento for sure,pcmasterrace,2025-12-09 20:14:53,1
Intel,nt64btr,Bakers estate definitely left an impression,pcmasterrace,2025-12-09 20:14:54,1
Intel,nt64cgl,RCPD,pcmasterrace,2025-12-09 20:14:59,1
Intel,nt64cuj,The baker house for sure,pcmasterrace,2025-12-09 20:15:02,1
Intel,nt64d74,RE7. When you first approach the house. That gave me the spooks!,pcmasterrace,2025-12-09 20:15:05,1
Intel,nt64dv1,always baker's house,pcmasterrace,2025-12-09 20:15:11,1
Intel,nt64equ,Definitely the opening sequence of RE7 Biohazard,pcmasterrace,2025-12-09 20:15:18,1
Intel,nt64ezr,Ive never played resident evil :/,pcmasterrace,2025-12-09 20:15:20,1
Intel,nt64fhj,Castle Dimitrescu,pcmasterrace,2025-12-09 20:15:24,1
Intel,nt64fsw,⁠Which of Resident Evil’s spooky locations left the biggest impression on you? Spencer Mansion? RCPD? One of many abandoned Umbrella labs?  The honest answer is…. I’ve never played any resident evil games.,pcmasterrace,2025-12-09 20:15:26,1
Intel,nt64fwf,The menu,pcmasterrace,2025-12-09 20:15:27,1
Intel,nt64gb5,***RCPD***,pcmasterrace,2025-12-09 20:15:30,1
Intel,nt64gzh,Spencer Mansion for sure,pcmasterrace,2025-12-09 20:15:36,1
Intel,nt64h2m,The Baker house,pcmasterrace,2025-12-09 20:15:37,1
Intel,nt64h8y,Baker house for sure!,pcmasterrace,2025-12-09 20:15:38,1
Intel,nt64hb1,Spencer mansion gets the cake!,pcmasterrace,2025-12-09 20:15:39,1
Intel,nt64hh2,Spencer mansion is just that memorable for me.,pcmasterrace,2025-12-09 20:15:40,1
Intel,nt64hjm,Baker house 100%! It brought the serious back!,pcmasterrace,2025-12-09 20:15:41,1
Intel,nt64iho,Baker house had me spooked the entire time.,pcmasterrace,2025-12-09 20:15:49,1
Intel,nt64inr,The baker house,pcmasterrace,2025-12-09 20:15:50,1
Intel,nt64iqg,Spencer mansion,pcmasterrace,2025-12-09 20:15:51,1
Intel,nt64j0c,"The Dollhouse in RE Village  That place is creepy af, never going back",pcmasterrace,2025-12-09 20:15:53,1
Intel,nt64jya,Doll house in the village,pcmasterrace,2025-12-09 20:16:01,1
Intel,nt64kus,The entire Village from 8,pcmasterrace,2025-12-09 20:16:09,1
Intel,nt64lk5,Spencer Mansion!,pcmasterrace,2025-12-09 20:16:14,1
Intel,nt64ls6,Fist time I booted into the rcpd station - the suffocation was fantastic for a first time player,pcmasterrace,2025-12-09 20:16:16,1
Intel,nt64m79,The first act of Resident Evil 2 classic,pcmasterrace,2025-12-09 20:16:20,1
Intel,nt64ma6,Hallo! I love PC!!! Woooooo!!! The RCPD building.,pcmasterrace,2025-12-09 20:16:20,1
Intel,nt64mlh,RCPD x2,pcmasterrace,2025-12-09 20:16:23,1
Intel,nt64mmj,House Beneviento. Both baddies are so creepy!,pcmasterrace,2025-12-09 20:16:23,1
Intel,nt64ndo,Baker house,pcmasterrace,2025-12-09 20:16:29,1
Intel,nt64ne4,Spencer Mansion,pcmasterrace,2025-12-09 20:16:29,1
Intel,nt64ni0,Baker House for sure,pcmasterrace,2025-12-09 20:16:30,1
Intel,nt64nq5,The Basement,pcmasterrace,2025-12-09 20:16:32,1
Intel,nt64nws,Baker House for sure. Every bone in my body wanted to get out of there and take the hottest shower imaginable to wash the mold off.,pcmasterrace,2025-12-09 20:16:33,1
Intel,nt64o6n,Thank you!,pcmasterrace,2025-12-09 20:16:36,1
Intel,nt64oah,RCPD in RE2 for sure,pcmasterrace,2025-12-09 20:16:37,1
Intel,nt64oc7,The mansion,pcmasterrace,2025-12-09 20:16:37,1
Intel,nt64ofp,The mansion Def.,pcmasterrace,2025-12-09 20:16:38,1
Intel,nt64omx,Umbrella labs,pcmasterrace,2025-12-09 20:16:39,1
Intel,nt64oye,Easily house Beneviento,pcmasterrace,2025-12-09 20:16:42,1
Intel,nt64pgh,Baker house! Was stressed all the way through the game,pcmasterrace,2025-12-09 20:16:46,1
Intel,nt64pk3,Baker house was extremely eerie imo,pcmasterrace,2025-12-09 20:16:47,1
Intel,nt64pkl,Rcpd,pcmasterrace,2025-12-09 20:16:47,1
Intel,nt64psl,"My first experience with RE franchise was with RE2 so RCPD is more iconic and spooky to me than the mansion, so I'll go with that. Thanks for the giveaway!",pcmasterrace,2025-12-09 20:16:49,1
Intel,nt64puj,Antarctic Base on RE CODE: Veronica. The impression that place gives you is unparalleled!,pcmasterrace,2025-12-09 20:16:49,1
Intel,nt64q2s,RCPD. Even opening a door animation was scary as hell to me.,pcmasterrace,2025-12-09 20:16:51,1
Intel,nt64q9j,Rcpd once you start being chased around in there for sure,pcmasterrace,2025-12-09 20:16:52,1
Intel,nt64qfh,Spencer Mansion Spend the lot of time in that!!,pcmasterrace,2025-12-09 20:16:54,1
Intel,nt64r2f,RCPD,pcmasterrace,2025-12-09 20:16:59,1
Intel,nt64r2s,Spencer Mansion. In the original when you go past the windows in the hallway the first time and nothing happens and then the way back and the dogs break through the windows young me nearly pissed his pants.,pcmasterrace,2025-12-09 20:16:59,1
Intel,nt64s1i,"Definitely Spencer Mansion, only one bathroom?",pcmasterrace,2025-12-09 20:17:07,1
Intel,nt64s1r,The caves in the re7 Chris DLC!,pcmasterrace,2025-12-09 20:17:07,1
Intel,nt64syb,RCPD.,pcmasterrace,2025-12-09 20:17:15,1
Intel,nt64t2o,Spencer Mansion,pcmasterrace,2025-12-09 20:17:16,1
Intel,nt64ug9,Gonna go with Baker House,pcmasterrace,2025-12-09 20:17:27,1
Intel,nt64ukx,Honestly the first zombie that turns around mid feast in the first game on ps1 scared me so much as a child. Nothing since has hit that level of immediate fear. I literally froze in place.,pcmasterrace,2025-12-09 20:17:28,1
Intel,nt64w6i,Laser hallway,pcmasterrace,2025-12-09 20:17:41,1
Intel,nt64w9z,"Ima be real, first thing I recall is ""EEEETHAAAAAAANNN"", but THAT area with HIM... (this is from RE... right..?)",pcmasterrace,2025-12-09 20:17:42,1
Intel,nt64wuy,The umbrella labs. The medical setting makes it even more terrifying.,pcmasterrace,2025-12-09 20:17:47,1
Intel,nt64wwo,The RCPD,pcmasterrace,2025-12-09 20:17:47,1
Intel,nt64wyd,RCPD! :D,pcmasterrace,2025-12-09 20:17:48,1
Intel,nt64x53,I will say House Beneviento,pcmasterrace,2025-12-09 20:17:49,1
Intel,nt64x7o,"Baker House, I love the backwoods creepy vibes, and the graphics at the time were just phenomenal",pcmasterrace,2025-12-09 20:17:51,1
Intel,nt64xmz,RCPD basement,pcmasterrace,2025-12-09 20:17:54,1
Intel,nt62tk7,"Bro, keep dreaming, you need to sell a newborn child for 16gb DDR5, you'd be lucky to get  half a gb for that GPU",pcmasterrace,2025-12-09 20:07:21,97
Intel,nt71q5a,Im just saying baker house because everyone else is lol,pcmasterrace,2025-12-09 23:02:17,14
Intel,nt69i3w,the AI copy and paste is killing me,pcmasterrace,2025-12-09 20:40:27,5
Intel,nt6650e,I got 32gb(2x16gb) for jsut under 300€ yesterday,pcmasterrace,2025-12-09 20:23:55,19
Intel,nt79t3d,"I've done extensive research about the objectively best area of the Resident Evil games, and that is Bakers House.      ^^^^^^Because ^^^^^^the ^^^^^^top ^^^^^^comment ^^^^^^says ^^^^^^so.",pcmasterrace,2025-12-09 23:48:22,6
Intel,nt69jrd,I got 64GB (2x32Gb 3200Mhz DDR4) kit for 110€ before the prices quadrupled,pcmasterrace,2025-12-09 20:40:41,3
Intel,ns3r9yu,yes the b580 unboxing experience was very impressive. i love the card,pcmasterrace,2025-12-03 18:00:20,1
Intel,nt2ufux,"Honestly, just horrible value wise. $1899 gets you a 5080 + 9800X3D prebuilt, $1500 gets you a 9800X3D + 5070 system.  $1700 buils should have no place for a 5060ti or AM4",pcmasterrace,2025-12-09 07:46:27,15
Intel,nt2v2io,"Why are you spec'ing an SFX power supply when you have an ATX case? You do get an adapter plate in the box, and in some cases it can make sense, but I wouldn't think this is one of them and you can probably get a better deal on a normal sized PSU in the same wattage.",pcmasterrace,2025-12-09 07:52:39,7
Intel,nt36rd5,"The main problem is that you are spending 1750$ on an AM4 computer on 2026  Investing that kind of money on abandoned tech might end up biting you on the long run when/if you want to upgrade since what you are building has no upgrade path at all.   [PCPartPicker Part List](https://pcpartpicker.com/list/cgyMqH)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core Processor](https://pcpartpicker.com/product/3hyH99/amd-ryzen-7-7800x3d-42-ghz-8-core-processor-100-100000910wof) | $365.99 @ Amazon  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | $35.90 @ Amazon  **Motherboard** | [\*Gigabyte B850 GAMING X WIFI6E ATX AM5 Motherboard](https://pcpartpicker.com/product/9xYfrH/gigabyte-b850-gaming-x-wifi6e-atx-am5-motherboard-b850-gaming-x-wifi6e) | $159.99 @ Amazon  **Memory** | [\*Klevv CRAS V RGB 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/XsMMnQ/klevv-cras-v-rgb-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-kd5agua80-60a300g) | $296.99 @ Amazon  **Storage** | [Western Digital Blue SN580 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/266NnQ/western-digital-blue-sn580-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-wds200t3b0e) | $174.99 @ Amazon  **Video Card** | [PowerColor Hellhound OC Radeon RX 9070 XT 16 GB Video Card](https://pcpartpicker.com/product/rGPv6h/powercolor-hellhound-oc-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-16g-loc) | $629.99 @ Amazon  **Case** | [Montech AIR 903 BASE ATX Mid Tower Case](https://pcpartpicker.com/product/kKcgXL/montech-air-903-base-atx-mid-tower-case-air-903-base-b) | $59.90 @ Amazon  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $82.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1806.65**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-09 04:45 EST-0500 |  DISCLAIMER  Im not in the US and pcpp insists to link to my local amazon if i clcik on items so i cant reliably check for availability and current prices, im going just on pcpp's word.  This would be my first stab at it sticking to amazon in the US since i dont know how confident you are with other merchants, but it can be made cheaper (for example buying the cpu and mobo from [msi for this combo](https://us-store.msi.com/Motherboards/AMD-Platform-Motherboard/B850-GAMING-PLUS-WIFI) that outright saves like 30$)  Anyhow  Best dollar to performance cpu  Fantastic air cooler that outperforms its price, its the next iteration of the one you picked and is usually considered better and similarly priced  A cheap but decent b850 board  Cheapest ram that pcpp can find in Amazon, that might need double checking since ram is a shit show  A well reviewed 2tb nvme, can save about 20$ going to a cheaper one or 50$ going to a 1tb one if you dont need the 2tb and add another drive in the future  A 9070xt that outperforms the 5060TI you picked by a decent margin  My favorite beginner's building case  A very well reviewed 850w gold fully modular psu   Im well aware this is above your budget but there is a bit of fat we could trim (again starting from different vendors).  This system will play anything at 1440p@120+fps for years to come and do some entry level 4k with clutches  Hopefully at least this gives you some inspiration, let me know if i can answer any question.",pcmasterrace,2025-12-09 09:52:14,7
Intel,nt2ts71,"Here is the PCPartPicker list for the link you provided.  [Here's how to do it on your own.](https://i.imgur.com/upztBRs.png)  ----  [PCPartPicker Part List](https://pcpartpicker.com/list/kCZMqH)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor](https://pcpartpicker.com/product/qtvqqs/amd-ryzen-7-5800x-38-ghz-8-core-processor-100-100000063wof) | $207.99 @ Amazon  **CPU Cooler** | [Thermalright Peerless Assassin 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3) | $34.90 @ Amazon  **Motherboard** | [Asus ROG STRIX B550-F GAMING WIFI II ATX AM4 Motherboard](https://pcpartpicker.com/product/K4LFf7/asus-rog-strix-b550-f-gaming-wifi-ii-atx-am4-motherboard-strix-b550-f-gaming-wifi-ii) | $129.99 @ Amazon  **Memory** | [Corsair Vengeance LPX 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory](https://pcpartpicker.com/product/W6ndnQ/corsair-vengeance-lpx-32-gb-2-x-16-gb-ddr4-3200-memory-cmk32gx4m2e3200c16) | $170.99 @ Best Buy  **Storage** | [Samsung 990 Pro 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/34ytt6/samsung-990-pro-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-mz-v9p2t0bw) | $198.99 @ iBUYPOWER  **Video Card** | [Asus DUAL OC GeForce RTX 5060 Ti 16 GB Video Card](https://pcpartpicker.com/product/ss8Pxr/asus-dual-oc-geforce-rtx-5060-ti-16-gb-video-card-dual-rtx5060ti-o16g) | $479.99 @ Best Buy  **Case** | [Corsair iCUE 5000T LX RGB ATX Mid Tower Case](https://pcpartpicker.com/product/9mqNnQ/corsair-icue-5000t-lx-rgb-atx-mid-tower-case-cc-9011298-ww) | $364.99 @ Amazon  **Power Supply** | [Corsair SF750 (2024) 750 W 80+ Platinum Certified Fully Modular SFX Power Supply](https://pcpartpicker.com/product/TsTZxr/corsair-sf750-2024-750-w-80-platinum-certified-fully-modular-sfx-power-supply-cp-9020284) | $149.99 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1737.83**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-09 02:39 EST-0500 |  This bot is in **no way** associated with PC Part Picker.  ----  *[I am a bot](https://i.imgur.com/hfRIhMe.png)* -  This action was done automatically. Please direct any questions or concerns ( or bug reports ) to [\/u\/eegras](https://www.reddit.com/message/compose?to=eegras&subject=&message=) - [About /u/PCMRBot](https://pcmasterrace.org/pcmrbot_old)",pcmasterrace,2025-12-09 07:39:51,5
Intel,nt3n6h9,"It's a solid PC - if you already owned some of those components. But I wouldn't buy that new. (Owning the same MOBO, RAM, PSU and roughly chip)  What you want is today's equivalent of that setup, which some have listed below.   The problem with this build is that AM4 is no longer in production, so if that 5800X gets a bit slow, you can't upgrade it alone, you'll have to upgrade the motherboard...which means also replacing the RAM.   So what you want is an AM5 setup where you CAN select cheaper components that you can then upgrade later if need be.  Good luck and welcome!",pcmasterrace,2025-12-09 12:20:08,3
Intel,nt2z2j6,"maybe consider AM5 rather locking yourself in am4, although you will have to pay for those hellish ddr5 ram prices. Plus with am5 you have access to pcie gen 5, and future proofing till 2027 case as said by others is a bit too expensive, you can shave mayb 150-200 for a good looking one and put the money in a better gpu / cpu and for psu, you can get a full atx one with gold or plat rating for less",pcmasterrace,2025-12-09 08:32:35,2
Intel,nt333hn,"https://pcpartpicker.com/list/PffWVF  I dont think I cooked, but Jesus Ram is fucked",pcmasterrace,2025-12-09 09:14:28,2
Intel,nt2vbjn,"My only suggestion is to not get a 2TB SSD. From my personal experiences having your SSD as purely systematic files allows the OS to boot quicker and update without fail. If you want the SSD to download games faster sure, just make sure you get 2 separate SSD cards. Alternatively most HDD will suffice as a cheap alternative to still have a butt ton of storage. Also 32GB ram is a good starting point, I don’t see anything eating that up immediately, but if you consider recording your game play, discord, 10+ web browsers and other background uses, not to mention the possibility of streaming, I advice getting more RAM (2x32GB is faster than 4x16GB) but that’s only if your plans are professional",pcmasterrace,2025-12-09 07:55:07,2
Intel,nt3476o,I don't know much about AMD cooler quality but they have their own coolers that come with cpu usually. So you can save money there,pcmasterrace,2025-12-09 09:25:49,1
Intel,nt40m1c,That is a lot of money for that PC. If it were me I would definitely drop the $400 case and get a 5070.,pcmasterrace,2025-12-09 13:48:09,1
Intel,nt4rs5x,"Bad value. I got this PC for $1800 in September:  https://www.microcenter.com/product/689586/powerspec-g722-gaming-pc  Even considering RAM prices, it would be still a bad value vs. what you can get in a prebuilt. Micro Center has great prebuilts.",pcmasterrace,2025-12-09 16:11:28,1
Intel,nt3rhqa,DDR5 ram may kill his budget.,pcmasterrace,2025-12-09 12:51:16,1
Intel,nt2um28,So I should check better prices for the CPU and GPU?,pcmasterrace,2025-12-09 07:48:10,-1
Intel,nt2vu8t,"oh thank you for that, TBH I didn't realize it was SFX. Are SFX more expensive that regular ATX? I definitely need to check that",pcmasterrace,2025-12-09 08:00:09,1
Intel,nt3q289,"Damn man, thank you so much for this. After reading comments, I get that AM4 isn’t a good option so yeah I was think about researching about AM5 to start with it and you dropped a full build so yea I can totally work from this, checking prices.  I’ll check the components in more detail later, but again thank you.",pcmasterrace,2025-12-09 12:41:22,1
Intel,nt4623o,"this is an excellent build, you can also probably shave off a good chunk of money by not exclusively looking at 6000 cl30 ram",pcmasterrace,2025-12-09 14:19:26,1
Intel,nt2tttc,thank you bot,pcmasterrace,2025-12-09 07:40:19,2
Intel,nt3ools,"Thank you so much, some others also mentioned about the AM5 and I think it would be the best option but now I need to research about it",pcmasterrace,2025-12-09 12:31:20,2
Intel,nt3q7fr,Thank you!!,pcmasterrace,2025-12-09 12:42:25,1
Intel,nt2ywv8,Would it be better like a 512Gb SSD and 1Tb HDD?  Also not thinking about streaming or something more professional but I will think about 2x32 ram. Thank you!!,pcmasterrace,2025-12-09 08:31:00,1
Intel,nt37q8x,Probably terrible coolers and loud,pcmasterrace,2025-12-09 10:02:00,2
Intel,nt3swo7,"And then he absolutely should not be building the system himself. Its just following clear instructions for a few hours for a beginner, that experience should not make you pay $200 more for AM4 + 5060 compared to a 9800X3D + 5070 a system",pcmasterrace,2025-12-09 13:00:50,1
Intel,nt2v2yz,"If that is your budget, I'd generally advice on not getting a $350 case lol. Try to adjust it so you can hop on to AM5, I can't really do the list right now.  Is there some realistic way you can come close to [this](https://www.costco.com/ibuypower-element-gaming-pc-desktop---amd-ryzen-7-9800x3d---nvidia-geforce-rtx-5070-12gb----windows-11-home---32gb-ram---2tb-ssd.product.4000384603.html)?",pcmasterrace,2025-12-09 07:52:46,12
Intel,nt57hbl,"Costco has rebuilds on sale until the 21st of Dec, pretty sure they are better deals than this. $1500 is 5070 and Ryzen 7 9800X3D",pcmasterrace,2025-12-09 17:26:43,1
Intel,nt2yco3,"Usually, yes. It generally costs more to make things smaller. Montech has a good deal on an 850w PSU that ranks well in the PSU tier list.",pcmasterrace,2025-12-09 08:25:17,4
Intel,nt3ruze,He provided a great build for you. I was gonna suggest you get an 9070 XT myself. What screen resolution are you planning to game at?,pcmasterrace,2025-12-09 12:53:47,2
Intel,nt3rc06,My pleasure  If its out of your budget you could start with a cheaper CPU as the 7800x3d is fantastic but a lot of people might never really use the extra L3 cache and you might be served well enough with a 9600x for example.  We can also take a step back on the GPU and go back to  the 5060TI you chose or a 5070 to also save some money but probably dollar/performance for straight out videogames nothing beats the 9070xt  Then as i mentioned if you are open to other vendors we can probably save some purchasing select pieces from newegg or the mobo/cpu combos from msi  Just let me know if i can help any further,pcmasterrace,2025-12-09 12:50:12,1
Intel,nt3sn0b,"No worries,  The biggest problem right now is that in the last 2 weeks, DDR5 ram has shot up 2-5x in price due to AI datacenter greed and such.  So suddenly a bunch of us are pumping new life into our existing AM4 setups instead of buying new AM5 PCs. Just today I replaced my 5600x CPU with a 2nd hand 5700x3d, which should max out the performance of my AM4 build.  I WAS planning on buying a new AM5 pc with a 7800X3D chip and 6000MHz ram, but the price suddenly jumped by a LOT!  Btw, if you have the budget, try and aim for a 70-series card or AMD equivalent like the 9070. I've always been a 60-series user (960, 3060Ti), but each time I buy it, I always regret not saving 2-3 months extra to get the better performing 70 series. They seem to be the sweet spot for price vs performance, even more so if you go with an AMD equivalent.",pcmasterrace,2025-12-09 12:59:02,2
Intel,nt43p38,"From what i know they ain't bad, they're mid. Which is sufficient",pcmasterrace,2025-12-09 14:05:57,1
Intel,nt2vkfz,">Is there some realistic way you can come close to [this](https://www.costco.com/ibuypower-element-gaming-pc-desktop---amd-ryzen-7-9800x3d---nvidia-geforce-rtx-5070-12gb----windows-11-home---32gb-ram---2tb-ssd.product.4000384603.html)?  thanks for that link, I'll try to adjust to something more like that and yeah you're totally right with the case it's really expensive compared to even other parts",pcmasterrace,2025-12-09 07:57:32,1
Intel,nt2vn98,"If you're going to go overboard anywhere, the case is probably the place to do it since you can carry it through multiple builds. That said, USD$350 is way more than I'd pay for a case personally.",pcmasterrace,2025-12-09 07:58:17,1
Intel,nt2ygrc,Tysm!,pcmasterrace,2025-12-09 08:26:25,1
Intel,nt3eef5,nowadays somehow prebuilds are cheaper than custom built computers. Check if any is available near you. coz 5060 ti for 1700$ is just too much.,pcmasterrace,2025-12-09 11:06:35,2
Intel,nt2w7uk,"That is true, I'd guess my answer came from the same place. $350 would be way too much for me lol",pcmasterrace,2025-12-09 08:03:51,3
Intel,nt2ynub,"In my mind was like… that’s the “cosmetic” part of the PC, like the RGB and stuff that would like to be nice on the pc so that’s why I picked that case but thinking the way you said it’s also a really good point",pcmasterrace,2025-12-09 08:28:25,1
Intel,nt3n6m1,Thanks,pcmasterrace,2025-12-09 12:20:09,1
Intel,nou1hyn,"I'm waiting on them to make something that can replace my 3070ti, the minimum performance I want for me to justify an upgrade would be a 9070xt / 5070ti  Intel, i'm waiting, and i'm willing, so show me what you can do.",pcmasterrace,2025-11-14 16:44:15,238
Intel,nouewiw,https://preview.redd.it/l09xx3bhg91g1.jpeg?width=1846&format=pjpg&auto=webp&s=85c81cc5c09ff61412c6b72e85e82902ce185990  Does it count?,pcmasterrace,2025-11-14 17:51:49,64
Intel,nou3l2x,"I was considering getting an Arc, but it didn't seem like a good fit for an AM4 board. It was also kinda too low end to be a worthwhile upgrade from my previous card, while the 9070 XT was one.",pcmasterrace,2025-11-14 16:54:36,14
Intel,nou1xr6,"I would've got the b580, but it only has 8 lanes.  So I got the 9060xt.  I'm playing on a gen 3 system, so l would loose a bit of performance.",pcmasterrace,2025-11-14 16:46:25,41
Intel,nou14lr,"Not touched arc yet, waiting for them to compete at the mid-high end before I take a serious look",pcmasterrace,2025-11-14 16:42:25,53
Intel,nouqmp5,"""yo intel arc is fucking horrendous""   - xXDÆTHZlay3erXx  (RTX 6000 PRO, 9950X3D,MSI GODLIKE X870E, 96GB 8200mhz)",pcmasterrace,2025-11-14 18:49:29,19
Intel,nou58od,"Got a B580 at a great price, small upgrade from my 6700XT which went to another build. Great 1440p performance paired with my 5900X.",pcmasterrace,2025-11-14 17:02:53,14
Intel,nouzy8h,"A770 Owner here. Was between 4060-7600XT-A770 this year, video games was a second focus but wanted to go more for productivity (in a budget). 4060 was fine, but the 8gb VRAM make me go for the other 2 options, mainly if I get to edit very big things in high res., it would be hard to do.  And between 7600XT and A770... The A770 gets 4070 performance on video edition, and Intel codes are far better than AMD. Gaming performance they are almost the same... So far very happy with my purchase.  Since the start of the year, doing the same workflow, with driver updates I got to shave a total of 5 minutes on each video render. At the start was 20m, rn is 15m with the same type of edition. And driver bugs? 0, enever needed to reinstall drivers.",pcmasterrace,2025-11-14 19:36:47,5
Intel,noukr29,The $250 price point is compelling to me for the specs.   But does it handle games well? That's what I want to know; $250 is a pretty big purchase for me right now.,pcmasterrace,2025-11-14 18:20:36,4
Intel,nouxqvu,"I mean i sort of do? My ultra 7 155h laptop has an arc igpu, if that counts",pcmasterrace,2025-11-14 19:25:25,4
Intel,nouhrp7,"Had a A770 and it was great, only reaon I moved away was VR. If they could support VR I would go back. Prices are better, it worked really well, the software was great and dev support was amazing.  Halo Master Chief Collection wasn't running well. Raised a support ticket and in a month they fixed it.",pcmasterrace,2025-11-14 18:05:56,6
Intel,nouynui,"They don’t make anything powerful enough for what I want out of a gaming PC but they’ve made strides and if they keep it up, I could see them continuing to improve, maybe having xx60 competitors in a couple of years.",pcmasterrace,2025-11-14 19:30:08,3
Intel,nov4dno,"I use an A770 16GB for gaming on the 4K TV. Mostly older or lighter games, but in Forza Horizon 5 with the help of XESS there's no problem with high-ultra settings in 4K. hitting rock solid 60fps",pcmasterrace,2025-11-14 19:59:49,3
Intel,nov63z8,I have a A770 and I'd say it performs about on par with my 3070 for most games I play. Honestly interested in seeing what more they can do with better hardware design and continued work on improving their drivers.,pcmasterrace,2025-11-14 20:08:53,3
Intel,novcztj,"A770, really liked it for it's price point, even though it was a pain at first. Solid at 1440p.   I'll get the b780 if it ever releases.",pcmasterrace,2025-11-14 20:45:33,3
Intel,novffom,I've got the A770 16 gig and it seems fine.  Hard to tell with games if it's my GPU or the game being next gen ready. I came from a gtx 970 so it felt like a massive upgrade. I remember Balder's gate being real low fps in act 1 on the 970 but very decent on the A770 at 1440. Hunt showdown also gets 90+ fps in busy areas but has had issues dropping to very choppy fps when things are happening close. With Hunt it's very hard to tell what the issue is as it has frequent issues that effect many gpus at the same time. Like a friend playing on amd had different issues at the same time as I did. Honestly couldn't tell you if I've turned the graphics up from low since I had that issue. A770 runs armored core 4 really well to the point that I haven't had to mess with any settings or check fps.,pcmasterrace,2025-11-14 20:58:24,3
Intel,novgcrp,"Been using my A770 since it launched. It's been great for a majority of the time, but the beginning was rough.",pcmasterrace,2025-11-14 21:03:19,3
Intel,nowo6qd,![gif](giphy|kSlJtVrqxDYKk|downsized),pcmasterrace,2025-11-15 01:18:39,3
Intel,noudbdg,"I love having more competition but I'm not touching it until they make a high end model, I just got a 5080",pcmasterrace,2025-11-14 17:43:58,5
Intel,nougnx2,My sons PC has an A770. Never really had any issues with it at all.,pcmasterrace,2025-11-14 18:00:22,2
Intel,nouha1a,At the very least it is damn good value for money.,pcmasterrace,2025-11-14 18:03:28,2
Intel,nousic2,"I have an Arc A750, and I'm pretty happy with it so far. I mainly play older games so I can get 60-100fps on 1440p ultra.",pcmasterrace,2025-11-14 18:58:44,2
Intel,nov0g45,"Been using B580 with 4K monitor for a while now. 7500F as CPU. With modern upscaling, it is very capable.",pcmasterrace,2025-11-14 19:39:24,2
Intel,nov24ks,I use one for an AV1,pcmasterrace,2025-11-14 19:48:07,2
Intel,nov9db5,Plex server,pcmasterrace,2025-11-14 20:26:14,2
Intel,now1pyd,The a750/770 wasn't bad for gaming and neither is the b570/580  I use an a370 for transcoding video and it does a great job.,pcmasterrace,2025-11-14 22:58:04,2
Intel,nowgg77,"I have a b580, and I absolutely love it. It is the perfect entry level 1440p card.",pcmasterrace,2025-11-15 00:29:39,2
Intel,nowrfek,"i have a b580. i just hate nvidia (and also use linux).  nvidia on linux is a pain in the ass. its getting better but i cba. i bought it a couple months ago. it was either amd or intel arc and i decided to just go for it.      Intel arc on linux is quite literally just plug and play. I can't express how much I love this card. i hope intel stays in the gpu market, but seeing recent news the situation seems dire.",pcmasterrace,2025-11-15 01:39:59,2
Intel,noxbpi3,B580 in my guest computer. Haven't had any issues with it playing all the LAN games my cousin and I play together every week.,pcmasterrace,2025-11-15 03:53:49,2
Intel,nou7hcw,"Been using an A770 for over two years! I definitely had my fair share of issues, especially early on, but over all it's been great",pcmasterrace,2025-11-14 17:14:13,3
Intel,noua0ne,i'm using [intel arc](https://www.reddit.com/r/pcmasterrace/comments/1ouj4en/im_glad_i_use_a_mesh_case/) and i'm very satisfied with it,pcmasterrace,2025-11-14 17:27:06,3
Intel,noucikm,A770 16GB LE here.,pcmasterrace,2025-11-14 17:39:54,4
Intel,nou0czb,"I've got two, an A310 and a DG1.  I have yet to get the DG1 working though.",pcmasterrace,2025-11-14 16:38:37,2
Intel,nou923w,"Here, just finished my build yesterday.",pcmasterrace,2025-11-14 17:22:15,2
Intel,noubmju,"I have three across two rigs. They may not the best, but I can do everything I want at pretty good settings and barely cracked $500 for all.",pcmasterrace,2025-11-14 17:35:20,2
Intel,noucmps,woulda bought it if not for vram alone,pcmasterrace,2025-11-14 17:40:29,2
Intel,noudt24,If they had a card that could compete with Nvidia and AMD high end I would consider it. At the current performance tier with B580 as the best card it doesn't interest me.,pcmasterrace,2025-11-14 17:46:27,2
Intel,noufuyl,"I love to not buy Nvidia, just as soon as someone makes a better product",pcmasterrace,2025-11-14 17:56:26,3
Intel,nouf9we,Had a a770 in my Linux but sadly I had to many issues getting it to play certain games and running it in my main  rig which at the time had a 6700xt it ran worse than the 6700xt so now it’s just sitting in it box not sure what to do with it,pcmasterrace,2025-11-14 17:53:36,1
Intel,noufd34,is it good for rendering  ? like blender/unreal  stuff  ?,pcmasterrace,2025-11-14 17:54:02,1
Intel,nouhmux,"I don't use it, but I appreciate there being more options available for people.",pcmasterrace,2025-11-14 18:05:15,1
Intel,nouinri,It's okay at best,pcmasterrace,2025-11-14 18:10:22,1
Intel,noulper,B580 at 1080 so far so good. Can lock 60fps ARC Raiders max settings or drop down from max a level and 120fps,pcmasterrace,2025-11-14 18:25:14,1
Intel,noumgg9,"I picked up a B580 for stoopid cheap secondhand and am using it to learn about local LLMs on my linux rig for future home automation. Works fairly decent, but my benchmark options are limited on linux for now. It runs a bit warm at idle without an undervolt (55° - 60° C).",pcmasterrace,2025-11-14 18:28:55,1
Intel,noumrkp,If they ever release a single fan battlemage card akin to the a380 I'd buy one for my sub 5L build for sure... Tempted as it is by the sparkle a380 but I keep hoping for better especially since the b50 pro is a thing.,pcmasterrace,2025-11-14 18:30:26,1
Intel,noumv3o,"Not me really, irs rare to find one here also",pcmasterrace,2025-11-14 18:30:54,1
Intel,noutyoj,It’s great for a budget build but I wanted something higher-end so I went for the 9070XT,pcmasterrace,2025-11-14 19:06:06,1
Intel,nouuj5w,I wish I could. But my friends have outspoken this idea from me. Ended up with a laptop and RTX card in it.,pcmasterrace,2025-11-14 19:09:00,1
Intel,nouxv8r,For me the problem is that Intel's best offering is about as fast as 4060. I got no use for that.,pcmasterrace,2025-11-14 19:26:04,1
Intel,nouy2kj,"I would swap to ARC, but the rest of my system needs an upgrade first. I don't even have ReBAR support.",pcmasterrace,2025-11-14 19:27:07,1
Intel,nouyskv,"Was looking at an intel one of them for a long time, until I realized most FEA & CFD software suites require nvidia graphics cards and so that killed that idea.",pcmasterrace,2025-11-14 19:30:49,1
Intel,nov17pf,I am interested but I don't think it is worth replacing my 2080 Super.,pcmasterrace,2025-11-14 19:43:23,1
Intel,nov5zlw,lol,pcmasterrace,2025-11-14 20:08:15,1
Intel,nov669r,"I just got an a380 for 120 and put it in my plex server.  Love it, barely spins up for 4K to 1080p transcodes including HDR to SDR mapping.  Makes zero noise.",pcmasterrace,2025-11-14 20:09:14,1
Intel,nov6uk6,"Yes, got a B580 for MSRP, and put it into my custom Steam Machine!",pcmasterrace,2025-11-14 20:12:48,1
Intel,nov8i0x,"I mean, is it? While the price to performance *seems* to be good, from what I’ve heard, the drivers kind of suck, and it doesn’t really work well with actual budget CPUs.  It’s good on paper, but it’s not really a mature product yet. I’d guess that by the next generation, they’d be more caught up with the competition at that price range.",pcmasterrace,2025-11-14 20:21:35,1
Intel,nov8u9z,"I have one in my jellyfin and its been amazing doing 3 transcodes at a time and not sucking up too much power, along with some misc server tasks like trickplay. Looking into the b580 for my personal computer currently. I've heard people say the drivers aren't good but personally I havent really had a problem with them.",pcmasterrace,2025-11-14 20:23:24,1
Intel,novb3gi,"I have an A770 in my rig, genuinely never had a problem unless it was Fallout 3",pcmasterrace,2025-11-14 20:35:28,1
Intel,novcv5j,I had a B580 near launch. Was my replacement for a 1080ti at the time. It was totally fine for my use case. I did upgrade to a 4080 later but that was more due to my use case changing (built a sim racing rig with a large format 4k TV) than the performance of the card itself. I was also quite partial to the understated looks of the card,pcmasterrace,2025-11-14 20:44:52,1
Intel,novda5p,"Ive used plenty of Arcs and have enjoyed the performance and compatability.  The 1 weird thing is in one older game I played, the game defaulted to my PCs AMD iGPU, and wouldnt run on the Arc without disabling the iGPU in the BIOS.   Probably just an issue with the engine only knowing to expect an AMD/Nvidia GPU",pcmasterrace,2025-11-14 20:47:03,1
Intel,novddt1,You think it would be the cheapest option to make a steam machine equivalent?,pcmasterrace,2025-11-14 20:47:34,1
Intel,nove9o5,I had the B580 for 4 months then switched to a 5070.,pcmasterrace,2025-11-14 20:52:15,1
Intel,novehw4,I downgraded from a 6950xt to a B580. honestly don't notice much if any difference in the games i play and the power consumption is signifcantly less.,pcmasterrace,2025-11-14 20:53:27,1
Intel,novfb5r,"b580 user here. paired with a 5700x3d.  not much to complain about when it comes to gaming performance. HOWEVER, video encoding is an absolute no-no with the b580. (streaming on discord or recording with hardware encoder in obs absolutely kills gaming performance.)",pcmasterrace,2025-11-14 20:57:45,1
Intel,novfitp,"I have one in my shop computer.....I have a gpu from all three in use. A 6800 in my wifes rig, I have a 4080, and the shop has the 580! Guess where all the gaming happens?",pcmasterrace,2025-11-14 20:58:52,1
Intel,novg2sv,I don't run arc on my personal system (have a 7800XT so it'd be a downgrade) but I build a budget system for my little brother using the B580 and he seems to like it,pcmasterrace,2025-11-14 21:01:50,1
Intel,novg7gh,"Upgraded from 1070 to b580, and have been very pleased with it. I mainly play dota, arpgs and do some photo editing in lightroom, and thought the b580 was the most priceworthy option for me!",pcmasterrace,2025-11-14 21:02:32,1
Intel,novgf1j,"We do. Arc 380. Best thing you can use for a Plex server.   Side note. I can't get DLSS to work on Battlefield 6. Before, it would hard crash. Now it just turns everything greyscale red. But XeSS works.",pcmasterrace,2025-11-14 21:03:39,1
Intel,novgndc,I would have happily gotten one if I didn't have a card more powerful already. I'm a big fan of what intel is doing so far in the GPU space.,pcmasterrace,2025-11-14 21:04:53,1
Intel,novhdz3,friend uses it on our private emby server for transcoding.,pcmasterrace,2025-11-14 21:08:46,1
Intel,novhqic,Decent choice for some price points but I wouldn't call it good good. It's definetely good for the market though. Waiting for some mid-high end moves,pcmasterrace,2025-11-14 21:10:36,1
Intel,novi5gj,I would 100% do it if I were in a situation of needing a 250$ GPU. Sadly Arc doesn't cover any other price category except if you wanna buy an old-gen card,pcmasterrace,2025-11-14 21:12:47,1
Intel,noviqo8,"A380 used here for my Plex Server, perfomance is incredible for transcoding media, especially given the price.",pcmasterrace,2025-11-14 21:15:49,1
Intel,novl32r,"i would gotten them but can't find any, only used one and even then, still pretty expensive since they are shipped from overseas, not in stores",pcmasterrace,2025-11-14 21:27:50,1
Intel,novlrhh,Intel cards aren't a bad deal for entry level rigs and I hear battlemage is much improved from the A generation cards.,pcmasterrace,2025-11-14 21:31:23,1
Intel,novmnsz,It’s a good graphic card. I have the A750. Runs oblivion remastered decently well at 1080p,pcmasterrace,2025-11-14 21:36:03,1
Intel,novnhsm,I got the b580 for my wife and it’s great she plays flight sim asserts corsa and I got the free bf6 from the deal which also runs great on b580,pcmasterrace,2025-11-14 21:40:24,1
Intel,novqpw6,"Speaking of, I'm planning to get B580 to replace aging 5700XT for mostly encoding to HEVC and light gaming on 1440p (Paradox games like EU5, CK3, Vic3, maybe F1, or other sports games, nothing competitive). Budget is 250 eur up to most 400 eur. For reference, I can get B580 for 250-270 easily, RTX 5060Ti 16GB for at least 430, so it's out of scope.  For US people, this is with VAT 20% included",pcmasterrace,2025-11-14 21:57:10,1
Intel,novrbgs,"I'd like to get one, but I'm waiting for it to be better than my 7900XT.",pcmasterrace,2025-11-14 22:00:21,1
Intel,novrmdm,I have one I bought to leave sealed as a collectible. I did buy another A770LE for personal use but sold it to a friend.,pcmasterrace,2025-11-14 22:01:58,1
Intel,novs58t,I got the Intel A580 last year at microcenter for $90 ☺️ I only game at 1080p medium setting,pcmasterrace,2025-11-14 22:04:47,1
Intel,novu6fo,"Just built my first pc with an arc b580 12gb and I'm loving it, been playing both flat-screen and vr and it rips",pcmasterrace,2025-11-14 22:15:53,1
Intel,novujep,"As soon as they optimize the performance in dx 11 and lower. Efficiency of battlemage is nice but drivers need the aforementioned tinkering. For now planning to go 4070, solid midrange choice and better than 5000 series (proper cuda, hotspot sensor is a must, dont care about dlss or framegen because i never use these).",pcmasterrace,2025-11-14 22:17:53,1
Intel,novus6m,It's fcking terrible.,pcmasterrace,2025-11-14 22:19:14,1
Intel,novutdc,Fantastic transcoding cards for servers.,pcmasterrace,2025-11-14 22:19:25,1
Intel,novv9ko,My 4070ti scoffs.,pcmasterrace,2025-11-14 22:21:55,1
Intel,novwdxj,"I considered for my latest build, upgrading from a 980ti.  But I was concerned about support and drivers and ended up finding a 9070 at msrp instead.  I would definitely consider Intel GPU's in the future though as they mature.",pcmasterrace,2025-11-14 22:28:06,1
Intel,novyq41,"Technically I use one, but not for gaming. I run a PleX server on a dedicated PC and I use an Arc A380. It's a beast for that usage, especially at that price point.",pcmasterrace,2025-11-14 22:41:04,1
Intel,novzhwv,Not me but I welcome choice in the gpu space. Sick of nvidia and amds pricing,pcmasterrace,2025-11-14 22:45:24,1
Intel,now139s,I want one for av1 encoding pbad. But other than it sitting in my server for that one use case it wouldn’t get used.,pcmasterrace,2025-11-14 22:54:29,1
Intel,now5fw5,Bad compared to a 5090. Sure.  Bad for price to performance. No.  My main problem with arc is that its still early days. If they make another generation assuming the rnd team survives lay-offs I might pick one up.,pcmasterrace,2025-11-14 23:20:09,1
Intel,now5pqe,"I don't. I'm all AMD, but I do like what they're bringing to the market. Would love to see them push out something to compete with the high end from Nvidia or AMD.",pcmasterrace,2025-11-14 23:21:47,1
Intel,now68w8,"I have one in my server, best media engine for transcoding on the market, for real! And linux support too!",pcmasterrace,2025-11-14 23:25:01,1
Intel,now7v0e,"I sometimes see the B570 at 250 USD (including import taxes and stuff where I live), but since I'm on AM4 I'm not sure if it's a good upgrade when I can save 50 USD more for a 5050",pcmasterrace,2025-11-14 23:34:56,1
Intel,now9shu,"I've only used the Arc 140m for a little bit, but it was shockingly good.",pcmasterrace,2025-11-14 23:46:57,1
Intel,nowagkq,"I use Intel gpu, just happens to be iGpu of the i3 2nd Gen (always thought it was 3rd but it's 2nd just found out today",pcmasterrace,2025-11-14 23:51:45,1
Intel,nowatqg,"I have a B580 Steel Legend, a super cool card, especially for budget builds. I paired it with a 5600x and I can run pretty much anything.",pcmasterrace,2025-11-14 23:54:27,1
Intel,nowbuqc,I current have the Intel Arc A770 LE. It's been running fairly well and hasn't caused too much problems with it. I def look forward to more software updates and see how well it'll perform overtime through its driver development.,pcmasterrace,2025-11-15 00:01:29,1
Intel,nowc713,Nobody was selling it when I build my current pc and I didn't feel like smuggling one trough the border,pcmasterrace,2025-11-15 00:03:39,1
Intel,nowcavy,They got to make something at at least the 5080 level for me to even consider an Intel card. What they have now can't handle native 4K.,pcmasterrace,2025-11-15 00:04:20,1
Intel,nowcn93,"If i ever build a pc, i will go for arc",pcmasterrace,2025-11-15 00:06:29,1
Intel,nowdfr6,"Technically yes. Not in my main gaming rig, but my Claw is Arc.",pcmasterrace,2025-11-15 00:11:19,1
Intel,nowdh8e,"I've had my 750 in a proxmox server doing passthrough for various things since it came out. When I first got it, I had lots of driver issues of course but that got smoothed out.",pcmasterrace,2025-11-15 00:11:34,1
Intel,nowgwc9,its ok,pcmasterrace,2025-11-15 00:32:23,1
Intel,nowh8v3,Great if you don’t plan to do anything other than game and don’t need hard hitting production equipment.,pcmasterrace,2025-11-15 00:34:30,1
Intel,nowhh2z,"I've got one, but it is used primarily to transcode videos on my jellyfin server.",pcmasterrace,2025-11-15 00:35:52,1
Intel,nowio4t,Have a ARC A770 16gb in my gaming server. One of these days I'll have to take it out and throw it into one of my other rigs to see how it does gaming.  https://imgur.com/gallery/CsqR093,pcmasterrace,2025-11-15 00:43:12,1
Intel,nowk3pt,I put one in my dads PC .  He only plays 10 year old games and it works perfectly.  No issues with it ever.,pcmasterrace,2025-11-15 00:52:15,1
Intel,nowk5iu,https://preview.redd.it/ctca68qjjb1g1.jpeg?width=9248&format=pjpg&auto=webp&s=797dea53fd3cf9096881d0a22fc85aab978d821a  My bros system but i guess it still counts.,pcmasterrace,2025-11-15 00:52:35,1
Intel,nowp9vg,I wanted to but scored a deal on a 6700 for 160 so couldn't pass it up,pcmasterrace,2025-11-15 01:25:46,1
Intel,nowpz9d,I got a cheap one as a secondary GPU to drive my secondary monitors,pcmasterrace,2025-11-15 01:30:26,1
Intel,nowtcb8,"Used A770 le 16Gb for about 2 years, very good gpu. Heating a bit  too much and too fast in 4K gaming, managed to deliver 4K high quality at 30-60 fps in most of the games. Now switched to Radeon RX 9070XT , this one doesn’t struggle with 4K towards 120 fps, and the ARC770 Sitting in her box for resting.",pcmasterrace,2025-11-15 01:52:19,1
Intel,nowx4kl,The CPU overhead issue on Intel still seems way to bad to consider it over AMDs entry level options instead imo.   It's great if you have a high end CPU but kinda falls apart with a CPU you'd expect to pair with it.,pcmasterrace,2025-11-15 02:16:51,1
Intel,nowxjod,When my 3080ti starts showing up in the bottom quarter of performance graphs I’ll look then. But it’ll be hard to justify enriching Jensen anymore. AMD has my vote.,pcmasterrace,2025-11-15 02:19:32,1
Intel,nowxt9a,I don't yet. I really want to though!,pcmasterrace,2025-11-15 02:21:15,1
Intel,nowxuss,This meme format is mid at best,pcmasterrace,2025-11-15 02:21:31,1
Intel,nowyyz5,It the lanes are only 8 a piece can I run two Arcs,pcmasterrace,2025-11-15 02:28:43,1
Intel,nox0ih5,"I used an a770 for a while, I liked it but it just barely wasn’t enough for the quality level and target fps in games I wanted to play, so I went back to the older more powerful card I had before. The hdr on the arc card was very pretty though.",pcmasterrace,2025-11-15 02:38:38,1
Intel,nox0mrg,"Grabbed the b580 for 235, replacing my 1660s, absolute world of difference, coincidentally my monitor is not 144hz as i previously thought... its 165hz lol rdr2 is amazing 90fps at 1440p, bf6 same story.",pcmasterrace,2025-11-15 02:39:24,1
Intel,nox0p9l,"I love my Bifrost A770 16gb, but I wanted to play vr and scored a refurbed 3080ti Fe for $450……..",pcmasterrace,2025-11-15 02:39:52,1
Intel,nox25pe,I was considering getting one to upgrade from my 3060 12gb when i had it. Got lucky and scored a 4070 for cheap or i wouldve bought one,pcmasterrace,2025-11-15 02:49:07,1
Intel,nox2age,"I built a couch gaming pc and im currently using my old 1080 in it, im really hoping they release the B770 so i can buy it to support arc. Imagine running AMD CPU with Intel GPU lmao",pcmasterrace,2025-11-15 02:49:57,1
Intel,nox2xwf,I love my b580,pcmasterrace,2025-11-15 02:54:10,1
Intel,nox5a2m,https://preview.redd.it/6yd6j8wx7c1g1.jpeg?width=4624&format=pjpg&auto=webp&s=d0a6420d49f4b717483a033444d22db1573743ce,pcmasterrace,2025-11-15 03:09:30,1
Intel,nox6odi,Next card I buy will be an Intel GPU for sure. Nvidia is pretty scummy and there's no way I'm putting an AMD anything in my computer lol  EDIT: I'm still very happy with my RTX 3070. It's still more than enough for the games I like.,pcmasterrace,2025-11-15 03:18:58,1
Intel,nox7jt3,"I recently bought a laptop with the Ultra 7 155H in it and that includes the Intel Arc iGPU. It also has a dedicated RTX 3050Ti, I have been surprised that leaving the RTX disabled I'm still able to emulate PS2 and 360 (Xenia) perfectly. I only use the RTX for heavier Steam games due to heat output and power consumption.  Also, when encoding 1080p video h.265/MP4 container in Handbrake using hardware acceleration the Arc is only 60-90 frames per second slower than the RTX.   I've been impressed I may check out Arc desktop options.",pcmasterrace,2025-11-15 03:24:59,1
Intel,nox7p81,A750. 👍,pcmasterrace,2025-11-15 03:26:00,1
Intel,nox9smj,Intel ARC A310 in truenas for AV1 encoding.,pcmasterrace,2025-11-15 03:40:33,1
Intel,noxecqy,"Not at the moment, but probably in the near future.   Need some affordable AV1",pcmasterrace,2025-11-15 04:12:54,1
Intel,noxfkj2,I have a Sparkle Titan b580 in my server for Plex. I’ve never used it for gaming.  Technically I use Arc in my MSI Claw 8 AI+. Works great in that.,pcmasterrace,2025-11-15 04:22:00,1
Intel,noxhvmz,it kicks ass in my pley server A380,pcmasterrace,2025-11-15 04:39:42,1
Intel,noxjk0l,If they can hit high end Nvidia gpus then sure. I'm not a mid range boy,pcmasterrace,2025-11-15 04:52:22,1
Intel,noxkax3,Was actually looking at their GPUs yesterday. Performances are quite good for the price. Maybe they should focus on GPUs more? Their Ultra chips dont come close to AMD.,pcmasterrace,2025-11-15 04:58:09,1
Intel,noxkt59,"Threw one in my son's computer, was a good bit cheaper than the equivalent Nvidia card and still a bit cheaper than the AMD.  Doom TDA is the most intensive game he plays and it has no problem with it so that's good enough for me.",pcmasterrace,2025-11-15 05:02:05,1
Intel,noxlgpy,"Have a b580 & so far so good. Bf6, cyberpunk, sm2, EfT, etc all run great.   I'm not frame chasing, as long as the games I play run with no stutter lags & look good I'm happy. I don't need to see the absolute detailed zit on a character's face in their stubble beard at 4k ultra settings at 196fps.   Would definitely like to see what a beefier card could be if they made one. You get a solid amount of vram, low price, & no burning cable. I hope it has a 1080ti lifecycle. But also waiting to see how much the C, D, etc improve.",pcmasterrace,2025-11-15 05:07:16,1
Intel,noxqa0y,A750 in a second build. Great for 1080p in most games,pcmasterrace,2025-11-15 05:47:07,1
Intel,noxvbxc,It's an amazing value. I used one after my 4090 and was kinda shocked how well it played COD and Warzone.,pcmasterrace,2025-11-15 06:33:07,1
Intel,noy0ero,"Got an Arc B580 for the price/performance ratio, coming from an RX 5700 XT. Was a good bit of an upgrade in a lot of newer games. Honestly think it's a great card for that purpose.  Unfortunately, I ran into poor performance that was a no-go for me. Yakuza Kiwami had a huge issue (ran at like 50-55 fps with horrible lows that made it jittery, while my 5700 XT easily pushed 144, which makes sense, it's basically a PS3 era engine with better assets). From what I can tell, 0, Kiwami 2, and YLAD all had the same issue, and they got fixed over the course of a few years. The problem with that is that 0 was reported on the forum like 2 years before it was fixed. I was in the middle of a playthrough and couldn't just wait for reasonable, playable performance. I even tried DXVK and got a big boost (like 90-100ish), but it was still very unstable.  So you really do have to beware of older games still, despite the drivers having improved a lot and being overall solid. Some other DX11 titles ran perfectly fine though.  OpenGL performance? Forget it. Necesse uses Java and OpenGL and I managed about 70fps before using Mesa for Windows to get a boost. Still wasn't ideal compared to my 5700 XT, once again (and AMD isn't known for OpenGL performance on Windows lmao).  So overall, I think it's a great piece of hardware with mostly solid drivers, but it's not good enough for me on the driver side and I don't want to wait. It lasted 2 weeks before I returned it and got a 9060 XT 16GB (which is again a good bit of an upgrade from the B580, so the extra cost doesn't hurt as bad. I've been waiting for so long for an upgrade that made sense to me).  This was all in the middle of last month. Got my 9060 XT on the 30th.  I'd also like to note that they did alleviate most of the CPU overhead issues recently, so my 5600G wasn't an issue in games that did run well, like The Finals (actually ran extremely well with maxed out settings, and XeSS is actually really good on the Arc cards).",pcmasterrace,2025-11-15 07:22:01,1
Intel,noy563j,"If I haven't got the RTX4060 LP, I would definitely vuy the B580",pcmasterrace,2025-11-15 08:09:50,1
Intel,noy7fmv,"I used a B580 as a media machine GPU for a bit.  The encoding was a little jittery when starting a file or stream, but smoothed out acceptably after 2-5 seconds of play.",pcmasterrace,2025-11-15 08:33:16,1
Intel,noyik65,Idk if an arc b580 would be good for me since Im still running a i7 6700k with a z170 mobo. But I have an Rx 580 that I've been wanting to upgrade from.   I fell like I need a full system upgrade to take advantage of any significant GPU upgrades.,pcmasterrace,2025-11-15 10:30:28,1
Intel,noyou2p,I have it in my NAS and for plex transcoding its a godsent. It just works.  Do be honest for my main rig it will not be an option for a long time. NVidia has with Cuda just to much stuff that i use daily.   AMD is years behind sadly.,pcmasterrace,2025-11-15 11:33:34,1
Intel,noz0hrm,I'd buy any companies gpu if its good price and good performance.   But I feel like Ibwould be a beta tester at this point.,pcmasterrace,2025-11-15 13:08:46,1
Intel,noz2vp3,Arc team reporting!,pcmasterrace,2025-11-15 13:24:26,1
Intel,nozgcu9,Replaced my 5700xt with a b580.   Performance is equal or slightly better at 1440p ultrawide.  The 5700xt was running out of VRAM in some games though and I was having very frequent driver crashes too.  The b580 has actually been a smoother experience.  The 5700xt has moved into a pc that had a 1050ti so it loves on being a very capable 1080p card,pcmasterrace,2025-11-15 14:47:29,1
Intel,nozgiex,I use the *word* arc. Is that the same thing?,pcmasterrace,2025-11-15 14:48:23,1
Intel,nozmzx0,I love these posts. Seeing the joker instantly tells me not to take it serious.,pcmasterrace,2025-11-15 15:24:45,1
Intel,nozorff,"Personally just not there yet for me, what they've offered has been interesting and I think intel should continue developing and release discrete gpus because I do believe they have a chance of rivaling amd and nividia for a share of the market. If they only keep improving on this",pcmasterrace,2025-11-15 15:34:17,1
Intel,nozrv0h,"Absolutely good, but also absolutely not great. I'd probably still rather just use an AMD card/APU FOR 1080p only because of driver optimization.",pcmasterrace,2025-11-15 15:50:31,1
Intel,noztt0j,They are great entry level gpus.  They are NOT great GPUs,pcmasterrace,2025-11-15 16:00:35,1
Intel,nozy1e6,"Planning on getting the Intel Arc Pro 16 GB for my server, for some video encoding. Such a monster for a little 70w card with 16 GB VRAM.",pcmasterrace,2025-11-15 16:22:55,1
Intel,nozzdwx,"I friend got an arc A750 for his PC. I think benchmarks showed around similar performance to a 3060 but I could be wrong. Anyways, it was only like... 270 bucks brand new. Looks fuckin sick too.",pcmasterrace,2025-11-15 16:30:05,1
Intel,np092sq,"My system is running i5 13600k and A770 (sparkle titan) for almost 3 years. Ngl, the GPU does exactly what I needed - video rendering in Premiere Pro and casual gaming. The drivers has been improved significantly.   Though, currently I'm not thinking about any upgrades, I might upgrade later - to newer GPUs from Intel.",pcmasterrace,2025-11-15 17:21:02,1
Intel,np0gu5i,"Been using the A750 since launch, got it for free during the Scavenger Hunt they held. It's been good so far, only recently showing signs of struggle as I'd like to enjoy Borderlands 4",pcmasterrace,2025-11-15 18:01:38,1
Intel,np1lqn4,Didn’t NVDA recently buy up a massive stake in INTC? Intel Arc is unlikely to survive the next 5-6 years.,pcmasterrace,2025-11-15 21:40:35,1
Intel,np6jp7y,"arc a770, it crashes alot",pcmasterrace,2025-11-16 18:14:14,1
Intel,np87kwm,It would have been good if Intel didn't scrap the idea for something better than the B580 because if we were to have the B750/770 there could have been a more affordable option to compete against the RX 9070 and RTX 5070 but of course Intel got rid of that notion and we would have to wait for the next series of GPUs if they do plan on making one which would most likely compete against the RX 9060 XT and RTX 5060 Ti 16GB cards.,pcmasterrace,2025-11-16 23:22:04,1
Intel,nph8frk,"I got one, it's meh in performance and drivers, but is heck of a good deal when it's on sale. Can't beat the FPS per money spent.",pcmasterrace,2025-11-18 11:58:43,1
Intel,nou8hqj,"Hell yea it is, my MSI Claw 8 is killing it out here.",pcmasterrace,2025-11-14 17:19:23,1
Intel,noucdkf,"The B580 is scratching an itch I cannot even begin to describe in adequate terms.  It is the default card for my 2-gamer builds now.  Since it's an 8x card, any consumer grade motherboard with two x16 slots that split the primary slot lanes when both are populated ultimately does not suffer a dip in performance.  Plus the linux friendly nature of the cards means that I can pack a double-bazzite PC in my bag and take anywhere.  It makes road trips and game nights significantly easier.",pcmasterrace,2025-11-14 17:39:12,1
Intel,nov90fw,come on this is a stupid question... everyone knows 99% of reddit is on 5090's. Hadn't you seen the Nvidia subreddit,pcmasterrace,2025-11-14 20:24:20,1
Intel,nouh0v0,Driver issues and compatibility in games frightens me. On paper they look great for the price but I'm worried.,pcmasterrace,2025-11-14 18:02:11,0
Intel,noutr0u,Good how? And for what? We talking mid gaming here?,pcmasterrace,2025-11-14 19:05:00,0
Intel,nov1f4q,Okay ngl this made me laugh so hard all of a sudden 🤣🙏🏻,pcmasterrace,2025-11-14 19:44:29,0
Intel,novu58e,Intel Arc Raiders?,pcmasterrace,2025-11-14 22:15:42,0
Intel,np2943u,I have b580 12gb gpu and amd is no go black screen central. Intel has no issues like those amd cards are trash.,pcmasterrace,2025-11-15 23:57:52,0
Intel,noufhb6,The odds of someone responding to this post that has an Intel Arc gpu is near zero. They have less than 1% market share and 0.16% of Steam users.,pcmasterrace,2025-11-14 17:54:35,-8
Intel,nough5r,![gif](giphy|1zSz5MVw4zKg0|downsized),pcmasterrace,2025-11-14 17:59:26,-3
Intel,nowh22v,![gif](giphy|iibEPf8xEDTedJcDJr),pcmasterrace,2025-11-15 00:33:20,44
Intel,now2cht,How about replacing 5090 without the flammable connector?,pcmasterrace,2025-11-14 23:01:45,16
Intel,novr9up,Me still happy with my 2080ti:,pcmasterrace,2025-11-14 22:00:07,20
Intel,nowbekd,Similar situation. 6800xt. If Intel can provide a meaningful upgrade to my 6800xt I'll seriously consider them. I've zero loyalty as every card I've bought has made me happy and been a genuine upgrade.,pcmasterrace,2025-11-14 23:58:33,6
Intel,npbm2yq,"Same, other than that I have a 3060ti. Budget cards are great, but i already have something better and im not willing to downgrade",pcmasterrace,2025-11-17 14:38:44,2
Intel,nownpmn,"The 3070Ti is barely 4 years old... thanks to the 8GBs, it was obsolete on its release. It could have had so much potential with 16 or 24GB",pcmasterrace,2025-11-15 01:15:31,2
Intel,nox3tga,"Same. Offer me good performance per price, enough vram, and solid drivers and my money is now yours.",pcmasterrace,2025-11-15 02:59:50,1
Intel,noxbbtu,Bmg g31 is going to be released q1. 2026. It will be anywhere from 32 to 44 xe2 cores.  I'm speculating that price tag is going to be around $500 USD.  Will most likely have 16 to 24 gigs of vram.  The next discrete cards after that will be xe3p. Those cards are currently insane. Testing just started for them. Most likely release will be 6 months after Nova lake.,pcmasterrace,2025-11-15 03:51:08,1
Intel,np8welp,Considering Nvidea bought a share of intel i would be surprised if ARC isnt dead in the water,pcmasterrace,2025-11-17 01:47:56,1
Intel,nou3bb3,Intel partnered with Nvidia to drive their APU's.  I don't think a new ARC will come out.,pcmasterrace,2025-11-14 16:53:16,-78
Intel,novxw1c,Newsflash: it isn't going to happen. AMD and nvidia are very much ahead. Intel isn't going to make up in a year what amd and nvidia have been doing for decades.,pcmasterrace,2025-11-14 22:36:24,-9
Intel,nov5era,Lossless scaling?,pcmasterrace,2025-11-14 20:05:12,26
Intel,now8c9p,"https://preview.redd.it/48776rh76b1g1.jpeg?width=2992&format=pjpg&auto=webp&s=9acb9751d120d4eecab4bd4cf1b532fcb37589b9  I feel you, great encoder so the main gpu can run free",pcmasterrace,2025-11-14 23:37:55,13
Intel,novu2un,Its in danger.,pcmasterrace,2025-11-14 22:15:21,5
Intel,nowny3f,Is the A310 connected to the chipset? I had that idea as well (with an Arc Pro A40) but i have already 3 PCie Cards in my system and the space is getting thight...,pcmasterrace,2025-11-15 01:17:05,1
Intel,noyw3h2,Adorable,pcmasterrace,2025-11-15 12:36:08,1
Intel,nou6cqs,"Honestly they're excellent for budget gaming PCs. Hella good performance for their prices.   For high end, though? I agree with you there.",pcmasterrace,2025-11-14 17:08:29,9
Intel,noua1tj,"I've got one paired with a 5950x and it runs great.  I'd like to see a b or c series 770+. But I do 1440 gaming, and I've been able to run anything I've thrown at it at 60+ fps.",pcmasterrace,2025-11-14 17:27:16,6
Intel,nouapa5,are you not bottlenecked by the 3900x in a lot of titles?,pcmasterrace,2025-11-14 17:30:36,1
Intel,nougv4r,"lose, not loose",pcmasterrace,2025-11-14 18:01:22,28
Intel,nou8r23,PCIe 3.0? You're basically losing no performance on a 16x interface with the 9060XT. Even a 5090 only loses about 2%.  VRAM limited cards on an 8x or 4x interface though... Oof...,pcmasterrace,2025-11-14 17:20:41,11
Intel,novn56r,Lmo rhere's like a 1% difference at most in 99% of gamws dude,pcmasterrace,2025-11-14 21:38:34,2
Intel,nowwug8,"I wanted one, but couldn't find them at MSRP. I bought a 6800 instead.",pcmasterrace,2025-11-15 02:15:03,1
Intel,nou2zjx,"Used the B580, even if it only has eight lanes, it still gives me some hella good performance at its price point.   $250-270 and I can play 1440p 60fps, that's all I could really ask for.",pcmasterrace,2025-11-14 16:51:38,29
Intel,now6g2q,I really wanted Amd and Intel to come out with a competitor that sits in the same category as the 5080.... AMD was rumored to have one but that never seemed to materialize.,pcmasterrace,2025-11-14 23:26:13,1
Intel,novr4tx,That’s never happening so I guess have fun waiting lol.,pcmasterrace,2025-11-14 21:59:23,0
Intel,now2jc0,If you look at the content in this sub you'd think 50% of people run a 5090/6000 Pro.  Bit funny cause even among members of this sub I doubt it's over 5%,pcmasterrace,2025-11-14 23:02:53,6
Intel,nov7fc8,"with their fake msrp/stock on release, and the overhead/driver issues, this gpu was DOA. Even if they completely resolve these problems, it's late, you can get way better products now at this tier, with way better support and resell value.",pcmasterrace,2025-11-14 20:15:52,-15
Intel,novkavh,"Ahh same, I really really wanted a B580 but they were not available in my country so I bought a used 6700XT instead.",pcmasterrace,2025-11-14 21:23:50,6
Intel,noy2nob,TIL the B580 has better performance than the 6700XT,pcmasterrace,2025-11-15 07:44:31,1
Intel,nov0ew3,"I'd say yes! I paired it with an i5-13400F and ran Forza Horizon 5 at 4K60, Space Marine 2 at 1440p60, Helldivers 2 at 4K40/1440p60, and Doom: The Dark Ages at 4K40/1440p60.   Settings were all at ultra, only had dynamic resolution set for Space Marine 2.",pcmasterrace,2025-11-14 19:39:13,3
Intel,nox1l3e,"It eats whatever i throw at it, got it for 235 from b&h photo, also came with bf6 a game i was never going to buy at full price that i now get to play at peak popularity.",pcmasterrace,2025-11-15 02:45:28,1
Intel,np0f42w,"I play Avowed on a 1440p ultrawide, no complaints here",pcmasterrace,2025-11-15 17:52:52,1
Intel,nowcx9k,"It does, it's arc",pcmasterrace,2025-11-15 00:08:12,1
Intel,nov01vh,"I heard that using Virtual Desktop is the best way right now.   However, I think someone who wants to use VR is likely not going to get an Arc GPU anyway due to the more budget nature of the GPU line.",pcmasterrace,2025-11-14 19:37:19,3
Intel,novgpcg,Yeah I made a support ticket about old Minecraft versions having rendering bugs with its old version of OpenGL and they fixed it also.,pcmasterrace,2025-11-14 21:05:10,1
Intel,nougrls,"For someone windows shopping, what kind of issues did you have?",pcmasterrace,2025-11-14 18:00:52,1
Intel,np07b2x,I’m on a regular 2080 and feel the same way.  Still hoping for a B770 release and that it would be a worthwhile upgrade.,pcmasterrace,2025-11-15 17:11:42,1
Intel,nov9oyg,"I'd say it's good with i5s, they just seriously improved the CPU overhead performance with i5 and i7.",pcmasterrace,2025-11-14 20:27:59,1
Intel,novgnwg,I do wonder if this is a problem with CPUs that don't have iGPUs.,pcmasterrace,2025-11-14 21:04:57,1
Intel,novgitf,Probably better due to the higher VRAM and clock speed.,pcmasterrace,2025-11-14 21:04:13,1
Intel,noy3m9q,"Actually, they just fixed that recently. Used an i5-14400F and it ran absolutely pristine.",pcmasterrace,2025-11-15 07:54:13,1
Intel,noxwf88,Fun thing: try and see if you can use lossless scaling and combine the rtx with the igpu.,pcmasterrace,2025-11-15 06:43:29,2
Intel,nozcetc,"Is amd years behind or just blocked out of the game.  Nvidia holds the market because of cuda, and cuda is proprietary to nvidia and very old tech so almost all programs are designed to take advantage of cuda.  Amd is not allowed to make a cuda equivalent and developers dont want to write their code for something new.    Thats not years of behind, thats a monopoly.",pcmasterrace,2025-11-15 14:23:49,0
Intel,np1of6b,"They're partnering up for integrated graphics because, let's face it, Nvidia doesn't do well in the CPU market.",pcmasterrace,2025-11-15 21:55:18,1
Intel,nouxbw7,"Budget gaming. I was able to play some good games from 2024-2025 at 1440p 60FPS with ultra graphics settings.   Also good for competition. Everyone complains that Nvidia is overpriced and AMD is underwhelming, but then where does Intel sit in the hate train?",pcmasterrace,2025-11-14 19:23:16,3
Intel,noupy00,"I'd wager the vast vast majority of those Arc users are enthusiasts, and those people are more likely to be on PC forums like this",pcmasterrace,2025-11-14 18:46:07,9
Intel,novmd4h,Common RTX 4090 L,pcmasterrace,2025-11-14 21:34:30,3
Intel,nox7q7g,Intel C990KS or Intel C990XE does have a ring to it ngl.,pcmasterrace,2025-11-15 03:26:11,4
Intel,noxmn5m,I bought a 5090. I undervolted and overclocked it and get a 3% performance boost while using about half the power of stock.  The connector issues are exaggerated and easily avoided.,pcmasterrace,2025-11-15 05:16:31,-6
Intel,novxo7v,me living with a turbine 2 ft away from me playing ck3 with my 2080 super:,pcmasterrace,2025-11-14 22:35:13,9
Intel,nowft0v,Me still happy with my rtx4000.,pcmasterrace,2025-11-15 00:25:43,1
Intel,nowqbe4,"Still using my 1080, it's a lil trooper",pcmasterrace,2025-11-15 01:32:40,1
Intel,nox73nc,"Not saying that my 3070ti isn't a good card, I'm playing in 1440p and in most situations it crushes it, but there are those couple of times where I find the performance lacking.",pcmasterrace,2025-11-15 03:21:53,1
Intel,np1vwyh,"I can understand that without judgement. I only just replaced my GTX 1080 a few weeks ago. Had that thing running for over 7 years and it's still got enough life left in it to go into a secondary computer for a few more years of service.  I don't know if I just don't tend to play demanding games or what but for example, I was still rocking out 60+ fps in Cyberpunk without turning the graphics settings down all that much.",pcmasterrace,2025-11-15 22:38:30,1
Intel,nox6tse,Same here. 6800xt for the last 3 years and still no issues with gaming at 1440 high in most games so I'm good until something is significantly better at a reasonable price.,pcmasterrace,2025-11-15 03:20:00,3
Intel,noyr6cv,Name a game and I’ve played it in 1440p on a 3070ti. Why do you people thing 8 gigs of vram is basically useless😭😭,pcmasterrace,2025-11-15 11:55:06,11
Intel,nowv7z7,Even just 12GB like the 4070 would have made it last longer   Same for the 3080,pcmasterrace,2025-11-15 02:04:35,2
Intel,noy0l9t,16 and 24 gigs wasted on it as the core can barely utilize 12gigs without shitting the bed.,pcmasterrace,2025-11-15 07:23:49,1
Intel,nouaj8j,"Unless intel confirms the cancellation of ARC officially as a whole, then that means pretty much nothing.",pcmasterrace,2025-11-14 17:29:43,44
Intel,nov1lib,"Maybe you don't get the reason for your downvotes, but at least one of them is that their cpu and gpu departments are completely separate, no that's why there was a big disconnect between Intel cpu and gpu quality, it's essentially 2 different companies sharing intel's name",pcmasterrace,2025-11-14 19:45:23,4
Intel,novma4m,Is that on paper? No? Then it doesn’t exist.,pcmasterrace,2025-11-14 21:34:04,2
Intel,now2vfm,"Intel doesn't compete at the high end, if anything it just takes even more market share from amd so idk why Nvidia would stop it.",pcmasterrace,2025-11-14 23:04:50,2
Intel,nowy981,"The partnership is because, as much as people in this sub argue otherwise, the age of large APUs has begun. They're likely not gonna catch on in DIY desktop, but for laptop and mini-PC's, giant APUs with large iGPUs are going to likely replace entry level dGPU.   So the partnership allows Nvidia to, instead of selling a GPU chip to a motherboard OEM to be soldered, will now sell that chip to Intel to be integrated into the SoC package as a chiplet.   It doesn't mean Arc is dead. It means Nvidia is ensuring they aren't squeezed out of this new emerging market",pcmasterrace,2025-11-15 02:24:06,1
Intel,noulj42,I'm aware,pcmasterrace,2025-11-14 18:24:22,1
Intel,nowr7ia,This is not how it works,pcmasterrace,2025-11-15 01:38:34,4
Intel,nows8mb,"By your logic, Tesla shouldn't have had the success it has now since Ford has been making cars for a lot longer.",pcmasterrace,2025-11-15 01:45:13,1
Intel,noy0n67,Oh intel pretty much can.,pcmasterrace,2025-11-15 07:24:20,1
Intel,nove8sk,Or video encoding. I've considered getting an A350 for AV1 OBS recordings,pcmasterrace,2025-11-14 20:52:07,22
Intel,novil9h,"Nope. Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",pcmasterrace,2025-11-14 21:15:03,6
Intel,nowvpgi,"While being strangled, JK but yeah slight airflow decrease.",pcmasterrace,2025-11-15 02:07:42,5
Intel,noy45ml,"It's not A310, it's A380 (although it's power limited to 43W) and it's not connected to the chipset, but directly to the CPU. I've picked up motherboard that has two large PCIe slots running at PCIe gen 4 speed, connected directly to the CPU, exactly for this reason. 2nd slot uses 4 lanes though, but it's more than enough for 3440x1440 170Hz uncompressed video.",pcmasterrace,2025-11-15 07:59:41,2
Intel,nox04e6,Would it be good for CAD modeling?,pcmasterrace,2025-11-15 02:36:07,1
Intel,noujxb8,"maybe a slight bottleneck in 1080p, but nothing to worry about.",pcmasterrace,2025-11-14 18:16:33,1
Intel,nove0mw,"It seems like I'm still more bound by the GPU and maybe something else. I can see in MangoHUD in e.g. Tiy Tina's Wonderlands my GPU at 60-70% and my CPU only at 15-20%.  The Ryzen 9 3900X might not be the newest CPU anymore, but it's also not slow. It's a 12-core (24 thread) CPU, which boosts up to 4,6GHz",pcmasterrace,2025-11-14 20:50:55,1
Intel,novk38z,Exactly.,pcmasterrace,2025-11-14 21:22:45,1
Intel,novunxn,Loosen up,pcmasterrace,2025-11-14 22:18:34,4
Intel,nouaj9w,"read the comment again, slowly this time",pcmasterrace,2025-11-14 17:29:44,-10
Intel,nozfzop,"I got the 8gb, so I might run low on Vram in the future.  So it might be a bigger difference when you runout of Vram.",pcmasterrace,2025-11-15 14:45:18,1
Intel,noyruun,"I found in the UK, the 8gb models were £240, while the 16gb were selling above £300 and I assume msrp.  The b580s were always cheap though, last I was aware they were going under msrp at £215 for some models.  And the b570s under £180.",pcmasterrace,2025-11-15 12:01:00,1
Intel,nougf04,"For this dumbdumb, what do you mean 8 lanes?",pcmasterrace,2025-11-14 17:59:09,9
Intel,now9wmb,Yeah great value for sure,pcmasterrace,2025-11-14 23:47:47,2
Intel,noxb7hg,Which is good enough for 98% of gamers,pcmasterrace,2025-11-15 03:50:15,1
Intel,novxrxz,"I mean I have a 7900xtx so I'm fine as is, but it would benefit everyone having some actual competition in the high end",pcmasterrace,2025-11-14 22:35:47,1
Intel,now75ly,"According to the Steam hardware survey we all run mid level cards. So yeah, not enough pro 6000s out there for the users here.   All joking aside... I really want to try Intel Arc one day.  I was planning a build for my younger brother, and I'm really considering a B580.... In my market Intel is really price competitive vs Nvidia, specially in the mid level cards like the 5060. I also considered the MSI Claw but it apparently sucked for another reason.",pcmasterrace,2025-11-14 23:30:33,3
Intel,novee48,That all got fixed in the first month,pcmasterrace,2025-11-14 20:52:54,7
Intel,noviykm,Its really funny you felt compelled to comment this as a reply to this specific comment and not as a comment as a whole,pcmasterrace,2025-11-14 21:16:57,4
Intel,novpl2s,Like?,pcmasterrace,2025-11-14 21:51:14,2
Intel,np0yp7m,"Not exactly better, but there was some performance increase compared to the 6700XT. In scenarios I could use Intel XeSS instead of AMD FSR, I got more frames with increased image quality.  For comparison, I paid for a brand new B580 the same price of a used 6700XT. It was a no-brainer and I am not suffering from CPU bottleneck, which isn't a big issue since the .8xxx drivers.",pcmasterrace,2025-11-15 19:33:21,2
Intel,nowh8v4,"Woo! I love that thing, its so good for an igpu",pcmasterrace,2025-11-15 00:34:30,2
Intel,now2iws,It's not that's its budget. It's the support full stop. They are not supporting a use case that many use. They are throwing away sales.,pcmasterrace,2025-11-14 23:02:48,2
Intel,nouoveh,"It was early adopter teething issues mainly. I haven't had any issues recently, Intel has fixed A LOT of stuff over time. My main source of complaint now is just that their driver software is lacking some features compared to my previous Radeon card. I'd rather Intel focus on perfecting the basics rather than implementing a bunch of fancy driver features though",pcmasterrace,2025-11-14 18:40:49,3
Intel,nowsky0,A lot of the early issues was with older games. Intel didn’t have years of optimizations to make sure they run well especially with a tech change I can’t remember the details on. I believe they fixed it.,pcmasterrace,2025-11-15 01:47:25,2
Intel,np0qiu4,"Yeah, I would like to see a 16gb version or at least a better 12gb.",pcmasterrace,2025-11-15 18:50:22,1
Intel,nova84b,"Okay, that’s good to hear, though isn’t that more of a budget card? I’d *assume* the most common setup would be with an i3.  The main point of criticism I heard is that you can get a last-gen used AMD card for around the same price and get better performance overall. If that’s still true, then I’d withhold on saying it’s truly a good card. Just a step in the right direction.",pcmasterrace,2025-11-14 20:30:49,1
Intel,novgbnx,"Yeah, we even have our own subreddit, r/IntelArc",pcmasterrace,2025-11-14 21:03:10,5
Intel,nphslob,If Intel's version of XT and Ti was a -K suffix that would go so hard,pcmasterrace,2025-11-18 14:03:34,1
Intel,noxtt61,Pay more than 1000 bucks for a card. You have to tweak the power consumption or it starts burning. Big clown face.,pcmasterrace,2025-11-15 06:18:58,15
Intel,now09b1,My 2080ti used to be too. Then I removed the dumbass fucking blower and mounted an aftermarket kit on it. Best money of my life. Was like 50 bucks. Temps dropped AND its quiet.,pcmasterrace,2025-11-14 22:49:42,8
Intel,now2exc,undervolt it,pcmasterrace,2025-11-14 23:02:09,2
Intel,noxqunr,My 2080 super is the same lol,pcmasterrace,2025-11-15 05:52:09,1
Intel,noxndez,GanG!  ![gif](giphy|ctkp9BZJ3zMkGhVZgC),pcmasterrace,2025-11-15 05:22:27,2
Intel,noutjh4,They've confirmed since this deal that Celestial will happen,pcmasterrace,2025-11-14 19:03:57,7
Intel,nowvh6c,I am interested in how it works. Care to explain?,pcmasterrace,2025-11-15 02:06:13,1
Intel,noy7r0k,Okay. How does it work then?,pcmasterrace,2025-11-15 08:36:39,1
Intel,noy7q57,"I just think people are sniffing copium. I want intel to do well in gpu department as much as the next guy, but reality is different. Often dissapointing.  The things that intel has to do to be on-par with current competitiors is astounding. However, I do hope I'm wrong and intel *can* find a way.",pcmasterrace,2025-11-15 08:36:25,1
Intel,novij2r,"Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",pcmasterrace,2025-11-14 21:14:43,9
Intel,nowhj3f,"Do you do a lot of video editing? I'm not familiar with this, and I'm wondering at what point I would need a separate GPU for video encoding.",pcmasterrace,2025-11-15 00:36:13,1
Intel,nox0wht,"Smart, are you using a display streaming like parsec?",pcmasterrace,2025-11-15 02:41:10,1
Intel,noy1fbf,"That's why I picked up single slot low profile version. Slightly more expensive and power limited, but it doesn't block that much.",pcmasterrace,2025-11-15 07:32:13,1
Intel,noxf6q6,Might be! I haven't tested that myself.,pcmasterrace,2025-11-15 04:19:05,1
Intel,novlpmk,"its a productivity beast, but if youre frames are uncapped and your gpu isnt getting to 100 percent usage, youre cpu bottlenecked, as most games only use 1 or 2 cores usage percentage means very little",pcmasterrace,2025-11-14 21:31:06,1
Intel,novy1xc,loosen up those pcie lanes step bro 😩,pcmasterrace,2025-11-14 22:37:20,7
Intel,noucvez,"I was agreeing, although it may not read like that...",pcmasterrace,2025-11-14 17:41:44,13
Intel,novdfy3,"Say this to yourself in the mirror 3 times, no more, no less. It MUST be 3 times for this to work  Come back to this thread  Then behold!  ![gif](giphy|12NUbkX6p4xOO4)",pcmasterrace,2025-11-14 20:47:54,5
Intel,nouo4a0,I'm assuming 8 pcie lanes,pcmasterrace,2025-11-14 18:37:06,19
Intel,novg6ny,"8 pcie lanes instead of the usual 16 so half the bandwidth for a given generation.  The PCIe bus is used to share information between the card and the rest of the system. It's usually not a bottleneck (the GPU can't process data as fast as it comes in through the bus), but it's a noticeable one when you're low on VRAM and the GPU needs to fetch data in the system RAM through the PCIe bus. Mostly an issue with 8 GB cards like the 5060.",pcmasterrace,2025-11-14 21:02:25,2
Intel,np1amfi,Ah. Shame that the Intel cards never got a market in my country. Wouldve definitely snapped one when I built my PC,pcmasterrace,2025-11-15 20:38:55,1
Intel,noupyn1,"If I were to get one for my kiddo, would I expect more troubleshooting for Steam games or ability to play less of them?",pcmasterrace,2025-11-14 18:46:12,1
Intel,npi1cpr,Yeah K or KS would be cool though i think XE as nod to HEDT and the X299 platform with the i9 9980XE and i9 10980XE could also go super hard.,pcmasterrace,2025-11-18 14:49:59,1
Intel,noxve3u,Its only 1000 now?,pcmasterrace,2025-11-15 06:33:42,1
Intel,noy7bbq,$3000* 5090 is stupid expensive,pcmasterrace,2025-11-15 08:32:01,1
Intel,noxw47t,"If you are calling me a clown by not only getting the best card on the market for gaming, but also being able to save nearly half of the electrical cost AND getting a small performance boost on top of that, then you are the clown.",pcmasterrace,2025-11-15 06:40:35,-8
Intel,noxvvcx,Deshrouding rules,pcmasterrace,2025-11-15 06:38:14,1
Intel,noy13mv,"No, I don't do a lot. The main goal wa to use it as a display GPU, but the AV1 encoding is a nice bonus. I run OBS in replay mode (10 minutes) in the background and this intel GPU is used for encoding.  I want my Nvidia GPU to not have any monitors connected to it, so I can switch between using it in my host OS and in the VM, here's an example of how it works: [https://youtu.be/3fiXFv85iRU](https://youtu.be/3fiXFv85iRU)  This allows me to for example stream a game directly from a VM to my TV and still use my PC to do something else, like for example few weeks ago we played Trine 2 with my brother, when I was running the game in both the VM and host (one copy on Nvidia and one on Intel) and we played multiplayer together.  I used to use the VM to play everything like year or two ago, but the Nvidia drivers on linux improved significantly, so nowadays I don't use it nearly as much, but I still do from time to time. Plus it's always nice to have a backup plan for running some anticheats.",pcmasterrace,2025-11-15 07:29:00,2
Intel,noy04zy,"I'm using looking-glass. It's a bit different than parsec, it works by copying frames from the main GPU's framebuffer into the shared system memory segment and then to the 2nd GPU's memory. No compression and very low latency (I use virtual display in my Windows VM that I can set to 500Hz, which gives me around 2.5ms of added latency in total).  I also use Sunshine and Moonlight when straming to a TV or remotely to my laptop or phone. I have the cheapest server in OVH that I use as a VPN, so I can access my PC or share some services over internet (like my personal cloud for example: [https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM](https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM) ), so I can game from anywhere, just login over SSH to my home server, wake on lan PC, start a VM and connect using Moonlight).",pcmasterrace,2025-11-15 07:19:22,1
Intel,now85cz,"Good point, I guess it isn't necessarily the best gaming CPU, but I quite like its speed for compiling etc.  Getting some 5xxx CPU seems like just a small step up and wouldn't be cheap. And anything even higher would mean basically replacing everything: Mainboard, RAM and CPU. Going with a lower core count and higher boosts might be better for gaming, but would feel like too much of a downgrade.  Ironically most games, which I play besides Tiny Tina's Wonderlands, are such old titles, that both GPU and CPU are complete overkill. And lately I found just far too little time for gaming in general to make it worth it with the current prices to get some DDR5.  Edit: I just checked and at least according to Wikipedia the Ryzen 5000 CPUs aren't clocked all that differently. Its successor the 5900X has a boost of 4,8GHz, but same core count and same L3 cache and same TDP.",pcmasterrace,2025-11-14 23:36:44,1
Intel,noupm2u,I don't know what that means in layman's terms. Does it cap bandwidth like older hdmi resolution? Does it limit my kid's Minecraft terrain generation range? Does it mean I only run 69 mods on Skyrim instead of 420?,pcmasterrace,2025-11-14 18:44:29,2
Intel,nout6ep,"To be fair I don't play *that* many different games, but I haven't come across one that simply doesn't work. The biggest game compatibility issue I ever ran into was with StarCraft Remastered not rendering at full resolution. My theory is that they were specifically checking for Intel (what would've been only slow iGPUs at the time) and disabling high resolution. Blizzard fixed it though thankfully.   This video is over a year old, so things have only improved since then. I wouldn't be surprised if a lot of the issues shown have been addressed: https://youtu.be/Y09iNxx5nFE",pcmasterrace,2025-11-14 19:02:06,3
Intel,nowjqbz,For what I've played using an A750 card on steam the only game I've ever had issues with is Kenshi which would crash out on me for whatever reason but nothing else seems to have any noticeable issues.,pcmasterrace,2025-11-15 00:49:53,2
Intel,noz3h95,WTF. I corrected it a bit.,pcmasterrace,2025-11-15 13:28:20,1
Intel,noy63gl,The point is that premium product should be perfectly tuned out of the box.,pcmasterrace,2025-11-15 08:19:22,7
Intel,noyazgp,"There is always room to tweak. Fact is, the burning connector should not happen on a product that expensive.",pcmasterrace,2025-11-15 09:10:39,1
Intel,noyssdb,"It’s cool you’ve got the best GP that’s ever U-ed and it’s even cooler that you made it suck less (power), but the reason people are diasgreeing with you is because you’re defending an inarguably bad design with the argument of “but it works on my machine”.",pcmasterrace,2025-11-15 12:08:53,0
Intel,noz4pma,"Your not a clown, you’re the whole circus buddy",pcmasterrace,2025-11-15 13:36:16,0
Intel,noy4scy,"Wait, if you don't connect your Nvidia to monitors, does that mean the thing you see on monitors is only rendered by the Arc?    So, is it like how you stream from game pass? Where you let other PC run and calculate the game (your VM in this case) and then stream the game on your host OS?    Are there advantages in doing this? How are the graphics compared to directly using your Nvidia?  When you say that you rarely play on the VM nowadays, do you mean your Nvidia is connected to the monitors? Or is there a way to split the task without VM?  Also when you played Trine 2, does that mean the game looks better on TV than your PC? Since Ark is weaker.  Sorry for asking many questions lol",pcmasterrace,2025-11-15 08:05:59,1
Intel,nouwn20,"😂 Just has less bandwidth, but still really damn good.",pcmasterrace,2025-11-14 19:19:45,7
Intel,nowythh,"It caps the bandwidth. But that's fine because it's not strong enough to even utilize the full bandwidth that 16 lanes would provide. And those extra lanes would just increase cost.   This *may* be a problem on an old motherboard, because 8 lanes of PCIe *may* be too little (idk, didn't see any tests). But 8 lanes of PCIe4 are more than sufficient.",pcmasterrace,2025-11-15 02:27:44,3
Intel,noy9d8k,"You need to change your expectations if that is your point.  If you buy a stick of RAM today, it is going to run at half speed if you don't go into the bios and correct it. If you don't want to tune your PC, don't build one yourself.",pcmasterrace,2025-11-15 08:53:32,-7
Intel,np2cs9i,Looks like we found someone poor AND stupid over here.,pcmasterrace,2025-11-16 00:20:04,1
Intel,noyj6di,"https://preview.redd.it/5utu2xxvee1g1.png?width=5240&format=png&auto=webp&s=a281c9815d6408496deeaa752ed61862de335754  No, I can choose which GPU I use for rendering. That's basically what laptops are doing this with multiple GPUs. I just set these environmental variables in the steam's launch parameters: DXVK\_FILTER\_DEVICE\_NAME=""NVIDIA"" \_\_NV\_PRIME\_RENDER\_OFFLOAD=1 \_\_GLX\_VENDOR\_LIBRARY\_NAME=nvidia   and it will run on Nvidia. I can do the same with any other app, like browser, blender, etc. The screenshot shows an example of the game running bare-metal on nvidia-gpu and being displayed on intel GPU.  To not repeat the same parameters with every game, I've made a simple bash script that just exports these variables (and few more, like DLSS upgrade, etc), so I just type ""setenv %command%"" in steam lauinch parameters and that's it.  When running a VM, I use looking-glass, which is different than streaming the game to another device over LAN (like in your example with gamepass). Looking-glass uses shared memory to copy frames from one device to another, so it only works within one PC, but the video is uncompressed and with minimal latency (like 2-3ms of added latency). This is essentially the same path as offloading rendering but with one additional limitation - windows forces vsync, so I need to run as high refresh rate inside windows as possible to reduce latency. I use Virtual display adapter that allows me to set up to 500Hz and run desktop without any physical monitors attached.",pcmasterrace,2025-11-15 10:36:56,2
Intel,noykase,"Trine 2 is not that demanding, both GPUs can run it at max settings just fine. This is also true with a lot of older games. With new games it wouldn't work like this, but there is another way. Linux has a built-in multiseat, that allows logging in 2 different users at the same time, giving them access to different monitors, input devices, etc. I could use a 4k dummy plug to enable multiseat on that device and stream from there to the TV, but I don't have it configured right now, as I still need VM sometimes, so usually it's enough.",pcmasterrace,2025-11-15 10:48:26,1
Intel,novcfue,"So like a v6 Camaro? Not as fast at the suped up v8, but still better than a Corolla?",pcmasterrace,2025-11-14 20:42:36,4
Intel,noybaib,"I think it’s less about “not getting the full performance”, and more about it being a fire hazard.   Especially when you pay that kind of money for it.    In fact, one could argue that the consumer shouldn’t be able to make your product into a fire hazard, even if they aggressively fuck around with software parameters. As long as no modifications were done to the hardware - it should be perfectly safe. Sure it may not function properly, but it shouldn’t melt itself.",pcmasterrace,2025-11-15 09:13:51,2
Intel,noyhewb,"Your ram clocks actually depends on your other hardware, its not the case for gpu. Its fine that you like it, but other may not, and nothing on the box say i need to understand something about overcklocking\\downcklocking. Also i doubt that some random dude will spend time to play with your gpu settings if you buy prebuilt, you will just get it as it is.",pcmasterrace,2025-11-15 10:18:19,0
Intel,novljpy,"More like the transmission than the engine. PCIe lanes determine how much compute (horsepower) can get to the CPU (Road) so if your GPU (Engine) is performing really well but is bottlenecked by PCIe lanes, you aren't really getting that level of performance.   It's also why this objection to the Intel cards is a bit of a moot point. If you are running a v6 than you don't need the best transmission, the car isn't making the power to necessitate more HP to the wheels.",pcmasterrace,2025-11-14 21:30:15,9
Intel,noydha4,"I'm the guy that bypasses restrictive software and hardware to do what I want, so I won't advocate for any more dummy proofing. That is one of the reasons I hate Apple.",pcmasterrace,2025-11-15 09:37:18,-1
Intel,now1zef,"So sounds like an Average Joe won't notice, especially compared to other cards of the same price?",pcmasterrace,2025-11-14 22:59:36,1
Intel,noylp79,"I don’t mind tinkering(in fact I enjoy it a lot, both at my job as a software dev, and as a hobby).  However, hardware+firmware(when not modified) should not allow software to set unsafe parameters, period. And the fact that those unsafe parameters are set from the factory - is all the more insane.",pcmasterrace,2025-11-15 11:02:45,1
Intel,now4agd,Of all the differences between cards at that price range the number of PCIe lanes it has will be the last thing you'll notice.,pcmasterrace,2025-11-14 23:13:14,3
Intel,nsxo1v3,"decent enough, don't listen to people who says it's underpowered. enjoy it to fullest, good luck, have fun.",pcmasterrace,2025-12-08 13:48:18,7
Intel,nsxrbml,1/10,pcmasterrace,2025-12-08 14:07:37,2
Intel,nsxo04x,9.8/10,pcmasterrace,2025-12-08 13:48:00,1
Intel,nsxo6q4,"I’m not sure that board supports the 2600. B550 boards typically dropped support for first and second gen Ryzen processors, it’s one of the reasons I went for a B450 board for my 1600AF build.",pcmasterrace,2025-12-08 13:49:06,1
Intel,nsxpvxr,Seems a decent enough budget build. As long as you don't plan on doing any heavy gaming with it (i.e. titles with high recommended / minimum specs) it should serve you well.,pcmasterrace,2025-12-08 13:59:07,1
Intel,nsxvvgo,"you can later upgrade to a RX 7700 xt or RX 9060 xt but you will be CPU bound at that point, RX 5700Xt is obsolete now but can pack a punch in that system, Nvidia 3060/ti too.  my recommendation would be the Rx 7700 xt or rx 9060 xt if you plan to upgrade later, at least you’ll have the gpu ready.",pcmasterrace,2025-12-08 14:33:54,1
Intel,nsy7bkf,it has RAM. thus it is a premium build.,pcmasterrace,2025-12-08 15:34:45,1
Intel,nt87ktw,Sorry but the 2600 is dogshit look for a 3500x or 3600 on AliExpress they go as low as 45 bucks   Gpu wise 5700xt you can get for about 100 used,pcmasterrace,2025-12-10 03:06:11,1
Intel,nsxoy1r,That's just a some parts.  Where build?,pcmasterrace,2025-12-08 13:53:36,1
Intel,nsxo5sw,Thanks!,pcmasterrace,2025-12-08 13:48:57,1
Intel,nsxrnxh,Ok buddy,pcmasterrace,2025-12-08 14:09:39,1
Intel,nsxqswv,It does I tested it,pcmasterrace,2025-12-08 14:04:31,1
Intel,nsxwft9,Well I was thinking of Rx 6600 it's a good enough card for me.,pcmasterrace,2025-12-08 14:37:07,1
Intel,nsyy5js,LoL yeah in my country the shortage isnt affecting us which is pretty lucky (well not yet at least),pcmasterrace,2025-12-08 17:46:59,1
Intel,nsxqztk,I mean case is not in the photo and the peripherals like monitor keyboard etc.,pcmasterrace,2025-12-08 14:05:40,1
Intel,nsxs7jl,"Its a PC with 7-10 yr old parts. Its not gonna run any modern game. I dont know what kind of sugrarcoating you expected here, but 1/10 is about right",pcmasterrace,2025-12-08 14:12:49,2
Intel,nsxwxya,"6600 is similar to a Rx 5700 xt, if you consider pricing the 9060 xt 8gb is around the same price as the 6600 xt so I do not see a point in buying the 6600xt",pcmasterrace,2025-12-08 14:39:58,1
Intel,nsxr9ip,Maybe actually build the thing first.,pcmasterrace,2025-12-08 14:07:16,0
Intel,nsxufrj,Yeah but that's the thing I'm not trying to start up black myth woukong with this thing and as I said it's a temporary card I will buy Rx 6600 when I get the money,pcmasterrace,2025-12-08 14:25:40,3
Intel,nsy1m2c,Yeah not in my country though it's a big difference in price over in my country between Rx 6600 xt and Rx 9060 xt,pcmasterrace,2025-12-08 15:05:19,1
Intel,ns23ryz,Have you updated your BIOS? Sometimes helps with memory errors.,pcmasterrace,2025-12-03 12:53:05,1
Intel,ns23z5a,"Are you overclocking or undervolting? This includes the AMD Curve Optimizer.  If no you can also try disabling XMP and rerun the memory test, see if that makes a difference.",pcmasterrace,2025-12-03 12:54:24,1
Intel,ns2amhj,I’m not sure if it’s fixed yet or not but there were issues with the 25.11.1 driver version. I downgraded to what 25.9.1 and stopped having the issues you did.,pcmasterrace,2025-12-03 13:35:26,1
Intel,ns26d59,"Not yet. Condidering doing that. Should I ?  EDIT : now runnning a MEMORY test on OCCT at 90%. So far so good, no more errors. Will update after a 10+ minutes test. Fingers crossed.   EDIT 2 : no error after 15 minutes. Thank you.",pcmasterrace,2025-12-03 13:09:45,1
Intel,ns24wdf,"No I did not over or underclock other than inside the AMD adrenalin program. But I got it back to default after getting those errors. But they keep happening.   Also, A-XMP is not active in my BIOS.",pcmasterrace,2025-12-03 13:00:23,1
Intel,ns2b2wy,It may be helping but I mainly play with two friends and they have 25.11.1 too and they do not have such problems... So I am considering other solutions first. Thanks anyway for your advice.,pcmasterrace,2025-12-03 13:38:08,1
Intel,ns26ogn,"Remove all but one stick of RAM and rerun the memory test, see if you can identify whether the errors follow one or more specific sticks.",pcmasterrace,2025-12-03 13:11:44,1
Intel,ns2cab6,"It doesn’t hurt anything to downgrade though? Also, 25.11.1 works for me now also but that was after a recent upgrade to an RX 9070 XT and R7 7800x3D.",pcmasterrace,2025-12-03 13:45:08,1
Intel,ns2co99,"no no ofc it does not, but I tried updating my Bios (an advice I got from another sub before yours) and now I am running a memory stress test and it goes without all the errors I got before. After that I will try to play for a few hours and if the crashes still occur, then I will tried downgrading my adrenalin drivers.",pcmasterrace,2025-12-03 13:47:21,1
Intel,ns2d20b,"Sounds good fam, good luck and see you topside (after this gruesome shift).🫡",pcmasterrace,2025-12-03 13:49:32,2
Intel,ns2dux5,o7,pcmasterrace,2025-12-03 13:54:08,2
Intel,ns4hewc,Surprising to hear them say that since they still have essentially F2P cosmetic monetization in the game? Unless their initial plan was to aggressively gate progression through currency or excessive grinding.,pcmasterrace,2025-12-03 20:04:57,6
Intel,ns4joqu,They felt they didn't need to make the game so grind based unlike f2p games that requires them to come up with grinding mechanics to keep players coming back and staying longer.,pcmasterrace,2025-12-03 20:16:00,2
Intel,ns4i6fg,If you make a free game you have to model it form the hround up with that in mind,pcmasterrace,2025-12-03 20:08:43,5
Intel,nsvvlqo,Have you tried verifying for a 100% fact that your GPU is not thermal throttling?,pcmasterrace,2025-12-08 04:27:14,3
Intel,nsvvung,"I would start to make sure that you have XMP enabled, That your system is clean, i.e dust/hair. I also have an 7900xt, but pair it with a 78003d. you should be able to get 120fps on Destiny at 1440p. Do you have AMD: Adrenalin? it will help tweak  your GPU and see if you could get some more fps out of games. Are temps abnormally high? I pull about 80-83c with mine Playing BF6 Ultra 1440p.",pcmasterrace,2025-12-08 04:28:55,1
Intel,nsvvw8z,What cpu? What are the temps like for gpu and cpu?,pcmasterrace,2025-12-08 04:29:14,1
Intel,nsvx0ix,I just repasted my XTX this weekend due to my hot spot hitting 106 degrees. They’re now at 65 and it seemed to have fixed my issues. My normal temps were in the 50s if it just looked at the overlay. Could be worth taking a look.,pcmasterrace,2025-12-08 04:37:04,1
Intel,nsw06g2,"I had this issue but after a rebuild. Cpuz was showing my pcie speed at x1. Should be at 16. I didn’t have my card seated correctly, which is crazy it still worked. Reseated my card and verified it was back at 16.",pcmasterrace,2025-12-08 05:00:07,1
Intel,nt4soh1,Vram at 80 whole the rest of the gpu is only 50 degrees. Only 60 on the hot-spot. How's that even possible. Does your case have some mad crosswinds only hitting the back of the GPU or something,pcmasterrace,2025-12-09 16:15:44,1
Intel,nsvvypk,"No, but this is a new perspective I haven't considered. temps are around 40-45 C during gaming despite high utilization",pcmasterrace,2025-12-08 04:29:43,2
Intel,nsvwey7,No actually temps are abnormally low during gaming. While having D2 running i’m getting 40-45 C. I definitely remember my card hitting around 80 before without issues.,pcmasterrace,2025-12-08 04:32:53,1
Intel,nsvw908,"13600K. during gaming the temps are around like 45-55 for CPU, 40-45 GPU. from what I remember my temps for both used to get a lot hotter during gaming.",pcmasterrace,2025-12-08 04:31:45,1
Intel,nsvx7bx,For me hot spot temps are only at around 48C so i’m not sure if there’s an issue with that.,pcmasterrace,2025-12-08 04:38:25,1
Intel,nsw0hob,where did you see that in cpu-z?,pcmasterrace,2025-12-08 05:02:28,1
Intel,nswglc8,You're monitoring the wrong GPU.,pcmasterrace,2025-12-08 07:23:23,3
Intel,nsvwu58,"Make sure the system is clean of dust and that all your OCs like XMP are proper.   Otherwise, idk. I used to have a 1080Ti that would get me +80-100 FPS on Tarkov at 1440p resolution, but nowadays it can't even do 60 and that's after an extensive cleaning, repaste, and overclock. But that's a much older card than 7900 XT.",pcmasterrace,2025-12-08 04:35:51,1
Intel,nswug1z,Wrong temps Check gpu Hotspot,pcmasterrace,2025-12-08 09:45:54,1
Intel,nswy2r6,under high utilisation? unless your room is like 5C ambient or you have a custom water loop that can't be right.  download hwinfo64 and check gpu hotspot and vram temps.,pcmasterrace,2025-12-08 10:22:48,1
Intel,nsx10jg,Impossible 45 is like idle for 7900xt. Also powedraw should be  higher 300+. I have like 330     Maybe he had Vsync on,pcmasterrace,2025-12-08 10:51:55,1
Intel,nsvxck5,"thats really low for temps, thats idle temp for most CPU/GPU. I'm thinking it may have to be a power delivery problem, or MAYBE it switch to integrated graphics instead of dedicated graphics, when running a game. I know it happens.",pcmasterrace,2025-12-08 04:39:25,4
Intel,nsw24e4,There is definitely something wrong there. There is no way any system could run at such low temps under full load. Can you run some tests in 3dmark / furmark and share results?,pcmasterrace,2025-12-08 05:15:01,1
Intel,nsw0q4j,"https://preview.redd.it/j4s6zwrdxw5g1.jpeg?width=3024&format=pjpg&auto=webp&s=309b47bc32b09cfdf696264694e326d88c1464b6  Current link Width , under main board",pcmasterrace,2025-12-08 05:04:13,1
Intel,nsz2rc4,https://www.gpumagick.com/scores/1937438  Wish i had screenshots of usage/temp in game but using furmark i was getting this. In game the hotspot wasn’t really getting over 60-65 despite drawing over 300W. I’ll do some more testing after work and see if i can get some screenshots during gaming.,pcmasterrace,2025-12-08 18:09:24,1
Intel,nsz2xjt,"I didn’t have vsync on, and have a 300hz monitor so that wouldn’t make sense why it was averaging around 120. And yes, i checked to make sure the refresh rate was 300.",pcmasterrace,2025-12-08 18:10:14,1
Intel,nsw9ho1,https://preview.redd.it/ubk5ek7dax5g1.png?width=2548&format=png&auto=webp&s=bf4499ce2bb3c491c58a0fbaf8f0dddca8e6aa80,pcmasterrace,2025-12-08 06:16:53,1
Intel,nsw9v6l,[https://www.gpumagick.com/scores/1937438](https://www.gpumagick.com/scores/1937438),pcmasterrace,2025-12-08 06:20:14,1
Intel,nsw57vz,"gotcha, yeah mine says x16 so can’t be that. thanks though",pcmasterrace,2025-12-08 05:40:05,1
Intel,nswz8dw,"I happen to have a 7900XT in 2nd rig, so went and did a quick furmark on the same settings (1440p) and can see very similar results, mine is running stock while I think you have Power Limit +10%/+15% hence the slightly higher power draw/clocks.  https://ibb.co/23Jg0kRJ  I'd look elsewhere for your issues, maybe overlays or stuff running in background, do you have screen recording/radeon relive/discord overlay/gamebar recording/etc.",pcmasterrace,2025-12-08 10:34:34,1
Intel,nswh6at,"As strange as it is, I can't see anything wrong there. Temps are ridiculous tho...   Well I can't help, maybe somone else could figure it out. The one more thing crossing my mind is that's something software related. And if so, then clean install of Windows won't hurt.",pcmasterrace,2025-12-08 07:29:09,1
Intel,nrpecd0,"tfw your friend REALLY wants you to play with him lmao     In any case congrats, enjoy your new build, a well deserved upgrade",pcmasterrace,2025-12-01 13:17:18,3
Intel,nrpvi7l,You think you waited long enough?,pcmasterrace,2025-12-01 14:59:23,2
Intel,nrpf5zl,You skipped DDR4 entirely!! Awesome upgrade 🤩,pcmasterrace,2025-12-01 13:22:33,1
Intel,nrpf49h,Thanks! Can't wait to be able to render literally anything in the distance. Feel like I've been playing a commodore 64 up to now.,pcmasterrace,2025-12-01 13:22:15,2
Intel,nrpxwmh,Haha! Time flies.,pcmasterrace,2025-12-01 15:12:37,1
Intel,nrpgmub,"I feel you, had about the same computer than your previous one still earlier this year, i7 4790K, 1080 Ti and 16Gb of DDR3, I can easily understand how you felt, luckily for me the 1080 Ti hold the ground quite well, I wouldn't have upgraded yet if it didn't die on me lol.     Like Careless said, we both skipped DDR4 ahah",pcmasterrace,2025-12-01 13:31:50,1
Intel,nrphmya,"I got the gpu before the rest, so I tried it in my PC but my CPU was running at like 130% for Arc Raiders lol I had no choice but to be friendly, god help me if I got into a shootout. And with BF6, I could only barely run redsec, I tried conquest and my PC nearly had a heart attack.",pcmasterrace,2025-12-01 13:38:02,2
Intel,nrpvffj,how do you know its the DPC latency,pcmasterrace,2025-12-01 14:58:57,1
Intel,nrqvklj,I'm using a dpc latency detecting software and have all the symptoms of high dpc latency,pcmasterrace,2025-12-01 18:01:36,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,84
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,76
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,19
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,11
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,16
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,8
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,26
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,5
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,6
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,5
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,17
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,6
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,4
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,8
Intel,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,1
Intel,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,Did AI create these new drivers?,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,1
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,0
Intel,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,22
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,6
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,27
Intel,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,4
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,4
Intel,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,3
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,99
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,3
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,5
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,7
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,6
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,6
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,11
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,6
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,18
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,5
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,6
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,7
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,3
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,6
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,5
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,1
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,4
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,4
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,5
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,10
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,13
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,16
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,18
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,16
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,22
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,14
Intel,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,4
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,6
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,5
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,11
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,4
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,6
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,4
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,6
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,8
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,6
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,1
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,6
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,1
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,46
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,13
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,7
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,23
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,1
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,5
Intel,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,3
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,5
Intel,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,3
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,4
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,2
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,1
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,3
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,22
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,7
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,7
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,10
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,6
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,4
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,4
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,1
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,1
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,158
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,82
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,49
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,4
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,5
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,5
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,48
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,19
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,15
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,12
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,9
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,5
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,3
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,6
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,4
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,5
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,5
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,22
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,14
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,6
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,31
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-6
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,8
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,80
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,12
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,24
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,17
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-17
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-16
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,5
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,10
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,56
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,21
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-13
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,45
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-4
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,24
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,2
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-8
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-9
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,16
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,4
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,12
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,4
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,31
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,9
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-2
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,5
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,21
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,13
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,5
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,10
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,3
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,3
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,223
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,25
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,108
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,17
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,30
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,121
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,70
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,8
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,36
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,34
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,60
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,25
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,21
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,27
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,5
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,13
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,20
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,4
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,1
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,2
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,3
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,3
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-7
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-22
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-4
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,43
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,26
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,16
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,34
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,20
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,7
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,5
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,7
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,3
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,5
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,29
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,9
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,7
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-5
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,23
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,4
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,2
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,7
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,3
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,4
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-14
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,14
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,0
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-1
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,29
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,5
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,43
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,30
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,5
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,12
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,3
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,8
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,9
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,0
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,0
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,9
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,20
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,5
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,4
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-14
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,-1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,31
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,30
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,-1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-4
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,8
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,4
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,11
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,-2
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,11
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,9
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,8
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,6
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-3
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,2
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,7
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-2
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,2
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-6
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,0
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,9
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-14
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-12
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,4
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,1
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,6
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,13
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,3
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,26
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,4
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,14
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,22
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,18
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,5
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,11
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,14
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,7
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,2
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,7
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,0
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,3
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-7
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-5
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-1
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-2
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,4
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,8
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,17
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,6
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,5
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-1
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,2
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-1
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,0
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,12
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,2
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,1
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,9
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,14
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,15
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,6
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,6
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,2
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,3
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,kb07k5w,"As you notice the photoshop version differs, so you can't compare them really",AMD,2023-11-27 18:28:41,5
Intel,kao517l,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),AMD,2023-11-25 07:14:52,2
Intel,k74d0ev,So basically PC games are never going to tell us what the specs are to run the game native ever again.,AMD,2023-10-30 18:21:26,542
Intel,k751wfu,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",AMD,2023-10-30 20:52:50,47
Intel,k748hf1,The true crime here is needing FSR to reach these requirements.,AMD,2023-10-30 17:53:41,193
Intel,k74fig7,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",AMD,2023-10-30 18:36:48,36
Intel,k74fyp7,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),AMD,2023-10-30 18:39:33,16
Intel,k74c3qr,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,AMD,2023-10-30 18:15:55,64
Intel,k7407mp,"well, at least the chart is easy to read, not a complete mess",AMD,2023-10-30 17:03:14,17
Intel,k7466bs,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,AMD,2023-10-30 17:39:45,11
Intel,k741zrd,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,AMD,2023-10-30 17:14:11,58
Intel,k748t1r,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,AMD,2023-10-30 17:55:39,51
Intel,k76jevo,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",AMD,2023-10-31 02:51:26,5
Intel,k76we4z,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",AMD,2023-10-31 04:46:34,9
Intel,k75qe4v,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,AMD,2023-10-30 23:33:24,5
Intel,k74d5ad,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",AMD,2023-10-30 18:22:16,25
Intel,k748nsg,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",AMD,2023-10-30 17:54:45,11
Intel,k74kjmk,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",AMD,2023-10-30 19:07:25,19
Intel,k7533wp,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",AMD,2023-10-30 21:00:16,3
Intel,k77mpgx,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,AMD,2023-10-31 10:35:56,3
Intel,k74hphk,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,AMD,2023-10-30 18:50:08,12
Intel,k773lbl,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",AMD,2023-10-31 06:11:46,6
Intel,k74eavi,I hope they will bundle this game with CPUs/GPUs,AMD,2023-10-30 18:29:20,2
Intel,k74fxtg,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,AMD,2023-10-30 18:39:23,2
Intel,k77022h,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,AMD,2023-10-31 05:27:42,2
Intel,k78gl5o,This game better look like real life with those specs. I does looks beautiful!,AMD,2023-10-31 14:43:10,2
Intel,k75as90,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",AMD,2023-10-30 21:48:53,5
Intel,k748ctr,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,AMD,2023-10-30 17:52:56,6
Intel,k74mjvt,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,AMD,2023-10-30 19:19:43,2
Intel,k74oqng,I love it how absurd these things are these days.,AMD,2023-10-30 19:33:13,2
Intel,k74awed,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,AMD,2023-10-30 18:08:29,1
Intel,l06s6dc,Does anyone know if it supports SLI or crossfire?,AMD,2024-04-18 19:16:23,1
Intel,k73yt0i,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,AMD,2023-10-30 16:54:36,0
Intel,k74kcre,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",AMD,2023-10-30 19:06:17,1
Intel,k74mb9y,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,AMD,2023-10-30 19:18:13,1
Intel,k74oabx,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,AMD,2023-10-30 19:30:24,1
Intel,k74sd5w,Why in the f*ck is upscaling included on a specs page?,AMD,2023-10-30 19:55:26,1
Intel,k77tobm,no more software optimization and full upscaling  bleah,AMD,2023-10-31 11:50:04,1
Intel,k749m34,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,AMD,2023-10-30 18:00:34,-3
Intel,k74d7ll,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",AMD,2023-10-30 18:22:41,0
Intel,k74phj0,"I've never even heard of this game, nor care about it, but these system requirements offend me.",AMD,2023-10-30 19:37:49,-1
Intel,k79vqgg,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,AMD,2023-10-31 20:00:03,0
Intel,k744khv,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",AMD,2023-10-30 17:29:53,-1
Intel,k79zrsa,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",AMD,2023-10-31 20:25:07,0
Intel,k7c9frd,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,AMD,2023-11-01 08:20:56,0
Intel,k75roib,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,AMD,2023-10-30 23:42:07,-3
Intel,k741y61,Nice,AMD,2023-10-30 17:13:54,-1
Intel,k765iq2,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",AMD,2023-10-31 01:14:49,0
Intel,k77fyxm,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",AMD,2023-10-31 09:08:01,0
Intel,k77zvvn,Native gaming died or what ? Wtf they turning pc gaming into console gaming,AMD,2023-10-31 12:44:58,0
Intel,k76r1ve,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",AMD,2023-10-31 03:53:47,-1
Intel,k74pme9,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,AMD,2023-10-30 19:38:39,1
Intel,k74qrim,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,AMD,2023-10-30 19:45:38,1
Intel,k74t7ty,"Omg, it would be a graphic master piece or  bad optimized thing.",AMD,2023-10-30 20:00:39,1
Intel,k74ybxr,First time I see matches recommendations for nv and amd GPUs...,AMD,2023-10-30 20:31:22,1
Intel,k756ha4,Rip laptop rtx 3060 6gb,AMD,2023-10-30 21:21:24,1
Intel,k759rkx,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",AMD,2023-10-30 21:42:14,1
Intel,k75e3m3,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,AMD,2023-10-30 22:10:21,1
Intel,k75ej2l,*NATIVE* resolution gang ftw!,AMD,2023-10-30 22:13:11,1
Intel,k75k1fl,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",AMD,2023-10-30 22:50:05,1
Intel,k75n4rl,Looks capped at 60fps?,AMD,2023-10-30 23:11:06,1
Intel,k75ops0,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",AMD,2023-10-30 23:21:57,1
Intel,k75qeie,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,AMD,2023-10-30 23:33:28,1
Intel,k760or5,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",AMD,2023-10-31 00:42:35,1
Intel,k77465m,Is it using UE5?,AMD,2023-10-31 06:19:31,1
Intel,k77bz45,"Ubisoft, rip on launch.",AMD,2023-10-31 08:09:13,1
Intel,k77htgp,guessing no DLSS3 then?,AMD,2023-10-31 09:33:36,1
Intel,k77l01d,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,AMD,2023-10-31 10:15:17,1
Intel,k77twmu,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",AMD,2023-10-31 11:52:12,1
Intel,k77x5ay,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",AMD,2023-10-31 12:21:45,1
Intel,k77z2e6,Farewell 1660ti… looks like it’s time for an upgrade,AMD,2023-10-31 12:38:12,1
Intel,k78p32u,well my 3300x is now obsolete for these new AAA games...,AMD,2023-10-31 15:37:57,1
Intel,k798boq,4k ultra right up my alley 😏,AMD,2023-10-31 17:36:35,1
Intel,k799x2x,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,AMD,2023-10-31 17:46:17,1
Intel,k7ec5fz,7900xtx will do 4k 120fps with FSR 3 then I guess?,AMD,2023-11-01 18:22:39,1
Intel,k7fkb1o,What must one do to achieve a higher rank than an enthusiast? A demi-god?,AMD,2023-11-01 22:56:10,1
Intel,k7gz4pf,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",AMD,2023-11-02 05:23:56,1
Intel,k7k2rqe,Is vrr fixed with frame gen then?,AMD,2023-11-02 20:32:54,1
Intel,k859q58,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",AMD,2023-11-07 00:16:09,1
Intel,k8kv1d8,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",AMD,2023-11-10 00:27:13,1
Intel,kaeixym,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,AMD,2023-11-23 05:18:15,1
Intel,k74h55l,They recommend upscaling even at 1080p.. disgusting ew,AMD,2023-10-30 18:46:41,218
Intel,k75hnu4,pretty much this we all knew they would start using upscaling as a crutch.,AMD,2023-10-30 22:34:04,14
Intel,k76ze7a,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,AMD,2023-10-31 05:19:49,5
Intel,k74f7tr,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",AMD,2023-10-30 18:35:00,25
Intel,k74e6u4,My thoughts too…,AMD,2023-10-30 18:28:39,4
Intel,k778snb,Thanks to all of you that were screaming dlss looks better than native lmao.,AMD,2023-10-31 07:23:19,5
Intel,k79niu3,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",AMD,2023-10-31 19:09:38,1
Intel,k77adqn,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,AMD,2023-10-31 07:46:18,1
Intel,k74x8ab,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",AMD,2023-10-30 20:24:48,-1
Intel,k74zbiw,Nope we as a community abused a nice thing,AMD,2023-10-30 20:37:19,1
Intel,k755j7t,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",AMD,2023-10-30 21:15:25,-6
Intel,k75o59y,Didn't take them long to make upscaling worthless.,AMD,2023-10-30 23:18:03,-1
Intel,k78qsdu,i smell a burgeoning cottage industry of game spec reviewers!,AMD,2023-10-31 15:48:28,0
Intel,k7hhgqi,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",AMD,2023-11-02 09:36:54,0
Intel,k7k4fzn,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",AMD,2023-11-02 20:43:01,0
Intel,k74js59,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",AMD,2023-10-30 19:02:44,-2
Intel,k78m5tc,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,AMD,2023-10-31 15:19:10,-1
Intel,k74pa1g,yeah this is the new standard,AMD,2023-10-30 19:36:32,1
Intel,k77dlbv,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",AMD,2023-10-31 08:33:14,25
Intel,k78n190,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,AMD,2023-10-31 15:24:48,1
Intel,k77nqtn,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",AMD,2023-10-31 10:47:59,-6
Intel,k74z5qq,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",AMD,2023-10-30 20:36:21,14
Intel,k76jxv9,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",AMD,2023-10-31 02:55:18,2
Intel,k76rqab,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,AMD,2023-10-31 04:00:06,2
Intel,k75qnpb,The actual true crime here is even having fsr to begin with. It should just have dlss,AMD,2023-10-30 23:35:12,-5
Intel,k76ju0a,Seems like they tried to cover every basis with these.,AMD,2023-10-31 02:54:33,7
Intel,k75ao0c,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",AMD,2023-10-30 21:48:08,10
Intel,k74htwr,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",AMD,2023-10-30 18:50:52,14
Intel,k74jsww,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,AMD,2023-10-30 19:02:51,3
Intel,k760d0a,"GCN support is over, RDNA1 is the lowest currently supported arch.",AMD,2023-10-31 00:40:29,6
Intel,k74k4hi,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",AMD,2023-10-30 19:04:51,3
Intel,k77v39i,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",AMD,2023-10-31 12:03:13,0
Intel,k74efc2,What do you mean forced raytracing?,AMD,2023-10-30 18:30:05,11
Intel,k77dids,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,AMD,2023-10-31 08:31:59,6
Intel,k74fapr,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",AMD,2023-10-30 18:35:29,16
Intel,k7gd959,It's actually 960p :(,AMD,2023-11-02 02:12:51,2
Intel,k78nhhp,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",AMD,2023-10-31 15:27:45,0
Intel,k74go33,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,AMD,2023-10-30 18:43:49,7
Intel,k78s16k,We'll see in a year.,AMD,2023-10-31 15:56:02,-1
Intel,k744q6u,AFAIK  &#x200B;  It is with RT,AMD,2023-10-30 17:30:51,24
Intel,k745n5v,Seems pretty good to me given it is at 4K with RT,AMD,2023-10-30 17:36:31,18
Intel,k743fzm,"Likely includes the RT features , its also 4k",AMD,2023-10-30 17:23:02,9
Intel,k743sfv,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",AMD,2023-10-30 17:25:06,-15
Intel,k74aacw,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",AMD,2023-10-30 18:04:43,35
Intel,k74c9q7,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",AMD,2023-10-30 18:16:57,23
Intel,k78pyma,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,AMD,2023-10-31 15:43:22,1
Intel,k771jw5,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",AMD,2023-10-31 05:46:01,1
Intel,k7aj8ac,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",AMD,2023-10-31 22:34:23,2
Intel,k77bs4b,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,AMD,2023-10-31 08:06:28,1
Intel,k75qm0a,Exactly! It even got xess so Intel users also can use xess,AMD,2023-10-30 23:34:53,2
Intel,k75k4vk,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",AMD,2023-10-30 22:50:44,3
Intel,k74wzhj,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",AMD,2023-10-30 20:23:22,6
Intel,k7708p1,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,AMD,2023-10-31 05:29:56,4
Intel,k77n6u7,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,AMD,2023-10-31 10:41:33,3
Intel,k75ya7o,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",AMD,2023-10-31 00:26:50,3
Intel,k7a5u7g,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",AMD,2023-10-31 21:03:27,0
Intel,k77o6pg,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",AMD,2023-10-31 10:53:04,1
Intel,k755rf9,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",AMD,2023-10-30 21:16:52,4
Intel,k75jput,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",AMD,2023-10-30 22:47:54,3
Intel,k761p0l,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",AMD,2023-10-31 00:49:12,2
Intel,k75i4pq,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,AMD,2023-10-30 22:37:13,2
Intel,k77cvge,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",AMD,2023-10-31 08:22:22,-1
Intel,k755zaf,"Mirage is PS4 game, Avatar is PS5",AMD,2023-10-30 21:18:15,7
Intel,k74a84y,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",AMD,2023-10-30 18:04:20,4
Intel,k75qyxk,Timed epic exclusivity? Aww man.,AMD,2023-10-30 23:37:17,3
Intel,k74vid8,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,AMD,2023-10-30 20:14:32,0
Intel,k74dkaf,You... are.. joking... right..?,AMD,2023-10-30 18:24:49,3
Intel,k770p8h,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,AMD,2023-10-31 05:35:28,2
Intel,k7ftdl4,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,AMD,2023-11-01 23:57:41,3
Intel,k74azia,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,AMD,2023-10-30 18:09:00,0
Intel,k749l36,Try ubisoft achievements :),AMD,2023-10-30 18:00:24,-1
Intel,k7a8c78,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",AMD,2023-10-31 21:19:36,1
Intel,k767klo,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",AMD,2023-10-31 01:28:47,1
Intel,k783twl,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,AMD,2023-10-31 13:15:34,0
Intel,k76sj05,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),AMD,2023-10-31 04:07:38,3
Intel,k76mhsf,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",AMD,2023-10-31 03:14:45,3
Intel,k747o6w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-30 17:48:49,1
Intel,k74w3s3,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",AMD,2023-10-30 20:18:07,0
Intel,k75if4b,yup that is why I will play 1440 UW native with a 7900XTX.,AMD,2023-10-30 22:39:08,1
Intel,k74vwla,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,AMD,2023-10-30 20:16:56,1
Intel,k75qeu1,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",AMD,2023-10-30 23:33:32,2
Intel,k75p425,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",AMD,2023-10-30 23:24:38,1
Intel,k75qigc,Very similar performance I guess,AMD,2023-10-30 23:34:13,1
Intel,k77bjls,Ye,AMD,2023-10-31 08:03:05,0
Intel,k7813wv,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,AMD,2023-10-31 12:54:43,0
Intel,k77obao,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",AMD,2023-10-31 10:54:29,1
Intel,k77o9a4,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",AMD,2023-10-31 10:53:51,1
Intel,k77udwp,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-31 11:56:37,1
Intel,k78piiq,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,AMD,2023-10-31 15:40:37,1
Intel,k7e7bz2,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,AMD,2023-11-01 17:53:25,1
Intel,k7eem4z,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,AMD,2023-11-01 18:37:35,0
Intel,k7fs4ss,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",AMD,2023-11-01 23:49:18,0
Intel,k7k3mjx,"with AFMF it works , didnt test with FSR3 now.",AMD,2023-11-02 20:38:03,1
Intel,k8l2h6h,Reading before raging :),AMD,2023-11-10 01:18:48,1
Intel,k74j51i,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",AMD,2023-10-30 18:58:47,66
Intel,k774q01,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,AMD,2023-10-31 06:26:48,3
Intel,k74rg7l,For 30fps even lol,AMD,2023-10-30 19:49:50,2
Intel,k74tk9h,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",AMD,2023-10-30 20:02:46,1
Intel,k779kpw,Or be happy your shitty video card is still supported.,AMD,2023-10-31 07:34:39,1
Intel,k78mpu0,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,AMD,2023-10-31 15:22:45,-1
Intel,k77deta,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,AMD,2023-10-31 08:30:28,7
Intel,k77kqzi,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,AMD,2023-10-31 10:12:09,1
Intel,k74w3jk,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,AMD,2023-10-30 20:18:05,14
Intel,k74frkt,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,AMD,2023-10-30 18:38:21,10
Intel,k74k70d,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",AMD,2023-10-30 19:05:17,4
Intel,k7a00mh,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",AMD,2023-10-31 20:26:38,1
Intel,k74w35a,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",AMD,2023-10-30 20:18:01,0
Intel,k77ddfz,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,AMD,2023-10-31 08:29:53,8
Intel,k79hiyj,But it does in many ways.,AMD,2023-10-31 18:33:00,6
Intel,k77ktee,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",AMD,2023-10-31 10:13:00,1
Intel,k78mx3z,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",AMD,2023-10-31 15:24:03,-2
Intel,k792qlt,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,AMD,2023-10-31 17:02:03,5
Intel,k77jb2i,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",AMD,2023-10-31 09:53:38,5
Intel,k776y63,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",AMD,2023-10-31 06:57:06,11
Intel,k75f8fi,speaking facts my guy,AMD,2023-10-30 22:17:50,0
Intel,k77ezjy,You shouldn't have downvote dood wtf redditors ?,AMD,2023-10-31 08:54:03,0
Intel,k79hyhx,People with AMD cards dislike upscaling more because FSR sucks ass lol.,AMD,2023-10-31 18:35:41,5
Intel,k77ebzi,"Uhm, me btw...",AMD,2023-10-31 08:44:25,1
Intel,k77kv35,"> Who actually plays games at native these days, if it has upscaling?  I do.",AMD,2023-10-31 10:13:35,3
Intel,k75rfzg,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",AMD,2023-10-30 23:40:30,2
Intel,k7kxjkt,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",AMD,2023-11-02 23:49:47,3
Intel,k7xpols,onto absolutely nothing.,AMD,2023-11-05 15:20:55,1
Intel,k785u5t,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",AMD,2023-10-31 13:30:13,11
Intel,k75w2l1,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,AMD,2023-10-31 00:12:04,6
Intel,k76yitl,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,AMD,2023-10-31 05:09:41,1
Intel,k77agw9,The game is raytracing only with a ridiculous ammount of foooliage.,AMD,2023-10-31 07:47:35,6
Intel,k77h1xr,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",AMD,2023-10-31 09:23:11,6
Intel,k78n7c0,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,AMD,2023-10-31 15:25:54,3
Intel,k74jbjo,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,AMD,2023-10-30 18:59:54,3
Intel,k77uxf6,False.  guy blocked me lmao,AMD,2023-10-31 12:01:42,1
Intel,k770n5h,"I thought that was on Linux, though I might be wrong",AMD,2023-10-31 05:34:47,0
Intel,k75wi30,1070 doesn't have hardware RT though.,AMD,2023-10-31 00:14:59,4
Intel,k74g227,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),AMD,2023-10-30 18:40:06,24
Intel,k78amg9,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,AMD,2023-10-31 14:03:39,0
Intel,k74qlaz,Completely agree.,AMD,2023-10-30 19:44:35,3
Intel,k7hr3n6,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,AMD,2023-11-02 11:28:02,1
Intel,k7476pu,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),AMD,2023-10-30 17:45:53,5
Intel,k75aybf,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",AMD,2023-10-30 21:50:00,8
Intel,k74gjdn,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",AMD,2023-10-30 18:43:00,11
Intel,k74gjiu,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,AMD,2023-10-30 18:43:01,1
Intel,k74d1es,"Derp, derp.",AMD,2023-10-30 18:21:37,0
Intel,k7gqdq3,Sounds like John on Direct Foundry Direct every week.,AMD,2023-11-02 03:55:28,1
Intel,k7721zf,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",AMD,2023-10-31 05:52:19,3
Intel,k77id8y,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",AMD,2023-10-31 09:41:01,4
Intel,k75quo4,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,AMD,2023-10-30 23:36:30,3
Intel,k75wl3m,Avatar is RT only. There is no non RT mode.,AMD,2023-10-31 00:15:31,7
Intel,k77ar4b,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,AMD,2023-10-31 07:51:38,6
Intel,k76ly3i,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",AMD,2023-10-31 03:10:28,2
Intel,k78qssi,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",AMD,2023-10-31 15:48:32,3
Intel,k75m1nd,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,AMD,2023-10-30 23:03:44,8
Intel,k75t1dz,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,AMD,2023-10-30 23:51:23,1
Intel,k74c5po,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",AMD,2023-10-30 18:16:15,13
Intel,k74ejc7,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",AMD,2023-10-30 18:30:46,5
Intel,k762gt6,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,AMD,2023-10-31 00:54:19,0
Intel,k7795r7,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,AMD,2023-10-31 07:28:35,5
Intel,k8xdnpw,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,AMD,2023-11-12 13:53:39,1
Intel,k77azgf,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",AMD,2023-10-31 07:54:57,0
Intel,k7ad42q,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",AMD,2023-10-31 21:51:09,-1
Intel,k771qsz,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",AMD,2023-10-31 05:48:28,1
Intel,k7baowx,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",AMD,2023-11-01 01:58:28,1
Intel,k76iowi,This is what I am thinking too.,AMD,2023-10-31 02:46:08,1
Intel,k7al3b9,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",AMD,2023-10-31 22:48:01,1
Intel,k78r7ik,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",AMD,2023-10-31 15:51:02,0
Intel,k7eeuc1,Let's hope so. 30fps is far from recommended for FSR 3,AMD,2023-11-01 18:38:59,1
Intel,k7kbe2u,Yeah I heard it works with that hoping it works with fsr3 now,AMD,2023-11-02 21:25:15,1
Intel,k74jv5m,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",AMD,2023-10-30 19:03:14,40
Intel,k770h31,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",AMD,2023-10-31 05:32:44,0
Intel,k74va9v,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",AMD,2023-10-30 20:13:10,-10
Intel,k78mlfi,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,AMD,2023-10-31 15:21:57,4
Intel,k776zix,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,AMD,2023-10-31 06:57:37,5
Intel,k77ebsi,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",AMD,2023-10-31 08:44:20,-1
Intel,k82kpsv,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",AMD,2023-11-06 14:19:02,1
Intel,k792d5j,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",AMD,2023-10-31 16:59:46,2
Intel,k77dtn2,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",AMD,2023-10-31 08:36:45,3
Intel,k77togb,Assassins creed origins and odyssey side quests/collectibles oh my god,AMD,2023-10-31 11:50:06,2
Intel,k74jzj8,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,AMD,2023-10-30 19:04:00,0
Intel,k74m5ig,I appreciate your insights and opinions. Thank you.,AMD,2023-10-30 19:17:14,-1
Intel,k775lx7,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",AMD,2023-10-31 06:38:51,1
Intel,k77sn6f,console games started upscaling way before PCs .,AMD,2023-10-31 11:40:07,5
Intel,k79hu3v,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,AMD,2023-10-31 18:34:55,4
Intel,k79vmwl,you forgot who owns one of the most popular engines out there?,AMD,2023-10-31 19:59:27,-1
Intel,k7abo7k,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",AMD,2023-10-31 21:41:33,-4
Intel,k77b8ol,downvoted by devs lol,AMD,2023-10-31 07:58:40,5
Intel,k79ib8i,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,AMD,2023-10-31 18:37:52,0
Intel,k78h0ux,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",AMD,2023-10-31 14:46:05,-1
Intel,k792yre,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,AMD,2023-10-31 17:03:29,0
Intel,k79ll9w,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,AMD,2023-10-31 18:57:46,2
Intel,k78xgtm,Why is there a dialog message about unsupported hardware when you try and run a 390X?,AMD,2023-10-31 16:29:37,3
Intel,k77blcl,It can do software based RT just like every other modern GPU out there.,AMD,2023-10-31 08:03:49,2
Intel,k74gdb3,Well then no wonder rx 5700 can't manage 30 fps lol,AMD,2023-10-30 18:41:59,27
Intel,k74m0rw,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",AMD,2023-10-30 19:16:25,9
Intel,k74ihd8,Avatars uses a Lumen like software RT solution.,AMD,2023-10-30 18:54:49,19
Intel,k78qd8n,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",AMD,2023-10-31 15:45:52,2
Intel,k74lfw8,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",AMD,2023-10-30 19:12:53,14
Intel,k75797r,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),AMD,2023-10-30 21:26:16,3
Intel,k74khy7,"Yeah, thatsl happened.",AMD,2023-10-30 19:07:09,0
Intel,k7a6im2,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",AMD,2023-10-31 21:07:50,1
Intel,k75wfsk,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",AMD,2023-10-31 00:14:32,1
Intel,k77chzy,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",AMD,2023-10-31 08:16:52,2
Intel,k77pinh,You didn't understand. It's another Swiss knife engine,AMD,2023-10-31 11:07:56,0
Intel,k76pny2,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",AMD,2023-10-31 03:41:22,1
Intel,k75nvkv,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,AMD,2023-10-30 23:16:13,-1
Intel,k77ap82,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,AMD,2023-10-31 07:50:56,-2
Intel,k78n2dv,The PS5 has a 6700.,AMD,2023-10-31 15:25:00,1
Intel,k7i9v2e,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",AMD,2023-11-02 13:58:02,1
Intel,k74jbph,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",AMD,2023-10-30 18:59:55,1
Intel,k8xdwme,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,AMD,2023-11-12 13:55:46,1
Intel,k77fln1,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,AMD,2023-10-31 09:02:53,0
Intel,k7ahuzb,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,AMD,2023-10-31 22:24:31,1
Intel,k78sexp,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,AMD,2023-10-31 15:58:21,1
Intel,k74o0rt,TAA,AMD,2023-10-30 19:28:44,53
Intel,k74rhkm,Temporal anti aliasing.,AMD,2023-10-30 19:50:04,6
Intel,k78mepd,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",AMD,2023-10-31 15:20:46,3
Intel,k78zoea,"Ahh, welcome to r/FuckTAA",AMD,2023-10-31 16:43:15,1
Intel,k7fo2qt,Even 4K looks blurry with some implementations of TAA,AMD,2023-11-01 23:21:50,1
Intel,k775ulz,they're giving you the bare minimum until your upgrade!,AMD,2023-10-31 06:42:13,0
Intel,k74lgqm,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,AMD,2023-10-30 19:13:01,-6
Intel,k7551yk,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",AMD,2023-10-30 21:12:24,12
Intel,k74xjh2,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",AMD,2023-10-30 20:26:40,12
Intel,k7549zz,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",AMD,2023-10-30 21:07:34,3
Intel,k78jvij,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",AMD,2023-10-31 15:04:19,6
Intel,k79h6tb,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,AMD,2023-10-31 18:30:53,2
Intel,k77gqlw,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",AMD,2023-10-31 09:18:51,1
Intel,k78mn3v,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",AMD,2023-10-31 15:22:16,4
Intel,k7kwbcl,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,AMD,2023-11-02 23:41:35,1
Intel,k783kme,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",AMD,2023-10-31 13:13:39,-1
Intel,k7adm22,No I havent. AMD is bigger than Epic.,AMD,2023-10-31 21:54:30,2
Intel,k7aqfmz,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",AMD,2023-10-31 23:27:21,4
Intel,k77exua,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",AMD,2023-10-31 08:53:22,0
Intel,k79j5la,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",AMD,2023-10-31 18:43:01,6
Intel,k78wlj1,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,AMD,2023-10-31 16:24:13,6
Intel,k79ipwc,It's software ray tracing which isn't accelerated by hardware.,AMD,2023-10-31 18:40:20,2
Intel,k79sb21,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,AMD,2023-10-31 19:39:06,1
Intel,k79a73q,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,AMD,2023-10-31 17:47:59,2
Intel,k74o26m,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,AMD,2023-10-30 19:28:58,20
Intel,k778s32,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",AMD,2023-10-31 07:23:06,1
Intel,k75bea7,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,AMD,2023-10-30 21:52:53,4
Intel,k75j5le,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,AMD,2023-10-30 22:44:05,6
Intel,k74nnyo,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",AMD,2023-10-30 19:26:34,4
Intel,k7a88v5,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",AMD,2023-10-31 21:19:00,1
Intel,k77pvj3,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,AMD,2023-10-31 11:11:45,4
Intel,k78lq5t,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",AMD,2023-10-31 15:16:17,3
Intel,k76zwtm,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,AMD,2023-10-31 05:25:55,2
Intel,k77ddap,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",AMD,2023-10-31 08:29:49,6
Intel,k77dhd7,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",AMD,2023-10-31 08:31:33,6
Intel,k77h10q,Both excellent 👌 Down the Rabbit Hole was another solid one.,AMD,2023-10-31 09:22:50,1
Intel,k7b776f,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",AMD,2023-11-01 01:31:29,1
Intel,k78ut1x,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",AMD,2023-10-31 16:13:03,0
Intel,k74xlfo,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,AMD,2023-10-30 20:27:00,26
Intel,k74uq1m,I have to turn it off in borderlands 3. Ugh.,AMD,2023-10-30 20:09:47,2
Intel,k78zmsb,r/FuckTAA,AMD,2023-10-31 16:42:58,0
Intel,k7n69qz,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",AMD,2023-11-03 12:46:56,1
Intel,k7gyunv,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",AMD,2023-11-02 05:20:45,1
Intel,k7bw0wl,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,AMD,2023-11-01 05:13:29,-1
Intel,k78p5rj,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",AMD,2023-10-31 15:38:25,2
Intel,k7kx549,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,AMD,2023-11-02 23:47:05,1
Intel,k7c5sa3,epic is tencent...,AMD,2023-11-01 07:26:14,-1
Intel,k79evb5,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",AMD,2023-10-31 18:16:40,0
Intel,k79vqds,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",AMD,2023-10-31 20:00:02,1
Intel,k79dbl9,You don't need RT hardware to do software RT. That's what I'm saying.,AMD,2023-10-31 18:07:06,3
Intel,k74wirt,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",AMD,2023-10-30 20:20:36,20
Intel,k755cwh,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",AMD,2023-10-30 21:14:19,19
Intel,k74ogkv,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),AMD,2023-10-30 19:31:29,15
Intel,k751p3z,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,AMD,2023-10-30 20:51:38,9
Intel,k75za9c,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",AMD,2023-10-31 00:33:28,8
Intel,k76k5hm,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,AMD,2023-10-31 02:56:53,3
Intel,k77u7zx,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,AMD,2023-10-31 11:55:06,2
Intel,k77anl2,Yes hah,AMD,2023-10-31 07:50:17,1
Intel,k78kxsq,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",AMD,2023-10-31 15:11:02,1
Intel,k75bm3k,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",AMD,2023-10-30 21:54:17,1
Intel,k75jy4k,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,AMD,2023-10-30 22:49:28,8
Intel,k75cf9l,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",AMD,2023-10-30 21:59:30,10
Intel,k74s4b6,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",AMD,2023-10-30 19:53:57,5
Intel,k78qh5r,Oh man the hair... its so crazy how much upscaling kills the hair..,AMD,2023-10-31 15:46:32,0
Intel,k74vpzd,i mean they already arent the smartest bulbs considering they went team green.,AMD,2023-10-30 20:15:50,-9
Intel,k7bwwni,Reading comrehension dude.,AMD,2023-11-01 05:24:02,1
Intel,k77rq8u,"I do, but thanks for your interest.",AMD,2023-10-31 11:31:01,0
Intel,k781oqh,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,AMD,2023-10-31 12:59:12,1
Intel,k77gylo,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",AMD,2023-10-31 09:21:55,0
Intel,k77prws,Nice. It’s on the list. Thanks man,AMD,2023-10-31 11:10:40,0
Intel,k79111l,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,AMD,2023-10-31 16:51:38,1
Intel,k74y62b,Probably because TAA is (unfortunately) more prevalent than it ever was.,AMD,2023-10-30 20:30:23,12
Intel,k77crxd,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",AMD,2023-10-31 08:20:57,4
Intel,k7ky7yi,Oh my bad if I read that wrong,AMD,2023-11-02 23:54:13,1
Intel,k79nw3z,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,AMD,2023-10-31 19:11:53,1
Intel,k74xr8d,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,AMD,2023-10-30 20:27:57,-10
Intel,k78nlj0,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",AMD,2023-10-31 15:28:29,1
Intel,k78nx4p,imagine being mad that 4 year old cards arent high end,AMD,2023-10-31 15:30:32,2
Intel,k78qsw0,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,AMD,2023-10-31 15:48:33,0
Intel,k75i9u4,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",AMD,2023-10-30 22:38:10,1
Intel,k77gx0m,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",AMD,2023-10-31 09:21:18,1
Intel,k74smha,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",AMD,2023-10-30 19:57:03,2
Intel,k78qwba,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",AMD,2023-10-31 15:49:08,0
Intel,k74wqog,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",AMD,2023-10-30 20:21:55,2
Intel,k7bxbe9,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",AMD,2023-11-01 05:29:04,1
Intel,k77y4x2,"It's for textures, not object edges from FSR use, lol. Two completely different things",AMD,2023-10-31 12:30:16,6
Intel,k793lf6,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",AMD,2023-10-31 17:07:23,-1
Intel,k77g0i6,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",AMD,2023-10-31 09:08:37,11
Intel,k77ubrs,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",AMD,2023-10-31 11:56:04,-2
Intel,k8f5yw4,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",AMD,2023-11-08 22:27:09,1
Intel,k79oe6d,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,AMD,2023-10-31 19:14:56,1
Intel,k753gel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",AMD,2023-10-30 21:02:23,12
Intel,k78lli0,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,AMD,2023-10-31 15:15:24,3
Intel,k7au2y4,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",AMD,2023-10-31 23:54:23,1
Intel,k75wc3v,5700 was probably the lowest AMD card they had to test with.,AMD,2023-10-31 00:13:51,1
Intel,k74tcrl,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",AMD,2023-10-30 20:01:29,5
Intel,k7byrit,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,AMD,2023-11-01 05:47:35,1
Intel,k7a8jw6,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,AMD,2023-10-31 21:20:59,1
Intel,k7kn3dl,AMD CAS is aimed to restore detail,AMD,2023-11-02 22:40:21,1
Intel,k7k5dha,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",AMD,2023-11-02 20:48:40,1
Intel,k7cl7iw,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,AMD,2023-11-01 10:59:12,2
Intel,k74ugbm,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",AMD,2023-10-30 20:08:12,0
Intel,k7bzcr6,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",AMD,2023-11-01 05:55:17,1
Intel,k7cntlm,Yeah but a 3060ti didn't,AMD,2023-11-01 11:27:22,1
Intel,k78mcrt,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",AMD,2023-10-31 15:20:25,2
Intel,k74zgnn,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",AMD,2023-10-30 20:38:09,5
Intel,k1rzmke,Confirmed Can it run CP2077 4k is the new Can it run Crysis,AMD,2023-09-22 22:22:20,592
Intel,k1s9f6d,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,AMD,2023-09-22 23:31:10,579
Intel,k1rtgmz,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,AMD,2023-09-22 21:41:53,308
Intel,k1tezss,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",AMD,2023-09-23 05:11:42,24
Intel,k1spmgk,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,AMD,2023-09-23 01:30:07,19
Intel,k1ryede,3080 falling behind a 3060? what is this data?,AMD,2023-09-22 22:14:11,123
Intel,k1say8p,wake me up when we have a card that can run this at 40 without needing its own psu.,AMD,2023-09-22 23:42:17,46
Intel,k1seqym,5090 here I come!,AMD,2023-09-23 00:09:51,26
Intel,k1sf8id,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",AMD,2023-09-23 00:13:23,39
Intel,k1samn8,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",AMD,2023-09-22 23:39:57,28
Intel,k1se7w7,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,AMD,2023-09-23 00:06:00,29
Intel,k1tcwd4,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",AMD,2023-09-23 04:49:38,6
Intel,k1swxw6,It's a good thing nobody has to actually play it native.,AMD,2023-09-23 02:26:41,19
Intel,k1sd0w5,Well good thing literally nobody is doing that…,AMD,2023-09-22 23:57:14,9
Intel,k1tv5m1,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,AMD,2023-09-23 08:24:44,10
Intel,k1svrss,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,AMD,2023-09-23 02:17:32,16
Intel,k1u5n8e,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,AMD,2023-09-23 10:40:09,4
Intel,k1t06iw,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",AMD,2023-09-23 02:52:43,10
Intel,k1sta67,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,AMD,2023-09-23 01:58:23,16
Intel,k1ru0dy,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",AMD,2023-09-22 21:45:23,26
Intel,k1sh29m,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",AMD,2023-09-23 00:26:40,19
Intel,k1rwvmw,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,AMD,2023-09-22 22:04:05,21
Intel,k1s5ads,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,AMD,2023-09-22 23:01:20,9
Intel,k1t5pl6,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",AMD,2023-09-23 03:40:25,8
Intel,k1sde74,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",AMD,2023-09-22 23:59:57,17
Intel,k1sbtem,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",AMD,2023-09-22 23:48:27,5
Intel,k1sr9s2,This doesn’t seem right…. 3080 performing worse than a 2080TI?,AMD,2023-09-23 01:42:55,2
Intel,k1t2kii,How tf is a 2080 ti getting more fps than a 3080?!?,AMD,2023-09-23 03:12:42,2
Intel,k1t66da,I'm not seeing the RTX A6000 on here...,AMD,2023-09-23 03:44:41,2
Intel,k1tglq7,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,AMD,2023-09-23 05:29:29,2
Intel,k1u9v0r,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",AMD,2023-09-23 11:28:08,2
Intel,k1ufe40,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",AMD,2023-09-23 12:22:22,2
Intel,k1witvs,Does anyone actually play this game? Its more of a meme game imho,AMD,2023-09-23 20:46:05,2
Intel,k1xd78i,4.3 fps 😂😂😂,AMD,2023-09-24 00:13:50,2
Intel,k1sczh9,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",AMD,2023-09-22 23:56:57,9
Intel,k1rumnu,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",AMD,2023-09-22 21:49:26,13
Intel,k1shlno,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,AMD,2023-09-23 00:30:36,3
Intel,k1t2byj,Lol the 3060ti is better than a 7900xtx...crazy,AMD,2023-09-23 03:10:40,2
Intel,k1vuadd,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,AMD,2023-09-23 18:12:06,3
Intel,k1s8b2u,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,AMD,2023-09-22 23:23:07,11
Intel,k1t61f2,ThE gAMe iS PoOrLY OpTImiSed!!!,AMD,2023-09-23 03:43:26,3
Intel,k1ttlda,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,AMD,2023-09-23 08:04:31,3
Intel,k1sgjyj,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",AMD,2023-09-23 00:22:53,4
Intel,k1sxb06,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",AMD,2023-09-23 02:29:34,2
Intel,k1sydqu,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",AMD,2023-09-23 02:38:06,2
Intel,k1tbdbm,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,AMD,2023-09-23 04:34:09,2
Intel,k1twz2u,"""4090 is 4 times faster than 7900XTX""",AMD,2023-09-23 08:48:39,2
Intel,k1uouoc,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",AMD,2023-09-23 13:40:46,2
Intel,k1wadup,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,AMD,2023-09-23 19:52:54,2
Intel,k1tl3qo,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),AMD,2023-09-23 06:20:29,2
Intel,k1s8g0f,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",AMD,2023-09-22 23:24:08,2
Intel,k1s8o3g,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",AMD,2023-09-22 23:25:45,2
Intel,k1sg5kw,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,AMD,2023-09-23 00:20:00,1
Intel,k1skakr,Native resolution is a thing of the past. DLSS looks better than native anyway.,AMD,2023-09-23 00:50:07,2
Intel,k1txff8,Is this with DLSS + FG?,AMD,2023-09-23 08:54:33,1
Intel,k1smbmr,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",AMD,2023-09-23 01:05:06,-1
Intel,k1s3u4i,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",AMD,2023-09-22 22:51:01,0
Intel,k1t2qkw,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,AMD,2023-09-23 03:14:12,1
Intel,k1sgzlr,No one is using these settings.,AMD,2023-09-23 00:26:07,2
Intel,k1spqtc,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",AMD,2023-09-23 01:31:01,1
Intel,k1shcs4,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",AMD,2023-09-23 00:28:49,1
Intel,k1sj5mo,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",AMD,2023-09-23 00:41:56,1
Intel,k1ulwx6,cyberpunk sucks don't worry about it,AMD,2023-09-23 13:17:47,1
Intel,k1rvukw,3080ti?,AMD,2023-09-22 21:57:22,1
Intel,k1sljwz,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",AMD,2023-09-23 00:59:20,1
Intel,k1sqr9g,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,AMD,2023-09-23 01:38:55,1
Intel,k1tt1b6,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",AMD,2023-09-23 07:57:19,1
Intel,k1tuc3v,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",AMD,2023-09-23 08:14:11,1
Intel,k1uq4ce,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,AMD,2023-09-23 13:50:25,1
Intel,k1s9ert,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),AMD,2023-09-22 23:31:05,-1
Intel,k1rzvbb,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",AMD,2023-09-22 22:23:58,-16
Intel,k1sfn3w,What body part do you think will get me a 5090?,AMD,2023-09-23 00:16:18,0
Intel,k1txo0m,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",AMD,2023-09-23 08:57:42,0
Intel,k1u7ngf,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",AMD,2023-09-23 11:04:00,0
Intel,k1uxont,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",AMD,2023-09-23 14:44:13,0
Intel,k1s5mej,That not even proper path tracing.,AMD,2023-09-22 23:03:45,-1
Intel,k1s6qt1,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,AMD,2023-09-22 23:11:53,-5
Intel,k1sb5wk,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",AMD,2023-09-22 23:43:51,-9
Intel,k1svwfa,Native is a thing of the past when dlss produces better looking image,AMD,2023-09-23 02:18:33,-3
Intel,k1sdfih,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,AMD,2023-09-23 00:00:13,-4
Intel,k1s7xzc,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,AMD,2023-09-22 23:20:30,-10
Intel,k1slr2s,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,AMD,2023-09-23 01:00:47,-1
Intel,k1sfvki,"Rather than having 2-3fps, I would go and see a video of path tracing.",AMD,2023-09-23 00:17:59,0
Intel,k1sihxn,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",AMD,2023-09-23 00:37:09,0
Intel,k1su9sh,cool idc i wont play that shit at such shit settings,AMD,2023-09-23 02:05:55,0
Intel,k1syk9g,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,AMD,2023-09-23 02:39:33,0
Intel,k1tpxos,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,AMD,2023-09-23 07:18:17,0
Intel,k1tvoqm,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",AMD,2023-09-23 08:31:41,0
Intel,k1tytwk,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,AMD,2023-09-23 09:12:53,0
Intel,k1tzugl,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,AMD,2023-09-23 09:26:15,0
Intel,k1uhusa,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",AMD,2023-09-23 12:44:21,0
Intel,k1v2ghf,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,AMD,2023-09-23 15:16:08,0
Intel,k1v8o7m,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",AMD,2023-09-23 15:55:59,0
Intel,k1vfxae,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,AMD,2023-09-23 16:41:57,0
Intel,k1vjs6i,"Looks like 4K is mainly just for videos, not gaming for the time being.",AMD,2023-09-23 17:06:09,0
Intel,k1vnxlu,How can you bring out such technology and no hardware can process it properly.  an impudence,AMD,2023-09-23 17:32:09,0
Intel,k1wm3gk,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,AMD,2023-09-23 21:06:13,0
Intel,k1ysyyz,You could just you know... disable path tracing and get 70 fps or go below 4k,AMD,2023-09-24 08:12:13,0
Intel,k1z173o,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",AMD,2023-09-24 09:58:57,0
Intel,k25idkz,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",AMD,2023-09-25 16:06:18,0
Intel,k1rv0mr,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,AMD,2023-09-22 21:51:56,-33
Intel,k1sbmt5,"so it is settled, Devs have zero optimization in games.",AMD,2023-09-22 23:47:08,-12
Intel,k1t5ilx,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",AMD,2023-09-23 03:38:40,-2
Intel,k1t8qzc,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",AMD,2023-09-23 04:08:39,-1
Intel,k1syqs1,Once again proving RT is useless,AMD,2023-09-23 02:41:00,-7
Intel,k1sfuie,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,AMD,2023-09-23 00:17:45,320
Intel,k1sw4a8,"""only gamers get this joke""",AMD,2023-09-23 02:20:15,0
Intel,k1tqj3o,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",AMD,2023-09-23 07:25:29,-3
Intel,k1uey5o,Starfield is peaking around the corner.,AMD,2023-09-23 12:18:19,0
Intel,k1ume3r,starfield as well,AMD,2023-09-23 13:21:33,0
Intel,k1smpf3,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,AMD,2023-09-23 01:07:57,407
Intel,k1sm1vi,The new Crysis,AMD,2023-09-23 01:03:02,21
Intel,k1t68ad,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",AMD,2023-09-23 03:45:09,36
Intel,k1tduwg,How long until a $200 card can do that?,AMD,2023-09-23 04:59:33,11
Intel,k1tl6bz,Most improvement is probably going to AI software more than hardware in the next few years.,AMD,2023-09-23 06:21:18,12
Intel,k1sui5w,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",AMD,2023-09-23 02:07:40,31
Intel,k1tggmb,GPU need to have its own garage by then,AMD,2023-09-23 05:27:53,7
Intel,k1sjd2a,Yep! Insane how fast things change.,AMD,2023-09-23 00:43:26,17
Intel,k1th7kz,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",AMD,2023-09-23 05:36:13,24
Intel,k1tksc7,Yeah rtx 12060 with 9.5 gb Vram will be a monster,AMD,2023-09-23 06:16:50,14
Intel,k1toc97,I'm willing to bet hardware improvement will come to a halt before that.,AMD,2023-09-23 06:59:10,5
Intel,k1tp7w0,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",AMD,2023-09-23 07:09:42,6
Intel,k1u5t5c,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,AMD,2023-09-23 10:42:10,2
Intel,k1u9trt,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,AMD,2023-09-23 11:27:46,2
Intel,k1umihs,But then the current gen games of that era will run like this. The cycle continues,AMD,2023-09-23 13:22:31,2
Intel,k1vcazz,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,AMD,2023-09-23 16:19:04,2
Intel,k1t8b6k,Who knows what new tech will be out in even 4 years lol,AMD,2023-09-23 04:04:25,2
Intel,k1tl2z8,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",AMD,2023-09-23 06:20:14,0
Intel,k1u0yj3,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",AMD,2023-09-23 09:40:52,1
Intel,k1u6uez,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",AMD,2023-09-23 10:54:31,1
Intel,k1sby3m,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,AMD,2023-09-22 23:49:24,311
Intel,k1rw4fg,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",AMD,2023-09-22 21:59:09,106
Intel,k1sbwfh,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",AMD,2023-09-22 23:49:03,11
Intel,k1t5qx0,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",AMD,2023-09-23 03:40:46,-1
Intel,k1tf1z9,I think that most of the progress will go together with software tricks and upscalers.,AMD,2023-09-23 05:12:21,3
Intel,k1s07rb,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",AMD,2023-09-22 22:26:17,128
Intel,k1rz345,That's VRAM for you,AMD,2023-09-22 22:18:45,130
Intel,k1tc7vb,we are counting decimals of fps its just all margin of error.,AMD,2023-09-23 04:42:40,12
Intel,k1s03of,If you buy a card with low ram that card is for right now only lol,AMD,2023-09-22 22:25:32,4
Intel,k1v5pv8,Wake me up when a $250 GPU can run this at 1080p.,AMD,2023-09-23 15:37:12,9
Intel,k1uiun2,Well DLSS isn't best. DLAA is,AMD,2023-09-23 12:52:43,7
Intel,k1svcks,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,AMD,2023-09-23 02:14:15,25
Intel,k1si3ng,Can’t you just disable TAA? Or do you just have to live with the ghosting?,AMD,2023-09-23 00:34:15,3
Intel,k1v2dce,TAA is garbage in everything. TAA and FSR can both get fucked,AMD,2023-09-23 15:15:33,2
Intel,k1svesl,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",AMD,2023-09-23 02:14:43,8
Intel,k1v301u,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",AMD,2023-09-23 15:19:41,2
Intel,k1v2lmw,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",AMD,2023-09-23 15:17:04,2
Intel,k1sv2p6,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",AMD,2023-09-23 02:12:07,7
Intel,k1vdbm6,Imagine buying a 4090 and then using upscaling.,AMD,2023-09-23 16:25:29,-4
Intel,k1scblp,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,AMD,2023-09-22 23:52:08,17
Intel,k1sfzkt,PC gaming is in Crysis.,AMD,2023-09-23 00:18:47,5
Intel,k1y7rss,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,AMD,2023-09-24 04:15:05,-1
Intel,k1t8lmy,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,AMD,2023-09-23 04:07:13,1
Intel,k1rxtnn,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",AMD,2023-09-22 22:10:20,8
Intel,k1u60lu,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",AMD,2023-09-23 10:44:39,14
Intel,k1sizpa,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",AMD,2023-09-23 00:40:43,-6
Intel,k1u5vqx,Yeh because native 4k looks worse than dlss + RR 4k,AMD,2023-09-23 10:43:02,8
Intel,k1xi8dd,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",AMD,2023-09-24 00:50:15,2
Intel,k1u5mhq,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,AMD,2023-09-23 10:39:55,10
Intel,k1sshhs,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",AMD,2023-09-23 01:52:13,8
Intel,k1sr5lk,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",AMD,2023-09-23 01:42:00,1
Intel,k1t41gi,VRAM,AMD,2023-09-23 03:25:30,3
Intel,k1uiuuv,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",AMD,2023-09-23 12:52:46,1
Intel,k1zcvhv,still better than 90% of games in 2023,AMD,2023-09-24 12:08:19,0
Intel,k1rvtrp,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,AMD,2023-09-22 21:57:13,21
Intel,k1s0kpr,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",AMD,2023-09-22 22:28:43,-1
Intel,k1sbnjt,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",AMD,2023-09-22 23:47:17,-8
Intel,k1u6o55,All graphics you see on your computer screen is fake,AMD,2023-09-23 10:52:26,2
Intel,k1t62rv,Path tracing is more demanding than Ray tracing,AMD,2023-09-23 03:43:46,5
Intel,k1u17zi,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",AMD,2023-09-23 09:44:22,4
Intel,k1s9p4d,I guess between the 3090 and the 4070,AMD,2023-09-22 23:33:11,0
Intel,k1sjydp,Yep hahaha,AMD,2023-09-23 00:47:40,2
Intel,k1v42p3,RemindMe! 7 years,AMD,2023-09-23 15:26:41,2
Intel,k1s3vi6,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",AMD,2023-09-22 22:51:16,18
Intel,k1s45n5,"This is an absurd, fanboyish thought lmao",AMD,2023-09-22 22:53:15,14
Intel,k1u3kdb,dont let novidia marketing see this youll get down voted into oblivion,AMD,2023-09-23 10:14:19,2
Intel,k1s2le1,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,AMD,2023-09-22 22:42:21,-7
Intel,k1sylo6,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,AMD,2023-09-23 02:39:51,3
Intel,k1sltal,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",AMD,2023-09-23 01:01:15,2
Intel,k1u6rsu,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,AMD,2023-09-23 10:53:38,2
Intel,k1t37ky,Fanboyism of both kinds is bad,AMD,2023-09-23 03:18:18,1
Intel,k1sbir3,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",AMD,2023-09-22 23:46:20,-1
Intel,k1u85m7,Since it needs more than 10gb vram,AMD,2023-09-23 11:09:41,3
Intel,k1tq0gr,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,AMD,2023-09-23 07:19:12,4
Intel,k1rw9dl,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,AMD,2023-09-22 22:00:02,28
Intel,k1sl1fh,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",AMD,2023-09-23 00:55:34,7
Intel,k1rx0ka,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",AMD,2023-09-22 22:04:58,-13
Intel,k1u2f5k,It's settled that you have no idea what you talking about,AMD,2023-09-23 09:59:44,4
Intel,k1sczqa,Its full path tracing u cannot really optimize this much.,AMD,2023-09-22 23:57:00,10
Intel,k1tpj68,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",AMD,2023-09-23 07:13:34,2
Intel,k1s9h3d,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",AMD,2023-09-22 23:31:33,2
Intel,k1sprwd,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,AMD,2023-09-23 01:31:15,144
Intel,k1tds1v,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",AMD,2023-09-23 04:58:42,28
Intel,k1u7ebf,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,AMD,2023-09-23 11:01:03,18
Intel,k1vo73s,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",AMD,2023-09-23 17:33:46,6
Intel,k1u7ms4,Amd had a tessellation unit. It went unused but it was present,AMD,2023-09-23 11:03:47,2
Intel,k1srvji,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",AMD,2023-09-23 01:47:34,160
Intel,k1sqn2i,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",AMD,2023-09-23 01:38:00,33
Intel,k1tzlza,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",AMD,2023-09-23 09:23:04,5
Intel,k1uatm9,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:38:16,5
Intel,k1u8wkg,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",AMD,2023-09-23 11:17:47,17
Intel,k1tdixd,"Oh damn, I forgot about those cards.  I wanted one so badly.",AMD,2023-09-23 04:56:03,3
Intel,k1u3mme,Remember when companies tried to sell physics cards lol,AMD,2023-09-23 10:15:05,3
Intel,k1thwg4,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,AMD,2023-09-23 05:43:55,15
Intel,k1tjl5e,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",AMD,2023-09-23 06:03:08,2
Intel,k1umriw,I remember when people had a dedicated PhysX card.,AMD,2023-09-23 13:24:29,2
Intel,k1un2sg,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,AMD,2023-09-23 13:26:56,16
Intel,k1tob83,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",AMD,2023-09-23 06:58:48,4
Intel,k1txgpk,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,AMD,2023-09-23 08:55:01,6
Intel,k1ugyqs,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",AMD,2023-09-23 12:36:34,4
Intel,k1ubuqx,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",AMD,2023-09-23 11:48:42,2
Intel,k1uc6ox,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",AMD,2023-09-23 11:51:55,9
Intel,k1uln5g,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",AMD,2023-09-23 13:15:37,4
Intel,k1ull2c,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,AMD,2023-09-23 13:15:10,2
Intel,k1tocdn,25 years,AMD,2023-09-23 06:59:12,6
Intel,k1u1mrk,"The software needs to run on hardware, right now it eats through GPU compute and memory.",AMD,2023-09-23 09:49:38,6
Intel,k1tqm89,And sadly ended up with 450 Watt TDP to achieve that performance.,AMD,2023-09-23 07:26:36,14
Intel,k1tvo5h,This. We'd be lucky to see more than 3 generations in the upcoming decade.,AMD,2023-09-23 08:31:28,4
Intel,k1u1jnl,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",AMD,2023-09-23 09:48:32,3
Intel,k1ucdje,Not with that attitude,AMD,2023-09-23 11:53:45,2
Intel,k1ugj3a,Love how it's still gimped on memory size 😂,AMD,2023-09-23 12:32:43,8
Intel,k1u3901,"They just need to render the minimum information needed , and let the ai do the rest",AMD,2023-09-23 10:10:17,3
Intel,k1u9l5i,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",AMD,2023-09-23 11:25:11,-4
Intel,k1u27rs,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",AMD,2023-09-23 09:57:10,4
Intel,k1ufc12,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",AMD,2023-09-23 12:21:51,2
Intel,k1t3hpt,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",AMD,2023-09-23 03:20:42,160
Intel,k1tek93,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",AMD,2023-09-23 05:07:01,12
Intel,k1uoawi,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",AMD,2023-09-23 13:36:30,3
Intel,k1su9mw,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,AMD,2023-09-23 02:05:53,-19
Intel,k1sbp7z,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",AMD,2023-09-22 23:47:36,84
Intel,k1sxtj8,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",AMD,2023-09-23 02:33:36,5
Intel,k1svx2t,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,AMD,2023-09-23 02:18:41,5
Intel,k1sbvoe,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",AMD,2023-09-22 23:48:54,15
Intel,k1slgjk,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,AMD,2023-09-23 00:58:39,8
Intel,k1tf7yk,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",AMD,2023-09-23 05:14:13,4
Intel,k1sad2c,So rendering at 960p? Oof...,AMD,2023-09-22 23:38:00,6
Intel,k1sd3lh,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",AMD,2023-09-22 23:57:47,-5
Intel,k1rxn27,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,AMD,2023-09-22 22:09:06,-21
Intel,k1s6fmd,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,AMD,2023-09-22 23:09:38,-24
Intel,k1tzl0f,Dont use useless raytracing and you wont have any problems lol,AMD,2023-09-23 09:22:43,-1
Intel,k1vpidg,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",AMD,2023-09-23 17:42:04,6
Intel,k23ed9f,AW2 looks insane. Can't wait to play soon,AMD,2023-09-25 04:14:36,2
Intel,k1si0t0,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,AMD,2023-09-23 00:33:40,23
Intel,k1s5qaj,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",AMD,2023-09-22 23:04:33,36
Intel,k1sor0g,My 3080 in shambles,AMD,2023-09-23 01:23:27,4
Intel,k1u5gbh,"It looks better than native even with fsr quality, the Taa in this game is shit",AMD,2023-09-23 10:37:48,-1
Intel,k1t69ca,You dont need to increase raw performance. You need to increase RT performance.,AMD,2023-09-23 03:45:25,31
Intel,k1vko89,Imagine!,AMD,2023-09-23 17:11:45,6
Intel,k1vz5q2,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,AMD,2023-09-23 18:42:28,6
Intel,k22igme,Only people who don't give a shit about high quality graphics don't care about ray tracing.,AMD,2023-09-25 00:09:28,3
Intel,k1tamw3,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,AMD,2023-09-23 04:26:51,2
Intel,k1s8m60,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",AMD,2023-09-22 23:25:22,4
Intel,k1v3dh2,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",AMD,2023-09-23 15:22:07,6
Intel,k1u1pck,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,AMD,2023-09-23 09:50:33,7
Intel,k1sl137,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",AMD,2023-09-23 00:55:30,3
Intel,k1u5xqe,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,AMD,2023-09-23 10:43:42,-4
Intel,k22yxwq,The Evangelical Church of Native,AMD,2023-09-25 02:08:44,1
Intel,k1vk8iw,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",AMD,2023-09-23 17:09:00,2
Intel,k22yk42,Nice but ive been playing video games since the 70s and its Shit end off..,AMD,2023-09-25 02:05:53,2
Intel,k1s3cm8,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,AMD,2023-09-22 22:47:37,-13
Intel,k1ssze4,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",AMD,2023-09-23 01:56:05,-4
Intel,k1tpcbe,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,AMD,2023-09-23 07:11:12,10
Intel,k1srkr7,"Yeah, sure, now go back to play starfield.",AMD,2023-09-23 01:45:16,7
Intel,k1siya9,"There literally is a /s, what else do you need to detect sarcasm?",AMD,2023-09-23 00:40:26,3
Intel,k1ycket,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",AMD,2023-09-24 05:03:07,1
Intel,k1wclz4,"I know, which makes path tracing even worse off imo.",AMD,2023-09-23 20:06:58,1
Intel,k1v47c6,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2023-09-23 15:27:32,2
Intel,k1vh7xa,haha this is gold 🥇,AMD,2023-09-23 16:50:10,2
Intel,k1sakbd,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",AMD,2023-09-22 23:39:29,-7
Intel,k1s5318,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",AMD,2023-09-22 22:59:52,7
Intel,k1t4yk0,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",AMD,2023-09-23 03:33:41,-4
Intel,k1sakwq,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",AMD,2023-09-22 23:39:36,-9
Intel,k1ryjta,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,AMD,2023-09-22 22:15:11,-2
Intel,k1stpfv,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,AMD,2023-09-23 02:01:36,41
Intel,k1w9mwj,Funnily Crysis still have better destructible environments than Cyberpunk has tho,AMD,2023-09-23 19:48:19,1
Intel,k1vonge,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",AMD,2023-09-23 17:36:37,8
Intel,k1txp0j,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,AMD,2023-09-23 08:58:05,17
Intel,k1tg0xs,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",AMD,2023-09-23 05:23:00,6
Intel,k1syqzz,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",AMD,2023-09-23 02:41:04,4
Intel,k1u23r0,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,AMD,2023-09-23 09:55:42,3
Intel,k1u67ug,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,AMD,2023-09-23 10:47:01,13
Intel,k1t2x0x,LOL I remember this exact scene also.,AMD,2023-09-23 03:15:45,32
Intel,k1ua0tu,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",AMD,2023-09-23 11:29:52,9
Intel,k1t6cm4,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",AMD,2023-09-23 03:46:15,19
Intel,k1tlbna,"I'd like to see ray tracing addon cards, seems logical to me.",AMD,2023-09-23 06:23:01,10
Intel,k1tyf7o,"TBH, I could probably run some of the old games I have on CPU without the GPU.",AMD,2023-09-23 09:07:31,3
Intel,k1ub8fu,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 11:42:32,3
Intel,k1uhy6a,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,AMD,2023-09-23 12:45:10,3
Intel,k1tcex2,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,AMD,2023-09-23 04:44:40,4
Intel,k1tsmml,What gpu you have,AMD,2023-09-23 07:51:58,2
Intel,k1tyt5q,Now it even runs fine on a Ryzen 2400G.,AMD,2023-09-23 09:12:37,2
Intel,k1t9hds,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",AMD,2023-09-23 04:15:43,40
Intel,k1vaqdn,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",AMD,2023-09-23 16:09:02,11
Intel,k1tf4z4,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,AMD,2023-09-23 05:13:17,3
Intel,k1tt5y9,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",AMD,2023-09-23 07:59:01,18
Intel,k1w7xof,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",AMD,2023-09-23 19:37:48,4
Intel,k1tyzar,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,AMD,2023-09-23 09:14:51,2
Intel,k1v3dv1,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",AMD,2023-09-23 15:22:12,2
Intel,k1uaqi2,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,AMD,2023-09-23 11:37:22,6
Intel,k1uke2m,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,AMD,2023-09-23 13:05:30,2
Intel,k1udkaf,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:05:15,5
Intel,k1tl1au,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",AMD,2023-09-23 06:19:42,24
Intel,k1t9pys,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",AMD,2023-09-23 04:17:58,17
Intel,k1u3wuq,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",AMD,2023-09-23 10:18:38,1
Intel,k1sx1p7,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",AMD,2023-09-23 02:27:32,41
Intel,k1t81d2,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,AMD,2023-09-23 04:01:49,5
Intel,k1szqei,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",AMD,2023-09-23 02:49:05,14
Intel,k1syew7,It's significantly better than raster. It kills fps but the quality is great,AMD,2023-09-23 02:38:21,9
Intel,k1ug9ty,Better graphics needing more expensive hardware is hardly a hot take.,AMD,2023-09-23 12:30:24,17
Intel,k1u52xg,"Yes, better graphics costs performance. SHOCKING",AMD,2023-09-23 10:33:12,12
Intel,k1t1ej8,People do it!,AMD,2023-09-23 03:02:50,7
Intel,k1tqio6,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,AMD,2023-09-23 07:25:19,11
Intel,k1t7tj4,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",AMD,2023-09-23 03:59:48,3
Intel,k1sx4o4,Sounds like most nvidia fanboys,AMD,2023-09-23 02:28:11,15
Intel,k1u2qx4,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",AMD,2023-09-23 10:03:52,1
Intel,k1u3no0,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,AMD,2023-09-23 10:15:26,-1
Intel,k1y7ifv,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,AMD,2023-09-24 04:12:33,-1
Intel,k1ss6lq,What? I thought amd was always more tuned to raster as opposed to reflections,AMD,2023-09-23 01:49:57,2
Intel,k1staby,Which is why nvidia is rabidly chasing AI hacks,AMD,2023-09-23 01:58:25,22
Intel,k1sv3ph,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,AMD,2023-09-23 02:12:20,4
Intel,k1sm72w,Say that to the people playing upscaled games at 4k (540p) on PS5.,AMD,2023-09-23 01:04:07,-1
Intel,k1s6hg3,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",AMD,2023-09-22 23:10:00,32
Intel,k1s7ntd,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,AMD,2023-09-22 23:18:28,10
Intel,k1slxqu,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",AMD,2023-09-23 01:02:10,12
Intel,k1s73qo,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",AMD,2023-09-22 23:14:27,-16
Intel,k1ttfgf,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,AMD,2023-09-23 08:02:21,12
Intel,k1tby96,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,AMD,2023-09-23 04:39:58,2
Intel,k1sb10i,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",AMD,2023-09-22 23:42:52,13
Intel,k1sf5sr,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",AMD,2023-09-23 00:12:51,2
Intel,k1u66ws,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),AMD,2023-09-23 10:46:42,7
Intel,k1t46jg,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",AMD,2023-09-23 03:26:44,-1
Intel,k1u6zio,You can already have 120fps on $1000 PC,AMD,2023-09-23 10:56:12,7
Intel,k1vnjmf,It improves both looks and fps so it is a win win,AMD,2023-09-23 17:29:44,0
Intel,k1scklw,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",AMD,2023-09-22 23:53:55,7
Intel,k1s9sl2,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,AMD,2023-09-22 23:33:53,5
Intel,k1s6x1t,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,AMD,2023-09-22 23:13:07,5
Intel,k1s97ev,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",AMD,2023-09-22 23:29:38,1
Intel,k1s30i9,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,AMD,2023-09-22 22:45:15,-9
Intel,k1s8eym,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,AMD,2023-09-22 23:23:56,-1
Intel,k1wd23h,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",AMD,2023-09-23 20:09:52,1
Intel,k1save0,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",AMD,2023-09-22 23:41:42,4
Intel,k1sama1,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,AMD,2023-09-22 23:39:52,-1
Intel,k1tastr,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",AMD,2023-09-23 04:28:29,21
Intel,k1tf175,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",AMD,2023-09-23 05:12:08,10
Intel,k1v57gq,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,AMD,2023-09-23 15:33:56,4
Intel,k1uehqr,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,AMD,2023-09-23 12:14:02,3
Intel,k1uvdfc,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,AMD,2023-09-23 14:28:22,3
Intel,k1uvhzl,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",AMD,2023-09-23 14:29:15,2
Intel,k1vxcz9,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",AMD,2023-09-23 18:31:17,2
Intel,k1u94ng,moving data between the two is the issue,AMD,2023-09-23 11:20:15,9
Intel,k1ud2dh,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:00:31,8
Intel,k1tzket,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",AMD,2023-09-23 09:22:31,2
Intel,k1tcnbk,Game physics doesn't seem to be a focus anymore though,AMD,2023-09-23 04:47:04,17
Intel,k1vx974,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,AMD,2023-09-23 18:30:38,3
Intel,k1y74dq,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,AMD,2023-09-24 04:08:48,0
Intel,k1zf2ze,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",AMD,2023-09-24 12:27:53,0
Intel,k1u0kty,"Yes, but not after Nvidia bought Ageia.",AMD,2023-09-23 09:35:56,5
Intel,k1y5xsz,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",AMD,2023-09-24 03:57:42,1
Intel,k1usgsa,I like how you said: static image  because in motion upscaling is crappier,AMD,2023-09-23 14:07:47,0
Intel,k1t97dw,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",AMD,2023-09-23 04:13:02,-3
Intel,k1t7en2,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",AMD,2023-09-23 03:55:55,4
Intel,k1t22gi,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",AMD,2023-09-23 03:08:23,1
Intel,k1u3zfu,"It *is* way slower, you're just compensating it by reducing render resolution a ton",AMD,2023-09-23 10:19:33,-8
Intel,k1ubnzq,If it was bloodborne i am guilty of that myself,AMD,2023-09-23 11:46:49,5
Intel,k1uerlc,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:16:37,6
Intel,k1y7wcn,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,AMD,2023-09-24 04:16:20,1
Intel,k1tarlv,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",AMD,2023-09-23 04:28:09,5
Intel,k1tfawo,Rasterisation is a “hack” too,AMD,2023-09-23 05:15:07,40
Intel,k1sxetf,If it works it works....computer graphics has always been about approximation,AMD,2023-09-23 02:30:22,34
Intel,k1t8lwj,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",AMD,2023-09-23 04:07:17,20
Intel,k1tqzxo,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",AMD,2023-09-23 07:31:22,4
Intel,k1vlzcm,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",AMD,2023-09-23 17:20:00,0
Intel,k1s7up5,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,AMD,2023-09-22 23:19:50,-10
Intel,k1sony7,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",AMD,2023-09-23 01:22:48,-6
Intel,k1sot43,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,AMD,2023-09-23 01:23:54,-6
Intel,k1s8jx2,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,AMD,2023-09-22 23:24:55,5
Intel,k1ubk56,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",AMD,2023-09-23 11:45:47,2
Intel,k1snovf,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",AMD,2023-09-23 01:15:25,3
Intel,k1sd914,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,AMD,2023-09-22 23:58:55,7
Intel,k1ubcdm,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",AMD,2023-09-23 11:43:38,-1
Intel,k1tno35,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",AMD,2023-09-23 06:50:58,10
Intel,k1ucpbc,Get that fps with a 5120*1440 240Hz monitor,AMD,2023-09-23 11:56:59,1
Intel,k1u928y,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",AMD,2023-09-23 11:19:32,-3
Intel,k1vo11q,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",AMD,2023-09-23 17:32:45,1
Intel,k1s3igv,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,AMD,2023-09-22 22:48:46,5
Intel,k1ugb6v,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",AMD,2023-09-23 12:30:45,1
Intel,k1wecws,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",AMD,2023-09-23 20:18:14,1
Intel,k1sbp25,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,AMD,2023-09-22 23:47:34,-6
Intel,k1sj47l,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",AMD,2023-09-23 00:41:39,-6
Intel,k1tbdg8,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,AMD,2023-09-23 04:34:12,9
Intel,k1tgkh1,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",AMD,2023-09-23 05:29:06,3
Intel,k1ueofm,8600gt in SLI man you are Savage!🔥,AMD,2023-09-23 12:15:48,2
Intel,k1v8q15,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",AMD,2023-09-23 15:56:19,3
Intel,k20zqc9,there there’s no way there will ever be another 8800 GT. you got so much for your money.,AMD,2023-09-24 18:32:27,3
Intel,k1v3f8y,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",AMD,2023-09-23 15:22:27,2
Intel,k1vqgag,Seemed like a perfect use case for the sli bridge they got rid of.,AMD,2023-09-23 17:48:01,3
Intel,k1y719d,Why would it need to send the data to the other card? They both feed into the same game.,AMD,2023-09-24 04:07:58,2
Intel,k1uqeyb,Big up the Vega gang,AMD,2023-09-23 13:52:34,3
Intel,k1tgpce,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",AMD,2023-09-23 05:30:35,35
Intel,k1u8rwh,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",AMD,2023-09-23 11:16:24,9
Intel,k1tffr8,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,AMD,2023-09-23 05:16:37,17
Intel,k1toby8,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,AMD,2023-09-23 06:59:04,3
Intel,k1w7uac,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",AMD,2023-09-23 19:37:13,2
Intel,k1thrmi,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",AMD,2023-09-23 05:42:24,3
Intel,k1t66vz,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",AMD,2023-09-23 03:44:48,3
Intel,k1ueiu5,We are guilty of the exact same sin.,AMD,2023-09-23 12:14:20,2
Intel,k1tzi21,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-09-23 09:21:41,0
Intel,k1u5aaz,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",AMD,2023-09-23 10:35:43,2
Intel,k1u0whe,would you care to explain ? Kinda interested to here this,AMD,2023-09-23 09:40:08,1
Intel,k1scoi2,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,AMD,2023-09-22 23:54:43,7
Intel,k1tcqbd,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",AMD,2023-09-23 04:47:55,7
Intel,k256x6l,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,AMD,2023-09-25 14:57:47,2
Intel,k1s8ne8,Turn on path tracing. Embrace the PowerPoint.,AMD,2023-09-22 23:25:36,4
Intel,k1ssjdy,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,AMD,2023-09-23 01:52:37,10
Intel,k1vpcr3,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",AMD,2023-09-23 17:41:03,0
Intel,k1s8f3n,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",AMD,2023-09-22 23:23:57,-4
Intel,k1wloml,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",AMD,2023-09-23 21:03:39,1
Intel,k1tpscr,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",AMD,2023-09-23 07:16:33,6
Intel,k1sen3q,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",AMD,2023-09-23 00:09:03,4
Intel,k1sdc8m,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",AMD,2023-09-22 23:59:34,-1
Intel,k1tcqfl,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",AMD,2023-09-23 04:47:57,3
Intel,k1ttjgv,Shame you don't. They did it for a reason.,AMD,2023-09-23 08:03:50,2
Intel,k1v42qp,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,AMD,2023-09-23 15:26:42,3
Intel,k1wmal2,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",AMD,2023-09-23 21:07:26,2
Intel,k1w2bdg,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,AMD,2023-09-23 19:02:32,3
Intel,k1wmeoj,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",AMD,2023-09-23 21:08:09,0
Intel,k1zjknf,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",AMD,2023-09-24 13:05:06,0
Intel,k1ue2iz,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,AMD,2023-09-23 12:10:02,3
Intel,k1tkit9,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",AMD,2023-09-23 06:13:48,4
Intel,k1t75sa,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",AMD,2023-09-23 03:53:39,3
Intel,k1v732r,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",AMD,2023-09-23 15:45:58,6
Intel,k1ub5y8,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,AMD,2023-09-23 11:41:49,13
Intel,k1t4gtg,Or a bag of fake tricks.,AMD,2023-09-23 03:29:16,11
Intel,k1u716i,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",AMD,2023-09-23 10:56:45,0
Intel,k1tnk4a,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",AMD,2023-09-23 06:49:39,0
Intel,k1szwwn,Have you tried Metro Exodus Enhanced?,AMD,2023-09-23 02:50:34,5
Intel,k1ttno1,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,AMD,2023-09-23 08:05:18,3
Intel,k1wlywq,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",AMD,2023-09-23 21:05:25,1
Intel,k1u602a,What is this? A reasonable comment in this dumpster fire of a sub?,AMD,2023-09-23 10:44:29,3
Intel,k1sg0o3,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,AMD,2023-09-23 00:19:01,2
Intel,k1tgd2j,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",AMD,2023-09-23 05:26:46,3
Intel,k1v2w6l,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",AMD,2023-09-23 15:18:58,3
Intel,k1wpc9k,Except it was just a rebadged 8800GTX/Ultra,AMD,2023-09-23 21:27:07,2
Intel,k1wso05,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",AMD,2023-09-23 21:49:06,1
Intel,k1tlekn,Pixel art go brr,AMD,2023-09-23 06:23:58,2
Intel,k1uefox,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,AMD,2023-09-23 12:13:29,3
Intel,k1tws4k,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",AMD,2023-09-23 08:46:03,0
Intel,k1shejh,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",AMD,2023-09-23 00:29:11,3
Intel,k1tykau,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",AMD,2023-09-23 09:09:21,2
Intel,jws0ze9,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",AMD,2023-08-18 21:33:20,71
Intel,jwrrxaq,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),AMD,2023-08-18 20:34:56,55
Intel,jwtujwf,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,AMD,2023-08-19 06:46:11,10
Intel,jws0ogv,I wonder if this will get added to Mangohud and Gamescope.,AMD,2023-08-18 21:31:17,9
Intel,jws656f,This is quality. Great work.,AMD,2023-08-18 22:08:17,8
Intel,jwtl7yn,Doesn’t capframeX uses presentmon as its monitoring tool?,AMD,2023-08-19 04:58:08,4
Intel,jwse5d4,I can finally see if it really is the ENB taking down my Skyrim gamesaves,AMD,2023-08-18 23:05:28,3
Intel,jwt3rjk,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",AMD,2023-08-19 02:18:52,9
Intel,jwv7vh1,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),AMD,2023-08-19 15:06:20,2
Intel,jwx068e,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",AMD,2023-08-19 21:46:34,2
Intel,jx6nzqo,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,AMD,2023-08-21 21:00:38,2
Intel,jwt26ko,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",AMD,2023-08-19 02:06:15,7
Intel,jwvd88w,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,AMD,2023-08-19 15:41:34,3
Intel,jwsaw5e,Thanks Intel! I will try this out at least since I hate MSI afterburner.,AMD,2023-08-18 22:42:00,4
Intel,jwss1oh,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,AMD,2023-08-19 00:48:31,4
Intel,jwsaaac,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",AMD,2023-08-18 22:37:37,-2
Intel,jwteu8t,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,AMD,2023-08-19 03:54:41,0
Intel,jwscc3e,And AmD gives far more than Nvidia.,AMD,2023-08-18 22:52:16,-10
Intel,jwwf6wi,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,AMD,2023-08-19 19:37:09,1
Intel,k25hh4a,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,AMD,2023-09-25 16:00:53,1
Intel,jwsaffm,People have reported that cpu usage in Radeon software is not very accurate.,AMD,2023-08-18 22:38:38,33
Intel,jwsejpm,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,AMD,2023-08-18 23:08:23,7
Intel,jwsu2it,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",AMD,2023-08-19 01:03:33,3
Intel,jwubtsm,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",AMD,2023-08-19 10:34:05,2
Intel,jws5mkd,Just use afterburner as OSD.,AMD,2023-08-18 22:04:42,3
Intel,jwtvx7w,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,AMD,2023-08-19 07:03:38,1
Intel,k3rohbn,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",AMD,2023-10-06 20:51:17,1
Intel,jwrzqfw,You beat me to it :),AMD,2023-08-18 21:25:03,4
Intel,jx08btc,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",AMD,2023-08-20 15:17:16,9
Intel,jwu03xe,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,AMD,2023-08-19 07:58:51,8
Intel,jx8l1n5,Technically it should be possible to add in MSI afterburner because it's open source,AMD,2023-08-22 06:01:19,1
Intel,jwtnlcl,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",AMD,2023-08-19 05:23:42,3
Intel,jwyec42,It was a pet project of one of the Intel engineers.   6/10 is not bad!,AMD,2023-08-20 03:57:36,4
Intel,jx6uru1,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,AMD,2023-08-21 21:43:19,1
Intel,jwtopeu,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,AMD,2023-08-19 05:36:25,6
Intel,jwus2bx,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",AMD,2023-08-19 13:12:58,2
Intel,jwwr3eh,I hate afterburner and RTSS. This is way better,AMD,2023-08-19 20:49:19,1
Intel,jwst65i,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",AMD,2023-08-19 00:56:51,8
Intel,jwtohd3,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",AMD,2023-08-19 05:33:51,-8
Intel,jwt37au,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",AMD,2023-08-19 02:14:23,2
Intel,jwsd0ai,"Well, everyone uses RTSS anyway and it gives you basically everything.",AMD,2023-08-18 22:57:07,2
Intel,jx7hc39,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,AMD,2023-08-22 00:20:25,1
Intel,jwscka1,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",AMD,2023-08-18 22:53:52,9
Intel,jwxcu6w,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,AMD,2023-08-19 23:14:33,1
Intel,jwsf4i1,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",AMD,2023-08-18 23:12:39,2
Intel,jwsnfu5,Afterburner fucks with my settings in adrenaline,AMD,2023-08-19 00:13:59,5
Intel,jwu3toe,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",AMD,2023-08-19 08:48:58,4
Intel,jwv9n3n,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",AMD,2023-08-19 15:18:05,3
Intel,jxi33zm,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,AMD,2023-08-24 02:22:09,1
Intel,jwsus0t,I get downvoted for asking a valid question?,AMD,2023-08-19 01:08:51,1
Intel,jwtt73f,Thank you for continuing to contribute Nothing to this conversation.,AMD,2023-08-19 06:29:18,7
Intel,jwslkce,Clearly not more than this beta of presentmon,AMD,2023-08-18 23:59:56,1
Intel,jwsd4sf,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,AMD,2023-08-18 22:58:01,-2
Intel,jwsfkyz,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",AMD,2023-08-18 23:16:00,9
Intel,jwxd4ky,Where are you seeing this?,AMD,2023-08-19 23:16:32,1
Intel,jwsfr15,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",AMD,2023-08-18 23:17:13,1
Intel,jwsno08,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",AMD,2023-08-19 00:15:40,13
Intel,jwuero7,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,AMD,2023-08-19 11:08:44,3
Intel,jwu8j4q,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,AMD,2023-08-19 09:51:11,3
Intel,jwv8abu,Great news to gamers though,AMD,2023-08-19 15:09:05,2
Intel,jwvbwid,ah sorry I meant NVK,AMD,2023-08-19 15:32:34,1
Intel,jwsuuv1,"I don't know, I didn't downvote you.",AMD,2023-08-19 01:09:27,6
Intel,jwsec7c,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,AMD,2023-08-18 23:06:52,6
Intel,jwt2g8b,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",AMD,2023-08-19 02:08:23,3
Intel,jwxvkn7,It’s where you oc in adrenaline,AMD,2023-08-20 01:27:06,1
Intel,jwuly2q,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",AMD,2023-08-19 12:21:17,5
Intel,jwv200x,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",AMD,2023-08-19 14:26:34,3
Intel,jwtzdkr,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",AMD,2023-08-19 07:49:04,5
Intel,jwuvpd6,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",AMD,2023-08-19 13:41:23,2
Intel,jwv1po6,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",AMD,2023-08-19 14:24:33,5
Intel,jwuapdo,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",AMD,2023-08-19 10:19:39,2
Intel,jx3nm3i,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",AMD,2023-08-21 06:56:43,1
Intel,jwv7qel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",AMD,2023-08-19 15:05:22,2
Intel,jwucdfz,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",AMD,2023-08-19 10:40:52,2
Intel,jwuwjmm,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",AMD,2023-08-19 13:47:45,1
Intel,jx4di9f,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",AMD,2023-08-21 12:07:55,1
Intel,jwuithf,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",AMD,2023-08-19 11:51:44,4
Intel,jwuq603,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",AMD,2023-08-19 12:57:40,3
Intel,jwuozk8,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",AMD,2023-08-19 12:47:47,2
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,52
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,9
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,4
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,3
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,3
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,5
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,2
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,4
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,1
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,3
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nsktadm,Will be a interresting CES,Intel,2025-12-06 11:12:47,19
Intel,nso11hn,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Intel,2025-12-06 22:37:18,7
Intel,nswpbo1,Merry Christmas everyone,Intel,2025-12-08 08:52:05,2
Intel,nswyceh,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Intel,2025-12-08 10:25:38,2
Intel,nsofcmj,They can't even ship B60's.,Intel,2025-12-07 00:01:12,2
Intel,nsp4pld,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Intel,2025-12-07 02:38:20,1
Intel,nsm5mtm,Aren't they always,Intel,2025-12-06 16:34:23,3
Intel,nt8d8jl,give it some time...,Intel,2025-12-10 03:42:11,1
Intel,nt8d10w,Merry Bitmas,Intel,2025-12-10 03:40:49,1
Intel,nsol1s5,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Intel,2025-12-07 00:34:27,7
Intel,nsq0noc,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Intel,2025-12-07 06:21:43,6
Intel,nswuidx,What is taking Nividia so long with the super cards?,Intel,2025-12-08 09:46:34,2
Intel,nt0mfun,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Intel,2025-12-08 22:47:54,1
Intel,nsn1l8j,There's definitely been lame ones.,Intel,2025-12-06 19:20:53,3
Intel,nt0k5mq,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Intel,2025-12-08 22:35:24,2
Intel,nt0ni97,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Intel,2025-12-08 22:53:46,1
Intel,nqdrca0,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Intel,2025-11-23 16:52:41,13
Intel,nqe5eoy,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Intel,2025-11-23 18:04:37,13
Intel,nr20ozq,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Intel,2025-11-27 14:11:52,1
Intel,nqga537,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Intel,2025-11-24 00:43:14,5
Intel,nqg407n,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Intel,2025-11-24 00:08:08,5
Intel,nqfqmb2,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Intel,2025-11-23 22:49:52,2
Intel,nnbivzp,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Intel,2025-11-05 22:28:54,36
Intel,nnbkl4o,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Intel,2025-11-05 22:37:53,14
Intel,nnbr1vs,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Intel,2025-11-05 23:13:11,6
Intel,nncz7bz,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Intel,2025-11-06 03:36:19,5
Intel,nnftti4,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Intel,2025-11-06 16:14:24,2
Intel,nndm2ga,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Intel,2025-11-06 06:31:30,3
Intel,nnc0ph2,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Intel,2025-11-06 00:08:27,-7
Intel,nnjsaw2,Will the 10 core Xe be better than radeon 890m or worse?,Intel,2025-11-07 05:14:22,0
Intel,nnbhb9s,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Intel,2025-11-05 22:20:34,-6
Intel,nnbj579,Still weaker than x3d,Intel,2025-11-05 22:30:15,-17
Intel,nnm29u0,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Intel,2025-11-07 15:46:06,3
Intel,npu5d43,Doesn't the B already serve that purpose?,Intel,2025-11-20 13:03:06,1
Intel,nnbmjlf,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Intel,2025-11-05 22:48:16,13
Intel,nnd5hpv,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Intel,2025-11-06 04:19:26,8
Intel,nnclctl,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Intel,2025-11-06 02:10:09,6
Intel,nnde8rz,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Intel,2025-11-06 05:25:14,10
Intel,nngpesf,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Intel,2025-11-06 18:45:02,0
Intel,nnklixi,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Intel,2025-11-07 09:57:31,2
Intel,nnlg7cm,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Intel,2025-11-07 13:50:18,1
Intel,nnbigvq,What're you doing on a laptop?,Intel,2025-11-05 22:26:37,19
Intel,nnbiwg3,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Intel,2025-11-05 22:28:59,9
Intel,nnbowvu,It's for handhelds and office laptops not hyper enthusiast shit.,Intel,2025-11-05 23:01:09,2
Intel,nncdk44,Nobody buys AMD laptops,Intel,2025-11-06 01:23:02,11
Intel,nnbk5wm,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Intel,2025-11-05 22:35:40,6
Intel,nnco8fw,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Intel,2025-11-06 02:27:36,1
Intel,nnz3axo,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Intel,2025-11-09 18:46:41,2
Intel,nnbn2as,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Intel,2025-11-05 22:51:02,10
Intel,nndnorc,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Intel,2025-11-06 06:46:30,8
Intel,nndrn8v,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Intel,2025-11-06 07:23:49,3
Intel,nnbivk1,Highly immersive porn on the go,Intel,2025-11-05 22:28:50,13
Intel,nnblwqh,"Idk, FEM sim of a pressure boiler?",Intel,2025-11-05 22:44:56,0
Intel,nnc0bgd,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Intel,2025-11-06 00:06:14,5
Intel,nnbkp8h,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Intel,2025-11-05 22:38:29,7
Intel,nnbkxvp,PTL extends up to the -H series too.,Intel,2025-11-05 22:39:46,3
Intel,nnbkts2,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Intel,2025-11-05 22:39:10,3
Intel,nnbl91n,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Intel,2025-11-05 22:41:25,2
Intel,nnc0woc,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Intel,2025-11-06 00:09:35,4
Intel,nnc2335,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Intel,2025-11-06 00:16:15,9
Intel,nnblgop,Just get a Vision Pro?,Intel,2025-11-05 22:42:32,6
Intel,nnbyre8,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Intel,2025-11-05 23:57:18,1
Intel,nnbmp20,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Intel,2025-11-05 22:49:05,4
Intel,nnbrc9f,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Intel,2025-11-05 23:14:48,0
Intel,nnc5j69,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Intel,2025-11-06 00:35:47,6
Intel,nnd2wbt,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Intel,2025-11-06 04:01:09,4
Intel,nnc1auf,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Intel,2025-11-06 00:11:48,1
Intel,nncm4mg,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Intel,2025-11-06 02:14:47,8
Intel,nnj3sul,"Two options seems right, either you care about it or you don't.",Intel,2025-11-07 02:28:49,1
Intel,nnh5cgt,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Intel,2025-11-06 20:02:16,1
Intel,nncnlt8,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Intel,2025-11-06 02:23:44,3
Intel,nncrc61,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Intel,2025-11-06 02:46:29,0
Intel,nnd4ak4,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Intel,2025-11-06 04:10:51,2
Intel,nndr8zp,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Intel,2025-11-06 07:19:59,4
Intel,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
Intel,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,3
Intel,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
Intel,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
Intel,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
Intel,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
Intel,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
Intel,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
Intel,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
Intel,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
Intel,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
Intel,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
Intel,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
Intel,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
Intel,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
Intel,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
Intel,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
Intel,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
Intel,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
Intel,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
Intel,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
Intel,nmoztyk,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Intel,2025-11-02 12:36:20,4
Intel,nmzufnv,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Intel,2025-11-04 02:52:11,2
Intel,nmquye0,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Intel,2025-11-02 18:34:43,1
Intel,nqarc70,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Intel,2025-11-23 03:04:10,1
Intel,noa52e1,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Intel,2025-11-11 13:57:33,1
Intel,nlbgoss,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Intel,2025-10-25 14:51:22,11
Intel,nlawueh,That naming scheme really is complete and utter dogshit,Intel,2025-10-25 12:59:00,6
Intel,nlp3wrh,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Intel,2025-10-27 19:03:55,1
Intel,nlrilyu,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Intel,2025-10-28 02:55:34,1
Intel,nlz6ywu,I'm looking forward to check how those series will perform!!,Intel,2025-10-29 09:04:56,1
Intel,nlbj26i,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Intel,2025-10-25 15:03:33,18
Intel,nlbihha,look forward to new APUs,Intel,2025-10-25 15:00:35,2
Intel,nlhde6w,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Intel,2025-10-26 14:36:10,2
Intel,nlpqfuu,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Intel,2025-10-27 20:58:10,1
Intel,nmbs4xl,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Intel,2025-10-31 06:11:07,1
Intel,nldt8zb,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Intel,2025-10-25 22:15:21,0
Intel,nlclyxd,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Intel,2025-10-25 18:23:40,-8
Intel,nlguu28,You’ll be waiting till 2027 on the amd side.,Intel,2025-10-26 12:44:06,3
Intel,nmbs0u8,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Intel,2025-10-31 06:09:58,1
Intel,nlq5vco,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Intel,2025-10-27 22:20:54,1
Intel,nlg1l4u,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Intel,2025-10-26 08:16:14,8
Intel,nlp4s9i,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Intel,2025-10-27 19:08:23,2
Intel,nlpqrfz,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Intel,2025-10-27 20:59:49,1
Intel,nlcna8j,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Intel,2025-10-25 18:30:27,9
Intel,nlp4f4m,Really? This should be good for Intel in 2026.,Intel,2025-10-27 19:06:31,1
Intel,nmbt7jh,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Intel,2025-10-31 06:22:00,1
Intel,nlqexwa,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Intel,2025-10-27 23:11:50,1
Intel,nmbsirj,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Intel,2025-10-31 06:15:03,1
Intel,nld2j3h,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Intel,2025-10-25 19:51:15,-5
Intel,nlqjpxg,👍,Intel,2025-10-27 23:38:19,1
Intel,nld2x4w,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Intel,2025-10-25 19:53:17,9
Intel,nld43st,You are right. I meant 'Gorgon Point'.,Intel,2025-10-25 19:59:34,1
Intel,nln841i,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Intel,2025-10-27 13:21:07,2
Intel,nlp48zs,My brain hurts and I’m still confused,Intel,2025-10-27 19:05:38,1
Intel,nmbrwqs,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Intel,2025-10-31 06:08:49,2
Intel,nkxsilf,"I think him saying ""unreleased products"" could mean it's still coming.",Intel,2025-10-23 11:21:44,26
Intel,nkz30mp,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Intel,2025-10-23 15:39:54,10
Intel,nkzp226,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Intel,2025-10-23 17:25:52,3
Intel,nl8a9mb,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Intel,2025-10-25 00:22:50,1
Intel,nkxmfto,Intel is not serious with dGPUs,Intel,2025-10-23 10:33:35,-6
Intel,nl0j2ic,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Intel,2025-10-23 19:51:32,8
Intel,nlec4sq,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Intel,2025-10-26 00:09:57,2
Intel,nm6dcp4,B50 is interesting once the software is there (planned Q4).,Intel,2025-10-30 11:58:35,1
Intel,nkxz8vy,"Yes, they are literally just playing. It's all a game lol",Intel,2025-10-23 12:08:42,11
Intel,nkz3604,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Intel,2025-10-23 15:40:37,2
Intel,nlbmybg,They are as serious as AMD.,Intel,2025-10-25 15:23:34,1
Intel,nl1d1w8,This interview happened during the quiet period so I don't think he could talk about the future,Intel,2025-10-23 22:26:07,8
Intel,nlbmm1g,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Intel,2025-10-25 15:21:47,2
Intel,nlc5u9q,Where did you hear that it was cancelled?,Intel,2025-10-25 17:01:38,1
Intel,nkzrbdu,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Intel,2025-10-23 17:36:29,3
Intel,nkzb6g5,There has been no update regarding celestial dGPUs internally.,Intel,2025-10-23 16:19:27,4
Intel,nl9n96c,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Intel,2025-10-25 06:15:43,1
Intel,nlc8fdp,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Intel,2025-10-25 17:14:34,1
Intel,nlcandy,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Intel,2025-10-25 17:25:50,1
Intel,nlbpug0,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Intel,2025-10-25 15:38:24,1
Intel,nlc7sgw,Ex-Intel coworkers/acquaintances.,Intel,2025-10-25 17:11:23,2
Intel,nl34or5,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Intel,2025-10-24 05:10:43,2
Intel,nl3q53x,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Intel,2025-10-24 08:35:10,2
Intel,nlafjvl,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Intel,2025-10-25 10:48:26,1
Intel,nlcioic,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Intel,2025-10-25 18:06:26,1
Intel,nlbtqtn,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Intel,2025-10-25 15:58:32,3
Intel,nlcbiv4,"So, no news story has come out stating that?",Intel,2025-10-25 17:30:18,3
Intel,noidhfz,Xe3P-HPM suggests otherwise,Intel,2025-11-12 19:47:04,1
Intel,nlc8k5a,"Yes, through my ex colleagues",Intel,2025-10-25 17:15:14,2
Intel,nleb7hi,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Intel,2025-10-26 00:04:10,1
Intel,nlbuyry,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Intel,2025-10-25 16:04:56,2
Intel,nmbst69,Why would Intel tell you? It would just stall selling all current ARC cards.,Intel,2025-10-31 06:17:59,3
Intel,nlchyo2,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Intel,2025-10-25 18:02:48,2
Intel,noifll4,What about it? That some reference exists in drivers?,Intel,2025-11-12 19:57:24,1
Intel,nlc2d7k,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Intel,2025-10-25 16:44:12,3
Intel,nlc7zs4,BMG was 0 margin product,Intel,2025-10-25 17:12:23,3
Intel,noiihla,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Intel,2025-11-12 20:11:56,1
Intel,nlc3nqx,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Intel,2025-10-25 16:50:50,2
Intel,noijing,They're also using Xe3p for NVL-P and that Island AI product.,Intel,2025-11-12 20:17:09,3
Intel,nlci8uq,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Intel,2025-10-25 18:04:14,3
Intel,noimd5x,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Intel,2025-11-12 20:31:37,0
Intel,nlck9s5,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Intel,2025-10-25 18:14:45,1
Intel,nled55i,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Intel,2025-10-26 00:16:21,1
Intel,nmbspxb,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Intel,2025-10-31 06:17:05,1
Intel,noinhcz,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Intel,2025-11-12 20:37:24,3
Intel,nkl7ghk,Think pretty easy naming. Any X infront just automatically means better iGPU,Intel,2025-10-21 12:17:01,36
Intel,nklikpx,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Intel,2025-10-21 13:28:12,18
Intel,nkodq7t,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Intel,2025-10-21 21:59:26,8
Intel,nkkzyp1,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Intel,2025-10-21 11:20:48,14
Intel,nkkoc9f,Naming for these chips are terrible,Intel,2025-10-21 09:31:57,11
Intel,nkmeuem,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Intel,2025-10-21 16:22:11,3
Intel,nkzbmky,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Intel,2025-10-23 16:21:35,1
Intel,nkld363,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Intel,2025-10-21 12:54:21,17
Intel,nkuhiob,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Intel,2025-10-22 20:56:33,3
Intel,nl9xb0y,"If the game could be 50–60% stronger, that would be That would be a killer",Intel,2025-10-25 07:53:56,1
Intel,nl9xpor,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Intel,2025-10-25 07:57:55,1
Intel,nkljsi3,"That’s been true for the past generations, but it looks like it will change this generation",Intel,2025-10-21 13:35:30,8
Intel,nkkrh6o,Still better than Ryzen 365 AI pro MAX+,Intel,2025-10-21 10:04:31,56
Intel,nkm3cl3,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Intel,2025-10-21 15:22:49,3
Intel,nkv8b22,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Intel,2025-10-22 23:24:43,1
Intel,nkm4j1i,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Intel,2025-10-21 15:28:57,3
Intel,nkksh88,Yea putting ai the model name is disgusting 😂,Intel,2025-10-21 10:14:17,22
Intel,nkl1j6x,I can't wait for the Ryzen 688S AI Pro MAX+++,Intel,2025-10-21 11:33:19,12
Intel,nkmi2u0,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Intel,2025-10-21 16:38:27,3
Intel,nkmgpw7,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Intel,2025-10-21 16:31:44,6
Intel,nkpnp7f,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Intel,2025-10-22 02:32:12,3
Intel,nklj3is,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Intel,2025-10-21 13:31:20,-2
Intel,nknc85p,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Intel,2025-10-21 18:59:33,2
Intel,nklps98,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Intel,2025-10-21 14:09:54,0
Intel,nko4b5d,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Intel,2025-10-21 21:10:14,6
Intel,nklngwf,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Intel,2025-10-21 13:56:45,13
Intel,nkns6e4,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Intel,2025-10-21 20:13:23,3
Intel,nkoestf,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Intel,2025-10-21 22:05:28,5
Intel,nkmk3eq,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Intel,2025-10-21 16:48:08,2
Intel,nkpsi6y,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Intel,2025-10-22 03:02:49,4
Intel,nkof5d7,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Intel,2025-10-21 22:07:26,1
Intel,nkq1gt0,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Intel,2025-10-22 04:06:17,2
Intel,nm4gnwa,Same core counts too,Intel,2025-10-30 02:20:15,1
Intel,njdkfg9,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-10-14 01:35:52,1
Intel,njo3me8,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Intel,2025-10-15 18:57:43,52
Intel,njp2gv2,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Intel,2025-10-15 21:57:08,23
Intel,njq65e2,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Intel,2025-10-16 01:51:35,10
Intel,njoj14y,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Intel,2025-10-15 20:14:46,6
Intel,njpl7pm,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Intel,2025-10-15 23:47:19,4
Intel,nkgif8b,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Intel,2025-10-20 11:51:27,1
Intel,nkzvqp0,"It's going to cost like $10,000, right?",Intel,2025-10-23 17:57:21,1
Intel,njo7pwi,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Intel,2025-10-15 19:18:08,25
Intel,njoyfid,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Intel,2025-10-15 21:34:53,6
Intel,nq5rfh4,seee ceerto seeeh,Intel,2025-11-22 07:44:01,1
Intel,njpivm1,"It really is a mess, someone should've been fired long ago.",Intel,2025-10-15 23:34:18,9
Intel,nmbtn83,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Intel,2025-10-31 06:26:27,2
Intel,njrrggy,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Intel,2025-10-16 10:08:28,1
Intel,njolo2l,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Intel,2025-10-15 20:28:01,16
Intel,njo9lo1,because 50% more cores need 50% more power for 50% more performance.,Intel,2025-10-15 19:27:37,30
Intel,njopbmv,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Intel,2025-10-15 20:46:36,15
Intel,njpsirt,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Intel,2025-10-16 00:28:36,4
Intel,njrpj24,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Intel,2025-10-16 09:49:54,-3
Intel,njsegc9,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Intel,2025-10-16 12:58:39,1
Intel,njopfv9,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Intel,2025-10-15 20:47:12,-4
Intel,njoa1f3,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Intel,2025-10-15 19:29:49,11
Intel,njqexqp,But LNL's TDP is too low compared to H45 cpu,Intel,2025-10-16 02:47:44,5
Intel,njrcz7w,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Intel,2025-10-16 07:38:36,2
Intel,nmbtgoa,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Intel,2025-10-31 06:24:35,1
Intel,njqijcd,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Intel,2025-10-16 03:12:16,3
Intel,njrgeuz,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Intel,2025-10-16 08:14:34,1
Intel,njote4v,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Intel,2025-10-15 21:07:31,22
Intel,njow153,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Intel,2025-10-15 21:21:46,18
Intel,njoudob,Intel is gonna have better integrated graphics than AMD,Intel,2025-10-15 21:12:50,4
Intel,njoar3w,I have no idea.,Intel,2025-10-15 19:33:29,11
Intel,njqtu4m,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Intel,2025-10-16 04:38:47,5
Intel,njqtb7t,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Intel,2025-10-16 04:34:25,4
Intel,njrhrow,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Intel,2025-10-16 08:28:47,2
Intel,njoxfxz,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Intel,2025-10-15 21:29:28,6
Intel,njp7qsx,Yeah people act like strix point is in that segment..... It's not.,Intel,2025-10-15 22:28:04,10
Intel,njp58ob,🫨,Intel,2025-10-15 22:13:13,3
Intel,njs0rkq,Wouldn't be the first time.,Intel,2025-10-16 11:27:26,1
Intel,njqutc3,"Oh, I guess it be like that.",Intel,2025-10-16 04:46:59,0
Intel,nis4cle,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Intel,2025-10-10 14:44:00,16
Intel,nirfu8b,"""we are tending to prefer e cores now when gaming""   That's very surprising",Intel,2025-10-10 12:29:22,24
Intel,nisac9a,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Intel,2025-10-10 15:13:09,8
Intel,niv55kq,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Intel,2025-10-11 00:24:27,4
Intel,niryzd1,I’m guessing Xe3P will be on Intel 3-PT.,Intel,2025-10-10 14:17:01,4
Intel,nitrjvx,"Tom is a funny guy, love it when he gets camera time",Intel,2025-10-10 19:37:13,4
Intel,nisfksg,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Intel,2025-10-10 15:38:45,2
Intel,niwlua9,"When is the panther lake reveal going to be, CES?",Intel,2025-10-11 07:13:32,1
Intel,nirs4bu,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Intel,2025-10-10 13:41:08,32
Intel,nirh07n,Wonder if there's some energy star requirements.,Intel,2025-10-10 12:36:46,5
Intel,nirq8f2,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Intel,2025-10-10 13:30:47,2
Intel,nitssdt,It's either N3P or 18AP.,Intel,2025-10-10 19:43:43,5
Intel,nirzgoz,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Intel,2025-10-10 14:19:27,17
Intel,niuj3v7,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Intel,2025-10-10 22:04:33,3
Intel,nirymua,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Intel,2025-10-10 14:15:14,5
Intel,nirya2r,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Intel,2025-10-10 14:13:27,2
Intel,nispl6v,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Intel,2025-10-10 16:27:41,7
Intel,niwprxq,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Intel,2025-10-11 07:54:44,3
Intel,niwoax2,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Intel,2025-10-11 07:39:15,1
Intel,nisife2,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Intel,2025-10-10 15:52:33,5
Intel,niwk1j6,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Intel,2025-10-11 06:55:10,3
Intel,nitfd2e,I'm tired of AMD slop consoles and handhelds,Intel,2025-10-10 18:34:17,-9
Intel,nixk3v6,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Intel,2025-10-11 12:43:47,2
Intel,nj1kiyw,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Intel,2025-10-12 02:44:36,1
Intel,niy0x5h,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Intel,2025-10-11 14:29:59,2
Intel,nix9wlx,Huh????,Intel,2025-10-11 11:23:59,2
Intel,nixt1zv,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Intel,2025-10-11 13:42:49,2
Intel,nj4298q,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Intel,2025-10-12 15:05:26,3
Intel,niy0ck1,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Intel,2025-10-11 14:26:44,1
Intel,niylmew,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Intel,2025-10-11 16:21:28,1
Intel,nizip6t,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Intel,2025-10-11 19:15:30,1
Intel,nj1tulq,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Intel,2025-10-12 03:51:16,2
Intel,nizcl4d,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Intel,2025-10-11 18:42:08,4
Intel,nj2hurj,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Intel,2025-10-12 07:32:20,2
Intel,nih90cf,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Intel,2025-10-08 19:56:24,18
Intel,nihman0,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Intel,2025-10-08 21:00:02,7
Intel,nigsicb,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Intel,2025-10-08 18:33:15,17
Intel,nij93k3,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Intel,2025-10-09 02:46:46,7
Intel,nii800b,Says nothing about discrete graphics. Think that's clearly dead at this point.,Intel,2025-10-08 23:00:42,-8
Intel,nihwdsn,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Intel,2025-10-08 21:53:14,2
Intel,niq882u,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Intel,2025-10-10 05:55:28,2
Intel,niljjpz,Sounds like you're clearly wrong.,Intel,2025-10-09 14:02:51,2
Intel,niicq4o,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Intel,2025-10-08 23:28:43,7
Intel,niihlwk,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Intel,2025-10-08 23:58:03,6
Intel,nilq0j4,"Lmao, sure. Any day now...",Intel,2025-10-09 14:36:08,0
Intel,nij8lxc,I don't think so considering how expensive LNL was,Intel,2025-10-09 02:43:39,-1
Intel,niv5lac,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Intel,2025-10-11 00:27:22,5
Intel,niqq5bp,What should be the desktop gpu equivalent?,Intel,2025-10-10 08:54:26,1
Intel,nkwiahy,So around a 3050 laptop performance?,Intel,2025-10-23 04:15:22,1
Intel,nobxw7e,"Xe2 wasn't a benchmark btw. Arc 140T is hot, can""t manage benchmarks staing under 100w, just to pair with an 980M.",Intel,2025-11-11 19:25:24,1
Intel,nipaq6n,The problem is how much is the price?,Intel,2025-10-10 01:57:39,0
Intel,nirogih,they should really do better on GPU,Intel,2025-10-10 13:20:46,0
Intel,nip7gnd,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Intel,2025-10-10 01:39:13,-4
Intel,njovxfo,"Just because you have 50% more physical units, doesn't mean you'll automatically get 50% more performance. Remember when the Z1E came out and it was supposed to be 50-100% more power than the steam deck?",Intel,2025-10-15 21:21:11,2
Intel,nouqj4l,Because Xe3 is like Xe2+,Intel,2025-11-14 18:49:00,1
Intel,niqqyxy,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Intel,2025-10-10 09:02:58,3
Intel,nlbl2xp,Yes,Intel,2025-10-25 15:13:57,1
Intel,nis9kyv,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Intel,2025-10-10 15:09:34,9
Intel,niq7bnn,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Intel,2025-10-10 05:47:17,14
Intel,nje1ogx,Maybe I'm confused about what they're advertising but weren't they just going head to head with 4060s with the b580? Wouldn't 3050 range be a huge step backwards?,Intel,2025-10-14 03:21:13,1
Intel,niqr96c,"Damn, still behind my old 1060 pc",Intel,2025-10-10 09:05:57,-2
Intel,nitxpqo,And some of the tiles are internal not on TSMC.,Intel,2025-10-10 20:09:28,3
Intel,nix2sd3,"On-package memory doesn't raise the device price from and end user perspective. It's a margin challenge for Intel because OEMs want the margins from the memory, so Intel needs to pass it along at cost.   Technically, on-package memory can even be cheaper because it can let you simplify the rest of the PCB. Iso-speed, that is.",Intel,2025-10-11 10:15:42,2
Intel,nje21sq,Are you saying you expect people to not want discrete gpu's in the future?  What about the direction of gaming would make you think that?,Intel,2025-10-14 03:23:34,1
Intel,nix2vkc,"> Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.  The 4 Xe tile is on Intel 3, so it'll have a significant clock speed deficit vs the 12Xe tile.",Intel,2025-10-11 10:16:39,0
Intel,niq9fth,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Intel,2025-10-10 06:06:48,-3
Intel,njxpw7k,The b580 is a full power desktop card. The one they're advertising here is a tiny little integrated gpu on a laptop processor,Intel,2025-10-17 07:48:12,1
Intel,niqri91,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Intel,2025-10-10 09:08:33,5
Intel,niydl4w,"We already knew they cancelled desktop variants though, Panther lake was going to be around Arrow Lake laptop performance no matter what.",Intel,2025-10-11 15:38:29,-1
Intel,njoe8zc,"If people can get a XX60ti level iGPU in a laptop, which means you don't have to worry about MUX switches (hardware or software), possibly have a cheaper platform (if scaled) since you don't need a separate GPU board, have access to a ton of RAM (many mid-range GPUs are RAM limited), and due to reduced complexity and the ability to better dynamically manage power can get a more effective slim chassis with less throttling... then yeah, it'd be pretty good.  The next big jump is LPDDR6 which looks to potentially hit a 50% bandwidth increase and marginally lower latency.  Combine that with a wide bus and you're looking at enough to power a XX70 series mobile chip.  The framework is immature but it could be competitive in the future.  We'll see!",Intel,2025-10-15 19:51:03,1
Intel,niqbe7s,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Intel,2025-10-10 06:25:06,11
Intel,niqal4v,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Intel,2025-10-10 06:17:27,9
Intel,niqaj3v,Isn't that lunar lake before and after?,Intel,2025-10-10 06:16:56,4
Intel,nir70pe,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Intel,2025-10-10 11:28:51,2
Intel,nje1eeu,"Dota2 mentioned, I like Intel.",Intel,2025-10-14 03:19:26,1
Intel,nj0ndfp,"this has been leaked a lot, but not confirmed",Intel,2025-10-11 23:11:37,2
Intel,njoo0kf,"Oh laptop gaming sure, but that's pretty low level gaming. Don't most of the people at that level just put up with whatever laptop/prebuilt stuff is available anyway? I really don't think it's going to move the discrete market that much?",Intel,2025-10-15 20:39:56,1
Intel,niqhv6g,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Intel,2025-10-10 07:28:09,3
Intel,niqhrao,Oh really? Wow thanks for the heads up. I really missed the context.,Intel,2025-10-10 07:27:02,1
Intel,niqhxw8,Apparently so! I made a mistake.,Intel,2025-10-10 07:28:56,1
Intel,niv48z3,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Intel,2025-10-11 00:18:22,1
AMD,nrw9wd0,"Because sure, why not.",hardware,2025-12-02 15:13:58,515
AMD,nrwc8fz,Yeah because one cannot pay more for ram than cpu.,hardware,2025-12-02 15:25:51,97
AMD,nrwcjud,"As someone who doesn't have a single piece of Intel silicon in my build, I've never understood people cheering on their downfall. We need competition, people, or shit like this happens.",hardware,2025-12-02 15:27:27,375
AMD,nrw8zib,Bought 64GB memory and 4TB storage in June. Have bought a Ryzen 7 9700X and RX 9070 this Friday.  I feel quite lucky.,hardware,2025-12-02 15:09:11,100
AMD,nrwbr9b,"I don't think this has anything to do with the memory shortage anymore, it's just pure greed.",hardware,2025-12-02 15:23:28,148
AMD,nrxgoqr,"Of course they do. Black Friday prices aren't supposed to be the new normal prices. I'm sure Intel and Nvidia are doing it too, this is not newsworthy.  In the Videocardz article about it they even admit this is not raising the normal prices, it's just prices returning to normal after Black Friday:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",hardware,2025-12-02 18:39:43,44
AMD,nrwkdd5,Won't someone think of the (checks notes) 355 BILLION dollar company!?!?!?!,hardware,2025-12-02 16:05:43,31
AMD,nrxtq3a,At this point I can see me running my 4670k GTX 970 build from 11 years ago until 2040 and beyond. Got my duct tape ready.,hardware,2025-12-02 19:42:31,4
AMD,nrwilkp,A decent PC about to cost the same as a new car,hardware,2025-12-02 15:57:06,18
AMD,nrwntbc,"“Hey people are still buying ram at these prices, let’s raise our prices too!”",hardware,2025-12-02 16:22:14,11
AMD,nrwitc7,"They did not get the memo, that people aren’t building PC due to memory prices?  Good luck AMD, I managed to get 9800x3d for 399€ brand new and I was still on the fence about it. Like really on the fence, it was a stretch. Cause they blow up!  Not to forget I am an enthusiast. I upgrade GPU every gen and always get the latest platform.  If this was a strech for me, then 90% won’t even look at those if you increase the price, especially now.",hardware,2025-12-02 15:58:08,14
AMD,nscnsbo,they’re ryzen the prices  i’ll go now,hardware,2025-12-05 01:46:21,3
AMD,nrwmtrc,"AMD: ""What are you going to do? Buy Intel?""",hardware,2025-12-02 16:17:28,8
AMD,nrx90dd,"Nice, now it's gonna be rising prices for RAM, GPU, and CPU!",hardware,2025-12-02 18:03:32,2
AMD,nrytxvc,"RAM makers are not going to like this, they rather prefer CPU prices being low because now when they ship DDR5 memory to margin highs  they want consumers to not second ask themselves regarding PC upgrades like they already do.",hardware,2025-12-02 22:39:36,2
AMD,nrzlzqo,people who built a pc at summer/spring must be happy af,hardware,2025-12-03 01:17:37,2
AMD,nrwkqqr,"This is what happens when there is no serious competition around the horizon, it's not the first time that AMD has done this btw.",hardware,2025-12-02 16:07:30,4
AMD,nrwlx74,I think AMD should reduce prices instead of increasing.,hardware,2025-12-02 16:13:11,5
AMD,nrwc3qk,"Intel is back, AMD should lower their prices. It would be better for their future.",hardware,2025-12-02 15:25:12,6
AMD,nrwj2qw,"Just bought mine, sorry everybody. Hopefully RAM and motherboard prices will crash for everyone else to make up for it, now that I ovepayed.",hardware,2025-12-02 15:59:24,1
AMD,nrwus2f,man i was JUST thinking of upgrading from my 3900x....,hardware,2025-12-02 16:55:26,1
AMD,nrwwxs7,I guarantee you they wont raise prices of their datacenter CPUs because you bet your ass their customers would switch to arm,hardware,2025-12-02 17:05:53,1
AMD,nryq4ns,"Proof that not having competition is bad.  Fast RISC-V chips soon, hopefully.",hardware,2025-12-02 22:19:47,1
AMD,nrzoltk,"If AMD holds the commanding lead in the retail market share with their Ryzen CPUs, there board and investors will probably be demanding more profit be taken.",hardware,2025-12-03 01:32:58,1
AMD,nrzvzd2,"Damn, didn't know CPU chips had tiny ram slots in them",hardware,2025-12-03 02:16:25,1
AMD,ns0angq,Good thing I got my 9800X3D before this happened.,hardware,2025-12-03 03:44:41,1
AMD,ns0oc5k,And they will not drop them until Intel gets their shit together,hardware,2025-12-03 05:19:56,1
AMD,ns1at1k,"Hehe, glad I got my Ryzen 9 a few days ago.",hardware,2025-12-03 08:38:40,1
AMD,ns758ny,I'm so glad I didn't wait longer to build my PC,hardware,2025-12-04 05:10:58,1
AMD,nrx43ck,"when intel was the top dog, everyone excused it by saying they had the best performance and stability, amd bulldozer was bad blah blah blah don't buy AMD  now that AMD is the industry leader all of a sudden competition is important  so obvious.",hardware,2025-12-02 17:40:26,0
AMD,nrxk0y5,I mean sure why not . Their gpus finally hit MSRP 8 months after launch so something gotta give,hardware,2025-12-02 18:55:23,3
AMD,nry8pik,Glad I bought a 9800x3d last week I guess,hardware,2025-12-02 20:55:29,1
AMD,ns0aqra,"Instead of reporting on your lack of information, be a farking journalist and actually research the answers before writing about it. Even AI can match this level of journalism.",hardware,2025-12-03 03:45:16,1
AMD,nrxhnxr,"Depends how much but I like to think this is my final amd product at this point. They have not been making the best decisions the past few years...   Like if intel is cheaper and more powerful at this point, might as well get that then.",hardware,2025-12-02 18:44:19,-1
AMD,nrx24b6,Because AMD is a damn corporation aiming to keep its margins while learning from the giants about consumer exploitations.,hardware,2025-12-02 17:30:51,0
AMD,nryoaax,"The setup looks clean, but the price news definitely stings.",hardware,2025-12-02 22:10:25,0
AMD,nsaxp48,"No, they're not:  https://www.tomshardware.com/pc-components/cpus/amd-isnt-increasing-prices-on-cpus-at-least-for-now-ryzen-appears-to-be-safe-from-the-ai-hysteria",hardware,2025-12-04 20:05:09,0
AMD,nrwikro,I think it’s way more likely rx 9000 has 0% than it does like 0.1% I mean it just makes sense 0 people bought it,hardware,2025-12-02 15:56:59,-6
AMD,nrwap32,Cache memory is going up in costs so AMD has to make up for it. /s,hardware,2025-12-02 15:18:07,165
AMD,nrxzpki,"You've been clickbaited. Yes, prices go up again after a sale. This is a non-story.",hardware,2025-12-02 20:11:35,32
AMD,nrxjsre,"This is not AMD raising prices. This is prices going back to normal after Black Friday is over.   The Videocardz article on this ""news"" says this is a return to normal, not the normal prices being raised:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",hardware,2025-12-02 18:54:18,47
AMD,nrwmktn,"""Eeh, might as well"" -Lisa",hardware,2025-12-02 16:16:17,16
AMD,nrwdjmx,Very good example being nvidia vs amd. Nvidia can keep their prices outrageous just because there’s no meaningful competition.,hardware,2025-12-02 15:32:23,116
AMD,nrwj4h3,Intel pricing is pretty competitive these days. As soon as gaming isn't your top priority they aren't a bad choice at all.,hardware,2025-12-02 15:59:38,48
AMD,nrwq0no,What model wifi card or Ethernet controller do you use? I've had really good luck with the Intel ones.,hardware,2025-12-02 16:32:47,10
AMD,nrwesiw,"But you see, good guy AMD would never do such a thing!",hardware,2025-12-02 15:38:34,18
AMD,nrwu9j3,People cheer on the downfall of Intel for the same reason they'll cheer on the downfall of a rival sports team.,hardware,2025-12-02 16:52:58,7
AMD,nrwd0cr,"last time after amd 64 intel got back swinging, This time it looks really sad",hardware,2025-12-02 15:29:43,6
AMD,nrwutgt,"The people that cheered for Intel's downfall were probably computer users from the time when it was Intel or nothing, that wasn't a great time because competition was low.",hardware,2025-12-02 16:55:39,3
AMD,nrx3roa,intel was a bad actor and actively tried to sabotage AMD and shun them from system integrators. sorry can't sympathize with intel.,hardware,2025-12-02 17:38:51,2
AMD,nry9p2h,"I bought about 80 in the last ten years, I'm doing my part.",hardware,2025-12-02 21:00:16,1
AMD,nry05hr,"I hate to break it to you, but the existence of Intel does not prevent sales being ended and prices returning to what they were previously.",hardware,2025-12-02 20:13:47,1
AMD,nry32tn,"> I've never understood people cheering on their downfall  They were almost a monopoly for a long time, excepting the AMD K8 era.  Intel's business practices and strategic decisions have been extreme short-term profit motivated since.. even before 2010? When they were dominating, they would just hold back from bringing tech to market to maximize profits. Deliberate decisions to rest on their laurels, to not invest into real R&D. It's been a slow motion train wreck, competitors with much more growth R&D momentum catapulting past them, leaving them in the dust.  It's kind of like a desire for justice? Cheering on an org reaping what they sow? Of course, in America companies like Intel, or certain industries, like in 2008, get bailed out anyway. No justice; moral hazards aplenty.",hardware,2025-12-02 20:28:07,1
AMD,nrx8kbo,"Well just read a article about intel making their own version of x3d cache for their next cpu linup, which will have lots of cache memory. If they actually perform for once and not take 300 watts to do it, could be enough competition to push AMD to lower prices again.  A big will see though.",hardware,2025-12-02 18:01:26,1
AMD,nrwd0vr,"Jealous. I bought 5x20TBs two months ago from Amazon, and they finally acknowledge it was lost somewhere and gave me a refund. I can only buy 3x20TBs now.",hardware,2025-12-02 15:29:47,24
AMD,nrwp833,I bought 64gb of ram 2 years ago for $130. 6000mhz,hardware,2025-12-02 16:28:58,4
AMD,nrwlmrk,"I bought my 7600X for $150 and DDR5 32GB kit for $80, and additional 1TB SSD for just $40. Now I see both the SSD and the RAM being 2 - 4x the price of what I paid for nowadays when I am browsing our local online marketplace. I can say that I feel the same.",hardware,2025-12-02 16:11:47,3
AMD,nrwiuo4,Damn.. I bought 64GB memory last week and a 9800X3D. Paid $380 for the memory.. at least I got the CPU before it increased.,hardware,2025-12-02 15:58:19,2
AMD,nrwuivc,"Yeah, I jumped a little earlier and I'm still sitting on AM4, but when it looked like tariffs were gonna blow up the PC component market I made sure that I had a 5950X, 64GB of Samsung B-die, an 8TB flash drive, and a 9070XT.  Planning to coast on this for the next few years and hope there's still a hobby on the other side.",hardware,2025-12-02 16:54:13,1
AMD,nrzb4qm,"I bought a 9950x3d and 192gb like 2 months ago, i am happy i did so ahahah",hardware,2025-12-03 00:16:30,1
AMD,nrwcqzn,The memory shortage is just one symptom of AI chewing through supply and increasing prices.   The DRAM still needs a CPU…,hardware,2025-12-02 15:28:27,43
AMD,nry1m9k,"""Black Friday sales have ended"" is not in fact a sign that we have been overtaken by greed.",hardware,2025-12-02 20:20:58,11
AMD,nry3qbp,"its... capitalism, from a publicly traded company. When was it ever not about greed??",hardware,2025-12-02 20:31:18,4
AMD,nrwl45v,"Nope, there isn't. It's simply a power move by AMD Ryzen because they know exactly the consumers will still choose them anyway over Intel that is already dragged on the mud by the reviewers and tech enthusiast community in general.",hardware,2025-12-02 16:09:18,7
AMD,nrxw923,"Please boost this, the entire thread is being ragebaited by an absolutely garbage article writing about a complete nothing burger. In the article it reads  ""The timing follows Black Friday and Cyber Monday discounts.....return to standard pricing.""  The article puts in complete rumor gibberish, and baits with RAM drama to confuse the reader because its literally just saying ""AMD cpu's went on a discount for Black Friday, they are now losing the discount."" Except the college student who wrote this had to pad the word count to 1000 on a 50 word article.   Completely nothing burger written for interaction bait.",hardware,2025-12-02 19:54:44,19
AMD,nrzguan,"I mean, their market cap is only a little higher than the GDP of Portugal.  They're only slightly too big to fail.  I'm pretty sure the other tech giants throw fries at them at the lunch table.  It's actually kind of sad, really.  NVIDIA could find the cash to acquire them between its couch cushions at this point.",hardware,2025-12-03 00:47:51,3
AMD,nrwoof8,Nah don’t worry new car prices are through the roof as well,hardware,2025-12-02 16:26:22,31
AMD,nrxcdld,"Yep, Ive been planning on building my dream PC since I can finally afford to.   Not going to happen if the whole industry decides to fuck us. I'll watch and laugh at them when AI pops and the market is flooded with their existing and planned hardware.",hardware,2025-12-02 18:19:18,0
AMD,nrwtlls,"It will be sooner than later I can feel. These companies will go extinct, they are too greedy to exist.",hardware,2025-12-02 16:49:49,1
AMD,nrwfff3,"Intel isn't back right now... Arrow Lake isn't getting much traction,  the refresh isn't getting much traction (though it's honestly pretty good), so desktop DIY seems to be AMD's market.  They hence have pricing power.  For mobile AMD isn't really a big player but you don't buy DIY laptop chips so it isn't relevant.  Lunar Lake is pretty good (it's efficient, though ARM solutions still beat it in performance per watt) and Panther Lake is sounding better (ARM still wins, but it's a good gap over AMD and most people ex-Apple want x86).  That's a different market than desktop though.  When Nova Lake comes out it'll be a much better chipset than ARL, we'll see if they have giant cache chips or not which is what a lot of DIY people want.  If that happens then maybe Intel can compete better and exert more competitive pressure on AMD.",hardware,2025-12-02 15:41:41,20
AMD,nrwj1b7,Intel is back in what? Panther Lake is only M2 level,hardware,2025-12-02 15:59:13,7
AMD,nsaxszc,"They're not raising them. In fact, the 9800X3D is cheaper now than ever.",hardware,2025-12-04 20:05:42,1
AMD,nry2fle,This article is complete clickbait about Black Friday sales ending. It applies to all of their competition as well.,hardware,2025-12-02 20:24:58,9
AMD,nry2s5f,There are people that prefer their cabling doesn't spontaneously combust.,hardware,2025-12-02 20:26:40,2
AMD,nrwjzgu,not to mention 3D in the title of the CPU...,hardware,2025-12-02 16:03:51,78
AMD,ns86bfm,considering how many reviewers pick a sales price and base their performance/dollar graphs based on that you may as well see this as raise compared to the misinformation you are fed.,hardware,2025-12-04 10:51:47,5
AMD,ns0ivkh,I guess we will know in less than 24 hours,hardware,2025-12-03 04:39:58,1
AMD,nrz8y0c,"prices going up above msrp 18 months after release isn't ""normal""",hardware,2025-12-03 00:03:51,17
AMD,ns06ixx,"This is just factually wrong. Look at GPU price history from MSRP to market price for the last year. GPU launches were incredibly inflated until these last 2 months, so what you think is ""black friday deals"" and ""cheap"" is what was supposed to be its original MSRP lol. Now this price hike is just to artificially raise the price to maintain the same profit margins they've had from the past year out of pure greed. No reason for AMD to raise prices on existing products on shelfs except for pure greed. No reason for RAM manufacturers to not offer long term contracts to large customers and OEMs when they are purposely causing a shortage, except for pure greed. No reason for Nvidia (who does not produce a large quantity of their FE GPUs themselves and relies on AIBs) to not bundle Vram with their dyes to the AIBs that have a smaller profit margin than Nvidia, except for pure greed.  20% of this is speculation of a AI bubble pop, the other 80% is companies seeing blood in the water and hopping on the price gouging train to artificially raise the prices for the whole industry. When all the businesses collude and work together to raise the price of computing power, which is now an essential commodity now that it powers the world, who stops them? I can't see it being government as they are apart of the majority of the demand for these AI datacenters.  Source you can check out: GPU Prices Crater Before Inevitable Opportunity to Screw Consumers - Gamer Nexus",hardware,2025-12-03 03:18:55,6
AMD,nrwj20j,"Counterpoint: Competition only works if there’s checks and balances to prevent price collusion. The stupid SSD mafia colluding and keeping prices high (DAE remember the great fire sale of SSDs in late 2023?). There’s no reason why a 2TB 990 Pro should be that close in price to a 9100 Pro. Were they losing money then? I am hard pressed to believe they were.  The skeptic in me however is willing to bet the RAM prices are never going to go down, and this will become the new normal, and they’ll just pocket the difference (unless there’s something major that happens like upstart Chinese suppliers flooding the DRAM market forcing them to).",hardware,2025-12-02 15:59:18,82
AMD,ns86g18,"and the solution to that isnt shitting on Nvidia, its for AMD to make better cards.",hardware,2025-12-04 10:52:57,2
AMD,nrwkjvt,"I mean, there is no competition for the high end. When it comes to medium-high performance, the competition is absolutely there. It's really just the 5090.",hardware,2025-12-02 16:06:36,4
AMD,nrwv4vz,"this is partially incorrect.  as there were lawsuits and settlements about amd/ati and nvidia price fixing.  if price fixing is happening, there is no competition.  it is a fake competition, just like the memory industry, where a memory cartel sets their prices through price fixing and unified supply control (let's all massively reduce production and increase prices for example)  BUT it can look to the average consumer to still be ""competition"" then.  is amd and nvidia rightnow price fixing?  well there sure as shit won't be an investigation into it rightnow. hell nvidia can triple down on fire hazards without a recall. and the pricing between nvidia and amd are surprisingly almost always very aligned.  what a coincidence.  amd is also not interested to sell anything aggressively, despite wrongfully claiming they would.  so there is no meaningful competition going on at all here anymore.  and it is reasonable to expect, that price fixing is going on  as well of course.",hardware,2025-12-02 16:57:08,-2
AMD,nrwwwb9,"Even with gaming, I just picked up a 225f for $155 on Amazon.  While not exactly on par with a 9600, it is close enough to save $40 on the CPU and saved $50 on the same brand's Intel vs AM5 ITX board.",hardware,2025-12-02 17:05:41,13
AMD,nrwmz1i,"Yeah, for productivity Arrow Lake currently offers better performance than similarly priced AMD competition. They also don't seem to be cooking themselves (so far) and don't suck back stupid amounts of power the way Raptor Lake did.   Intel's lack of platform longevity is still a pain point, but if that doesn't matter to you and you just care about getting the best bang for your buck right now I wouldn't fault anyone for going Intel.",hardware,2025-12-02 16:18:10,27
AMD,nrya9l2,just purchased an i5 14600KF after selling my Ryzen 5 5600 just because I had DDR4 ram to utilize it with.  I believe there is a lot of misinformation online on how the 14th gen is still messed up to this day but I'm having zero problems (updated BIOS to be sure). Ran BF6 earlier (CPU demanding game) and CPU was running 70% - 80% at 58 to 60 degrees at stock lol,hardware,2025-12-02 21:03:04,2
AMD,nrwy8zu,"I've been out of the loop with pc components for the last 3/4 odd years. My build is primarily music production-focused but I do play games on it quite a bit (primarily PvE, I don't really need anything beyond 4K @60fps): AMD Ryzen 3600, Nvidia GTX 1660Ti, 32 GBs of DDR4 RAM, Asus TUF B450M pro-II. What should I be looking at if I wanted to upgrade without dumping half of my wage for DDR5 sticks?",hardware,2025-12-02 17:12:13,1
AMD,nrxs5b0,"For homelabbing, Intel seems almost purpose-built for this. Lots of cores for cheap (never thought I’d say this about Intel), decent enough single-thread, and that excellent Quicksync.",hardware,2025-12-02 19:34:49,1
AMD,nryw89h,Isn't Intel in the middle of divesting themselves from their networking business?,hardware,2025-12-02 22:51:45,3
AMD,nrwrfpq,"I dunno, I just use the sharkfin antenna that came with my motherboard. Works fine.",hardware,2025-12-02 16:39:33,-6
AMD,nry0k9l,"The article is clickbait. Yes, black friday sales are over, so prices will rise again. This also applies to their competition.",hardware,2025-12-02 20:15:48,5
AMD,nrwoqer,"Amd was better than intel around 2000s, then intel bounced back, now amd is top again, intels gonna bounce back again",hardware,2025-12-02 16:26:38,10
AMD,nryr27t,"You forgot the (brief) period where AMD K7 was dominant.  Also, INTC did not purposely hold back development for years. It literally fell apart when 10nm was delayed. The chip and fab business was so tightly bound that any delays in the fab (10nm) caused the chip design business to stall and make silly workarounds (e.g., Coffee Lake, Rocket Lake, Ice Lake, etc).  You can point to the massive $100B in stock buy backs, but INTC during that same time also spent more on R&D than AMD and TSMC combined.",hardware,2025-12-02 22:24:33,6
AMD,nrzn82d,> not invest into real R&D  [$8-14B/yr](https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expenses) on what?,hardware,2025-12-03 01:24:48,6
AMD,nrwhum9,"In a similar situation just on a smaller scale. Bought a 20TB disk for 300€, the store sent it in just a cardboard box and it was obviously DOA.  The return department dragged their feet with the replacement for 2 fucking months until one day they just randomly closed the case with a refund. The price of the same drive is now 450€.  The store is Senetic btw, any European shoppers avoid it. Shit packaging, completely unresponsive for any support apart from the initial (legally mandated) return ticket and fucked me over in the end when it was in their financial interest to do so.",hardware,2025-12-02 15:53:29,11
AMD,ns0ec1w,"Unless you really need them new, I'd check ebay refurbs for 20tbs. I got my 14tbs for like 180 a couple months ago. you might get lucky, however they were WD white enterprises.   i can send a link if you'd like",hardware,2025-12-03 04:08:40,1
AMD,nrwtyju,"I bought 48gb 6000 cl30, Trident Neo Royal Z (the silver with sprinkles) and felt outrageously lavish for paying 320€ this September.    Now they're more than 500€. Jesus Christ. At least the 9800X3D just hit record low with 440€ where I live.",hardware,2025-12-02 16:51:32,1
AMD,nrywhhl,"I'm glad I grabbed a 2x32GB DDR4-3200CL20 kit for my laptop earlier this year, paid $90 brand new",hardware,2025-12-02 22:53:08,1
AMD,nrwo2gz,for what do you need 64gb?,hardware,2025-12-02 16:23:26,-1
AMD,nryp9vb,Did you read the article or just comment capitalism bad immediately when you saw the headline,hardware,2025-12-02 22:15:24,1
AMD,ns13l5h,They can control the diy market all they want. They still don’t have the real important markets of pre built computers and laptops.,hardware,2025-12-03 07:28:49,1
AMD,nrwv8mg,"Genuinely, who is buying this shit? Average new car price is ~$50k now.  I make 6 figures and the most I've ever spent on a car was $22k, and even then I kinda regretted it (until I was able to sell it at a profit during the pandemic...)  Apparently several manufacturers are straight-up discontinuing base trims next year in an effort to boost that average sale price even higher.  This can only end in tears.",hardware,2025-12-02 16:57:38,5
AMD,nrwvngx,What do you mean intel isn't back? Intel has a 75% market share on CPU's.,hardware,2025-12-02 16:59:35,-4
AMD,nrwqj4d,Thats a stretch but yes they aren't back yet,hardware,2025-12-02 16:35:15,0
AMD,nry47f9,Unfortunately I couldn't open the article. Thanks for clarifying!,hardware,2025-12-02 20:33:38,2
AMD,nrwse2w,Extra 3 dollars at least just for that.,hardware,2025-12-02 16:44:04,29
AMD,nrzwidk,Source? There is nothing to suggest that's what's happening.   The Ryzen 9950X MSRP is $649.99 at launch (August 2024). I bought it in November 2024 for $599.99. It's currently $539.99 on Amazon. It would take a $110 increase to hit MSRP. This would be a MASSIVE increase and doesn't seem very likely.   My prediction: It'll hit $599 again at most,hardware,2025-12-03 02:19:28,17
AMD,ns1bhnn,"What's the MSRP? What's the current price?  That's all that matters.  Not whatever you imagine is ""supposed to be its original MSRP lol"".",hardware,2025-12-03 08:45:30,3
AMD,ns0v5ew,"Look, AMD is way behind Nvidia in market cap. By paying these increased prices, we're helping AMD stay competitive in the one arena that really matters. I think I speak for all gamers when I say that increased competition benefits us all.   /s",hardware,2025-12-03 06:14:17,0
AMD,nrx2p1x,"RAM prices have to come down. The entire client market, especially OEMs, will be at risk of collapse otherwise. There's no reason to think a rapid 500%+ increase in memory prices due to a shortage (and the resulting panic buying) is permanent.   I dont believe we're about to witness the collapse of the client PC, smartphone, and tablet markets.",hardware,2025-12-02 17:33:39,28
AMD,nrx5pt7,"The RAM situation is going to kill the entire PC market entirely if it stays this way.  That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  Literally everything like PC's, laptops, smartphones, consoles, etc will all have to go up in price quite a bit.  It's not sustainable.   Also, SSD prices have been very reasonable overall for a while now.  And yes, the latest SSD's will cost more, but come down in price fairly quickly all told.  This is really not an issue.",hardware,2025-12-02 17:48:12,15
AMD,nrwu0fq,I fully support China eating the revenue of these companies,hardware,2025-12-02 16:51:46,36
AMD,nrzp3uy,"...so you're describing the lack of competition, aka anti-competitive price collusion. 😅",hardware,2025-12-03 01:35:57,1
AMD,nsv6yh8,"Lay persons are going to be priced out of the market, we're going to all be on thin clients eventually as our hardware dies, paying monthly for a resolution/framerate package that doesn't meet it's advertised performance.",hardware,2025-12-08 01:53:43,1
AMD,nrxdk7l,"> Counterpoint: *Competition only works if there’s checks and balances to prevent price collusion*.  That's what I'm saying since years now; Gamers blindly buying nVidia for the sake of it, ruined the GPU-market.",hardware,2025-12-02 18:24:54,-8
AMD,nrxbm8f,"Even in medium-high segment, there is no competition, it's an absolute dominance of the 5070 and 5070ti against the 9070 and 9070XT. It's not even close.",hardware,2025-12-02 18:15:46,-7
AMD,nrzejc2,"Brother, that's a whole lot of stupid.",hardware,2025-12-03 00:35:06,3
AMD,nryc109,"> Nvidia doesn't keep their prices high because of ""no competition""... they do it because they are greedy as fuck and have always been that way.  No, Nvidia keeps the prices high, because they CAN — People buy their cards, without thinking, no matter what.  > Even back in the day when AMD/ATI was crushing their shit GPUs by huge margin they still charged more because they handed out bribes to game developers for years and always got their BS software gimmicks like Phys-X in new games. They also spent way more on marketing because AMD had to spend huge amounts on CPU.  There you have it, the answer on why people buy Nvidia-cards. Not because those would be (always) necessarily better, but mostly out of the market's *brand-perception* — nVidia spent billions to fabricate their leader-image.  Same reason forwhy so many people still stick to Intel and answer questions about their CPUs with *""It's a i7!!""*.",hardware,2025-12-02 21:11:34,-1
AMD,nrxb94i,And with the price of RAM right now those sorts of savings do mean a lot. 32GB+ of fast RAM has become as expensive as the GPU for a new computer.,hardware,2025-12-02 18:14:04,3
AMD,nryan22,If Intel comes out with something that is like the x3d chips then I'd consider them. That extra cache is too awesome.,hardware,2025-12-02 21:04:52,0
AMD,nrwuzc3,I got another 3+ years out of my old PC simply by upgrading from 1700x Ryzen to 5600 Ryzen. That system is still viable I just wanted to upgrade to 9800x3d. Platform longevity should not be underestimated.,hardware,2025-12-02 16:56:24,15
AMD,nrwzx37,"Mmm I’m a bit confused about this, I thought due to AMD being on a smaller node they were more power efficient…",hardware,2025-12-02 17:20:15,-1
AMD,nryd0a8,"> Intel's lack of platform longevity is still a pain point […]  Kind of blows one mind, how Intel still sticks to their idiotic 2-CPUs-per-socket mantra no matter what …  As if it didn't cost them already a good amount of users switching sides, due to AMD have the better cards here.",hardware,2025-12-02 21:16:18,-4
AMD,nrxlgr8,"A big upgrade would be something like a Radeon 9060 XT 16GB, which is about 2x as fast as your current GPU. You can also get a used CPU, something like 5600X or 5700X should be available for not much money and will work, just update your BIOS first.",hardware,2025-12-02 19:02:13,1
AMD,nrx0nhf,Ryzen 5000x3d is a drop in upgrade.,hardware,2025-12-02 17:23:48,1
AMD,nrz9iz7,Not sure. But I still use and have seen many Intel networking chips in the wild.,hardware,2025-12-03 00:07:17,2
AMD,nrxp028,Well Intel may be the one who made the chips that handle your wifi. They make a huge portion of networking chips.  So you very well might have a partial Intel system without knowing it!,hardware,2025-12-02 19:19:18,6
AMD,nrwvcwq,"the situation looks dire for intel, they just sold everything off, a lot of engineers are leaving, the money cushion is gone",hardware,2025-12-02 16:58:12,-2
AMD,nrwoh62,"I run multiple VMs, some docker containers, gaming, software development, etc",hardware,2025-12-02 16:25:24,7
AMD,ns3jr6e,"I was playing some Clair Obscur last night, had a few chrome tabs open (youtube etc) to look up certain builds or whatever, discord, etc. I don't even rice my desktop experience with widgets. But after a ~4hr gaming session my RAM was at 27GB. I have 64GB.  One thing I noticed in the past from my years of PC experience is that your system will generally use less RAM if its going to be approaching the limit. This suggests that theres algorithms that will use disk space instead and/or do memory reallocations. There is a performance cost to this.  That said I think 32GB is a good amount of RAM, I rarely approach it.  However you never know what apps or games are leaking memory so having a lot is nice.",hardware,2025-12-03 17:24:11,1
AMD,nrx6719,Consumerism just never stops.  It boggles my mind how completely thoughtless most consumers are with how they spend money.,hardware,2025-12-02 17:50:27,6
AMD,nry4rm8,> Average new car price is ~$50k now  Because of the high end dragging the average up. Entry level cars have been pretty consistently priced after accounting due inflation for quite a while now,hardware,2025-12-02 20:36:25,4
AMD,nrzc9tb,"A lot of people think they deserve a treat, the treat being a $75k new car. To the point that “paying off the car” is a common financial milestone.",hardware,2025-12-03 00:22:48,3
AMD,ns87quw,the carbrain insanity is very sad to see. People buying cars multiple times their yearly income in costs then wondering why they are getting fleeced. An average person spends close to half of lifetime income on a car.,hardware,2025-12-04 11:04:45,1
AMD,nrwxq97,"You say this as if their market share hasn't been steadily decreasing over the past couple of years.   What's even worse is that Intel's market share shrink has largely been slowed down by them pushing a bunch of high volume, low margin chips. If you look at revenue share in desktop, AMD is already at 40% share.   Intel's competitive position in desktop has only been deteriorating since X3D came out. Something which Intel's own executives have acknowledged, multiple times, in different conferences and earnings calls.   It's a lil insane people are still denying what Intel's leadership themselves have admitted is a problem.",hardware,2025-12-02 17:09:43,10
AMD,nrxc6im,I think they mean amongst youtubers.,hardware,2025-12-02 18:18:23,-1
AMD,nrwrhjy,"They did something AMD can't yet, ARM level idle but in PPA,PPW and raw performance both AMD and Intel need to work on it.",hardware,2025-12-02 16:39:48,6
AMD,nry2xyb,You get an extra fiddy from the 9850X3D,hardware,2025-12-02 20:27:28,9
AMD,nrxxpb9,">The RAM situation is going to kill the entire PC market entirely if it stays this wa  ""Hi, I'm Michael Dell""  ""and I'm Tim Apple!""  ""And together we're excited to announce our new industry-wide pricing initiative!""  ""To help alleviate the burden on consumer PC pricing, we're pioneering the launch of a new initiative that has been in the works for some time: 30-year PC Mortgages with fixed-rate APR!""",hardware,2025-12-02 20:01:46,9
AMD,nrzjnzr,"> The RAM situation is going to kill the entire PC market entirely if it stays this way. That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  We're lucky that internet infrastructure is so terrible in North America, otherwise Big Tech might succeed by sheer force in a pivot to replace local PCs (and ownership) with pure streaming clients.",hardware,2025-12-03 01:04:02,3
AMD,ns7j9de,"Yeah effectively. Competitors != competition. When the industry has super high R&D and setup costs (like CPU/GPU design and manufacturing) it's very difficult for competitors to enter and disrupt the market. Just look at Intel ARC, they are a billion dollar company and yet they are still having difficulty in GPU development.",hardware,2025-12-04 07:07:41,1
AMD,nrxdqet,Its literally close according to every single benchmark but whatever.,hardware,2025-12-02 18:25:42,9
AMD,nryjyuj,"sure, the x3d makes a huge difference, but for my needs I didn't need the extra power or cost associated with an x3d chip for this more budget build.",hardware,2025-12-02 21:49:26,2
AMD,nrx3yc4,"Platform longevity has to have some kind of financial value placed on it.   The maximum value of long platform longevity is the price of a motherboard. And you maximize this value if you buy into the very first generation on that platform. So every subsequent generation you enter in, the value of platform longevity goes down.   You also have to consider what impact generational improvements have on platform longevity. Zen 1 -> Zen 3 was a huge jump. Assuming Zen 7 on AM5, I don't think we're gonna see that level of improvement with Zen 4 -> Zen 7.  Let's say someone upgrades from AM4 to AM5 with the launch of Zen 6. They receive less value from platform longevity than someone who entered at Zen 4. And someone who bought in at Zen 4 will likely receive less value than someone who bought in at Zen 1.  So yes, platform longevity does have value. It's better to have platform longevity than to not have it. But best case scenario, that value is the price of a motherboard, and we could argue on the whole that you subtract from that value based on the circumstances.",hardware,2025-12-02 17:39:45,14
AMD,ns86w60,"99% of users do not even know what a CPU is, let alone know how to upgrade one.",hardware,2025-12-04 10:57:03,3
AMD,nrxbix5,"1700x was already obsolete when it was released, my 4 cores non ht 4670k was superior for gaming with a basic overclock and it was released almost 5 years before the first gen ryzens. I see Soo many people braging about upgradability but all of them could've bought a 8700k a few months after the release of Ryzen and had a system stronger than 1000, 2000, 3000 series and slightly weaker than the 5000.",hardware,2025-12-02 18:15:20,4
AMD,nrx3v4i,"They were comparing to Raptor Lake. Also, ARL is on a smaller node than Zen 5.",hardware,2025-12-02 17:39:19,9
AMD,nrx4dv1,"AMD isn't on a better node. And power efficient is a difficult metric to measure because it varies wildly on context: ISO-Performance, ISO-Power, ISO-task that isn't time sensitive (web browsing). It depends on power profile (high-performance will sacrifice efficiency for performance). It depends on where in the efficiency curve you are. It depends on how high in the product stack you are (a 7500F is more efficient in gaming than a 9950X, but may be less efficient in highly threaded productivity apps, etc.)  edit: Downvoted for being factually correct. You don't just run CB24 at full power and divide scores to determine efficiency. That's way too simplistic.",hardware,2025-12-02 17:41:51,-1
AMD,nrxph8e,"Forgot to mention I edit video in DaVinci Resolve so CUDA cores are kind of a must for me, but yeah, the GPU and CPU are definitely getting swapped before anything else",hardware,2025-12-02 19:21:40,2
AMD,nrza1xt,"Oh absolutely! The latest generation 10 gig chips from them are really good, very power efficient compared to prior offerings. But Intel networking stuff has (mostly) always been solid.",hardware,2025-12-03 00:10:21,2
AMD,nrxqcbq,"Ah, I didn't know that. Not sure why I got downvoted for answering your question, though.",hardware,2025-12-02 19:25:55,2
AMD,nrx193r,"Wouldnt say that too early, amd suffered same fate like current intel and bounced back",hardware,2025-12-02 17:26:40,5
AMD,ns02gn2,Yeah but everything seems to be more or less tracking with inflation except wages for some odd reason.,hardware,2025-12-03 02:54:08,-1
AMD,nrwyi7c,"Losing market share isn't great.  But they still have ~75%.  Saying a company with a 75% market share ""isn't back"" is dumb.",hardware,2025-12-02 17:13:27,-6
AMD,nrx3t10,AMD's PPA is the best on the market. Just not for light workloads no one cares about. Run some database benchmarks.  Nobody cares what Geekbench gamers think.,hardware,2025-12-02 17:39:02,-4
AMD,nryeezf,Ninety-eight fiddy ex three dee,hardware,2025-12-02 21:23:06,1
AMD,nryozpg,"more like  ""Introducing out new lineup running Windows/MacOS SE, designed for just 4GB of DDR3! And it still starts at only $999!""",hardware,2025-12-02 22:13:59,5
AMD,ns86kwh,"you could buy computer parts with a fixed rate loan for a very long time. You shouldnt though. If you cant afford it without a loan, you cannot afford it period.",hardware,2025-12-04 10:54:13,1
AMD,ns86os3,you need to solve communication at faster than light speeds (good luck) to make everthing a streaming client. Anything that isnt local server streaming is absolute ass.,hardware,2025-12-04 10:55:11,1
AMD,ns86s01,"Only if you ignore all the features. You know, the features buyers DONT ignore.",hardware,2025-12-04 10:56:00,1
AMD,nrxdv3q,I was talking about sales though?,hardware,2025-12-02 18:26:19,-2
AMD,nrznbpo,Sorry I didn't mean to say it is a one size fits all solution. I'm just hoping that Intel figures something similar out to really push AMD down in price or get them to innovate further,hardware,2025-12-03 01:25:24,2
AMD,nrx7dry,"Time is also a factor, the amount of time I spend swapping a motherboard is far more than the time I spend swapping a CPU.       Also potentially getting a bad motherboard if I was also swapping a motherboard (has happened twice).       That means loss of more time and effort to reinsert the old motherboard, RMA/return the new board, and then swap the board again again if I manage to get a replacement. Getting an RMA repair/replacement again isn't a 100% guarantee in this day and age with how stingy companies are. So I mean depending on how far you want to take this. By the end of this I could be out an extra $100-$200 in time.",hardware,2025-12-02 17:55:56,-2
AMD,nrxibdg,"IDK about anyone else, but I had some bad experience with intel and wanted to support AMD. Helps I got the x370 Taichi motherboard as a gift. I still have 2x 32GB Kits of DDR4 that I upgraded to 64GB on two machines. I wonder if I should sell those. I think I should sell my 64GB Kit of DDR4 as well.",hardware,2025-12-02 18:47:20,6
AMD,nrxvjjc,"In that case a 5060 Ti 16GB or something used, e.g. a 4070. I also use Davinci for editing on a 5060 Ti, for Fusion I'd welcome even more power tbh, but everything else works great.",hardware,2025-12-02 19:51:19,2
AMD,nry3m4s,"I guess it's the smugness of ""not a single piece of silicon from Intel"" when most likely you and most people do in fact probably have something from intel.   It's r/hardware anyway, most people here are teens that worship youtubers, dont sweat it too much.",hardware,2025-12-02 20:30:44,7
AMD,nrx4u4v,"Bulldozer era AMD was truly awful, their GPUs weren’t good, their CPUs weren’t good, and their server CPUs weren’t good either. Their turnaround is crazy.",hardware,2025-12-02 17:44:01,4
AMD,ns037aw,[Inflation adjusted incomes have been trending higher for decades and are up about 5% in the last 5 years](https://fred.stlouisfed.org/series/MEPAINUSA672N),hardware,2025-12-03 02:58:33,2
AMD,nrx0xjn,"How can a company ""be back"" if you are literally losing share?   At best one can claim a company is ""back"" if they *stopped* the bleed, but even that hasn't occurred.",hardware,2025-12-02 17:25:09,15
AMD,nrxvukm,"Is it really that difficult to understand the semantics of ""being back""? Good heavens.",hardware,2025-12-02 19:52:48,3
AMD,nrxvlka,Geekbench is the opposite of gaming benchmark   And AMD wins ofc in benchmarks that rely on fat caches,hardware,2025-12-02 19:51:35,1
AMD,ns7t0p0,and your specific device will be abandoned by the OS in 3-5 years,hardware,2025-12-04 08:40:56,1
AMD,nrxf533,"Good. I wasn't. You can clearly read ""performance' in my message.",hardware,2025-12-02 18:32:22,2
AMD,nrxtwcy,I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old. Although I don't really have the data.  What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?,hardware,2025-12-02 19:43:21,7
AMD,nrxmbxt,I went from a Ryzen 1700 to now a 5800X3D on a x470. The value of platform longevity can not be understated!,hardware,2025-12-02 19:06:24,2
AMD,nrxyqwc,"Yeah, I'm thinking that 5800XT + 5060Ti 16GB might be the best combo I can aim for until DDR5 is back to normal prices again.  How about ""higher tier"" cards compared to the 5060Ti? 5070s, 5080s etc.?",hardware,2025-12-02 20:06:53,1
AMD,nry4omu,"No worries, it's just internet points at the end of the day, I was just a little confused when I went to respond to his comment and saw the vote count.  I wasn't even trying to be smug, it was more of a ""I don't own anything Intel related (so I have no reason to be going to bat for them), and I still think them failing to compete with AMD sucks for everyone"".",hardware,2025-12-02 20:36:00,3
AMD,nry03bh,7970Ghz Edition was pretty beastly,hardware,2025-12-02 20:13:28,2
AMD,nrx5hwa,"game cpu king, productivity king, server? performance king but the industry is shifting to computer per watt",hardware,2025-12-02 17:47:10,-2
AMD,nrx1lpi,"Intel isn't ""back"" because it never ""left"". They have always retained an extraordinarily high market share.",hardware,2025-12-02 17:28:22,-5
AMD,nrxwqaj,"I didn't say geekbench was a gaming benchmark. But that people treat geekbench like a gaming benchmark and also that geekbench scores are easily gamed (benchmark runs so quickly that the CPUs can easily go beyond their thermal throttling point before the benchmark completes)..  Also plenty of workloads Zen excels at that don't get the benefit from cache. Like I mentioned database benchmarks, where SMT can give you 50% more IPC. Meaning better perf. and better perf/watt.  You know workloads that pay the bills. Not Geekbench. Of course Geekbench also pays the bills by misleading consumers.",hardware,2025-12-02 19:57:03,1
AMD,nrxf9zo,Weird. Considering we're in a price related thread. Whatever I guess.,hardware,2025-12-02 18:33:00,-1
AMD,nrzmi23,> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  With all the stuff you have to disconnect/rehome/recable? I think for the average home builder closer to an hour.,hardware,2025-12-03 01:20:35,5
AMD,nrzgqzc,"Maybe for an experienced builder, but to me (someone who's only built 1 PC) the prospect of just changing the CPU seems like a far less daunting task.  If you just want to replace the CPU, you unscrew 4 screws to remove your cooler/AIO pump head, pop open the socket, put your new CPU in, repaste and remount the cooler, plug some fans back in if necessary, and you're good. Maybe remove the GPU if you're really pressed for space.  When changing an entire motherboard, you have to unmount the cooler, remove the GPU, unplug every cable plugged into the motherboard, possibly remove some AIO fans/the radiator (or just fans in general) from the case if they're getting in the way of the motherboard screws, remove the motherboard, and remove any SSDs you want to reuse.   Then you have to put everything (CPU, SSDs, RAM) on the new motherboard, put the new motherboard in, repaste and remount the cooler, remount any fans you had to take off and plug them back into the motherboard, plug all your PSU cables again, then put your GPU back into place.  You basically have to redo a big chunk of the entire build process. Not to invalidate what you said, maybe you are proficient enough that all that would only take you 10 additional minutes. But I would not bank on the average DIY PC owner being able to do all that so quickly.",hardware,2025-12-03 00:47:21,3
AMD,ns8ctjf,"> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  Maybe if your build has basically nothing in it, mine has an AIO I would need to remove, PCI-e cards, GPU, 2 SSDs that would need swapping. I think it's more like an hour.  CPU, you just pop off the AIO, swap it out and put it back on.",hardware,2025-12-04 11:47:47,2
AMD,nrxzruh,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  You might be surprised. Reliability of these things can very much follow a bathtub curve,hardware,2025-12-02 20:11:54,1
AMD,ns0w5fv,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  The failure rate for a brand new board is thousands of times higher than the failure rate of a three-year-old board.,hardware,2025-12-03 06:22:39,1
AMD,nrxzf8z,"QC for PC parts seems to have been rather lax lately, I received a board with multiple damaged pins, and another one with a bad bios, I also got a bad CPU. This was all on the build I did around this time last year. Because of these issues it wasn't up and running till February. I also remember when I was a young teenager I got a bad board from Frys in the late 90s. My friend also received an Asrock board a few months into 2025 that basically melted with stock settings.  For some on compact builds multiple hours. Multiple factors go into this. Swapping a motherboard can be a major PITA. Not to mention when dealing with multiple thousands of dollars of components I have to take breaks to deal with the stress.",hardware,2025-12-02 20:10:11,1
AMD,nrymmux,"5070 has a reasonable price, but it's somewhat limited by 12GB of VRAM. 5070Ti is a great card at a still somewhat reasonable cost and 5080 is probably the worst offender, it's way too expensive for what it is.",hardware,2025-12-02 22:02:13,1
AMD,nrz6clo,"Productivity king is pushing it, intel still has the lead in that",hardware,2025-12-02 23:48:42,6
AMD,ns87azq,"productivity is clearly in Intels court right now. Unless you need AVX512, but the vast majority never will.",hardware,2025-12-04 11:00:47,1
AMD,nrx6yl4,"I agree, currently they’re the best in the very same categories they were awful at a decade ago. Productivity can be argued that Intel is still better for some things. Still, in a good number of use-cases, AMD is still fast than intel per watt. Most of the Core Ultras are using a shit ton more power to match AMD’s performance.",hardware,2025-12-02 17:54:00,-2
AMD,nrx25z4,And are going bankrupt while doing so,hardware,2025-12-02 17:31:04,6
AMD,nrx5qmt,The term ‘Is Back’ refers to a positive directional change and not a current status. Their current status is collapsing market share and margins.,hardware,2025-12-02 17:48:19,6
AMD,nrx2x7h,"With a bunch of lower end, cheaper chips. They simply aren't competitive.   Here's a quote from the CFO of Intel:   >""As you know, we kind of fumbled the football on the desktop side, particularly the high-performance desktop side. So we're -- as you kind of look at share on a dollar basis versus a unit basis, we don't perform as well, and it's mostly because of this high-end desktop business that we didn't have a good offering this year,""",hardware,2025-12-02 17:34:45,4
AMD,nrzncjt,"10 more minutes vs a CPU swap, so not counting that time or cooler installer.   Move RAM over, move GPU over, move storage over. Cables are already routed and sitting right there.    Maybe 10 minutes is optimistic, but I think if its not your first build, an additional hour to swap the Mobo vs just the CPU seems pretty long.",hardware,2025-12-03 01:25:32,0
AMD,ns8d48e,"I wouldn't want to do it that quickly, I like my cables nice and neat and shit.",hardware,2025-12-04 11:50:08,1
AMD,nrxanww,"but ARM is winning in compute per watt, AMD is only a leader in x86 in that category",hardware,2025-12-02 18:11:19,2
AMD,nrzrmnz,"I was thinking swap as in same case.  Cable locations vary by mobo, although generally not by a lot.",hardware,2025-12-03 01:50:52,3
AMD,ns87dom,"ARM isnt winning in compute per watt, Apple design team is winning in compute per watt. Other ARM offerings arent any better than say arrow lake.",hardware,2025-12-04 11:01:29,1
AMD,ns50sh2,Laptop CPU naming scheme is a real mess,hardware,2025-12-03 21:38:18,72
AMD,ns5egm2,I LOVE LAPTOP CPU NAMING SCHEMES THAT MAKE 2 GEN OLD CHIPS SOUND MODERN!!!!,hardware,2025-12-03 22:47:34,25
AMD,ns76hx4,4 core/4 threads vs 8 cores/12 threads.   AMD really went light on this CPU. Only one full Zen core.,hardware,2025-12-04 05:20:34,10
AMD,ns4yz1k,"I don’t understand why they pit them together: sure 10-30% more performance, _at 50% more power consumption_.  Why not comparing it to an AMD with the same power consumption? It may give better performance…  Edit: I also like that they say “Intel consumption is _slightly higher_”, and on the next sentence “…would draw 68W or _almost 50% more_”. Article is clearly written by Intel PR…",hardware,2025-12-03 21:29:35,37
AMD,ns645l4,Yeah sure not in a similiar configuration.,hardware,2025-12-04 01:14:47,7
AMD,ns59itk,Don't see any reason to buy anything on intel 7 unless at a large discount.   The intel stuff on tsmc 3nm is much better,hardware,2025-12-03 22:21:14,3
AMD,nscjcz8,So there are two laptops at same price just one has the Ryzen AI 5 330 the other has the Core 5 210H which one is better? I just saw this review of a laptop with the intel [https://www.youtube.com/watch?v=q5OFCLvxA6U](https://www.youtube.com/watch?v=q5OFCLvxA6U)  and it looks very very bad for battery life not even very light gaming.,hardware,2025-12-05 01:20:05,1
AMD,ns5tumu,"I would like laptop without AI please, especially CoPilot, that product is hot trash",hardware,2025-12-04 00:14:44,1
AMD,ns8x9tm,"What a silly comparison, they aren't even close in price... The AMD only has 4 cores, of course it's going to take a huge hit in multithreaded...   Also, I just realised this is a repost from 9 months ago, maybe prices were different then? Why post something so old now though that's not even relevant anymore?",hardware,2025-12-04 14:04:03,1
AMD,ns94yw5,The real budget performance winner is an m1 macbook air.,hardware,2025-12-04 14:47:16,1
AMD,ns4xtet,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-03 21:24:03,0
AMD,ns58jh0,Yup. Now it seems like Apple is the only company with good CPU names.,hardware,2025-12-03 22:16:12,33
AMD,ns53lic,"Was gonna say price, but looking at Lenovo's website and I couldn't even find the Ryzen AI 5 330 in my country, but the option with Ryzen AI 7 350 is still cheaper than the option with the Intel Core 5 210H.   So this is a really odd choice for comparison.",hardware,2025-12-03 21:51:39,29
AMD,ns52zbw,50% higher power consumption for *50% higher performance (in nT benchmarks),hardware,2025-12-03 21:48:44,24
AMD,nscbrg8,"68W is only the peak at the start, otherwise the chip stays at around 50-58W during gaming. Even the AI 5 330 can peak at 58W and 43W in games too according to their review so it isn't that much of a difference either way.",hardware,2025-12-05 00:34:20,2
AMD,ns6fr7l,The better comparison should be 210H vs AI240  13420h reskin vs 7640HS reskin  But that would be boring,hardware,2025-12-04 02:24:07,2
AMD,ns5mn0u,"And probably on par in consumption, since the node is smaller.  This comparison right here is like comparing an i3 to an i5 and saying the i5 has better performance…",hardware,2025-12-03 23:33:23,3
AMD,ns95pps,That's Alder Lake. Can't remember anything later on the Intel 7. There used to be some decent 12-14th gen CPU for it.,hardware,2025-12-04 14:51:13,1
AMD,nse32ur,"Eh, I'm not a fan of the Pro Max Ultra shenanigans.",hardware,2025-12-05 07:55:58,3
AMD,nse3wqf,Well you can configure the cpu and GPU cores to be different and still have the same name so no they're not better,hardware,2025-12-05 08:03:56,1
AMD,ns5dvw5,"My gripe with Apple is it’s TOO simple. M4, pro, max, ultra.   Doesn’t really say how may CPU or GPU cores, as it varies by device SKU (some got the binned cpu like the air got 1 less gpu core)",hardware,2025-12-03 22:44:28,-14
AMD,ns5jw88,I see a $100 price difference in the US with the ryzen at $500 and Intel at $600 through Lenovo. However the thing is the core 7 240H model is only $1 more than the core 5 and the ryzen 7 with 860M graphics is only $525. So why bother with either model at that point?,hardware,2025-12-03 23:17:43,20
AMD,ns5m5hq,"Where I'm from, 330 is very sparse and for some WILD reason more expensive than similar 350 options.  At the same time, 350 is more expensive than 225h... (Edit: re-checked, 225h isn't cheaper anymore it seems)",hardware,2025-12-03 23:30:33,5
AMD,ns5pltx,"Yeah. That’s like comparing an i3 with an i7. An i7 will consume more, and will have better performance. That’s why the comparison is odd to me: why comparing a Core 5 with an AMD that has to be compared with a Core 3?",hardware,2025-12-03 23:50:33,2
AMD,ns5ueib,yeah i've never even heard of the ai 330. Crazy they gave it the 5 specification when it has 4 cores and three of them are C cores with only one full zen 5 core.  [https://imgur.com/a/lMajlf6](https://imgur.com/a/lMajlf6)  generally speaking amd is better efficency wise although intel does make up for it with the lower idle draw and lower draw in low-medium loads. The higher binned hx stuff have better efficency but you have to power limit them bc out of the box they go like 100w+     edit: from what im seeing both intel and amd have given up on the i3/3 naming in the laptop space but I still wouldn't say it is a fair comparision bc u should compare it to the 226v which has the NPU like the ai 330,hardware,2025-12-04 00:17:50,8
AMD,ns5rc3w,Go ask why amd names it a ryzen 5 and not a ryzen 3? Hmm?,hardware,2025-12-04 00:00:21,8
AMD,nsay4pn,No the 210h is on intel 7 same with the 240h,hardware,2025-12-04 20:07:22,3
AMD,nsedc3v,"The 210H is ""Raptor"" Lake(Alder, since the P Cores have 1.25MB L2 instead of 2MB like actual Raptor Lake)   12450H -> 13420H -> 210H are the same CPU, with clock speed bumped up with each iteration",hardware,2025-12-05 09:36:28,1
AMD,ns5r68c,Well they always list the core count directly in the full product title on their site,hardware,2025-12-03 23:59:25,28
AMD,ns5w0b9,"It's not.   The Core 210 is Intel's budget chip.   Both chips are priced similar. It's a Core **5** vs a Ryzen AI **5**  Both are positioned as ""5"".  Intel adding E cores to boost the nT performance on their products has been giving them the nT lead in the lower end segments for a few generations now",hardware,2025-12-04 00:27:01,16
AMD,ns5sgm8,Whatever that has to do with technical specifications…,hardware,2025-12-04 00:06:47,1
AMD,ns6t6q7,"That and this comparison also saw the Intel version having better gaming percent, but it is only like a 5-10% difference across the board, with more games favoring Intel and some other favoring AMD",hardware,2025-12-04 03:46:20,3
AMD,ns5ts76,Why did YOU bring up i3 vs i5 then? Huh?,hardware,2025-12-04 00:14:20,6
AMD,ns67zx3,"Because they are both from Intel, I was trying to convey the fact that they are not on the same category. I need to refer to the same family somehow…  Now, saying Ryzen 5, 7, 9, etc. makes only sense within AMD, same for Intel within Intel. But different manufacturers have different nomenclature, and even different ways to measure TDP.  TDP on Intel is true TDP at minimum power, for AMD you can go lower than the TDP they publish for PL1.  So, at same power, AMD is also competitive, just that they have different nomenclature.",hardware,2025-12-04 01:38:06,3
AMD,ns6ugtj,"They *are* the same category. The two companies just approach that category differently. Intel decided to have 4 extra E cores to boost nT performance, but at the expense of higher power consumption if you load those cores.  This is the lowest end """"""current"""""" gen CPU H series CPU from Intel.",hardware,2025-12-04 03:54:42,0
AMD,nsx9thj,Bought an XT last week cause stock has largely been sorted in the last few months and quite a few models have been under MSRP and that situation could change in the coming months because of the DRAM situation. I also wanted to go AMD since I moved to Linux so I've been pretty pleased. For as much as we complain this isn't really a bad time to buy a gpu.,hardware,2025-12-08 12:10:09,22
AMD,nswxbmf,TLDW:     **7 Game Average (Low/Medium):**         1080P:       RX 9070 16GB is:     ~28% faster than the RX 7900 GRE 16GB    ~24% faster than the RX 7800 XT 16GB    ~33% faster than the RX 7700 XT 12GB    ~80% faster than the RX 6700 XT 12GB       ~159% faster than the RX 5700 XT 8GB     1440P:     RX 9070 16GB is ~96% faster than the RX 6700 XT 12GB       4K:    RX 9070 16GB is ~113% faster than the RX 6700 XT 12GB       **7 Game Average (High/Ultra):**         1080P:       RX 9070 16GB is:     ~25% faster than the RX 7900 GRE 16GB    ~30% faster than the RX 7800 XT 16GB     ~60% faster than the RX 7700 XT 12GB       ~100% faster than the RX 6700 XT 12GB       ~160% faster than the RX 5700 XT 8GB,hardware,2025-12-08 10:15:19,55
AMD,nsx9wbb,"Adjusting for inflation doesn't really help, a lot of the older AMD cards dropped below MSRP pretty shortly after release (excluding 5700XT). I ended up getting my 6700XT for ~$290 USD.  Also in Australia it's currently possible to get a 9070 for a theoretical $425 USD excluding tax if you are able to use the $60 AUD store credit, $50 AUD gift card (easy since it can be Amazon), $20 USD Steam credit and ~$10 AUD cashback.",hardware,2025-12-08 12:10:46,18
AMD,nsyx7gv,I like these types of videos that compare against previous gens.  It's very useful for potential buyers to see generational gains and determine if it's a justified upgrade,hardware,2025-12-08 17:42:23,10
AMD,nsxbix5,I'm happy to see 9070XT is doing well. Having said that my 7900XT Nitro+ ain't going anywhere any time soon.,hardware,2025-12-08 12:23:31,9
AMD,nsx25lp,"Got a 9070XT on Black Friday, now that they're so cheap I couldn't resist. Really impressed with it, performance is great, runs pretty cool and Adrenaline is really nice to play around with undervolting and overclocking. Got a real nice undervolt on it.",hardware,2025-12-08 11:02:51,34
AMD,nswxor7,It's the best value for money gpu,hardware,2025-12-08 10:18:58,42
AMD,nsx10eq,"As a day 1 RX9070 owner, I can say I'm quite happy with it. I had some stability issues in windows early on, though those went away with driver updates and probably would've been completely avoided if I did a clean install, and it does suck when games only support the Nvidia features even though AMD has an alternative. Aside from that, performance has really solid. I play at 1440p, so regardless of the game I'm playing I know I can crank basically all the settings to the max, not bother with any of the upscaling, and still get FPS numbers well above 100.  Also, it does hurt knowing the ""best value GPU"" in 2025 is more expensive than *my entire rig* was back when I started. Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.",hardware,2025-12-08 10:51:52,10
AMD,nsx0a1d,"Just bought the XT version of this card for 4K gaming on my TV and it’s an absolute beast.   Loving it, especially with a slight Undervolt to lower to power consumption.",hardware,2025-12-08 10:44:43,4
AMD,nsxudcm,Best graphics card for linux.,hardware,2025-12-08 14:25:16,5
AMD,nsxd264,"After going from AMD to Nvidia, raw perf doesn’t really mean anything to me anymore. Transformer DLSS is so good it beats AMD at native res.",hardware,2025-12-08 12:35:15,1
AMD,nsz9hb1,plenty of cards in stock at your local Micro Center...,hardware,2025-12-08 18:41:59,1
AMD,nt38b67,Bought a 6800 reference card back in December 2020 and just got the 9070 today to replace it. For me the biggest draw was power to performance 220W vs 304W of the XT,hardware,2025-12-09 10:07:46,1
AMD,nt4ma3e,"I've been rocking my 6700XT for years, it might be time for an upgrade, holy cow!",hardware,2025-12-09 15:45:00,1
AMD,nsxufaf,"Do they talk about the value with ""maintenance mode"" or ""extended support"" at 3.5 years, and without it?",hardware,2025-12-08 14:25:35,1
AMD,nsxa1sf,While the raw performance is good I am still concerned about the adoption rate of FSR4 and Redstone.  I'm waiting another gen to see if it catches momentum.,hardware,2025-12-08 12:11:59,1
AMD,nsxvnqp,Buying the 7900 XTX for $889 a couple years ago was the best decision I made. Card kicks ass,hardware,2025-12-08 14:32:41,1
AMD,nt1y1hm,Been having driver problem lately with my Rx 9070. :/,hardware,2025-12-09 03:26:36,1
AMD,nsxsrkc,"For people that havent been following since 2019:  5700XT 2019 launched at $400 (adjusted from $500), rivaled the 2070 super but had bad drivers for at least a year. Overall, underwhelming  6700XT launched at ??? But once available in 2022 it was generally under $430. Good drivers, and value. Overall, great GPU crushed under pressures of pandemic.  7700XT, trash. Dont even wanna talk about it.  7800XT launched $500 late 2023, delivering around 25% more perf than the 6700XT. Overall good launch/drivers.  7900GRE launched at $550, 2024. This GPU was an absolute mess. It launched super late, had weird underclocked memory issue at launch (despite it being around for a year in China already) and was priced too closely to the superior 7900XT.  9070 launched at $700 in 2025. Good GPU but completely crushed by the 5070 by value/availablility. FSR4 launched with limited implementation and heavy RT perf was only about a rtx 4070. Path tracing not really possible.",hardware,2025-12-08 14:16:03,0
AMD,nsx9zqb,guessing it not worth upgrading from the 7900xt looking at those GRE numbers,hardware,2025-12-08 12:11:32,-2
AMD,nsxeicq,"Any explanation for why the 7800XT beats the 7900 GRE at 1080P low/medium?  Is it that the greater amount of MCD:s in the 7900 GRE vs 7800 XT puts more pressure on the CPU, which results in a bottleneck on the 7900 GRE? Or is it just more MCDs = more latency = worse performance with high FPS due to latency?",hardware,2025-12-08 12:45:53,14
AMD,nsy7qyr,Cannot do sound now but at no point in graphs i saw mention what RT settings they used.,hardware,2025-12-08 15:36:58,1
AMD,nszk1vm,"That's basically the same difference in their memory bandwidth, could have saved themselves a lot of testing and just used those stats.",hardware,2025-12-08 19:33:40,1
AMD,nszh4t7,"Yup. Bought my 5700 non-XT well under $300 (MSRP 349). The 6700XT did eventually drop below MSRP, and stayed there for a while. But do remember that it took a long while for that to happen.",hardware,2025-12-08 19:19:03,3
AMD,nsyzx2m,The 6700xt was around $350-$400 when the 7800xt released for $600 in my country.,hardware,2025-12-08 17:55:35,3
AMD,nt8ga6z,6700 XT was going for 900 during 2021. It didnt start getting cheap until LONG after it's release.,hardware,2025-12-10 04:02:15,1
AMD,nsyzxnx,"And with how idiotic MS is liable to get with Windows, with the AI bubble visibly approaching explosion, the better Linux driver sit only makes a good deal better.",hardware,2025-12-08 17:55:40,10
AMD,nszfish,"Since you've not had it long, let me pass on some unsolicited advice:  Leave the undervolt off when not gaming. You won't eat up any extra wattage or anything, but it will ensure that you don't run into any issues with stability at low power states. Not that you're guaranteed to have issues, just that it sure makes things easier to diagnose when you KNOW it's not your ""OC."" Adrenaline makes it so easy that you might as well.   Additionally, when starting a new game and facing crashing, the absolute first thing you should do is cut your undervolt in half. It's astounding how much variation in stability you'll face with undervolts and different games.   For example, my 9070 XT is stable at -70mv in any synthetic benchmarks I can throw at it. But it wasn't 100% stable in Horizon Forbidden West until I dropped it down to -65mv. Then in Tiny Tina's Wonderlands I had to drop it down to -45mv to avoid the occasional crash.   But if I load up Horizon again, -65mv for 10 hours straight no problem.   Play around and figure it out. It's fun.",hardware,2025-12-08 19:11:06,6
AMD,nswynlv,"I mean, technically true but boy what a lame way to win. Only 20-25% faster than the 7900gre it replaced. I guess Moore’s Law really is dead",hardware,2025-12-08 10:28:45,27
AMD,nswzn3a,"Which is amusing, because at launch it was the worst - clearly positioned to be an upselling device to make the 9070XT look better.",hardware,2025-12-08 10:38:33,12
AMD,nsy6mb4,Except for when it isn't: [https://www.youtube.com/watch?v=mPQWoSEYMLs](https://www.youtube.com/watch?v=mPQWoSEYMLs),hardware,2025-12-08 15:31:10,1
AMD,nsx04v6,"Really? The last time I looked, the 5070ti, 9070XT and 5070 are in the top sellers list. not the 9070.",hardware,2025-12-08 10:43:20,-8
AMD,nsx718i,Okay but like how about just the best GPU?,hardware,2025-12-08 11:47:06,-4
AMD,nsx73ns,>Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.   Have 9th or 10th gen Intel office PCs flooded the market yet? Those types of desktops used to be good for an low budget gaming / workstation PC.,hardware,2025-12-08 11:47:41,3
AMD,nsy8o1a,"> Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.  Before the RAM/Storage prices went insane last month, people just accepted that you either build a budget rig (still stronger than a console) or expect to spend >1000 on it. Now though i would suggest agaisnt buying a new build until memory prices get better.",hardware,2025-12-08 15:41:36,2
AMD,nsx4zhi,When consoles are $600+ what people consider good value changes. Welcome to modern electronics. Hell welcome to modern everything. A carton of eggs is over $5 now,hardware,2025-12-08 11:29:05,0
AMD,nt0abwa,"Which is more important then you'd think with the total estrangement from reality down in Redmond nowadays. Even if you're not immediately planning to switch to linux or go dualboot (like me), with how loony toons shit is getting in that operation, being able to as painlessly GTFO of the MS ecosystem as possible on short notice is a capability worth planning for in a build for if and when MS finally does a dealbreaker for you.",hardware,2025-12-08 21:43:56,4
AMD,nt10uez,"been extremely pleased with my 9600x/9070 system on Linux for at least 8 months or so now.    Only somewhat miss Adrenaline, and setting up fsr4 being slightly less annoying on Windows",hardware,2025-12-09 00:10:46,3
AMD,nsy9ds5,I had the oppotunity to test out DLSS vs FSR native implementation by developers in same game a few times and DLSS is just so much better. I always use DLSS quality if its an option now. It has better AA than native.,hardware,2025-12-08 15:45:13,2
AMD,nsyqohs,"> 7700XT, trash. Dont even wanna talk about it.  Agree in general but it was on sale multiple times for ~$360 with 2 bundled games, if you wanted or sold those it was a solid buy, arguably the best value card on the market at those discounts. 9060XT 16GB is marginally slower for the same money, albeit with better RT, upscaling and more VRAM.",hardware,2025-12-08 17:10:08,1
AMD,nt4memd,It doesn't help that it is now a felony to play games that don't have PT or DLSS.,hardware,2025-12-09 15:45:37,1
AMD,nsxfj4q,Man what is it with you people and needing to upgrade every damn generation?,hardware,2025-12-08 12:53:07,16
AMD,nsxgklq,"Both actually have the same number of active MCDs. On paper, the only spec that's worse on the 7900 is the memory bandwidth due to running the VRAM at lower clocks.",hardware,2025-12-08 13:00:22,23
AMD,nszzs5h,"Oh right, I think maybe covid was still affecting things back then, not sure.",hardware,2025-12-08 20:51:53,2
AMD,nsz1z6x,"I bought 12/07/2023 so before the 7800XT release, the 6700XT was quite a bit cheaper after that release, they went down to ~$263 USD before tax but I pulled the trigger early because it came with Starfield which was... let's just say I haven't even bothered trying to play it because the stories and gameplay I saw didn't seem that interesting...",hardware,2025-12-08 18:05:35,3
AMD,nt10a0a,"> It's astounding how much variation in stability you'll face with undervolts and different games.    I've  benchmarked with as low as -125 on my 9070    -77 seems to be the limit for Helldivers, and seems to be rock solid for everything else as well so that's where I stay",hardware,2025-12-09 00:07:29,3
AMD,nsxfosc,"Getting 30% gen-on-gen in pure raster, with an even bigger uplift in RT and ML workloads on a smaller die is pretty good. To do it at a 40W lower TDP is really impressive. To get those uplifts purely from architectural improvements, with no real node shrink to speak of is completely absurd.  Also there are still very big gains to come with RDNA 5/UDNA.  Next gen will almost certainly be produced on N2P, skipping N3 entirely. It should yield performance uplifts almost as great as Maxwell -> Pascal, assuming AMD/Nvidia don't decide to shrink the dies massively.",hardware,2025-12-08 12:54:14,58
AMD,nsxlss4,"Well if we think of it in terms of release in the US it comes across great. 6 months later, same price, less power, +28% is decent",hardware,2025-12-08 13:34:33,10
AMD,nszg5hd,"If we knew the actual margins AMD and NV were taking, we'd have a much better idea of the state of Moore's Law. That and the cost of designing more and more complex ICs creates it's own growing cost outside of ""cost of number of transistors per sq mm."" And the more and more complex software environments.",hardware,2025-12-08 19:14:11,5
AMD,nsxc4s7,Thats the sad reality train we are on now. Pray that we actually even get these when quantum tunnelling gets worse.,hardware,2025-12-08 12:28:09,11
AMD,nsxd1ex,"Probably more like 40%+ in ray tracing. It is still dead, but  also this architecture is more forward thinking, and spending its transistors on next generation stuff.   I'd imagine RDNA5 will be another big leap, and this will age more poorly in some regardless, compared to the RTX 5000 series from Nvidia. If Nvidia ages worse, it's because of artificial implanted software limits. Like they blocked frame generation on the RTX 3000 series.",hardware,2025-12-08 12:35:05,9
AMD,nswzydl,"Yep because it's using same die as 9070 xt, it's benefiting from driver performance boost",hardware,2025-12-08 10:41:36,5
AMD,nsyzmn8,"Ehhh, yes and no. Worst of Radeon, still less of a turkey then that 12 gig 5070.",hardware,2025-12-08 17:54:11,2
AMD,nsxacb3,Top seller and value for money are two entirely different and unrelated concepts,hardware,2025-12-08 12:14:19,30
AMD,nsx9l54,"Do you not understand the difference between ""best selling"" and ""best value""?  The 9070 gives you the most fps/$",hardware,2025-12-08 12:08:15,28
AMD,nsy86mm,"Really, where do you see top seller list for all GPU SKUs combined, comparison?",hardware,2025-12-08 15:39:10,1
AMD,nsx0b2m,Because of the recent performance boost 9070 is a lot closer to 9070 xt there's only 8-10% performance difference,hardware,2025-12-08 10:45:00,-5
AMD,nsxlwyc,"That's the 5090, and it's not even close. No one is cross shopping a 5090 and a 9070 though.",hardware,2025-12-08 13:35:17,8
AMD,nsx9giz,"Maybe, I haven't checked.  Talking about office PCs, post covid IT spend at companies shifted massively towards laptops, now that work from home / hybrid work is a common thing. Buying a cheap, old office PC will be far harder in a couple years time, simply because supply is lower than it used to be. I doubt we'll see floods of 12th+ gen office desktops anywhere near the scale we saw with 5th/6th gen.",hardware,2025-12-08 12:07:13,8
AMD,nsx9yz9,The PS3 (60GB) launched at $599 in 2006.,hardware,2025-12-08 12:11:22,8
AMD,nsy8vm5,Eggs is a very specific US issue due to chicken pandemic. A carton of Eggs (10 units) here in eastern europe is 2.15€.,hardware,2025-12-08 15:42:40,2
AMD,nszbty3,"The problem with the 7700XT wasn't necessarily that it was a bad card.  The problem was that the 6800 non-XT offered basically the same performance, basically the same efficiency, and was very often cheaper throughout most of that card's effective life.  I know someone who snagged a 6800 brand new for $330 more than a year into the RDNA3 era. Also, supplies didn't dry up for a **long** time in many/most markets. It was basically a no-brainer. It was an equivalently-priced (or cheaper) 7700XT with more VRAM. There was zero reason to even consider a 7700XT throughout most of its run.",hardware,2025-12-08 18:53:06,2
AMD,nt4tw7o,"Retroactively, the 6800 has been $340-$380 as well. The 6800XT was $400-450.",hardware,2025-12-09 16:21:31,1
AMD,nt4tqob,"LOL, its not a crime but if you are looking for the best, its objectively better.",hardware,2025-12-09 16:20:48,1
AMD,nsy9n90,It will depend on your wealth level. If GPU is not a meaningful expense for you then performance will be the only question.,hardware,2025-12-08 15:46:31,1
AMD,nsxq64f,"So I guess my theory that it's latency limited would be correct, then? Memory bandwidth likely isn't an issue at 1080P low/medium, but high latency very much could be, especially with the RDNA 3 MCD designs that I assume come with a latency penalty due to the memory bus and compute die not being on the same die..",hardware,2025-12-08 14:00:47,4
AMD,nt1eqea,"Cyptomining 2: Crypto-bro boogaloo.  Late 2020 through 22 was prime crypto-bro and scalper hell. Even 6700XTs were going for $800 for a while there because you couldn't get anything above a 6700XT at all. They sold out to bots immediately, and some humans. Within minutes of going into stock.   6700XT's were less susceptible because the 192bit mem bus made it less profitable for eth mining assho... folks. But someone, whether it as AIBs, retailers, someone, was still jacking prices sky high on them. Not talking about scalpers, but cards you could buy directly from Newegg.",hardware,2025-12-09 01:33:04,2
AMD,nt094zw,Stop basing your opinions off other people's.   Form your own. Starfield is a great game and offers a fantastic experience.   I got my copy with the 7800xt when it released and I remember it still being quite close in price. Much lower than what it was when I got my 6700xt for $900 because my laptop decided it was time to go to Laptop Heaven,hardware,2025-12-08 21:38:02,2
AMD,nt8gdv0,"You bought like 1.5 years after release, that's not exactly ""shortly after release.""",hardware,2025-12-10 04:02:56,1
AMD,nt1dr41,"-75, you mean? Because AFAIK, it still only works in .05mv increments.  I mean, I've benchmarked as low as -95mv on Steel Nomad. Didn't mean it would pass 2 hours of benchmark loop. And to me, stable requires hours of testing minimum.    If you can get -75mv stable in a handful of games, you're over the 50 percentile line in the silicon lottery.   But I maintain that you'll find a game that doesn't like -75mv at some point. Or maybe not, and other games you could get even more than -75mv. Shrug.",hardware,2025-12-09 01:27:14,2
AMD,nszjwyf,"RDNA 3 was a undercooked bad gen. RDNA 4 fixing all the issues, chopping LLC and Mem phys by a third and going monolithic makes perfect sense. N5 -> N4C helps a little as well.      N2 is highly unlikely while N3P is pretty much confirmed by Kepler\_L2 for GFX13 across all products using it.  Maxwell -> Pascal level perf gains is overly optimistic. Node progress doesn't permit another 30 -> 40 series perf uplift.  But insane gains can be expected in ML and PT over RDNA 4 fs.",hardware,2025-12-08 19:32:59,9
AMD,nszk3n9,Wafer pricing is going from $16-18K 5nm to 30K 2nm so don't expect the same die sizes.,hardware,2025-12-08 19:33:55,5
AMD,nsz04l6,"And that's before whatever perf is shaken out of RDNA 4 by Redstone and other software upgrades to come, which may be significant.",hardware,2025-12-08 17:56:36,2
AMD,nt0og7l,"ML is irrelevant. You aren’t getting this card if you want to run LLMs. RT is borderline irrelevant. If you want to run RT in the latest games you will need something much more powerful than a mid range card like this. And lastly it isn’t 30% raster, its 20% in 1440p and 25% in 4k which is pretty disappointing.  The only thing that is pretty good about this card is that everything else is worse. It wins by virtue of being just disappointing in a market flooded with absolute garbage",hardware,2025-12-08 22:58:58,0
AMD,nszkk03,"Fs and I can't wait to hear more about RDNA 5, but unfortunately it seems like it'll be radio silence until well into 2027. Look at how AMD handled RDNA 4 as well, nothing until one week prior to launch. Maybe a Cerny teaser for µarch at GDC 2027 with Road to PS6?  Yeah and FG limiting egregious especially after the new model got rid of OFA entirely. Fingers crossed they unleash it when 60 series launches as a good gesture similar to DLSS TF across all RTX cards.",hardware,2025-12-08 19:36:11,5
AMD,nsy8232,yep the 9070 is just defective 9070XT dies being put to use.,hardware,2025-12-08 15:38:32,-2
AMD,nsx0hkt,"Are you talking about the 26.6.3 drivers performance boost or is there a newer one recently? If it's really within 10% of the XT, then it makes sense to buy the 9070 yes.",hardware,2025-12-08 10:46:46,4
AMD,nsxm1lb,Sure but people were cross-shopping a 4080/90 and a 6900/7900 tier card,hardware,2025-12-08 13:36:05,-3
AMD,nszk2e1,The only desktops we have left at my job are incredibly cheap mini pcs or insanely expensive dual quadro towers.  Nothing really in the middle that would be viable for upgrading for home use in the future anymore. :(,hardware,2025-12-08 19:33:44,3
AMD,nsxkmjl,And the PS4 launched at $399,hardware,2025-12-08 13:27:09,8
AMD,nt7zpu1,my best friend just went to leavenworth for playing at native  how dare you,hardware,2025-12-10 02:19:29,1
AMD,nsyx6pa,"Even if you're rich, you don't need to upgrade every generation. That's just mindless consumerism. And judging from the fact that this guy is even asking the question, then he's not loaded.",hardware,2025-12-08 17:42:17,6
AMD,nt1hpf4,"> -75, you mean? Because AFAIK, it still only works in .05mv increments.    LACT in linux allows .01, on windows I did have it at .75     Never had it crash on anything at that, did pretty good in the lottery it seems.",hardware,2025-12-09 01:50:56,1
AMD,nt4fdtg,"Yeah, great proof of concept of GPU semiMCM and heralds great things for shit after RDNA 4, but the silicon itself was borked.",hardware,2025-12-09 15:10:26,3
AMD,nt4fkav,"It did Control at 1440p perfectly acceptably. RT isn't the prime determinant yet, but RDNA 4 is pretty competent at it.",hardware,2025-12-09 15:11:22,2
AMD,nsx0ojl,https://youtu.be/YWUqsqcM4Hs  Watch this,hardware,2025-12-08 10:48:39,-6
AMD,nsy8d55,You mean a 4070TI and 7900 cards? These perform about equally.,hardware,2025-12-08 15:40:04,1
AMD,nszr5va,"The [Tiny / Mini / Micro](https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/) PCs have their fans because you can cluster them together and use them as compute nodes for a home server setup, but they're in no way good for gaming.  Just to make the situation even more dire, low profile GPUs are mostly dead now. The best half height, slot power card you can buy is an RTX 3050. If you have an 8 pin available, your only modern option is a Gigabyte 5060. Even if you managed to find a middle of the road 12th+ gen office desktop, you're going to struggle buying a GPU to go with it.",hardware,2025-12-08 20:09:06,2
AMD,nt3lftf,"You don’t need a gpu if we’re going to go down that road. Worrying about how other people spend money, now that’s a waste.",hardware,2025-12-09 12:06:37,0
AMD,nt1stnb,"Damn, thwarted by the limitations of windows bullshit. Didn't know you could adjust in smaller increments under linux.   Yeah, certainly sounds like you've landed a winner.",hardware,2025-12-09 02:54:47,1
AMD,nrwxan4,Just wait  2̶ ̶m̶o̶r̶e̶ ̶w̶e̶e̶k̶s̶ ̶ 2 more years and prices will drop again 🥳,hardware,2025-12-02 17:07:37,140
AMD,nrxhuoc,"I bought a 9070 XT just after the RAM price increase was announced. I already missed a good time to buy GPUs in the past, because I didn't react to industry news. But not this time.",hardware,2025-12-02 18:45:11,14
AMD,nrwvdr3,"Oh boy, I love when things become more expensive!",hardware,2025-12-02 16:58:18,45
AMD,nrxn80x,"So, has anyone considered switching to hiking? Definitely  more affordable than gaming these days.",hardware,2025-12-02 19:10:41,34
AMD,nrxndim,"I'm honestly surprised it's so little, although I suppose this reminds us just how little 8 or even 16GB of GDDR6 was costing AMD and Nvidia until very recently.",hardware,2025-12-02 19:11:26,5
AMD,nrwx0bu,"If this turns out to be true, I would be shocked that AMD raises prices before Nvidia.  Nvidia is addicted to money, and I feel like they will exploit any excuse they could to make more money.",hardware,2025-12-02 17:06:14,15
AMD,nrwyxaj,Not really anything AMD can do about RAM prices. They were probably priced with low margin to begin with,hardware,2025-12-02 17:15:28,15
AMD,nrwwhe5,Well the 8GB cards were not good value and arguably AMD and Nvidia can stop making them to use the same VRAM chips for higher margin SKUs,hardware,2025-12-02 17:03:40,5
AMD,nrx29x2,I grabbed a 9070 2 weeks ago in off the price hike hype bubbling up,hardware,2025-12-02 17:31:35,1
AMD,nrwuotv,"Hello JohnSteveRom2077! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-02 16:55:00,1
AMD,nry23c7,And I thought I was an idiot for building my PC 3 months ago and not waiting a few more months for black Friday deals.,hardware,2025-12-02 20:23:17,1
AMD,nry8mpa,"yeah snagged prebuilt 9800x3d w/9070 XT and some newegg bundles to build a second, tried for the 499.99 9070xt today that failed lol, bot scalped in less than second lol.   So got the 599.99 asrock challenger.  My 2600x and 1080ti are still playing stuff surprising well at least on lower setting at 1440p, but time for an upgrade and teenager needed something he's been using an old 1660ti but playing mostly older stuff.  ram and vram are not going to get cheaper anytime soon and chip producers have little incentive to increase capacity when they can still rake in profit at higher per unit profit vs. more volume and without the risk of building out capacity.",hardware,2025-12-02 20:55:06,1
AMD,nry93vq,"Well se them next time, I'll buy a new TV or a 3d printer, no need to give them money.",hardware,2025-12-02 20:57:25,1
AMD,ns2gmm0,"I've been watching the prices over the last couple weeks since I was in the market for a 9070 XT. Prices went down over Black Friday/Cyber Monday, but they're now back to pre-BF/CM level.  Maybe, just maybe, there's a hint about the reliability of this source that he claims there'll be no new graphics cards from AMD until 2028?",hardware,2025-12-03 14:09:59,1
AMD,ns86vge,steam deck is looking better and better every day,hardware,2025-12-04 10:56:52,1
AMD,nsvpky6,20-40 dollars right now\*,hardware,2025-12-08 03:47:59,1
AMD,nrykzxd,AMD making sure it beats Intel's 1% market.,hardware,2025-12-02 21:54:20,2
AMD,nrx6x5f,"I love this Nvidia does nothing... AMD we are starting to come into line with peoples price expectations. We are starting to slowly claw back sales from the grips of Nvidia.  AMD Video team ""Can't have this we need to full stupid into the wall. Consumers can't forget we are the under dog and if we eat 20-40 dollar increase that just isn't happening""",hardware,2025-12-02 17:53:49,-2
AMD,nrwwyys,"Another proof that AMD Radeon also doesn't give a damn about their consumers and GPU market share and will happily just pass the price increase of the components to the consumers instead of taking this as an ""Opportunity"" to gain marketshare because Nvidia is also expected to have price increase on their GPU soon.  Speaking of Nvidia I wonder why they still haven't increased their GPU prices yet? Could it be that they are less affected by this dram shortages? I have heard that GDDR6 had a higher price increase over GDDR7 for this very reason, because there are a lot more products out there that is using GDDR6, compared to the GDDR7, which is what the RTX 50 is using.",hardware,2025-12-02 17:06:03,-3
AMD,nrwzv0v,Buying the recent dip in prices to below RRP might have proved to be the right call.,hardware,2025-12-02 17:19:59,0
AMD,nrzwi3i,"AMD just got their products to MSRP, and now if we go back into rebate part trois - woof!  AMD just can't catch a break - well shouldn't hurt much they only shipping like 15 GPUs per week (hyperbole).",hardware,2025-12-03 02:19:25,0
AMD,nrxoqbr,"This makes no sense.  AMD supplies the GPU die not the VRAM.  ""Hey AIBs, you are paying more for your gddr, so I better raise the prices of the GPU for reasons"".",hardware,2025-12-02 19:18:00,-5
AMD,nrz3ckp,"Woah.. at this pace, a RTX 5090 may actually be viewed as the card for future-proofing and investment into future price hikes..  Yikes!!!",hardware,2025-12-02 23:31:18,-4
AMD,nrwyhmb,"I always hated the conversation “should I wait for this?” “Should I wait for the next gen?” No one knows what the future holds, even the companies releasing these products!  I always say just buy what you need when you need it, do your research, and enjoy!  Edit: not throwing any shade to you, just mentioning a related topic that I come across a lot!",hardware,2025-12-02 17:13:23,43
AMD,nrx1ugz,Yea like finally buying an affordable 6600XT in the year of our Lord 2023 when RDNA3 and Ada are already out.,hardware,2025-12-02 17:29:32,9
AMD,nry3bc6,"Yup same, got a 9070 XT the other day for around €500ish. Had to get it for that price, and now I’m not going to be worried about “missing out” just like when the Crypto phase popped in and fucked us over.",hardware,2025-12-02 20:29:16,7
AMD,nrx69dj,Because it is better right?,hardware,2025-12-02 17:50:45,8
AMD,nrygsjs,"As the old joke goes, the graphics are amazing but the gameplay sucks.",hardware,2025-12-02 21:34:22,23
AMD,nrz21wi,"Went outside, wasnt impressed with the graphics. Lack of DLSS support in 2025 is ridiculous.",hardware,2025-12-02 23:23:53,21
AMD,ns1idw8,"I'll just stick to 5+ y/o games, or non first person shooters",hardware,2025-12-03 09:54:48,4
AMD,nrxy2f8,"Jogging around local park, assuming your local park is safe can be decent alternative? Maybe riding some cheap bikes also work.",hardware,2025-12-02 20:03:33,2
AMD,ns8e880,"as far as hobbies go, PC gaming still one of the cheapest. hiking is certainly up there in cost comparisons though.",hardware,2025-12-04 11:58:52,2
AMD,nrx2dp4,"NVIDIA's rumored (heavy pinch of salt) to be decoupling VRAM from their GPU trays. If that's true, you're going to see a steep price increase as well because the AIBs are going to be fighting with one another to get their memory modules from vendors who aren't keen on expanding GDDR7 production beyond their initial roadmaps.",hardware,2025-12-02 17:32:06,26
AMD,nrwxf9e,Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.,hardware,2025-12-02 17:08:15,40
AMD,nry95t5,Pretty sure Nvidia doesn't ship memory out to board partners.,hardware,2025-12-02 20:57:42,3
AMD,nrwxpjg,Every corporation is addicted to money and stock value. AMD isn't our friend.,hardware,2025-12-02 17:09:38,14
AMD,nrxgwrs,AMd didnt have a load of GDDR6 piled up.,hardware,2025-12-02 18:40:46,4
AMD,nrwzipq,nvidia isnt providing dram anymore. its up to the partners to raise prices.,hardware,2025-12-02 17:18:20,5
AMD,nrwyzo7,"Nvidia has a FAT margin on GPUs,AMD not as they try to reclaim parts of the market",hardware,2025-12-02 17:15:47,-5
AMD,nrx2lu9,8GB is likely higher margin than 16GB.,hardware,2025-12-02 17:33:12,5
AMD,nrxyedw,Five days ago someone see $560 9070 XT on microcenter.,hardware,2025-12-02 20:05:11,0
AMD,ns2hns7,"They currently have the #1 spot on Amazons best selling GPU's list. Both in the US/global and in Europe (Amazon.de).  The 9070 XT in particular is very popular with DIY'ers and enthusiasts. Nvidia is doing well in the OEM and laptop markets, but for us DIY'ers and enthusiasts, AMD is offering very competitive products. Still behind Nvidia for sure, but doing better than it used to.  And for people rooting for Intel, I know this is an Intel sub mainly, but if you wish more competition, we shouldn't look at Intel. Intel is only eating at AMD's market share, not Nvidias. AMD is the only credible competition for Nvidia, but buying Intel only weakens the only credible competition, and makes Nvidias dominant position stronger.",hardware,2025-12-03 14:15:47,0
AMD,nry7fyk,"Do you seriously think NVidia is not going to raise prices at all?  At this point it is a matter of when, not if.  (unless this all comes crashing down shortly, which is unlikely).  We already know they have postponed the 5000 series ""super"" release due to VRAM availability and cost.  (Their 6000 series workstation/server Blackwell GPUs use 96GB of DDR7 each).  They make a lot more money per RTX 6000 Pro Blackwell than a 5090, so they'll prioritize those and hike consumer prices in order to reduce demand there.",hardware,2025-12-02 20:49:26,8
AMD,nrxquvq,"AMD (and Nvidia) care about their data center consumers first because that’s where the money and the high margins are. Every wafer allocation and memory module that’s allocated to a gaming GPU is one less that could have gone into a data center GPU. When the AI bubble pops and data center demand plummets, prices will go down across the board.",hardware,2025-12-02 19:28:25,7
AMD,nrx1xjh,Nvidia is just [passing the cost](https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own) to board partners instead of being forthright about it.,hardware,2025-12-02 17:29:57,12
AMD,nrx1ief,Not just Radeon look at their CPUs they dont give a two things about us. Their rather deprecated and sell us expensive piles of scrap while making Apple now seem like a value proposition. Truly preposterous times for us with no proper competition and a lack of market interest due to high prices of everything.,hardware,2025-12-02 17:27:56,2
AMD,nry9sve,They supply both.,hardware,2025-12-02 21:00:48,6
AMD,ns1abww,"Has pretty much always been the case. What was the longest lasting 1000 series card? The 1080ti. The longest lasting 2000 series card? The 2080ti. The longest lasting 3000 series card? The 3090...    Etc. etc.    They're the fastest card in the stack and also tend to have more vram. They've always been the most future proof, but they also cost and arm and a leg.",hardware,2025-12-03 08:33:53,2
AMD,nryf1m6,[I certainly regret not buying ram a couple months ago like I was considering](https://imgur.com/a/iVsxlWV),hardware,2025-12-02 21:26:05,16
AMD,nryzksx,There are definitely times where that question can be easily and usefully answered,hardware,2025-12-02 23:10:01,17
AMD,nrzjzen,"In general hardware has aged like open yoghurt out of the fridge.  Sometimes anomalies happen, it's not the first time.  I remember major price hikes and poor performance increases after the 9800pro ( x800 and x1800 sucked, hd series was little better until the hd4000) nvidia sucked after geforce 4 until the 8000 series.   People who bought gpus during the crypto boom at insane prices got fleeced. People who bought a 3060 ti or 3080 right before got lucky, people  who waited for a 4070super also got a better deal.  By the time the prices have started rising its too late.  If a gpu is available at msrp at launch you can rarely find better value later in the gen as prices dont drop anymore until the next gen. So yeah either buy at launch and msrp or wait for the next cycle   Imo if you are on a complete potato gpu now and need a cheap upgrade then second hand 3060ti are very cheap (190 euros) , else a 5070ti is the only thing id consider to have any performance/price right now. Everything else is either way more expensive for little gain, or has too little vram for the price. Or is too small an upgrade for the money over a cheap second hand 3060ti, or has poor second hand pricing.   5060 would be more interesting if the base model came with 12 GB and was priced at 250",hardware,2025-12-03 01:05:53,6
AMD,nrxifv0,"I remember 2 to 4 weeks before the 9070xt launch people were telling others to ""just buy a 7900xt"" for $700. I guess they got 4gb of extra VRAM out of it, but 95% will never use it. But I think there is a time to actually just wait, instead of buying $700 product that will be worth $500 in less than a month.",hardware,2025-12-02 18:47:54,5
AMD,nry8czu,"500€? New? God damn thats a steal.  There used to be a general idea for buying hardware, *Just buy it when you need it, there is always the next big thing coming*, which still is in part true, but man with the volatility of the market since Corona Im 100% certain you can time your buys.",hardware,2025-12-02 20:53:49,4
AMD,ns8e30a,so you got it for two thirds the price it actually costs?,hardware,2025-12-04 11:57:43,1
AMD,nrzz1op,Gotta redo your skill tree but it can get boring pretty easy without the right setup,hardware,2025-12-03 02:34:20,5
AMD,nrzk5pr,r/outside is leaking,hardware,2025-12-03 01:06:53,6
AMD,nrylxgs,"Could be a pretty massive difference. Each AIB fighting to source memory on individual smaller contracts instead of NVIDIA just making one big deal for the entire production run, taking advantage of the scale to get it cheaper.",hardware,2025-12-02 21:58:48,6
AMD,nry8z3j,Sure they are expanding GDDR7.  But for servers mostly.  RTX 6000 Pro Blackwell (96GB GDDR7) is a hot server GPU for inference use cases that aren't just the giant LLMs.  It is much more cost efficient than the B200 HBM offerings for all sorts of use cases that don't need to access too much RAM concurrently.  Several NVidia competitor hopefuls have also gone the GDDR route for inference focused AI accelerators.,hardware,2025-12-02 20:56:47,1
AMD,nrx8c75,They're already sacrificing margins on making consumer GPUs instead of AI GPUs,hardware,2025-12-02 18:00:23,17
AMD,nrx3wkf,"There are rumors that Nvidia will no longer be bundling VRAM with their GPUs in the face of rising memory prices, and AIBs will have to source it on their own.  While Nvidia may be ""absorbing costs"" on their FE models which have very limited production, for the vast majority of consumer graphics cards they're going to be passing on costs to the AIBs and thus consumers.   They have absolutely no incentive to absorb costs, anyway. Their entire consumer gaming graphics division is already being run at a massive opportunity cost to their real money maker, datacenter GPUs. It's frankly a miracle that they haven't decided to entirely scrap it yet when the same dies they're selling to consumers for a few grand at most can be sold to AI datacenters for tens of thousands.",hardware,2025-12-02 17:39:31,21
AMD,nry7whv,"\> absorb the costs if they desire  What makes you think they desire? They have a massive fraction of the market, and from a profit/investor POV raising prices is win/win here unless it somehow risks significant market share loss.",hardware,2025-12-02 20:51:37,7
AMD,nrwy5ga,"Agreed.  I’m just sort of surprised they would absorb the cost at all, tbh.",hardware,2025-12-02 17:11:45,6
AMD,nrx0qho,>Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.  Which won't happen. Nvidia doesn't want to be a charity.,hardware,2025-12-02 17:24:13,2
AMD,nrwy23i,"True, and I recommend people get cards based on their needs/desires, never blanket recommend 1 company over the other.  I’m just surprised it’s AMD who might raise prices before Nvidia, even if they can absorb some of the cost.",hardware,2025-12-02 17:11:19,6
AMD,nrx00ex,That's only a rumor and has not been confirmed.,hardware,2025-12-02 17:20:42,5
AMD,nrwzw1e,"Why are AMD's margins slimmer?  They're manufacturing at the same place using very similar technology. Their equivalent product is actually a little smaller than Nvidia's in terms of die size... they're a big TSMC customer, while obviously not as big as Nvidia they're not some small timer who can't negotiate a favourable deal either.  On this basis I would assume AMD and Nvidia have very similar margins on GPU sales to board partners.",hardware,2025-12-02 17:20:07,6
AMD,ns8f0ow,then why are AMD cards more expensive?,hardware,2025-12-04 12:05:00,1
AMD,ns2cdyz,Still seeing $579.,hardware,2025-12-03 13:45:44,3
AMD,ns2n3b0,"When you've been there for 20 years and kept losing market share in the last decade continuously, you are in deep problem. But for AMD, right now they prefer being the SOC for gaming and have little interest in putting effort in dGPUs.",hardware,2025-12-03 14:45:37,-1
AMD,nryt96r,"I doubt Nvidia will raise prices they will most likely just stop producing FE""s. Which isn't the same as eating the cost or raising it.   Then AIB's will most likely raise pricing. That isn't due to Nvidia thats AIB's doing what they gotta do.   As far as priority gaming has never been the priority for Nvidia this generation. There priority is AI chips. Everything else is secondary to that.   AMD CPU division is basically printing money now. They want there GPU division to go anywhere. There going to need to be more competitive, right now there not.   They don't offer a better software stack, Ray tracing or anything to match the 5080 in performance. They also have a reputation of dropping support a LOT sooner then Nvidia  That means they need to become a value leader and why in the world would you pay the same or more for a 9700xt over a 5070 Ti?   Specially if the two are with in 50 dollars of each other or even 100 dollars of each other.",hardware,2025-12-02 22:35:59,-2
AMD,nrx2myq,"Whelp if that rumors become the truth, then it is definitely right to expect the Nvidia GPUs will also get a price increase, i just wonder when it will happen and why AMD did it first with their RDNA 4...",hardware,2025-12-02 17:33:22,-2
AMD,ns1c8ok,"Usually it's best not to buy during the post-launch rush when prices are way inflated, unless one can get one of the few actual MSRP offers.  Other than that, I totally agree that trying to predict the future is largely futile. Especially now that generational improvements have become so slow and low. Hardware holds its value for longer than ever before and the risk of buying a product only to see it depreciate it soon after is therefore also low.  People who bought any decently reviewed GPU at MSRP since the RTX 20-series have gotten pretty good value by now.",hardware,2025-12-03 08:52:59,7
AMD,ns29jnm,"9070 xt shouldn't be too bad either, considering it can be had for less than $/€600. Unless you're a big fan of AI upscaling.  Waiting for delivery now, I guess the shop has a bit of backlog after BF/CM.",hardware,2025-12-03 13:29:03,2
AMD,ns7ejsg,Either way the days of the affordable 200-300$ value king are long gone.  Anomalies like the Nvidia 8800GT or the 1080 Ti will never happen ever again.,hardware,2025-12-04 06:26:10,2
AMD,nryvjjn,>95% will never use it  And the other 5% could use 4 or 100? Or is the XTX too cheap for that?,hardware,2025-12-02 22:48:06,5
AMD,nrxkyy6,"Sure, but how often can you predict something will drop that much? When the 5090 was announced didn't people sell their 4090s for as much as they bought them or slightly more? Also, if I'm going to use a GPU for 3 months, I'm not going to wait for it to drop in price.",hardware,2025-12-02 18:59:51,7
AMD,ns22x9g,"I like to buy every other year now, while my current GPU still holds some value.   GTX 970 -> 980 Ti -> 1080 -> RTX 2070 Super -> 3080 -> RX 9070 XT",hardware,2025-12-03 12:47:24,3
AMD,ns8h0ks,Brand new in the EU I’ve seen them go brand new for as little as €609 actually. He had it listed for €540 but I managed to haggle it down to €500. So roughly €100-€110 off MSRP?,hardware,2025-12-04 12:20:06,1
AMD,ns8e95a,and proper setups cost quite a bit too.,hardware,2025-12-04 11:59:04,1
AMD,nrzxpiq,"Also means there's probably going to be a lot more variability in performance from vendor to vendor than we're used to, due to the sourcing of VRAM potentially being from different places.",hardware,2025-12-03 02:26:29,3
AMD,ns246j6,They have 92% of the gaming sector. It's easy money and a failsafe if the AI bubble goes bust.,hardware,2025-12-03 12:55:44,1
AMD,nrwyn5s,Nvidia is also likely in a position where can get better prices on memory than AMD. So they might not be facing reduced margins yet.,hardware,2025-12-02 17:14:06,8
AMD,nrx12xw,Im surprised that people even consider AMD its not priced well against Nvidia and Intel is coming hot on its heels. Their attitude towards consumers is frankly appalling considering that they are just fighting for scraps in this space,hardware,2025-12-02 17:25:52,1
AMD,nry9xqh,"Why is it ok to talk about AMD rumors but not NVidia ones?  The seemingly official quote in this article is clearly at least partially wrong:  It states no new products through 2027.  Do you really believe AMD won't be producing any new GPU SKUs for over 2 years?  I think in both cases these rumors are likely true, but may be a bit off given that both are sloppy third hand leaks without any confirmation from the source or quality multiple sources.",hardware,2025-12-02 21:01:28,5
AMD,nrx49kx,A 9070 XT's die is almost as large as a 5080 but are selling it for far less. Also nvidia is doubt able to just get a better price on components.,hardware,2025-12-02 17:41:16,6
AMD,nrx0zzl,"They have far lower market share, they most likely price their stuff with a lower margin. Nvidia can do whatever they want.",hardware,2025-12-02 17:25:28,2
AMD,nrx4mut,"Nvidia has a 378 mm2 GPU that has a MSRP of $1000 and $750, while AMD for the same die size has $550 and $600 MSRP. In this case, nvidia makes between 50-80% more $$$ per die.  Nvidia has a 260 mm2 GPU has that a MSRP of $550, while AMD has a 200 mm2 for $300 and $350.  And so on....  A wafer of chips produced at TSMC generates nvidia much more money that AMD's one, even though the prices per wafer must be similar.",hardware,2025-12-02 17:43:04,3
AMD,nrx2q64,"The 9070 XT is priced closer to an RTX 5070 right now, performs like a 5070 Ti but is much closer in terms of hardware to an RTX 5080. So while AMD does use similar manufacturing, they have to price lower than Nvidia for roughly the same hardware (similar GPU die size, 256-bit VRAM bus with 16GB of VRAM).",hardware,2025-12-02 17:33:48,-1
AMD,nrzccrw,RDN4 is on a more expensive and smaller node.,hardware,2025-12-03 00:23:14,0
AMD,ns2u0rp,"Their market share just went up, not down. But it's not where it should be, I'm just happy they have success in the DIY/enthusiast market, since that's where I am. I don't care much about the OEM or laptop markets.",hardware,2025-12-03 15:21:05,2
AMD,nrx3sm8,Nvidia has about $50 billion more cash on hand to stretch it out and not be a first mover if they wish.,hardware,2025-12-02 17:38:59,5
AMD,ns8fw69,"Apple has insanely good CPU hardware. But they have zero value, because it comes tied to a mac, so thats an automatic nonstarter for most.",hardware,2025-12-04 12:11:41,1
AMD,nrxkr9d,"Macbook Air M4 is $750 most of the time now and there's no Windows laptop at the price that can compete on build quality, battery and experience, if you don't plan on doing any laptop gaming.",hardware,2025-12-02 18:58:50,1
AMD,nsaj2on,Unless the AI bubble pops and chipmakers find themselves sitting on a huge pile of chips that they can't sell.,hardware,2025-12-04 18:52:22,2
AMD,nsbd7ha,"The usual trick is to list it as a business expense and claim the VAT back, but it wouldn't knock more than 5% off the price, else there'd be no profit for the seller.  It was either used or fallen off a truck.  Legit brand new was €589 in Germany around BF and CM.",hardware,2025-12-04 21:23:34,1
AMD,ns2bsx5,>It's easy money and a failsafe ~~if~~ when the AI bubble goes bust.  FTFY,hardware,2025-12-03 13:42:21,2
AMD,ns2hgu6,Google’s TPUs are already showing a lot of promise and its been used to train the best AI model currently,hardware,2025-12-03 14:14:42,1
AMD,nry8pg3,I switched to AMD mainly because of Nvidias shit drivers on Linux. Which is funny because I had previously switched to Nvidia because of AMDs shit drivers on Windows.,hardware,2025-12-02 20:55:28,4
AMD,nry0eq6,"Not everyone lives in the USA.  In my country it's priced competitively with a 150€ price difference to the 5070 Ti.  If you don't care about the RT difference and CUDA, going with AMD is a no brainer",hardware,2025-12-02 20:15:02,4
AMD,nrxdsol,"The 5080 die is the same one as the 5070 ti, which is the direct competitor of the 9070xt and sells more than the 5080. Average margins will be slightly higher for Nvidia, but not by much. Especially considering AMD uses GDDR6.",hardware,2025-12-02 18:26:00,5
AMD,nryaake,GDDR6 and GDDR7 do not have the same cost.,hardware,2025-12-02 21:03:12,5
AMD,nrxa9kz,"Ah, I was comparing 9070XT to 5070Ti.  They're all remarkably small dies but I guess that's progress.",hardware,2025-12-02 18:09:27,2
AMD,nrycpmz,"Certainly NVidia has notably higher margins here, but you're likely over estimating it somewhat.  GDDR6 and GDDR7 do not have the same cost.   The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).    Additionally, Its not clear how much of the actual sale (vs MSRP) NVIdia earns vs AMD -- there are various people making profit between them and the consumer.  We also know that NVidia sells the vast majority of its gaming GPUs not directly to consumers but to OEMs for pre-builds.  We don't know if NVidia sells GPUs to those OEMs at the same price they sell to D2C retail brands.  Do the big OEM system builders (NVidia's largest customers) get bulk discounts others do not?",hardware,2025-12-02 21:14:53,2
AMD,nrx7oxq,Oh they're about £100 or more different in price where I am.,hardware,2025-12-02 17:57:23,4
AMD,ns37fnc,Yearly it went down.,hardware,2025-12-03 16:25:20,0
AMD,nsgab3x,"I wouldn’t put it past people doing that but there are plenty of used 9070 XT’s in Europe selling for under €550 on some sites, just seen one sell for €530 a few hours ago and another for €550. Not uncommon.",hardware,2025-12-05 17:01:28,1
AMD,ns8eoqk,outside of US AMD is way too expensive to even consider for a GPU.,hardware,2025-12-04 12:02:25,0
AMD,ns0zqhp,I dont also live in the USA. 150 pricce difference with no guarantee when they will cut first day game driver updates. Please id rather spend that extra 150 and get better support lol.,hardware,2025-12-03 06:53:33,-5
AMD,ns14z34,">GDDR6 and GDDR7 do not have the same cost. The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).  Hope you're not trying to say that GDDR7 is responsible for the increase of $400 of board costs. Come on! A little research showed that there's just a difference of $2-3 per GB, which comes around **$30-50 for 16GB**. Please don't try to say the PCB circuitry is $300 because PCB's is one of the cheapest components of a GPU.  LE: You say the costs are influenced by cooling. FYI, the TDP increase for 5080 vs 9070 is just 45W (350W vs 304W). That's $10 **max** in cooler costs if you've been closely following the BOM cots for the industry.",hardware,2025-12-03 07:42:02,1
AMD,ns3oh52,"That doesn't mean AMD isn't selling well, it can mean that Nvidia happens to sell even better. But mostly in the OEM and laptop market, not the DIY and enthusiast market that interest us.  The 9070 XT is the best selling video card on Amazon in the US and in Europe. And the best selling card on Mindfactory and various price aggregators. So it's actually looking pretty good for AMD, the 9000 series is a pretty big success.",hardware,2025-12-03 17:47:02,1
AMD,ns8erwc,I literally listed an European country where it is cheaper and the other guy did too while being way cheaper,hardware,2025-12-04 12:03:05,1
AMD,ns22kkt,Cheapest RX 9070 XT in my Amazon (Ireland) is 702eur  Cheapest RTX 5070 Ti is 963eur (+261eur),hardware,2025-12-03 12:45:02,3
AMD,ns8fjqf,Not sure how that can be considering neither AMD nor Nvidia has publicized their contracts and we know they arent buying GDDR off the market,hardware,2025-12-04 12:09:04,1
AMD,ns8k73r,Either your country is an anomaly in pricing or you got things wrong.,hardware,2025-12-04 12:43:19,1
AMD,ns2buk8,"Tip: some German shops ship to Ireland.  [https://www.alternate.de/listing.xhtml?q=\*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price\_asc](https://www.alternate.de/listing.xhtml?q=*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price_asc)  Add €18.90 for shipping and maybe €20 for VAT adjustment, it's still €628. Quick, it's almost sold out.",hardware,2025-12-03 13:42:37,2
AMD,ns8k8op,"No, my country is normal like a lot of others",hardware,2025-12-04 12:43:37,1
AMD,ns2hciw,"That listing is gone, so now the cheapest from there is    €634 ->**€655.31 + p&p**",hardware,2025-12-03 14:14:01,1
AMD,ns2hj4t,Check other retailers. There are multiple storss in Germany selling 9070XT's for 600€,hardware,2025-12-03 14:15:03,2
AMD,ns2jviz,"Bad luck, it was there an hour ago.  Here's this one, but they'd deliver in January  [https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en\_US](https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en_US)",hardware,2025-12-03 14:28:02,1
AMD,nsa4y1d,No reason to buy a new cpu when you have to sell a kidney for ddr5 ram,hardware,2025-12-04 17:45:04,154
AMD,nsafakr,I've been holding onto my used ddr4 for no reason for the last 2 years. Turns out maybe being lazy and waiting actually benefited me.,hardware,2025-12-04 18:34:25,39
AMD,ns9zdf7,If the demand for these in data centers was actually there wouldn't the prices have raised already?,hardware,2025-12-04 17:17:50,21
AMD,nsch63v,I don't believe they'll keep that promise because their sales will drop when the memory situation impacts all their sales channels.,hardware,2025-12-05 01:06:30,10
AMD,nsc9f4m,Can we just use banks of CPUs as RAM?   It would only take 333 copies of a 7800x3d to make its 3D cache add up to 32GB!,hardware,2025-12-05 00:20:51,7
AMD,nsgdfsk,Honestly just relieved AMD didn’t bump the prices. I can finally upgrade without crying.,hardware,2025-12-05 17:17:06,3
AMD,nsa1fcr,"None of this is helping anyone. All it's proving is that customers must buy on initial release date, as the future could skew product pricing at a later time. Which doesn't help anyone and encourages scalping even more.",hardware,2025-12-04 17:27:47,6
AMD,nspg6gk,"They never were going to. I told you so. The source that said they would, also said AMD is not going to release a new GPU until 2028.",hardware,2025-12-07 03:53:03,1
AMD,ns9xqad,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-04 17:09:48,1
AMD,ns9zgvc,"Good to see AMD hasn't forgotten its roots. Also, Zen6 is probably going to come in higher",hardware,2025-12-04 17:18:17,-17
AMD,nsb82mm,they better not. if they get greedy and start raising prices just because they can then I'm done with them.,hardware,2025-12-04 20:57:54,-6
AMD,nsd7hai,I was born with two for a reason.,hardware,2025-12-05 03:43:31,20
AMD,nsdxbr2,\+300 euros for a 32 GB Ram kit.  Absolutely insane,hardware,2025-12-05 07:01:07,6
AMD,nscp19d,I'm so glad I bought 64gb of ddr5 for £170 right the price hike.,hardware,2025-12-05 01:53:46,4
AMD,nsa9qxc,Unless you have socket compatibility... Intel better get with the program here.,hardware,2025-12-04 18:08:03,-6
AMD,nsaan9v,"Sure DDR5 went up, but is a couple hundred more really gonna make or break say a $1500 computer that now costs $1700?  I think people are way overthinking this or worrying way too much about it.",hardware,2025-12-04 18:12:21,-42
AMD,nscczqk,For the past 1.5y I couldnt get rid of my two Aliexpress Corsair 16gb sticks that i bought for $30 a piece but were not a kit.   Nobody wanted single 16gb sticks - they all wanted 32gb kits and who can blame them when they were as low as $79.  Now I got $230 in total for them.  Crazy world.,hardware,2025-12-05 00:41:24,20
AMD,nsdoxgj,I built a media server around my old ddr4 and boy am I fucking glad I did,hardware,2025-12-05 05:47:39,3
AMD,nsx0wfc,sir this is a casino,hardware,2025-12-08 10:50:47,1
AMD,nsadrej,"yes, but it's also about manufacturing capacity. Data centers aren't using consumer grade ram, but they have booked nearly all the capacity at the fabs that manufacture consumer grade ram.",hardware,2025-12-04 18:27:09,40
AMD,nsamh6a,"The DRAM cartel sees the bubble coming and wants to make huge profits by making the bubble bigger. They are going to scam these companies (other than OpenAI) by refusing to increase production and charging insane prices even though they could easily (and were originally planning) to raise production and keep prices reasonable. That was when these scumbag ""AI"" companies weren't trying to corner the market to screw over their competitors  OpenAI's Sam Altman is the one who started all of this. If he can buy up 40% of the DRAM supply he thinks he can stop his competition from being able to source ram at reasonable prices. The fact that everyone else on earth gets fucked over doesn't matter to him at all. The DRAM manufacturers are playing along because they'll get insane profits. When the bubble finally bursts they will go back to normal operations and prices will crash but they think they'll walk away rich first",hardware,2025-12-04 19:08:53,23
AMD,nse81j9,"There's already separate CPUs specifically for HPC / HBM / whatever feature is required for server style ai computing, so idk why there's worry for regular CPUs, especially when they can be very inefficient for server computations.",hardware,2025-12-05 08:45:24,1
AMD,nseiymz,That is the good news about chiplets. Server CPUs will just absorb the slack.,hardware,2025-12-05 10:31:16,5
AMD,nspghgh,Even less if you use 9950X3D2's. Probably cheaper too.,hardware,2025-12-07 03:55:09,1
AMD,nsa4vn1,"I bought a 7800X3D (tray version), 2 years after launch for 50% of MSRP so not sure what your argument is?",hardware,2025-12-04 17:44:45,31
AMD,nsa483g,People have needed to buy on release date for 7+ years now. People got ass mad when I was saying those about the 2080 Ti because the premium models always have more limited availability.   The best time to buy is almost always at launch now.,hardware,2025-12-04 17:41:34,19
AMD,nsclbqk,Buy on initial release date? When companies release said like raptor lake? No thanks.,hardware,2025-12-05 01:32:00,3
AMD,nsc01bp,"I think right now you buy a ""justifiable"" MSRP day 1 or wait for the end of the line platform to build and buy around.",hardware,2025-12-04 23:25:28,1
AMD,nsc3qri,"Indeed, false reporting of price increases that don't exist do not help anyone.",hardware,2025-12-04 23:47:21,1
AMD,nsa7wxc,"Roots? You people have the memory of a goldfish.  AMD does the same shit as everyone else, charge the most the market will bear.",hardware,2025-12-04 17:59:14,42
AMD,nsab9dt,"It's not about roots. The PC market makes up a huge portion of both Intel and AMD's sales in a way that it doesn't for Nvidia, Micron, or Samsung.",hardware,2025-12-04 18:15:15,9
AMD,nsa6e8v,"They happily would if the demand were there. We've seen that across their GPUs.. they're not being benevolent, they're just spinning.",hardware,2025-12-04 17:52:00,11
AMD,nsa1j36,"I seriously hate this argument.  If you had a chance to get a 200%+ higher paying job, would you? Hypocrites.",hardware,2025-12-04 17:28:16,10
AMD,nsdlev2,Roots of charging 300+ dollars for a ryzen 5 with 6 cores since zen 3?,hardware,2025-12-05 05:19:29,0
AMD,nsx25ys,"So you were done with them since 2004, right?",hardware,2025-12-08 11:02:57,0
AMD,nsdqbvr,How many ram sticks you would like to buy? /s,hardware,2025-12-05 05:59:16,6
AMD,nsahqf1,"Intel doesn’t change RAM with every socket either, so socket compatibility isn’t really required to keep your RAM",hardware,2025-12-04 18:46:01,23
AMD,nsaaygt,A couple hundred? The exact kit I'm using now cost a little under $300. Now it's $1100.  Don't downplay this shit.,hardware,2025-12-04 18:13:51,51
AMD,nsc4to8,"I had to settle for 32GB DDR5 at the start of November at $259 AUD, that same kit today is $739 AUD.  At todays prices, RAM alone would have cost more than the rest of my build and I wouldn't have been able to justify it.",hardware,2025-12-04 23:53:45,5
AMD,nsap771,Not everbody got rich parents bro,hardware,2025-12-04 19:22:23,8
AMD,nsagc6s,you're talking to the same people who contemplate whether it's a good idea to save $50 by not buying an nvidia gpu,hardware,2025-12-04 18:39:25,-8
AMD,nsf51gu,Same here for my homelab.. 64gb for ~£70. Wild to think how much money I have in ram just in my room alone,hardware,2025-12-05 13:24:48,3
AMD,nsf35m3,"Isn't the chip is the same ?   They have one more for ECC (on DIMM, not the on chip bullshit) and a controller (maybe for RDIMM ? )",hardware,2025-12-05 13:13:10,3
AMD,nsfpkmo,"yea they are scooping up all the LPDDR too, we are so fucked lol",hardware,2025-12-05 15:19:24,3
AMD,nsbzx0x,"Problem as with the Crypto it takes longer to prices to go down than prices to shoot up. I mean it took ages for GPU pricing to get back to ""normal"" and arguably never did. RTX 40 had a huge markup with the 4080 being a 3070 successor with a massive markup.",hardware,2025-12-04 23:24:46,8
AMD,nsasi6v,"I mean, can you say that isn't the right call.  if everyone knows its a bubble, why would you start making new capacity that in a few years will at best ride the end of the bubble, if not completely burst by then.  so yeah, doubly smart",hardware,2025-12-04 19:38:58,28
AMD,nse3pb2,"Fabs take 3 to 5 years to make     They arent increaseing production becose they cant without the wait, they bet that the buble would pop before they could payoff their new fabs",hardware,2025-12-05 08:01:57,3
AMD,nscak7g,"There's an insane doomerism bandwagon going on over hw spaces on  reddit, for a while now. From bad reporting to bad reading skills, people are doubling down on the worst possible interpretation and outcomes as guaranteed truth.",hardware,2025-12-05 00:27:24,24
AMD,nsj0iuq,* RTX 50 series are cheaper now than at launch * DDR5 was much cheaper around mid-2023 to mid-2025 than it was at launch in 2021,hardware,2025-12-06 01:59:25,4
AMD,nsrgnvw,"I genuinely can’t believe the conclusions (more like delusions) people come to on this subreddit, like AMD has been the good guy any time in the past several years.",hardware,2025-12-07 14:06:50,3
AMD,nsan9dx,Their price increases are due to the memory increases.,hardware,2025-12-04 19:12:43,-1
AMD,nsx22r5,"> If you had a chance to get a 200%+ higher paying job, would you?  depends on a job. I could do that right now, but i know too many people in that job that end up with permanent disability before 40 from job requirements.",hardware,2025-12-08 11:02:06,1
AMD,nsa3u3z,"The argument I'm making is that instead of raising prices, AMD is not cutting them and Zen 6 will come in higher.   Not sure what you're rambling about",hardware,2025-12-04 17:39:40,-8
AMD,nsk5gav,"More like [$195](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof), the launch prices are just inflated.",hardware,2025-12-06 07:11:52,1
AMD,nse8hum,1 kidney for 1 ram is only fair,hardware,2025-12-05 08:49:57,6
AMD,nsdqoto,I'd settle for a chip.,hardware,2025-12-05 06:02:18,1
AMD,nsappks,Rich kids got not sense for prices under a couple grands; therefore they can't understand the problem since their parents will buy them everything to make them shut up,hardware,2025-12-04 19:24:57,16
AMD,nsx14v5,the exact kit you are using is irrelevant. A typical 32GB of DDR5 can be got for 300 dollars now. Only a few extremists need the best frequencies and timings.,hardware,2025-12-08 10:53:02,0
AMD,nsc4ot1,You can definitely find a kit under 500,hardware,2025-12-04 23:52:56,-6
AMD,nsabn3e,"You don't need to look at the exact same kit...  Look for a current kit with similar specs...  Most people don't need more than 32GB (and even that's more than most really need), and that was $100 and now it's about $300 for a similar kit.",hardware,2025-12-04 18:17:03,-24
AMD,nsbk5p3,a 300 dollar kit im assuming is 96gb  Most people dont need that much   Motherboard prices are coming down a lot to offset some of the increase in ram price,hardware,2025-12-04 21:58:08,-15
AMD,nsak345,Are these the same people who spend a bunch extra on RGB lighting and noctua or maglev fans then too? lol,hardware,2025-12-04 18:57:11,-5
AMD,nspglyd,I suddenly feel rich with my 7900XT.,hardware,2025-12-07 03:56:02,1
AMD,nsx1l6e,"For server DDR5 yes, for HBM no.",hardware,2025-12-08 10:57:25,2
AMD,nsg0gep,"I admit that I don't know the difference between ECC and non ECC DDR5 memory chips.    I assumed that it was the case since that's how it goes over at TSMC for logic chips. I don't have time to look into it now. If that assumption was wrong, then I look dumb and I'll ""take the L"".",hardware,2025-12-05 16:13:27,1
AMD,nsetd0i,Part of that markup is from the die area getting more expensive. The other part is Nvidia further increasing margins and implementing more expensive memory for what is arguably a net negative. The extra bandwidth or some ten watts of power saving does basically nothing on most of the stack and instead you get gimped amounts of memory at higher cost.,hardware,2025-12-05 12:03:37,5
AMD,nsk4kek,"OTOH, SSD prices did take a sharp dive for a glorious moment.",hardware,2025-12-06 07:03:19,1
AMD,nsx1qtu,new nodes are more expensive than old nodes nowadays. So prices for GPUs will keep increasing just from basic manufacturing costs.,hardware,2025-12-08 10:58:56,1
AMD,nsx1vfs,"Its not just HW spaces, reddit is very doomer outlook in general.",hardware,2025-12-08 11:00:08,1
AMD,nsa40u3,"""Forgotten its roots.""",hardware,2025-12-04 17:40:35,13
AMD,nsqionh,Dual channel and dialysis it is then.,hardware,2025-12-07 09:14:42,1
AMD,nsd9zxa,Under $500 is still absurd for 64 GB of DDR5.,hardware,2025-12-05 03:59:49,11
AMD,nsac53x,Even that's a three fold increase. You get low end 32 GB kits for $300 whereas for the same price I got a Corsair Dominator Titanium 64 GB kit.  The market is fucked and it's entirely the fault of the corporate AI fad.,hardware,2025-12-04 18:19:25,25
AMD,nsblmwb,Nope it's a 64 GB Corsair Dominator Titanium kit.,hardware,2025-12-04 22:05:41,14
AMD,nscrv0s,My 64GB kit went from $135 at the start of this year to $780 now.  https://old.reddit.com/r/buildapcsales/comments/1i21ta2/ram_patriot_memory_viper_venom_64gb_2_x_32gb/  https://www.newegg.com/patriot-memory-viper-venom-64gb-ddr5-6400-cas-latency-cl32-desktop-memory-matte-black/p/N82E16820225335?Item=N82E16820225335,hardware,2025-12-05 02:10:32,5
AMD,nsx1eeo,Fun fact: RGB fans are so popular now that they are usually cheaper than non-RGB ones. So you can buy the cheaper RGB variant and just turn the lights off.,hardware,2025-12-08 10:55:37,1
AMD,nsgav86,"No ""L"" to be taken, it's greed from manufacturer any way \^\^",hardware,2025-12-05 17:04:14,2
AMD,nsx6gva,"True but AFAIK Nvidia could've sold a full AD103 die with a cheaper board and cooler at 300W priced at 800 USD. But Nvidia slipped a pricey cooler, board, pushed wattage and charged 400 more for that.  Crypto IMV really did a number because they thought they could get away with it. Ofc the OG 4080 sold poorly and never sold out.",hardware,2025-12-08 11:42:15,1
AMD,nsacdte,"Yeah, 3x increase for 1 part, one of the cheaper parts, making a $1500 PC using a typical 32GB kit be about 13% more expensive now than before. Hardly breaking the bank IMO.  I am curious how much difference in spec this kit for example is vs the one you got for $300 for 64GB before.  https://www.amazon.com/gp/product/B0BPHSVVS5/ref=ox_sc_act_title_1?smid=AWD7GDDT7Q2ZT&th=1  You are claiming 64GB is $1100, but this is less than half that price for 64GB 6400 CL34 which seems decent enough to me. I wonder what are the specs of your $1100 64GB ram then that it's that much better.",hardware,2025-12-04 18:20:34,-25
AMD,nscwfva,No one is paying 800 dollars for that   The ask price is not the true market price  There are klevv sticks on amazon that is more reasonable,hardware,2025-12-05 02:36:38,4
AMD,nsx1akr,im seeing equivalent specs for under 600 now.,hardware,2025-12-08 10:54:36,1
AMD,nsxzmfl,"And yet despite selling poorly as you claim, the 4080 sold more than the entire RDNA3 lineup combined. So i guess it offered something enough people wanted.",hardware,2025-12-08 14:54:42,1
AMD,nsag690,"This is just the start of the price hikes as well. It's going to keep getting worse until at least 2028.  Plus it's not just system RAM. It's going to affect VRAM too, as well as consoles, phones etc.",hardware,2025-12-04 18:38:38,15
AMD,ns9x83o,its a feat of engineering for a modern kernel to continue to work with a driver compiled in 2006,hardware,2025-12-04 17:07:19,34
AMD,ns9k1xs,"Yeah, PCI to PCIe bridges exist.  Of course the impressive part is getting the driver running in modern Windows.",hardware,2025-12-04 16:03:26,53
AMD,nsa2e7x,That's a blast from the past.,hardware,2025-12-04 17:32:32,10
AMD,ns9i5mm,Before I even clicked I knew Omores had something to do with this,hardware,2025-12-04 15:54:19,6
AMD,nsadfnn,I still have a couple of those oldies around. Had a Obsidian x-24 for a while there. Still have a Canopus Pure3d 2. Nice one that was actually different with s video. (Actually it's a proprietary connector),hardware,2025-12-04 18:25:34,2
AMD,nsi1knx,Wow finally found a purpose to my 3dfx collection   https://drive.google.com/drive/folders/1Pnl3R-4WgTXfKO33bEUqBbu9jT5mU54g,hardware,2025-12-05 22:24:09,2
AMD,nsfn0bb,"I remember when I had to use a separate 2d and 3d graphics card in my PC. I think the Voodoo Banshee was the first combined card I owned. I bet I still have it in a box somewhere lol, I wonder if that card will still work :)",hardware,2025-12-05 15:06:04,1
AMD,nskgk2r,Please credit the absolute legend /u/O_MORES who pulled this off!,hardware,2025-12-06 09:03:00,1
AMD,ns9g2zd,I didn't even know they had SLI back then. Or that Voodoo supported SLI.,hardware,2025-12-04 15:44:17,0
AMD,ns990w5,"Hello tuldok89! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-04 15:08:47,1
AMD,nsav14f,"Ancient? Buddy, I'm not even 40 and I played with Voodoo2 cards when they were current. Antiques are defined as >99 years old. The word you're looking for is ""vintage.""   (Yes, I know that's the Toms headline. We really need to ban TH posts.)",hardware,2025-12-04 19:51:46,-5
AMD,nsc74aw,I am using a soundcard from 2008 and its working perfectly,hardware,2025-12-05 00:07:30,10
AMD,nsdqezg,Modular kernel design FTW.,hardware,2025-12-05 05:59:59,2
AMD,nsfkcou,"I just listened to Jensen on Joe Rogan and I have to go back and relisten to it.  He explained I think the dtx was their game changing tech?  I don't understand what Nvidia did that made them leave all their competitors in the dust, technology wise.",hardware,2025-12-05 14:51:48,1
AMD,nsdljh2,"> Of course the impressive part is getting the driver running in modern Windows.  Shouldn't be that hard since, simply put, PCi-Express not only is just a newer revision of PCI itself (technically speaking), but also from a information-technology standpoint it's basically fairly the same.  Since to this day, all x86-chipsets still have a *PCI-to-PCi-Express* bridge and would be physically able to house age-old PCI-slots, if boards *wouldn't* have essentially beheaded the chipset off its physical PCI-slots …  *Microsoft didn't had Windows fundamentally drop support for PCI-slots nor -cards, or did they?*  ---- For comparison on what today's chipsets are actually capable of, just take a sharp look at recent Xeon- or EPYC-boards for embedded industrial PCs — They still often offer lots of PCI-slots, PS\/2 and whatnot.  So PCI, PCI-X and PCi-Express are more or less interchangeable and absolutely being concurrently featured onto boards *all at the same time*, even alongside some AGP-slot. Only ISA is harder to implement, yet not impossible.",hardware,2025-12-05 05:20:28,5
AMD,nsa9din,It was my first GPU I bought on the Pricewatch site. I could now compete with the rest of my dorm  in UT.  It was wild how fast things improved yearly back then.,hardware,2025-12-04 18:06:17,8
AMD,nsjy95s,"For the sake of your wallet, I hope you put together that collection before the prices for Voodoo cards skyrocketed.",hardware,2025-12-06 06:04:18,2
AMD,nskgf7h,Good god man,hardware,2025-12-06 09:01:38,2
AMD,ns9k6g3,"Get off my lawn.  3DFX *invented* SLI... but it's not the SLI you probably know, much more primitive but was very effective at the time.",hardware,2025-12-04 16:04:02,40
AMD,ns9iehj,3dfx SLI has almost nothing in common with nvidia's SLI implementation,hardware,2025-12-04 15:55:30,14
AMD,nscd44m,"The acronym SLI was made by 3dfx! The were the first dual-""GPU"" consumer offering.   Of course, they weren't called GPUs back then...",hardware,2025-12-05 00:42:06,3
AMD,ns9gsga,"All the Voodoo cards had it, but it was only exposed on consumer cards from Voodoo 2 and forward.",hardware,2025-12-04 15:47:44,7
AMD,nsbzx9c,I believe there were different ways it could work. One method would have each card render a different line.,hardware,2025-12-04 23:24:48,3
AMD,nsdngv3,"> I didn't even know they had SLI back then. Or that Voodoo supported SLI.  Their SLI was actually *one of the main reasons* for nVidia in the first place even overtaking the scraps of nigh bankrupt 3Dfx back then to begin with … Well, aside of their patent-disputes of IP-theft Nvidia did to 3Dfx, of course.  Also almost broke young Jensen back then was basically running around frantically (to one investor after another, begging for millions) after some on-going law-suits (at which nVidia rightfully got dealt severe blows over stolen technology and patent-infringement from 3Dfx), only to get together the money to overtake 3Dfx and essentially nip all the legal process-proceedings hammering in over Jensen in the bud …   … by overtaking 3Dfx, which at that point back then was going bankrupt in mere months (with the law-suits with Nvidia having very much to do with it to begin with …), only to  thus clear all legal dust for nVidia altogether.  ---- If nVidia **wouldn't** have been able to get together the money for a ~~de facto-settlement~~ *hostile overtake* of 3Dfx, nVidia *wouldn't* exist today. Yet Jensen got his way and nVidia did so, and went on to loot 3Dfx off everything they had instead … The rest is shady history.  As a result, ""Nvidia's"" graphics-cards \*magically\* sky-rocketed performance-wise shortly after — Go figure.",hardware,2025-12-05 05:35:49,3
AMD,nsdo792,"> The word you're looking for is ""vintage.""   I'd even say it's considered *retro* or just mere 'legacy'.",hardware,2025-12-05 05:41:44,2
AMD,nseevys,antiques are actually defined as >25 years.,hardware,2025-12-05 09:51:52,1
AMD,nskctln,"""Your soundcard works perfectly""?!",hardware,2025-12-06 08:24:40,2
AMD,nsk1e7n,Bro hears in monochrome,hardware,2025-12-06 06:33:22,2
AMD,nsebjpi,I was more talking about the difficulty of getting a 32bit Windows NT 4 driver running in 64 bit Windows 11 but it seems the hard part was done by someone in 2006 who wanted to do the same for Windows XP x64.,hardware,2025-12-05 09:18:22,9
AMD,nsd4y8j,"My first as well - actually can't remember if it was a 2 or a banshee, but it was definitely PCI.  I'm almost positive I never actually used it properly 😂.  After getting it, it was like nothing changed in games.  I was 12 and didn't understand that you had to go into game settings back then and turn on hardware acceleration/select the GPU.  I figured it out for one game like right before I got a new PC with an AGP card and Windows 95, and games worked by default then but I also knew to check every time lol",hardware,2025-12-05 03:27:29,2
AMD,nskvj05,"Some of them are from my own/family pc purchases between 96 and 2001  For example, the voodoo 1 orchid righteous, the voodoo banshee and the voodoo 5 5500 agp  The other were mostly bought between 5$ and 40$ cad between 2001-2004",hardware,2025-12-06 11:34:09,2
AMD,ns9ld9u,"AFAIK Nvidia never used the ""scan-line interface"" SLI from 3DFX. That method died with the purchase of 3dfx by Nvidia, and just the initials were reused later for a totally different way of doing multi-GPU.",hardware,2025-12-04 16:09:51,19
AMD,nsdojo4,"> 3dfx SLI has almost nothing in common with nvidia's SLI implementation  Well, except the very origin: *3Dfx inventing the technique* (as in principle).",hardware,2025-12-05 05:44:30,0
AMD,ns9kcpk,"Did Voodoo3 even support it? I don't remember.  I had Voodoo2 12MB in SLI because that was the style of the time, gave them away when Geforce 256 launched though.",hardware,2025-12-04 16:04:54,3
AMD,nsedsxg,GeForceFX was a dumpster fire,hardware,2025-12-05 09:41:11,1
AMD,nsgvku1,"https://en.wikipedia.org/wiki/Antique  >The common definition of antique is a collectible object such as a piece of furniture or work of art that has an enhanced value because of its considerable age, but it varies depending on the item, its source, the year of its creation, etc. The customary definition of antique requires that an item should be at least 100 years old and in original condition.[3] (Motor vehicles are an exception to this rule, with some definitions requiring an automobile to be as little as 25 years old to qualify as an antique.[4])",hardware,2025-12-05 18:44:39,1
AMD,nt6auj1,Enjoying yourself?!,hardware,2025-12-09 20:47:02,1
AMD,nskgaaq,Is be surprised if there were any significant differences in quality between a sound card from 2008 and today,hardware,2025-12-06 09:00:13,5
AMD,nst93zl,"It took until dx7 before it was finally at least on par with glide and most games still supported it by that time, if not even still run better with it than directx. Hardware T&L was also a standard feature on arcade hardware by the early 90s. Even the 90s consoles had hardware-accelerated T&L.  The game changer for Nvidia (and conversely for 3dfx) wasn't anything technology-related, but down to business strategy decisions.  Nvidia chose to target OEMs as their primarily customers, since prebuilts were (and continues to be) where the majority of GPU sales took place and go for cashflow over outright margins. Whereas 3dfx moved towards trying to muscle in on and eventually cut out partner vendors by doing their own AIB manufacturing, as well as focusing too much on the higher margin enthusiast retail part of the market.",hardware,2025-12-07 19:38:17,0
AMD,nsm31wl,"Funny how things people regarded as junk and sold off for pennies, or literally threw out for scrappers to take can appreciate in value so wildly. Same thing happened for CRTs - I wonder what the next big thing is?   From 1999 all the way to 2005, my family PC had a Pentium (MMX?) ranging from 166-233MHz and, wait for it...a Virge DX (2MB/4MB?). I would say we should have stuck a Voodoo 2 or 3 in there on the cheap, but my dad somehow failed at installing RAM in it, so I doubt either of us would have known how to actually enable hardware rendering in most games anyway.",hardware,2025-12-06 16:20:43,1
AMD,ns9obn7,"Yes, the last Voodoo models still supported it.",hardware,2025-12-04 16:24:05,1
AMD,nsfo3jb,"It at least brought large performance-increasements, even if Nvidia botched the first implementations, no?  *The GeForce line was at least a huge improvement on graphics for them …*  Since before that, nVidia failed hard and their very first graphics-card line *NV1* (and the *NV2*, which never really saw the light of days and was canceled prior to release), only supported proprietary *quadratic texture mapping*, which was fundamentally incompatible to anything OpenGL or DirectX — Both NV-line cards failed hard and nVidia ended up on the brink of bankruptcy because of it.",hardware,2025-12-05 15:11:50,1
AMD,nskjraf,Well idk. But its still better than the one built into my mainboard😂,hardware,2025-12-06 09:36:20,2
AMD,nsm87ie,"The Voodoo 1 Orchid Righteous 4mb was paired with a Matrox Mystic 4mb in a 01 166mhz mmx, 3gb hdd and was wonderful to play Moto Racer, Starcraft, Diablo 1, Duke Nukem, Red Alert",hardware,2025-12-06 16:47:54,1
AMD,ns9pmhm,"Not only that, the voodoo5 5500 I have downstairs does single board SLI to use both chips.",hardware,2025-12-04 16:30:25,7
AMD,ns9uogr,How did it work? They don't seem to have the header for the cable,hardware,2025-12-04 16:54:53,3
AMD,nslbfyz,> But its still better than the one built into my mainboard  unlikely. unless you're using something 15 years old,hardware,2025-12-06 13:40:17,-2
AMD,nsa6hrb,It was internal SLI only to combine the chips that are on a single board. Those models didn't support multi-card SLI.   In reality the original SLI only existed during the Voodoo 2. Voodoo 1 didn't have it for consumer cards and Voodoo 3+ didn't have the header for the ribbon cable.,hardware,2025-12-04 17:52:28,4
AMD,nsa41ya,"I don't have personal experience with it, but I believe it simply sent data over PCI. Some later cards from Nvidia or AMD/ATI could also do SLI/CrossFire without the physical bridge, just over PCIe.",hardware,2025-12-04 17:40:44,0
AMD,nspjiuf,"Audio was effectively solved on PC by the late 90s. Once they hit 48000hz/16bit Stereo it became good enough, everything beyond that is mostly for engineers, audiophiles and specialist setups (surround etc).  Not that there wasn't and doesn't continue to be audio quality differences, but that has always come down to the quality of components and circuit after the DAC does it's work. As a result, good sound cards from back then still sound good. If anything, sound cards have gotten more simple over time, they used to have onboard processing for audio mixing, discreet chips to handle MIDI and FM synthesis. These days it's almost entirely CPU driven, audio chips barely do anything at all.",hardware,2025-12-07 04:15:13,2
AMD,nslrmg2,Well the soundboard might be a few years newer than 2008 but not much lol. Recently downloaded the drivers and noticed how old tehy are,hardware,2025-12-06 15:18:11,1
AMD,nsusefd,"Sound has mostly been processed on the CPU for 25 years now, the sound chip on your motherboard doesn't really do much.",hardware,2025-12-08 00:26:31,0
AMD,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,7
AMD,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
AMD,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,10
AMD,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,11
AMD,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,11
AMD,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,8
AMD,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,3
AMD,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,1
AMD,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,1
AMD,nsz85x6,What's the difference between B650 and B850 apart from PCIe 5.0 GPU support?,hardware,2025-12-08 18:35:38,20
AMD,nsz8aq7,Why would they make more motherboards when sales are dropping due to unaffordable RAM?,hardware,2025-12-08 18:36:17,32
AMD,nsz7w0i,Article says that motherboard sales were down 50% in November compared to October. I mean I know the numbers would fall but 50% in a month? Thats a different league to the crypto bubble...,hardware,2025-12-08 18:34:17,25
AMD,nszo0gn,Maybe software will start getting rid of bloat?,hardware,2025-12-08 19:53:26,7
AMD,nt2o8re,I still find it disappointing that the CPU to chipset bus on the B850 is PCIe 4,hardware,2025-12-09 06:47:31,2
AMD,nsz0ouj,"Hello constantlymat! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-08 17:59:21,1
AMD,nt2fnp9,"I'm confused by the reporting on this, or maybe the marketing spin.  Other reports are that motherboards sales and future projections are expected to be way down as a effect of high memory prices. So why would you want to expand motherboard production when demand is expected to drop?  B650 and B850 production cost difference also I doubt are that significant from an end user perspective. If you only charged users the actual BoM difference would people really want to save the $10 or so?   It seems more likely that there is existing B650 stock that they were not able to deplete through the holidays sale season due to an overall drop in sales from rising memory prices. They'll want to clear these without also lowering margins on B850.",hardware,2025-12-09 05:33:00,0
AMD,nszaf28,Main differences as far as I can tell:  * One primary PCIe 5.0 M.2 SSD slot: Was optional with B650 and is mandatory with B850 * PCIe 5.0 GPU support was a rare BIOS update with non-guaranteed functionality on B650 boards and is now guaranteed * USB connectivity: B650 had at most optional USB 3.2 support whereas now the optional USB support jumps up to USB 4.0 * Wifi: B650 boards featured Wifi 6 and 6E whereas most B850 boards afaik have Wifi7,hardware,2025-12-08 18:46:25,25
AMD,nt0snmq,"The funny thing is that many B650 boards actually do now support Gen 5 on both the x16 and top NVMe slot with BIOS updates.  For example, I’m getting Gen 5 x16 on a Gigabyte X AX v2 B650 motherboard.",hardware,2025-12-08 23:23:01,9
AMD,nt0uwex,"it's about B650 vs B850 production - they want to prioritize the cheaper motherboards, to offset increased system costs a bit and still get people to buy.",hardware,2025-12-08 23:36:07,16
AMD,nsz8eqj,"Once we get the sales data that fully includes Samsung & Co's price-hike from mid November to $135 per 16GB DRAM module (according to Reuters), I wouldn't be surprised to hear that motherboard sales shrink by 75% year-over-year.",hardware,2025-12-08 18:36:50,27
AMD,nt1ci7p,I'd expect the sales of laptops and smartphones to also dramatically drop off in the months to come as the OEMs use up their existing memory inventory and contracts. Samsung's mobile and memory division already had a public battle over the memory supply.,hardware,2025-12-09 01:19:42,6
AMD,nt1c1s8,"Gearbox CEO Randy Pitchford: *laughs*  https://www.pcgamer.com/hardware/borderlands-4-is-a-premium-game-made-for-premium-gamers-is-randy-pitchfords-tone-deaf-retort-to-the-performance-backlash-if-youre-trying-to-drive-a-monster-truck-with-a-leaf-blowers-motor-youre-going-to-be-disappointed/  > ""Borderlands 4 is a premium game made for premium gamers. Just as Borderlands 4 cannot run on a PlayStation 4, it cannot be expected to run on too-old PC hardware,"" he posted on Saturday. ""This is not a game made to run on 10-year-old PCs… if you're trying to drive a monster truck with a leaf blower's motor, you're going to be disappointed.""  ...  > our Nick found that 120 fps was a long way off on a max spec PC in his early Borderlands 4 performance testing. Running the same RTX 5090-based GPU and CPU setup as the X user above (albeit at stock clocks) resulted in a mere 40 fps average at Native, without any DLSS or MFG help. A hefty dose of DLSS Performance boosted the frame rate up to 80 fps (with the odd hitch), but it's still a poor showing for the $2,000+ GPU.  Don't forget about Microsoft keep finding new ways to shove Copilot into things. Or Electron-based apps continuing to be popular and RAM hungry.",hardware,2025-12-09 01:16:54,11
AMD,nt52srs,"It's about the alignment of economic incentives. The PC game development companies don't pay for your hardware so they have little incentive to write efficient code. Their incentive is to minimize developer time and cost.   There used to an old saying: Andy giveth and Bill taketh away. It refers to Andy Grove, former chairman of Intel and Bill Gates, former chairman of Microsoft.  Intel keeps making faster CPUs and Microsoft keeps finding ways to make slow.",hardware,2025-12-09 17:03:52,1
AMD,nt3tlh0,it is actually the same physical chip in both chipsets.,hardware,2025-12-09 13:05:26,3
AMD,nszk53l,"So slightly cheaper to produce if the old production line is still open then, less traces for PCIE, and cheaper wifi radios I guess. I can't imagine its a big difference though.",hardware,2025-12-08 19:34:07,15
AMD,nt3tco6,"These are not actually chipset differences, but AMD different requirements for branding.",hardware,2025-12-09 13:03:49,2
AMD,nt4za3p,Do these boards actually have wifi built into the chipset? In the past desktop boards seemed to just have an m.2 device added on.,hardware,2025-12-09 16:46:59,2
AMD,nt0v9ll,"Yeah, I just checked mine and it says the GPU is running at PCIe 5.0. Weirdly enough, I don't recall it being an option in the BIOS menu but I'm not complaining given I paid £130 for my motherboard lol.",hardware,2025-12-08 23:38:19,5
AMD,nt0xzwn,"Hm, somehow I don't think saving 10 bucks on the chipset will make up for the memory price increase.",hardware,2025-12-08 23:54:21,14
AMD,nt3a97m,"The price increase for both is only going to be ~20% so it will dent sales, its not likely to be the complete collapse that PC components are likely to face e.g. Motherboards, PC PSU's, Cases.  Its almost certain the number of PC component manufacturers is going to be significantly smaller by this time next year, especially case manufacturers.",hardware,2025-12-09 10:26:53,2
AMD,nsztj8j,"B650 is the basis for Epyc 4004/5 systems, so B650 was probably always going to remaining in production for a while. Or at least until AMD is prepared to spend the time + resources to validate and certify B850 for it.",hardware,2025-12-08 20:20:56,9
AMD,nt4q1hm,Where do you check that it’s running at pcie 5.0?,hardware,2025-12-09 16:03:04,1
AMD,nt1bf86,Going to need to bundle free boards and CPUs with RAM kits to drive sales at this rate.,hardware,2025-12-09 01:13:06,12
AMD,nszxwgy,"It's all *the same chip*; different names on the same piece of silicon. B650 vs B650E vs B850 is just a matter of which other features the motherboard is required to implement in order to qualify for the different branding options, and most of those differences don't matter to a low-end server.",hardware,2025-12-08 20:42:43,19
AMD,nt4vjhn,CPU-Z.,hardware,2025-12-09 16:29:19,2
AMD,nt5ryqk,"GPU-Z or CPU-Z on Windows.  LACT on Linux for a GUI view or just “lspci -vv” - 32 GT/sec is Gen 5, 16 GT/sec is Gen 4.",hardware,2025-12-09 19:13:29,1
AMD,nt00yu8,"Yes, I'm aware the silicon itself is the same. The entire AM5 platform uses the same promontory 21 chip. I suppose I could've said ""B650 mobos are the blah blah"" for better clarity.",hardware,2025-12-08 20:57:44,7
AMD,nt0f51h,"There's still *nothing to validate*, especially from AMD. A motherboard vendor implements the features they want for their low-end server board, then checks with AMD's marketing department to confirm what branding that feature set qualifies for.",hardware,2025-12-08 22:08:25,3
AMD,nt1z332,"B840 is PROM19 (B550), but yeah, it’s essentially just board level feature differentiation.",hardware,2025-12-09 03:33:02,3
AMD,ns64f73,"The FPGA have a Gen4x4 interface. A short summary from the article:   >Enable high-speed connectivity through 16.3G transceivers and PCIe Gen 4 interfaces, delivering fast data transfer and low latency for machine vision and networking systems  >Support mixed-voltage I/Os including 3.3 V and lower-voltage standards, allowing easy integration of both legacy and high-speed I/Os on a single device  >Simplify system integration and accelerate design cycles by allowing engineers to seamlessly connect and synchronize diverse sensors, optical modules, and control interfaces  >Spartan UltraScale+ SU45P and SU60P FPGAs will have early access Vivado tool support in 1H'26 with silicon availability in 2H'26.",hardware,2025-12-04 01:16:24,7
AMD,ns6gsy8,I’m really curious how much these cost.  The data rate is high end but the logic amount attached to it is not super high.  Would AMD sell these to you for $60 each if you were a customer they cared about?,hardware,2025-12-04 02:30:16,6
AMD,nsgoo7k,Sounds like something you can put on an M.2,hardware,2025-12-05 18:11:15,1
AMD,ns6ql8s,"digikey, mouser, etc. will eventually show their one piece pricing. You can already see pricing for some of the bigger parts there like the SU65P.",hardware,2025-12-04 03:29:54,5
AMD,ns85gh3,"The Spartan name traditionally is their ""low end"" FPGA.  The way the (4) families are divided that they have different mix of resource that address areas of the market.  The prices are likely ascending from left to right.  If you need more logic, you would pay for their higher series.  Manufacturers typically have supply contracts with their high volume preferred customers that bypass the usual distributors chain.",hardware,2025-12-04 10:44:04,2
AMD,ns6or52,Would be nice but I have no idea.,hardware,2025-12-04 03:18:20,1
AMD,ns6rhth,I’m kind of interested in this one so I may quote it.  The Digikey pricing is weirdly divergent from reality on FPGAs.  Where for a lot of other categories it’s usually pretty accurate.  Maybe because the turnover is poor?,hardware,2025-12-04 03:35:36,3
AMD,nsgm4a2,"TL;DR: the HDMI Forum sucks. Use Displayport instead if connecting to a monitor. Use the one and only DP -> HDMI adapter by Cable Matters which (sometimes) works if you want 4K 120Hz HDR VRR with full 10bit on your TV like me.  And once again, the HDMI Forum sucks. Pricks.",hardware,2025-12-05 17:59:00,724
AMD,nsh8wpy,It's a shame TVs didn't continue with Display Port like they used to with VGA.,hardware,2025-12-05 19:51:45,79
AMD,nsgl6j1,As a Linux user I have been following this drama since HDMI 2.1's release. Hopefully valve with their larger influence can convince the HDMI forum to change their minds on allowing an opensource driver implementation.    I am worried though that the HDMI forum will grant some sort of special license to valve and the steam machine will become the only linux device to support 2.1,hardware,2025-12-05 17:54:34,332
AMD,nsgsg52,Isn't the HDMI drama all boiling down to licensing bullshit? My understanding is displayport is pretty much free for manufacturers but the owners of the HDMI license charge by the port and are pretty expensive to get,hardware,2025-12-05 18:29:18,112
AMD,nsgui4d,"TL;DR, hardware interface standards should not be proprietary.",hardware,2025-12-05 18:39:22,105
AMD,nsh76d1,"Honestly HDMI Forum is terrible, I wish manufacturers would start phasing out HDMI",hardware,2025-12-05 19:42:47,72
AMD,nsik4lw,HDMI is one of the worst ports ever made.,hardware,2025-12-06 00:15:29,18
AMD,nshqq3e,Will anything even happen with steam machines with the current price of RAM,hardware,2025-12-05 21:25:27,10
AMD,nsgop2f,AMD tried something like this and the HDMI Forum quickly shut them down. Might even be related to this device though it was a while ago they were trying to push it through. Personally I use DP when I can and am looking forward to [GPMI](https://en.wikipedia.org/wiki/GPMI).,hardware,2025-12-05 18:11:22,25
AMD,nsi48i8,HDMI has always been like it's the exact reason why VESA wanted to kill it many years ago.,hardware,2025-12-05 22:39:13,13
AMD,nsgzgjt,"Oh so that's why my linux laptop can't leverage HDMI 2.1, TIL.   I wonder if a thunderbolt to HDMI 2.1 adapter will work or not... (my guess is no)   Unfortunately many monitors only have one displayport input.",hardware,2025-12-05 19:03:38,6
AMD,nshzamb,Why not use DisplayPort,hardware,2025-12-05 22:11:29,5
AMD,nsgp36m,"Oh, so it's HDMI 2.0 bandwidth with chroma subsampling... People were hoping for HDMI 2.1 bandwidth without HDMI 2.1 features.",hardware,2025-12-05 18:13:13,16
AMD,nsgzfsy,“Digging in” haha it’s like the most surface level shit lol,hardware,2025-12-05 19:03:32,9
AMD,nshqfvx,The HDMI Forum needs to be dissolved.,hardware,2025-12-05 21:23:58,8
AMD,nsgx73y,"I'm wondering if it will have surround sound capability? I bought a Minisforum mini gaming PC, about the size of an Xbox Series S, and installed Bazzite on it. Only to discover that over HDMI, only stereo is supported. I have to reboot into Windows if I want surround sound. To be fair, that's really just for movies, but it'd be nice if it worked in Bazzite.",hardware,2025-12-05 18:52:34,3
AMD,nsii4yl,Feels like HDMI standards are drama every year.,hardware,2025-12-06 00:03:14,5
AMD,nsiwk9o,Can't the driver interface with some proprietary blob that acts as a middleman between the open source driver and the HDMI 2.1?,hardware,2025-12-06 01:33:27,4
AMD,nsqq5xn,"One error in that article, they say DP 1.4 has more bandwidth than HDMI 2.1 which definitely isn’t true.",hardware,2025-12-07 10:29:47,4
AMD,nsgn4am,"Valve should work with some budget TV company and release some 55-75"" rebranded TV's without HDMI and just displayport. Keep the optical audio port. I need that. Pack in HDMI adapters. Someone needs to champion displayport on televisions",hardware,2025-12-05 18:03:48,23
AMD,nsh2pfp,Let's pray HDMI forum sees sense with open source,hardware,2025-12-05 19:19:47,3
AMD,nsj1d3w,"I'm honestly annoyed that a pure optical cable didn't just become the standard. A single optical cable is absolutely capable of carrying the bandwidth necessary for 4K+ streaming plus uncompressed audio, and over much longer distances. If this became the standard years ago we wouldn't have so much HDMI cable waste from having to upgrade so many times.",hardware,2025-12-06 02:04:48,5
AMD,nsh4wfg,Can valve do what compaq did and clean room solution this?,hardware,2025-12-05 19:30:57,2
AMD,nslr2oz,"Just to add some additional info, this is only a problem for AMD (any maybe Intel?) on Linux. Nvidia fully supports hdmi 2.1 and all of its features. This is because the Nvidia driver is still mostly proprietary even though they have open sourced the kernel modules.",hardware,2025-12-06 15:15:04,2
AMD,nsqq265,Why can’t AMD just make some closed source drivers?,hardware,2025-12-07 10:28:43,2
AMD,nsgxw44,"I don't follow this as much since I don't use Linux to that extent, but doesn't Nvidia support HDMI 2.1 in Linux (both the closed and open source drivers) because they use a closed source binary blob for it?  If so this seems like it's also addressable on the AMD and Valve side as well. However is there an ideological road block related to not wanting to implement a closed source solution? From the article it seems like a road block is also wanting to remain open source on AMD's side -   >“At this time **an open source HDMI 2.1 implementation** is not possible without running afoul of the HDMI Forum requirements,” AMD engineer Alex Deucher said at the time.  If so it seems like the question should be asked is if that ideological stance is worth it at the expense of some consumers (depending on their view). As in if AMD/Valve could support HDMI 2.1 fully but via a closed source binary blob but chose not to because of their stance on being open source, how would the consumer feel about it?   An interesting extension of this is if/when AMD/Valve release drivers for Windows for this will it support full HDMI 2.1 in Windows? Or would they artificially restrict it for feature parity?",hardware,2025-12-05 18:55:58,4
AMD,nsk7pcz,"This is the exact reason I can't buy a Steam Deck or Steam Machine now. I would LOVE to buy a Steam Deck if I could use it both to game on and to feed 4K@120Hz VRR HDR 4:4:4 to my TV via moonlight. I wish HDMI would just fucking die already. DP 2.1 already supports upto 80Gbps whereas HDMI is going to crawl up in bandwidth step by step to milk as much money in forced upgrades as they can, in addition to blocking open-source drivers.",hardware,2025-12-06 07:33:44,2
AMD,nsgwh82,"Man screw HDMI, display port is here anyways",hardware,2025-12-05 18:49:04,4
AMD,nsqa5vn,"It would be nice to move away from HDMI all together. In my opinion it would be nice to just have audio and video a separate connection, they won't do it because of DRM and Dolby but it would be nice.",hardware,2025-12-07 07:50:52,2
AMD,nsjlr2z,HDMI is just DisplayPort with different branding that you have to pay for 4 times over.,hardware,2025-12-06 04:23:20,1
AMD,nshao5w,"I didn't realize that was a thing.. I can do 4k 120 out on linux with my displayport to hdmi cable, I thought it was 2.1?",hardware,2025-12-05 20:00:55,1
AMD,nsrkiwl,Corporate greed. Use display port,hardware,2025-12-07 14:30:19,1
AMD,nsz0kg3,"In addition to HDMI 2.1 proprietary nonsense and anti-consumer restrictions on Linux..., don't forget that many big companies represented by HDMI Forum also enjoy snooping on user content for marketing purposes with the feature called ACR, which stands for Automatic Content Recognition. It is a digital fingerprint of all content watched on TVs, from internal apps to external devices connected to it via HDMI ports. Owners of those TVs would need to dig deeply into settings menu to switch it off. The ACR sends into cloud a few kilobytes of data almost every minute with information what is watched, when and for how long. Please check your settings and watch any YT video about it and how to disable it.  There is no other way to fight for opening HDMI 2.1 standard for Linux devices than taking political action and rallying local consumer right and protection groups that could then make representations before national consumer protection agencies, and ultimately national parliaments. As HDMI is a global standard backed by big, rich, global corporations, it's the only way. Those companies will not cut the hand that feeds them on their own.",hardware,2025-12-08 17:58:46,1
AMD,nsi1z7i,It doesn't even need hdmi 2.1,hardware,2025-12-05 22:26:26,1
AMD,nsgw01z,Given that the one thing companies like most is saving money i cant understand why HDMI is still used and they didn't all just switch to DP,hardware,2025-12-05 18:46:44,0
AMD,nssj8w0,"the hdmi forum is SCUM.  they are an active enemy of gnu + linux.  and this situation isn't new.  amd basically bagged this scum organization to get ""hdmi 2.1"" working on gnu + linux for ages and all they did was show amd the middle finger.  the quotation marks are for partially meaningless hdmi 2.1 is, as people can put hdmi 2.1 on boxes with hdmi ""1.4"" bandwidth. another scam run by the hdmi forum, although dp does it too now, but dp at least has the direct bandwidth marketing option as well next to it if desired.",hardware,2025-12-07 17:34:22,0
AMD,nsgz3sz,"I can understand Linux not paying because it's free software and it's unmanageable, but this is a physical box. How much could it be, one dollar?",hardware,2025-12-05 19:01:52,-10
AMD,nsj6zq2,"I guess, I feel like I don't understand why bother dealing with all of the licensing drama for a discount PC that most people will not have it hooked up to more than a cheap 1080p TV.",hardware,2025-12-06 02:41:13,-1
AMD,nsjy0m6,Big meh. The HDMI license and the closed-source requirement for 2.1 features is the cost of doing business. Valve is a multibillion dollar company so this is really just a matter of them being stubborn with SteamOS. If sub-$500 TVs can get HDMI 2.1 then so can the Steam Box.  Everyone who uses a credit card pays a 2-3% credit card fee for every single transaction. Hypocritical if you use credit cards but then complain about HDMI.,hardware,2025-12-06 06:02:10,-12
AMD,nsgoudq,This is why the Intel Arc GPUs only support DP and the graphics card has a built in DP to HDMI adapter on the board for the HDMI port. So the driver only needs to support DP.,hardware,2025-12-05 18:12:03,295
AMD,nsiexdy,Here's the kicker most of the time VRR does not work on that adapter.,hardware,2025-12-05 23:43:18,17
AMD,nsgyzov,What do you mean there's only one DP > HDMI adapter?,hardware,2025-12-05 19:01:17,14
AMD,nsh5176,But can the DP adapter do hdmi arc?,hardware,2025-12-05 19:31:38,9
AMD,nsjwhmm,One of the main reasons to get the Steam Machine is the HDMI-CEC support.,hardware,2025-12-06 05:48:55,3
AMD,nskk463,> TL;DR: the HDMI Forum sucks.  :),hardware,2025-12-06 09:40:02,3
AMD,nsjx5up,This being the adapter in question: https://a.co/d/j9BTuKl,hardware,2025-12-06 05:54:40,2
AMD,nskvmim,"Here is what valve should do: Design a nice sticker that says ""Steam Machine native 4K"" and ""license"" it to TV manufacturers that have a DP on their TV so they can slap it on their boxes.",hardware,2025-12-06 11:35:04,2
AMD,nsmbi33,"The adapter has a high success rate as long as you don't care for VRR. Once you care for VRR, the chances of it working come down a lot and depends a lot on the display you're plugging it into and your GPU model.",hardware,2025-12-06 17:05:19,2
AMD,nsirx4j,Can you link? I thought Cable Matters explicitly lists it as not VRR compatible,hardware,2025-12-06 01:03:20,1
AMD,nt371rq,"> TL;DR: the HDMI Forum sucks. Use Displayport instead if connecting to a monitor. Use the one and only DP -> HDMI adapter by Cable Matters which (sometimes) works if you want 4K 120Hz HDR VRR with full 10bit on your TV like me.  That isn't at all true for all TVs. LG OLED TVs from 2020 and before that at the very least (but likely later sets as well, I just happen to have a 2020 set) do not support VRR over DP-to-HDMI adapters and with a lack of DSC also don't support full 120 hz at 4K with 10 bit 444.",hardware,2025-12-09 09:55:11,1
AMD,nsh0kyn,"Is it that straight forward?   From what I've read HDMI 2.1 is supported in Linux on Nvidia hardware from both the closed source and ""open source"" drivers.  The problem seems like it stems from the open source issue as even the Nvidia ""open source"" driver has closed binary blobs, which they used to support HDMI 2.1  Even from the article it suggests it seems like the hangup is also an ideological one in part -   >“At this time an open source HDMI 2.1 implementation is not possible without running afoul of the HDMI Forum requirements,” AMD engineer Alex Deucher said at the time.  This suggests they could implement HDMI 2.1 support in the same way.",hardware,2025-12-05 19:09:09,-8
AMD,nshfw4e,"Is dumb that they don't because it's an open standard, at least on more premium TVs. But of course it all comes down to what most people are familiar with, which is HDMI, and money, it's cheaper to not include it.",hardware,2025-12-05 20:28:35,32
AMD,nsl00vo,Very few TVs had VGA. Only really high end models.,hardware,2025-12-06 12:15:05,1
AMD,nshj4jp,AMD had a working opensource driver with the HDMI firmware that loaded somewhat like Nvidia does and it was rejected by the HDMI Forum.,hardware,2025-12-05 20:45:47,126
AMD,nsgvckn,I think that would be objectively worse indeed. Would kinda defeat the whole point.,hardware,2025-12-05 18:43:32,55
AMD,nsh96tb,I really doubt it as it would require a custom AMDGPU driver patch,hardware,2025-12-05 19:53:13,44
AMD,nsihdam,>  Hopefully valve with their larger influence can convince the HDMI forum to change their minds  https://media.tenor.com/QgTx6fv4IpAAAAAM/el-risitas-juan-joya-borja.gif,hardware,2025-12-05 23:58:22,10
AMD,nsiio1c,If Microsoft and Nintendo can’t influence it I doubt valve can man,hardware,2025-12-06 00:06:31,12
AMD,nsigk09,Do Tizen TVs not support it?,hardware,2025-12-05 23:53:23,2
AMD,nsvykab,"Why would you even still want HDMI, the inferior of all?",hardware,2025-12-08 04:48:12,1
AMD,nsgu6w3,"Blocked on linux, even if you have infinite money. Thats the problem. Pure insanity by hdmi forum.",hardware,2025-12-05 18:37:51,125
AMD,nsgugn1,It's not about licensing. AMD and gpu/laptop manufacturer already pay for it. It's about HDMI forum not wanting driver for HDMI 2.1 to be open source.,hardware,2025-12-05 18:39:09,67
AMD,nsh2gnk,"hdmi is drm, in a way. it's always down to ~~licensensing~~ money",hardware,2025-12-05 19:18:33,28
AMD,nsm67ih,"So that's why every GPU I've gotten recently has 3 DP ports and only one HDMI. I thought they were just out to get me and my never ending stash of old HDMI 1.2 cables(which are perfectly fine for 99% of stuff I want to plug in).  Down with HDMI, long live DisplayPort!",hardware,2025-12-06 16:37:24,3
AMD,nsrmy6j,HDMI's *existence* is sort of down to licensing and DRM bullshit.,hardware,2025-12-07 14:44:04,1
AMD,nsi0x7j,"I wish displayport was even remotely reliable in comparison to HDMI. On the surface it seems like such a better standard, but dude the number of displayport cables/adapters that die if you roll them out in a large number is insane",hardware,2025-12-05 22:20:29,-7
AMD,nsjm6ec,"Ahem.   ""ECOSYSTEM""  My most hated word. -runs-",hardware,2025-12-06 04:26:32,21
AMD,nsh6u2d,Amen.,hardware,2025-12-05 19:41:00,5
AMD,nsih1ux,The amount of HDMI out in the world pretty much means that won’t happen.,hardware,2025-12-05 23:56:25,26
AMD,nsjrpgs,Not going to be replaced in the TV segment without their say.,hardware,2025-12-06 05:09:23,2
AMD,nszp4qm,"Yup, it's basically DVI with added sound and subtracted screw fitting, with a much flimsier plug.",hardware,2025-12-08 19:58:56,2
AMD,nsl2l2d,"GPMI protocol will use the USB-C connector, and it's unclear whether or not it will be free. What are the odds that a standard developed by a Chinese company is fully USB compliant? I don't have high hopes. If we're talking about display interfaces that use the USB-C connector why not look forward to devices implementing DP 2.1 Alt mode, or heck, even Thunderbolt?",hardware,2025-12-06 12:36:43,4
AMD,nsi229o,It could. Video over usb-c/thunderbolt is transported using DisplayPort protocol.,hardware,2025-12-05 22:26:54,7
AMD,nsiys4j,Because they need to be as universally compatible as possible. Not many people have TVs with DisplayPort.,hardware,2025-12-06 01:48:04,14
AMD,nsjmcll,"FRL is specifically the thing being gatekept, even though FRL is barely different from DisplayPort HBR.  And much of the secrecy is to keep you from realizing it is stolen from VESA.",hardware,2025-12-06 04:27:48,11
AMD,nsqqz0k,Yeah this is pretty disappointing. This makes it significantly less capable than all modern consoles.,hardware,2025-12-07 10:37:57,1
AMD,nsjxb9v,"If I had surround sound downstairs, I would absolutely game on it in surround",hardware,2025-12-06 05:55:57,1
AMD,nsqrsdt,It looks like SteamOS only does Stereo sound for games so it’s not looking good.,hardware,2025-12-07 10:46:06,1
AMD,nskfkej,Yes,hardware,2025-12-06 08:52:57,3
AMD,nsgr5q3,DisplayPort has no alternative to eARC and therefore you can't fully get rid of HDMI in the TV space.,hardware,2025-12-05 18:23:03,49
AMD,nsh1fl9,"I can't imagine many people being okay buying a TV with NO hdmi. HDMI + Displayport is a far friendlier solution, and more convenient for just about everybody.   Sure, its less of a protest, but that HDMI adapter isn't suddenly going to make eARC and CEC stuff work nicely",hardware,2025-12-05 19:13:25,11
AMD,nsgo77z,"which chipset do they use? most brands use off the shelf chipsets so they need to find one with display port. take for example the Mediatek pentonic 1000, a high end off the shelf chip used in expensive TVs yet it doesn't support display port [https://www.mediatek.com/products/pentonic/1000](https://www.mediatek.com/products/pentonic/1000)",hardware,2025-12-05 18:09:00,7
AMD,nsi1x93,"Optical audio is not ideal for PC gaming either because you can't actually output 5.1 surround sound for games unless it's a dolby digital 5.1 or dts 5.1 bitstream (which are compressed and lossy surround sound formats) And the problem with that is that only works if you have premade content like a video file with DD5.1 or DTS tracks built in that you can passthrough to the receiver, but a standard PC cannot encode general audio TO DD5.1 or DTS in real time unless your soundcard on the PC supports a uncommon feature called Dolby Digital Live. Consoles DO have this capability to encode to DD5.1/DTS5.1 in real time but PCs don't, which is where the confusion often comes from on PC side.",hardware,2025-12-05 22:26:07,7
AMD,nsk84wb,"That would cut out like 10 forced upgrade cycles across billions of cables, TVs, GPUs, peripherals and cost all of those industries billions of dollars. I'm honestly quite sure that's the only reason we see these stupid standards inch up their bandwidths step by step instead of just fixing it one go.",hardware,2025-12-06 07:37:59,4
AMD,nskex5o,HDMI uses the same physical standard for ages at this point.  With optical audio you would have the same exact problems that you had now with HDMI,hardware,2025-12-06 08:46:18,2
AMD,nsha1i5,It should be open source and I’d like them to be vocal about it. It would be nice if they had the hardware capabilities for 2.1 and then roll out a software update later if they ever make progress on it being open source.,hardware,2025-12-05 19:57:39,3
AMD,nsmb6n2,">  I wish HDMI would just fucking die already.   Keep dreaming, just like how RCA composite held off every other ""superior"" analog input for TVs until HDMI finally toppled it. Never saw anyone complaining that most TVs didn't have VGA input. Baragin-bin TVs _still_ have RCA composite input over VGA/S-Video/Component.  At this point with HDMI 2.2 supporting 96gbps, the only practical use for DisplayPort for most people is the one-cable solution for laptops to plug into a monitor dock and charge the laptop at the same time. 96gbps still isn't enough for 8K120 without DSC, so there will inevitably be something better.",hardware,2025-12-06 17:03:37,3
AMD,nskaywe,"Steam machine does have displayport 2.1 though. And the deck has displayport 1.4. If you want hdmi to die then buy displayport devices, like th exact ones you are complaining about not having hdmi.",hardware,2025-12-06 08:05:57,0
AMD,nsreqbm,> It would be nice to move away from HDMI all together.  Won't happen. RCA Composite cables dominated the scene until HDMI arrived even though there were superior analog options. HDMI cables are also very very cheap to produce. DisplayPort has a locking mechanism but that increases the cost of the cable. Same reason why RCA became mainstream over BNC.,hardware,2025-12-07 13:54:51,1
AMD,nskfjno,Your converter is likely hdmi 2.1,hardware,2025-12-06 08:52:44,1
AMD,nsh1m3c,"Because HDMI is deeply entrenched in the whole ecosystem and DisplayPort doesn't cover all features that are useful in that space. (e)ARC, CEC, Lip sync correction, etc. If you want to offer devices with DisplayPort support, you'd need a loooong transition period where you offer both, so why even bother? HDMI license fees are not that much to begin with (in addition to the annual flat fee, it is $0.04 per device if you implement HDCP which you probably have to do in the TV space anyway).",hardware,2025-12-05 19:14:19,17
AMD,nsh0w31,I always thought it was:  1. Profit 2. Shareholders 3. Not paying taxes.  4. Screwing over brand loyal customers by reducing quality/functions while increasing prices. Etc…,hardware,2025-12-05 19:10:42,-2
AMD,nskfctn,cheap 1080 TVs are dead by this point.  We aren’t in the 10s,hardware,2025-12-06 08:50:49,3
AMD,nsgmzf7,people be yapping anything just to shit on AMD,hardware,2025-12-05 18:03:08,8
AMD,nsgmr8h,"brother read the article, or even OPs summary",hardware,2025-12-05 18:02:02,7
AMD,nskjxaa,"AFAIK HDMI foundation doesn't want money but a ""closed driver"" and AMD doesn't want to make a ""closed driver"" like nv did.",hardware,2025-12-06 09:38:03,4
AMD,nsgz46k,"It varies from one individual card to another, but Realtek Protocol Converters were indeed used on Alchemist series models to provide (partial) HDMI 2.1 output. Depending on the specific type of color space, bit depth, refresh rate and display mode needed, [it got a bit complicated](https://community.intel.com/t5/Graphics/HDMI-2-1-UHD-144Hz-Arc-A750-A770/m-p/1452946). It's also part of why A- series cards are often a pain in the ass to get working with some TVs or older displays (the other part is poor EDID handshaking). No VESA VRR or Auto Low Latency Mode support, either.  More recent Battlemage cards, however, no longer use a PCON and support native HDMI 2.1a output, avoiding all of the above mess.  HDMI Forum does indeed still suck, regardless.",hardware,2025-12-05 19:01:55,79
AMD,nsgvoxq,"apple do the same, the display controllers are all DP display controllers and then if there is a HDMI port that is powered by active DP to HDMI chips on the motherboard.  Does lead to some compatibility issues that the vendor cant easily fix as the DP to HDMI converter tends to not be something they can flash new firmware onto.",hardware,2025-12-05 18:45:13,67
AMD,nshhbiv,"That was only for the A series and they did it because writing good drivers takes time and supporting HDMI on top of DisplayPort would make things more difficult for the driver team, which needed as much help as they could get for the launch of the first cards. The Intel B series cards have true HDMI ports and suffer the same problem as AMD on Linux with HDMI 2.1.",hardware,2025-12-05 20:36:09,29
AMD,nshmboq,"And Valve should have done the same with their hardware, I assume they have enough control over the final product to do so and also to verify that it works correctly, including vrr.",hardware,2025-12-05 21:02:24,11
AMD,nsl3vsq,"AMD had to do the same thing. They had open source drivers ready for full HDMI 2.1 support, but they could not release them because the HDMI forum sucks.",hardware,2025-12-06 12:47:10,4
AMD,nsh0nzl,"There's only one adapter that works reliably, the Cable Matters one. All other DP -> HDMI adapaters fail to pass through a 4K 120Hz HDR VRR 10 bit signal semi-reliably.  Other adapaters will be able to do the same specs, but not all at the same time. For instance, you'll have 4K 120Hz HDR, but VRR will not work. Or you'll have VRR but the HDR won't work. Or you'll have both, but the image will be 8 bit (HDMI 2.0 levels with 8 bit 4:2:0), causing colour issues. Or the image will straight up bug out/break/disconnect.  The Cable Matters is the only adapter the community has found which can do all the above somewhat reliably. The adapater is at its fucking limit, so sometimes it bugs out, needing to be unplugged/plugged in, or a restart; but in my experience I get 4K 120Hz HDR with VRR at 10bit most of the time. I use Bazzite on an LG C1 TV with an AMD 7900XT.  I would consider the experience absolutely usable and not a dealbreaker. It just works most of the time. I just have to unplug it and plug it back in, or restart my console PC under the TV a couple times a month.  Now we wouldn't need that adapter if the HDMI Forum allowed HDMI 2.1 on Linux without proprietary drivers (which AMD do not have). And unfortunately HDMI has a monopoly on TVs, so we're stuck with either HDMI 2.0 (which is open source on linux, but looks like shit with HDR and VRR enabled due to 8bit 4:2:0), or using a janky ass adapter to use HDMI's more reasonable competitor - Displayport.",hardware,2025-12-05 19:09:34,61
AMD,nsigz2c,"No, because the underlying spec is DisplayPort, and it's converting to HDMI signaling.  It doesn't add features that DisplayPort lacks, unfortunately - DP to HDMI is generally a one-way connection, so anything coming back over HDMI will be lost.",hardware,2025-12-05 23:55:58,24
AMD,nsh5r1b,"I'll let someone else answer that, because I've only used Arc cards on Linux servers for compute/Quicksync. I've never used the video output on them.  From what I understand about Arc (and what someone seems to have commented under me too), they do not have actual HDMI hardware, just a DP -> HDMI converter built into the card itself. This should bypass the issue entirely.",hardware,2025-12-05 19:35:23,4
AMD,nsjegnp,"ARC is between your TV and your receiver (HDMI to HDMI). Assuming you're using the adapter from your PC to your TV (Displayport to HDMI), you should be fine. The video card will still send audio.",hardware,2025-12-06 03:30:53,1
AMD,nsnumto,"TVs with DP support are incredibly rare, right?",hardware,2025-12-06 22:01:01,1
AMD,nsqqbo0,TVs don’t have DP,hardware,2025-12-07 10:31:24,1
AMD,nskc08v,"From reading forum posts, it's flaky. So it's a crapshoot regardless.",hardware,2025-12-06 08:16:21,2
AMD,nsqc8sw,"I have this adapter (CM 102101) and with my AMD 6900 XT it did not work. However with my new 5070 Ti it does (VRR really activly working, not just “on” in the driver) - while it works fine on my LG 42C2 it causes image errors every 5 seconds on my older 55C9, but in general working here too.  With the Adapter the LG 42C2 only shows ""VRR"" instead of ""Gsync"". And I activated ""Reference"" mode for colors in the new nvidia app, because HDR was a bit washed out at first.  (4K 120 HDR VRR FULL RGB) The Adapter maybe needs a firmware downgrade to .120 https://kb.cablematters.com/index.php?View=entry&EntryID=185  Some time ago they patched it out because it could cause problems.",hardware,2025-12-07 08:10:51,1
AMD,nsh2yk9,"The open source Nvidia driver ""works"" because it is modular. The part that the HDMI Forum refuses to open source is separate and proprietary.  Intel Arc skips the problem entirely afaik by having the DP -> HDMI converter on board the card itself. Intel Arc cards don't actually have HDMI, just a converted Displayport.  So you could use Nvidia on Linux. But then you'd have to deal with the plethora of Nvidia-specific issues on Linux: lower FPS than Windows, bad frame times, low 1% lows, bad Wayland support, no Gamescope support, etc... It's usable and mostly works if you don't look at the numbers. But when a 9070XT catches up to a 5090 in averages on Linux, and beats it in 1% lows, you know something's wrong.  About your second point, sure it's ideological in the sense that AMD decided a decade ago to make their Linux drivers open source, and part of the Linux Kernel itself. (AMD users on Linux do not update drivers, they just update the Kernel. Nvidia users have to install separate drivers like Windows). And they're paying for that (otherwise good decision) today.  But the problem in reality is practical, because it means that AMD would have to rewrite their entire driver architecture for Linux because one pesky organisation refuses to give an inch.  It's absurd to blame AMD driver design which has always been the best Linux GPU experience, when it's one organisation refusing to budge an inch and asking AMD to reinvent the wheel instead. The worst part is the HDMI 2.1 firmware isn't black magic. There's no trademark secret here that risks being exposed; let alone capabilities which Displayport doesn't already have. They're litterally just being asses about it.",hardware,2025-12-05 19:21:04,23
AMD,nsh53sf,"No it's NOT that straightforward. Nvidia and AMD have very different GPU architectures. Nvidia has the GSP which offloads a lot of hardware management from the OS, including apparently HDMI-related functionality. The GSP is essentially an entire sub-OS that runs on the GPU that is launched via firmware provided by the OS. This has several benefits such as being separated from the OS so that they can have the OS-specific driver side be simpler with a shared GSP firmware. It also can help reduce load on the system CPU since many tasks are being handled directly on the GPU instead of needing to run on the CPU. And also what is probably the most important reason to Nvidia that it allows them to ""lock"" features in a way where someone can't hack the driver to unlock them (their consumer GPUs basically have all the same hardware as their enterprise ones but the consumer ones have severe limits because they want companies to buy their more expensive ones). The con of it is that it makes the GPUs slightly more expensive (pennies compared to what they get by forcing companies to buy enterprise GPUs instead) and that the firmware blobs are very large because they're essentially an entire minimal OS image.  By contrast the AMD GPUs have much simpler firmware components and delegate much of what the GSP does to the driver. This is cheaper for them but it means that they have to write the functionality multiple times for the OSs they support and they have no real mechanism to prevent people from buying their cheaper consumer GPUs instead of their more expensive ones (besides the latter having more memory and being under a support contract).",hardware,2025-12-05 19:32:01,14
AMD,nsh22jg,"Not quite sure of the full details, but essentially it boils down to NVIDIA doing things differently.  They have a closed source firmware blob that handles most low level card functionality unlike AMD and Intel, and thus they can offer HDMI 2.1 functionality by not having that functionality as part of the driver itself, but rather handled by the firmware.",hardware,2025-12-05 19:16:35,3
AMD,nshse49,It's also because TV manufacturers make money from HDMI licensing,hardware,2025-12-05 21:34:19,-12
AMD,nsypcuc,No idea what you are talking about. It was super common in the early days of LCD TVs.,hardware,2025-12-08 17:03:34,2
AMD,nsji8od,Bastards,hardware,2025-12-06 03:57:20,39
AMD,nshcw7c,"Yeah, this is the crux of the issue  `amdgpu` is fully open-source. The HDMI forum refuses to allow AMD to put support there because of their approach to their ""intellectual property"" of how HDMI 2.1 works.  Theoretically, a binary-only module could include support, but that's not a good approach either  If one were to make hardware-specific (GabeCube/SteamDeck only) support in software, it would still expose the implementation details, and would be trivial to bypass.  As I understand it, Intel ARC has HDMI2.1 in Linux by implementing it in hardware, so if anything, Valve could maybe take that approach with a built-in DP->HDMI converter for instance.",hardware,2025-12-05 20:12:40,65
AMD,nsijmvn,Most consoles get custom drivers,hardware,2025-12-06 00:12:29,-5
AMD,nsk2lji,This is why open standards matter. I've been going out of my way to make sure I have Display Port in all my displays.,hardware,2025-12-06 06:44:42,9
AMD,nskug20,was MS and Nintendo asking to open 2.1?,hardware,2025-12-06 11:23:54,2
AMD,nsitsw2,Hisense also has its linux os.,hardware,2025-12-06 01:15:24,2
AMD,nt4qgm1,"Because if you want to directly connect to a TV, it’s the only game in town, outside of buying an unreliable DP to HDMI adapter.",hardware,2025-12-09 16:05:07,1
AMD,nsiduys,Not insanity. Dickhead behaviour.,hardware,2025-12-05 23:36:40,47
AMD,nsmiyro,"I totally agree, but wish GPUs would have mini-DP and at least one USB-C with DP-Alt.  They take less space so allow more exhaust vent.  Also, it will be easier to get TVs manufacturers to adopt USB-C (which is doing DP) because they cater to the average consumer.",hardware,2025-12-06 17:45:42,3
AMD,nsn5ajj,Would honestly be open to GPU makers dropping HDMI altogether. Pulling an apple on us as a forcing function to switch off hdmi. Maybe TVs and consoles would catch up  In an ideal world this could all be tucked into a USB-C cable too,hardware,2025-12-06 19:40:25,2
AMD,nsi0gsr,"You left off the first part of the quote, ""my understanding is..."" And this misrepresented my comment. That aside, thank you for the rest of the information regarding pricing.",hardware,2025-12-05 22:17:58,7
AMD,nsjlz0j,"I've had zero issues with the Cable Matters displayport cables, but you do have to buy the right version for your use case. Also there's no long distance displayport 2.0 unless you use fiber, I think?",hardware,2025-12-06 04:25:00,2
AMD,nsh72hq,We should have some sort of international agreements in place for developing open protocol standards for hardware and software.,hardware,2025-12-05 19:42:13,5
AMD,nsjlxrt,Then just reject hdmi 2.1 and use DP for modern features instead.  There isn't that much hdmi 2.1,hardware,2025-12-06 04:24:44,5
AMD,nszsg4e,"And horrible licensing requirements, extreme cable length limitations, etc",hardware,2025-12-08 20:15:29,1
AMD,nst7lft,GPMI supports USB-C but it also has it own connector for maximum capabilities,hardware,2025-12-07 19:30:49,1
AMD,nsj4k2l,And many TVs use eARC to get the audio from their tv smart apps and streaming boxes to their speakers.   And if not you’d use the pass throughs on your amp which are hdmi because it has audio.   DisplayPort just doesn’t do the things needed for home theatre use.,hardware,2025-12-06 02:25:37,8
AMD,nsgsf02,Damn I do believe I use eARC or maybe it was CEC and I use optical for audio. It'd be nice to have HDMI and eARC then,hardware,2025-12-05 18:29:08,5
AMD,nsgvlo8,You don’t need eARC on the HDMI if you have a dedicated optical cable going from your TV to your AVR.,hardware,2025-12-05 18:44:46,-3
AMD,nsgvyc7,Display port over TB does.  you have lots of extra bandwidth options here.,hardware,2025-12-05 18:46:30,-1
AMD,nsjmpjp,eARC really sucks.  So does CEC.,hardware,2025-12-06 04:30:33,-2
AMD,nshqqze,"Use both at the same time lol, the HDMI 2.0 for CEC and audio, and use the DP for the video feed",hardware,2025-12-05 21:25:35,1
AMD,nsgpog0,"Damn. That is a problem. Can they stick the cheapest brand of N100 mini-PC's into a 55-75"" television and make a SteamOS TV",hardware,2025-12-05 18:16:03,6
AMD,nskf8gr,"We have had very few changes in the industry in recent decades, even for cables.    Having an optical base standard wouldn’t change anything from this point. You would still be forced to upgrade if you wanted the latest feature. Sure the cable could still be the same, but it’s always the least expensive item inside a home theatre setup.",hardware,2025-12-06 08:49:32,1
AMD,nskpi4m,> Steam machine does have displayport 2.1 though.  [Specs say DisplayPort 1.4](https://store.steampowered.com/sale/steammachine).  Which seems weird to me because RDNA3 should do DP 2.1,hardware,2025-12-06 10:35:18,1
AMD,nspygjd,"I don't think you understand - I know the Steam devices do, but the TV's I want to connect them to don't. There are almost no TVs at all with DP. Only PC monitors have DP inputs.",hardware,2025-12-07 06:03:35,1
AMD,nsxceal,You listed it backwards.,hardware,2025-12-08 12:30:11,1
AMD,nsh115y,Ehh it all comes down yo having more money one way or the other,hardware,2025-12-05 19:11:25,1
AMD,nsjlf2s,"when it comes to  VRR to HDMI though DP, literally every company has issues with it.",hardware,2025-12-06 04:20:48,7
AMD,nsm5440,"I mean...the use-case is a bit different.  I don't think the average person is connecting an Intel Arc GPU to a TV and the average person buying one, is probably more willing to troubleshoot or find solutions. But the average person, is at least *expected* to have the option to use a Steam Machine as a Home Theater PC, with little resistance.   If you're having every day normal, mainstream customers troubleshoot why HDMI isn't working on their Steam Machine, then you've lost the battle before it even started. Those customers aren't going to turn to Google to figure out why its not working. They are just going to ask for a refund and stick with conventional consoles.",hardware,2025-12-06 16:31:37,1
AMD,nshj725,"Hmm. What's the 'limit' related to exactly? Heat? EMF? It's an active adapter, yeah?",hardware,2025-12-05 20:46:08,6
AMD,nshwrcr,Can you provide a link to the adapter?,hardware,2025-12-05 21:57:38,4
AMD,nsiiydv,"Aw dam that sucks but thanks! Had to use the only hdmi port on my gpu for better hdr on my monitor, guess I'll have to play hot potato when I want to use my tv.",hardware,2025-12-06 00:08:18,3
AMD,nsh6a1u,Sorry my fault I meant eArc not Intel arc cards :p.  My sound system uses eArc so when I connect my tv to my gpu via hdmi it passes the sound to my soundbar/sub,hardware,2025-12-05 19:38:06,9
AMD,nsjm9yb,"Yup from gpu to TV and the sound bar is hooked up to the arc hdmi on the TV. Also keep forgetting my soundbar is arc, not eArc.",hardware,2025-12-06 04:27:16,1
AMD,nsxaw0n,being able to advertise compatibility with a sticker might improve chances of them existing.,hardware,2025-12-08 12:18:35,1
AMD,nsxax5l,"Some do, its just quite rare for TVs.",hardware,2025-12-08 12:18:50,1
AMD,nshj2bs,"It's not really about the driver itself being modular with Nvidia, but the fact that the kernel driver doesn't actually do any low level management of the card at all. Instead, the card has a processor on it (called the GSP) which manages all the functionality of the card. That's why the firmware is like 100MB; it's basically an entire driver. The OS kernel driver just tells the GSP what it wants to do and the GSP takes care of the implementation details. AMD and Intel can't do the same thing with their cards and it would require a complete rework of how the cards/GPUs are designed. It's also why Nvidia's Linux kernel driver is now open-source; any trade secrets and implementation details about their cards are part of the firmware, which is closed source.",hardware,2025-12-05 20:45:28,6
AMD,nsiqhe3,">It also can help reduce load on the system CPU since many tasks are being handled directly on the GPU instead of needing to run on the CPU.  I'm not terribly knowledgeable on the subject, but it does appear as though this statement flies in the face of all the disparity between AMD and NV GPUs running (in Windows) on low perf CPUs. Hardware Unboxed did several videos showing that overall driver CPU overhead was notable higher on NV.",hardware,2025-12-06 00:54:20,2
AMD,nsh3f7w,"Which is why I'm not sure framing this entirely as an issue with the HDMI forum is accurate.   Yes one might not like the stance from an ideological perspective. But it does seem technically speaking it's possible to implement HDMI 2.1 on Linux. There is no hard restriction.  So if AMD or Valve chose not to implement it just based on that, should the consumer who might not share such rigid ideological stances also not pressure AMD or Valve to take a more pragmatic approach?",hardware,2025-12-05 19:23:24,-9
AMD,nski5vw,Afaik DP does support a functional equivalent to audio return channel. Not sure if anything supports it tho.,hardware,2025-12-06 09:19:42,13
AMD,nsjz37a,What's the background of that? Is sound more like a usb device on displayport? Somehow I've never had a problem the last 15 years playing audio over displayport?,hardware,2025-12-06 06:11:52,4
AMD,nt33s83,"Well nobody is saying ""ONLY support DP"". You only need a single HDMI port with eARC, rest could be DP.",hardware,2025-12-09 09:21:32,1
AMD,nsih980,"Brother, TV manufacturers lose money from HDMI licensing lol, they have to pay for that shit, cable manufacturers too.     With that said, despite display port being free it's not actually cheaper than a roughly equivalent HDMI cable, often it's more expensive.",hardware,2025-12-05 23:57:40,25
AMD,nsov8e5,"> Valve could maybe take that approach with a built-in DP->HDMI converter for instance.  Only problem is I’m pretty sure the Steam Machine would be hardware final already, so the only chance Valve has is convincing the HDMI Forum to allow support in the open source driver, which AMD already tried to do and failed.",hardware,2025-12-07 01:38:31,3
AMD,nsxb8h2,"AMD has no trouble using binary blobs elsewhere, so why not here?",hardware,2025-12-08 12:21:17,1
AMD,nsuphnn,"OTOH, the refusal to compromise is on the open source projects. It's their own choice to say no to a binary module, just as it is HDMI author's choice to allow putting their IP in open-source code, or to choose to demand it being provided in binary form only.  Since the IP of HDMI is likely interconnected with copy protection technologies, if they made open saucers happy (for a moment at least, until the next outrage), they would piss off content providers.",hardware,2025-12-08 00:10:30,-2
AMD,nsiyezg,Most consoles are not built with GPL software. Custom Linux driver will either be limited to non-GPL API or it will be required to be open source.,hardware,2025-12-06 01:45:40,13
AMD,nsjlw77,"well, its becasue they don't wangt HDMI to be open source, and by nature a linux implememntation will bascially be open source.",hardware,2025-12-06 04:24:24,4
AMD,nsuq7se,"No, it's probably technical. HDMI involves DRM to protect stuff like Netflix streaming content from being copied too easily. Open sourcing would potentially compromise the DRM bits, so the DRM players don't want it, and hence HDMI Forum wants the implementations to be binary only/obfuscated, to satisfy the needs of those users.",hardware,2025-12-08 00:14:32,1
AMD,nsjo756,"The issues aren't really as visible on a smaller scale. Displayport was my preferred format until I had to deal with it on a large scale.   Perhaps a quarter of the PCs at my job have displayport as their only video output. Those DP cables make up over 90% of the display cables we have to replace. When we had significant amounts of VGA / DVI those basically never died even when they got beat up badly by the users. HDMI isn't quite on that level, but they mostly only die from users smooshing their PCs against the wall and bending the hell out of the cable ends. I replace a dead HDMI cable maybe once every couple months due to this.  Our DisplayPort cables never get beat up due to the PCs associated with them being under counters in a spot where users have no reason to move them around, but despite this they die like crazy. We replace maybe three or four a month. We've tried a variety of brands including Cable Matters, because we'd really like to stop dealing with this. No dice.",hardware,2025-12-06 04:42:00,4
AMD,nsmjuan,I really think all DP and HDMI cables (and ports for that matter) should list the version on them.,hardware,2025-12-06 17:50:12,1
AMD,nshidpw,If only we had an International Organization for Standardization,hardware,2025-12-05 20:41:50,15
AMD,nsjnhaq,DP has no replacement for eARC,hardware,2025-12-06 04:36:29,26
AMD,nsjwqnb,HDMI-CEC,hardware,2025-12-06 05:51:03,16
AMD,nsxc3xs,"if you are transfering video and audio through external device to the TV, TV does not need to run its own audio.",hardware,2025-12-08 12:27:58,1
AMD,nsgu8cq,eArc is the audio return channel. Why would you need both optical and eArc at the same time?,hardware,2025-12-05 18:38:03,5
AMD,nsgygph,"Optical is also extremely limited on audio capability/quality. So it’s a dead technology to any of us with 7.1, much less Atmos",hardware,2025-12-05 18:58:45,24
AMD,nsh370v,Optical cable is a dead standard at this point,hardware,2025-12-05 19:22:15,16
AMD,nsgweyt,"But then as a side effect you lose the volume control via HDMI CEC, so with optical you're forced to use the AVR remote for just the volume control and TV remote for everything else.",hardware,2025-12-05 18:48:46,15
AMD,nsgxpvz,The bandwidth alone doesn't help if there is no standard around it. Or is there something?,hardware,2025-12-05 18:55:07,8
AMD,nsh3b44,You need a common standard such eARC,hardware,2025-12-05 19:22:49,4
AMD,nsk75gr,How does eARC suck?,hardware,2025-12-06 07:28:18,1
AMD,nshr34w,hence my comment about the TV needing both. His comment said to avoid HDMI entirely.   A TV with a handful of both ports would be excellent. A TV without any HDMI will be returned heavily,hardware,2025-12-05 21:27:23,4
AMD,nskent8,We are talking TVs not a pc,hardware,2025-12-06 08:43:40,2
AMD,nsn795c,"Yes, devices on either end would need to be upgraded over time but we would have way, way less cable waste was more my point. How many HDMI cables do people have laying around from needing to upgrade to higher bandwidth cables, or worse threw them away and they're in landfill somewhere.",hardware,2025-12-06 19:50:54,2
AMD,nsh1btf,If only the Notorious B.I.G. had been correct…,hardware,2025-12-05 19:12:54,1
AMD,nsmb80n,"Fully agree, and that's why Valve needs to get it right, by making sure their converter chip works reliable with their Gpu in their Steam machine.",hardware,2025-12-06 17:03:49,2
AMD,nsjbem4,"Probably signal integrity because of the sheer amount of data, 4k 120 is like 48 gigabits per second. Anything slightly off in the timing, and the signal drops",hardware,2025-12-06 03:10:09,20
AMD,nsqecso,"Its the Cable Matters 102101 - can confirm that it can really do VRR, depending on your GPU / Display. On my 4080 Laptop (via USB-C to full size DP) and  6900 XT it did not work, on my 5070 Ti it works with LG 42C2 and 55C9. Unfortunately on the 55C9 it causes major image glitches every 5 seconds. The 42C2 is fine.  (4K 120 HDR VRR FULL RGB)  The Adapter maybe needs a firmware downgrade to .120  [Cable Matters Firmware Update Tool \[Enable VRR on Windows OS\] - Cable Matters Knowledge Base](https://kb.cablematters.com/index.php?View=entry&EntryID=185)  Some time ago they patched it out because it could cause problems.",hardware,2025-12-07 08:31:50,3
AMD,nsj8coc,"Unfortunately, yes, if you can't use DisplayPort from your PC to your monitor.",hardware,2025-12-06 02:50:02,3
AMD,nshc2yg,"Ah I see! Honestly no clue since my sound system still uses Toslink (optical)! So for me the audio output is baked into the video feed with HDMI from the PC, and the TV just outputs that over Toslink.  Although I'm interested in the answer too, since I wanted to upgrade to an eARC setup one day.",hardware,2025-12-05 20:08:22,4
AMD,nsjxbr8,"Yeah, adapter should work fine. 👍",hardware,2025-12-06 05:56:04,1
AMD,nsjlniy,"Just because the Nvidia driver is offloading more tasks to the GPU it does not mean that it uses less CPU than the AMD GPU. It just means that the Nvidia driver would be using more CPU compared to not doing so, but at the end of the day the hardware is different and the drivers are completely different implementations. Perhaps the AMD driver is in fact more efficiently written than the Nvidia one or perhaps the Nvidia driver is doing something else that the AMD driver isn't.   Without actually profiling what the driver is doing while it's scheduled on the CPU you'd have no real way to know the reason and even if you had profiles of both the AMD and Nvidia drivers they'd be so different in terms of codepaths that it would be almost impossible to do a direct comparison.",hardware,2025-12-06 04:22:35,1
AMD,nsh4t7u,"It’s not a choice between ideology or pragmatism, rather simple technological constraints.  Fundamentally, AMD and Intel GPUs can’t use this approach, since NVIDIA has a CPU on their GPU since Turing allowing for them to create this more powerful firmware.  AMD and Intel don’t, so they can’t do anything remotely to this level.     They could always make a closed source driver of course, but this would mean essentially maintaining a completely separate driver module out of the kernel, which would mean spending resources to ensure constant compatibility.",hardware,2025-12-05 19:30:29,13
AMD,nskjjt0,"Audio via HDMI supports return channels, DP doesn't. HDMI also simply has a lot more investment going into it",hardware,2025-12-06 09:34:04,10
AMD,nstojq1,"Who are they paying though? Like seriously, I've never actually thought about this and just assumed they just paid. And while they do as HDMI does have a licensing cost, where does that money actually go? All the major manufacturers are members of HDMI Forum.",hardware,2025-12-07 20:54:27,1
AMD,nsp9952,"Right, but that wouldn't preclude them from having possibly included a hardware converter like Intel did on ARC. They don't have to convince the hdmi forum if they ship a hardware implementation with the GPU just outputting DP signals.  Of course, all speculation until the final hardware is out",hardware,2025-12-07 03:07:27,1
AMD,nsuw7ql,"Generally speaking, no you can't put binary modules in kernel modules even if you want to, because they effectively become GPLv2 themselves when linked into the kernel.  Nvidia does some questionable things with their binary module to try to get around this by calling out to the binary from the kernel (which I've heard referred to as a ""legal prophylactic"" lol), but even this is questionable on whether it follows the licence text. Mainly, that hasn't been tested in court, and may or may not stand up to lawyering.  Firmware is a different story, since the kernel would generally be loading the firmware into the device to make it work, rather than that code being linked against the kernel. So, a future design from AMD could place the HDMI handling in the firmware module and work fine on Linux.  Copy protection, and security through obscurity are likely factors in why the HDMI forum is so shitty about this, but I think the bigger thing is the licensing revenue. If an open implementation exists, then it could be copied into an uncertified device, and the HDMI forum would have to buy one fewer exotic car or something.  As a counterpoint, DisplayPort does HDCP (copy protection) just fine without this draconian licensing issue, and is equivalent (or better than) hdmi 2.1 in features. The only drawback to it is lack of support from TV manufacturers.",hardware,2025-12-08 00:48:17,2
AMD,nsxbhq2,Linux kernels are full of binary blob implementations for things that cannot be open sourced.,hardware,2025-12-08 12:23:15,1
AMD,nt07qu8,"To my knowledge, the only ""DRM bits"" in HDMI would be HDCP - which isn't specific to HDMI 2.1 (or even to HDMI in general), though.",hardware,2025-12-08 21:31:04,1
AMD,nsxbrk9,ive never seen a DP cable die unless it was physically damaged by the user. Same goes for all other cables except USB to be honest. Didnt consider this to be such a common issue.,hardware,2025-12-08 12:25:21,1
AMD,nsjqzdm,I wonder what's going on there lol.,hardware,2025-12-06 05:03:32,1
AMD,nsji7q6,ah yes... iOS.,hardware,2025-12-06 03:57:08,7
AMD,nsxbyyz,how commonly do you need a return channel for audio though?,hardware,2025-12-08 12:26:55,1
AMD,nsy5cuu,"No but none of those devices have displayport. No streaming box or AVR that I am aware of does anyways.    It would have to be some kind of setup where the streaming box connects to the AVR with HDMI, then your AVR outputs to the TV with displayport?",hardware,2025-12-08 15:24:43,1
AMD,nsgwzl0,I don't. I used to use eARC but switched to optical for my cheap class D amp. I used to use eARC. Memories flooding in. I'll probably need eARC again in the future when more class D amps have eARC ports on them and I upgrade,hardware,2025-12-05 18:51:31,1
AMD,nsjmyo9,Unless your tv doesn't happen to support pass through of the codec you want because absolutely everything that touches the stream must be licensed for that specific codec.,hardware,2025-12-06 04:32:29,1
AMD,nsjmigu,Ugh why didn't they just come up with a dedicated audio connection instead.,hardware,2025-12-06 04:29:03,1
AMD,nsk4258,there are multiple ways to expose an audio device over TB.  It is completely possible for a TB display to also act as a bridge so that it forwards all other attached TB device to the host (video source) meaning from the video source you can then select what audio output as it would show up just the same as if you attached that audio output directly to it.,hardware,2025-12-06 06:58:26,1
AMD,nsk47qq,USB and PCIe are both standards that are channeled through TB.   A thunderbolt display can act as a TB/USB multiplexer so it exposes attached USB/PCIe devices to the upstream video source.   Thus letting any audio device attached to it be directly addressable from the video source.,hardware,2025-12-06 06:59:55,1
AMD,nskeg6e,In no way.,hardware,2025-12-06 08:41:29,1
AMD,nsnfqjf,">Yes, devices on either end would need to be upgraded over time but we would have way, way less cable waste was more my point   But you can still use older cables, even with full-bandwidth HDMI 2.1 devices. It's not recommended, but nobody will stop you. If it doesn't work, an Ultra High Speed certified cable is only about $10/€10. That's not much when the AV receiver or soundbar alone costs one or more orders of magnitude more.   >How many HDMI cables do people have laying around from needing to upgrade to higher bandwidth cables, or worse threw them away and they're in landfill somewhere.  They can still use them.",hardware,2025-12-06 20:38:01,0
AMD,nsmc5g2,"Well, if it were easy to make a converter chip that works reliably, then we wouldn't be having this conversation.   Its not like Intel reached this crossroad and was like 'lol lets make it unreliable hehehe'",hardware,2025-12-06 17:08:51,3
AMD,nsjrqdm,"Right, but signal integrity is affected by things like conductivity (heat) and interference (EMF). If it's a signal processing chip bw limitation, I'm surprised some premium cable company hasn't just put a more powerful chip in. In fact I'm surprised a premium cable company hasn't made a short-distance super thick monstrously overkill adapter/cable for this purpose.",hardware,2025-12-06 05:09:36,6
AMD,nstw25g,"It goes to the HDMI forum, being a member of the forum means you get a say in the spec and features of HDMI, you still have to pay licensing fees to use the port and to use certain branding words.",hardware,2025-12-07 21:31:26,2
AMD,nspbhf4,"If they have done that then this wouldn’t be an issue because they would just use the hardware converter, but seeing as it is an issue, then they don’t have a hardware converter in there, the older Intel Arc GPUs don’t have both a straight HDMI connection from the GPU and a converted HDMI, the ones that had converted HDMI *only* had converted HDMI.",hardware,2025-12-07 03:21:58,1
AMD,nsv9ftj,"Well but that is the fault of the licence the open source project has picked. It's an issue purely caused by the FOSS project themselves, the GPL is actually meant precisely to not allow such things. They ruled themselves out by their choice. It's kinda their responsibility to deal with it whichever way they want - to convince others to comply, to find some Nvidia-like workaround, or to not have the feature if they don't want to.  HDMI creators don't have to comply, open source projects aren't magically entitled to have everybody go out of their way to serve their needs.  WRT DisplayPort, it's complicated. In theory it looks better just for the reason it is free so devices could be cheaper/have more ports etc. But for example DP 2.x was years late, so HDMI 2.1 kind of earned its place as the high-bandwidht solution for over 4K60 resolutions. There are the audio and synchronization things. AFAIK HDMI is better at working over longer cables, which DP 2.x has big issues with (the 80Gbps mode). Sadly we can expect HDMI 2.2 with 96Gbps bandwidth to again come to market much sooner than the DisplayPort alternative (though arguably, 96Gbps is not that much higher than 80Gbps, so as long as DP 2.1 UHBR20 actually becomes commonplace, maybe the alternative is here already).  So you can't really say the HDMI does nothing to justify its premium.  I also hear repeat complains about DisplayPort monitors having annoying quirks when going to sleep (icons/windows rearranged iirc). No idea if the prevalence of such problems is really bigger than on HDMI, of course. I don't quite remember having issues with HDMI myself.",hardware,2025-12-08 02:08:27,-2
AMD,nsxg5ij,It's basically how every mainstream TV soundbar setup functions these days,hardware,2025-12-08 12:57:29,6
AMD,nsyapnr,we are working in fictional scenario where displayport is an option here. But also i guess it depends on your setup? Everyone i know just connects their devices directly to TV rather than any AVR.  Most people have simple setups and have 1 or 2 devices connected max. Even cable TV now is an app inside the TV rather than a seperate box.,hardware,2025-12-08 15:51:50,1
AMD,nskecey,"Display port have to walk with its own leg, Thunderbolt (or USB 4) won’t be used on TVs. You have to be able to to the exact thing that you do with eARC.",hardware,2025-12-06 08:40:24,2
AMD,nso2bqj,"I was talking about the waste more than the cost (but that too). And when you get up to HDR Vision with lossless Atmos audio, a lot of times the older HDMI cables do not hold up. I have dozens in my closet that can't handle that bandwidth, nor the bandwidth of modern PC displays with high refresh rates and ultra wide resolutions. PCs would have also benefit from the optical cable solution.",hardware,2025-12-06 22:44:33,3
AMD,nsk5jmz,"It'd be nice, but it would require a more powerful chip to exist - it's a pretty specialized thing, and you'd have a make back all of the money on designing and fabricating the chips (plus chip manufacturers are pretty slammed these days) and the only real use case I know of for these is for connecting TV to older GPUs or multiple TVs to GPUs with a single HDMI port.   I'd also have to see if you could draw enough power off of the power pin on the DP port to power anything significantly better  I don't really know, just throwing out some ideas of why they might not be gunning to do that right now",hardware,2025-12-06 07:12:46,4
AMD,nspdywn,"My point is we don't know yet, but we DO know they didn't do that on the Steam Deck.  It remains an issue for HDMI on Linux and AMD hardware either way, even if they do end up building in a workaround.  I hold more hope for that idea than Valve/AMD/anyone convincing the HDMI forum to stop being asshats about it though",hardware,2025-12-07 03:38:39,1
AMD,nsvep9f,"Sure, the HDMI forum has that right and I have a right to call them assholes for it. They'll still be getting their licensing fees on the hardware either way, and it's doubtful HDCP would be any less secure because of an open implementation. Previous implementations have never been an issue in that regard.  There's also practical reasons why a closed source module is a bad idea. It becomes a debugging black box if there's ever an issue caused by that code.  I think the main thing with stuff rearranging on sleep is from the monitor appearing to be disconnected. I've seen the same thing happen with HDMI, but as with all things, your mileage may vary",hardware,2025-12-08 02:39:55,1
AMD,nsxzpz3,"Ah, i never used a soundbar or know anyone who does.",hardware,2025-12-08 14:55:13,1
AMD,nt1s5in,"A lot of the ""just use DP, not sure why you'd need HDMI"" comments on this subject feel like they're coming from people who just watch everything on their computer. The concept of a living room with a TV and attached soundbar is foreign to them. Not judging but it's definitely something I've noticed, you see it all the time on the various linux subs too.",hardware,2025-12-09 02:50:55,1
AMD,nsyyp1n,Ya true I’m just saying that ya it would work for the one situation where you’d be doing everything through the AVR and then ya it uses DisplayPort without sound.   But many may use eARC so since HDMI works with sound etc I don’t see it going anywhere unless they implement something similar.,hardware,2025-12-08 17:49:39,1
AMD,nsknfbu,if you were to get rid of HDMI then why would one not replace that with a simple TB/USB4 connection.,hardware,2025-12-06 10:14:08,1
AMD,nsodzr1,">And when you get up to HDR Vision with lossless Atmos audio, a lot of times the older HDMI cables do not hold up.  Nothing is stopping you to use them with HDR, Dolby vision,  lossless Atmos or what ever.  >I have dozens in my closet that can't handle that bandwidth, nor the bandwidth of modern PC displays with high refresh rates and ultra wide resolutions.  because they were poorly manufactured in the first place and they can't handle high enough bandwidth. I you buy good cable today they will still be good cables even tomorrow.  >PCs would have also benefit from the optical cable solution  Optical cable solution is even worst in the PC space because nowadays you expect 1 cable that carry everything.",hardware,2025-12-06 23:53:11,-1
AMD,nsw17el,"Ironically, Steam Machine doesn't actually offer DisplayPort 2.1 either, I just noticed.  The GPU used in the box could do it (40Gbps UHBR10 mode only), but Valve only exposes DisplayPort 1.4.",hardware,2025-12-08 05:07:52,1
AMD,nsym8ed,sound bars the the most popular ways of getting better audio out of TVs.  And relatively cheep/simple compared to a full home audio set up.,hardware,2025-12-08 16:48:16,2
AMD,nskolru,"Because a simple TB/USB4 needs the support from the SOC maker and it would be a more expensive solution than just a display port input.  Display port should be able to walk with its legs, in order to star a transition (not a replacement because you still need HDMI support for a long time) it has to have the same feature as HDMI. It must have an eARC replacement, CEC etc.  HDMI is used because it has everything a TV needs and it's licensing cost are negligible.",hardware,2025-12-06 10:26:07,1
AMD,nsofnto,"What are you even talking about? You can't use the vast majority of the older cables with the latest bandwidth requirements. You end up with signal dropouts. And no I did not cheap out. It's just the reality that if you're taking years ago, the cables were only made to the specs required at the time.. It's like having a cable now that supports 48Gbps and in 10 more years you need a cable for 16K video or something (theoretical, obviously) and the best of the best cables of today probably wouldn't support 192Gbps or whatever the new bandwidth requirement is. We reached a point where the quality of the HDMI cable absolutely does matter now where years ago it did not because of far less bandwidth requirements. This is my point of repeated obsoletion because the cables are only made to be just enough for whatever the current best quality is.",hardware,2025-12-07 00:03:01,2
AMD,nsw22nh,"DP 1.4a can do up to 8k@60, 4k@120, or 1440p@144 with HDR and VRR, so it's definitely in the ""good enough"" camp for most, unlike the rather gimped HDMI2.0 fallback modes.",hardware,2025-12-08 05:14:38,1
AMD,nskpab3,it requires you put in a USB-4 Doc within the TV.   You would not directly attach the TV SOC to the USB4 as those chips do not support the needed dock like features that would let the TV pass through other devices as audio targets.  why would display port add eArc when the entier point of modern display port is that can be tunnelled within USB so can use that ecosystem and protools for device handshakes.,hardware,2025-12-06 10:33:06,1
AMD,nswaoi4,"DP 1.4 really gets saved by the DSC compression these days though, too bad it was not retrofitted to HDMI 2.0 modes.",hardware,2025-12-08 06:27:35,1
AMD,nskwva9,"""it requires you put in a USB-4 Doc within the TV""  It's frankly a very Frankenstein solution for something that is not needed. You have already a SOC that handle input/output of the TV.  >why would display port add eArc   Because that's the current workflow in the audio TV market. You connect your device to the TV and then you use the eARC port to connect the AV receiver, everything is handled by the TV.  If you want to make a monitor just make a monitor.",hardware,2025-12-06 11:46:40,1
AMD,nsa0t02,"I used to be a call center tech for them 13 years ago, based in the US. The writing has been on the wall for Micron closing Crucial for over a decade now. Even then, we had the perception that Crucial was like 1% of their business and an afterthought.  They were also the owner of SD Card manufacturer Lexar, before they sold it to a company in China maybe 8-10 years ago. Same thought process, it was just even smaller of a piece of their business.  Then maybe 6 years ago they moved Crucial's customer service out of the US, so more fat trimming.  Crucial finally closing is just a business efficiency decision, and they can focus more on B2B so that other companies can sell consumer products.",hardware,2025-12-04 17:24:46,213
AMD,ns9s2nx,Perfect time for the Chinese to move right in.  In no time the Triplopoly will be broken.,hardware,2025-12-04 16:42:20,79
AMD,nsd1qw7,My company booked a large % of microns 2026 production. I’m sorry gamers.,hardware,2025-12-05 03:07:40,14
AMD,ns9lwuu,">This coupled with the recent shortages of RAM for consumers and subsequent rise in their prices has got me worried. If this trend continues and AI race actually takes off, where does that leave normal PC enthusiasts / DIY culture that started in 1980's. We can't assemble computers without RAM, SSDs or GPUs.  DIY PC pays by far the least for silicon of any market, so it eats last. You can expect DIY PC builds to not really be sensible for many quarters.   Regrettably, we are in the very early innings on this trend. Things will get a lot worse before they get better. The first sign that things are normalizing will be sensible pricing on OEM PC builds. They are able to make volume contracts than retail can't support.  >Plus, the recent thrust by both Intel and AMD to go for APU / integrated architecture makes me believe that the industry is pushing consumers towards locked hardware that cannot be customized, and we all would eventually be forced to use NUCs or laptops that come with soldered RAM and CPU or even worse, integrated SOC with GPU.  APUs make a lot more sense than low-end dGPU products. Making a dGPU requires a bunch of infrastructure components that add cost and size to the design. Those things make sense if you are building something big and powerful. If you are instead making a mainstream 1080p graphics solution, an integrated graphics solution with on-package memory is cheaper and better.  >Would love to know your views on how this thing will eventually play out. Do you think that this AI bubble will eventually pop bringing normalcy or can this bring out seismic shift in how we see computers?  ""The cure for high prices is high prices"". The memory market will eventually see either a new entrant or major new capacity coming online. However, a fab that broke ground today would have a hard time ramping to sufficient volume to change pricing before 2028. The existing players have a long track record of cartel behavior.  I don't expect a seismic shift in how we see computers. I expect it to be like crypto but bigger.",hardware,2025-12-04 16:12:27,47
AMD,nsax7zi,"Things are bad, and we are entering a period of likely further global turmoil.  Companies all want a slice of the AI pie, even if it dooms us all.  I joked somewhere else that if someone could make a bio-reactor, Kellog's and Nestle would probably start making food to feed them, so we could spin up more AI compute hardware.  When the AI bubble bursts, just remember which companies left us hanging.",hardware,2025-12-04 20:02:42,23
AMD,ns9mrrn,Other dont want risk to expand their production in case AI bubble pop,hardware,2025-12-04 16:16:35,6
AMD,nsa4daz,"I don't think so. We are just seeing a shift because of the AI. I think developers will have to start actually optimizing games now because more and more people entering PC gaming will be on ""budget builds"" aka whatever they can get their hands on",hardware,2025-12-04 17:42:18,4
AMD,nsa7rpn,"This whole thing seems really overblown. Micron are shuttering Crucial, but will continue to supply 3rd-party OEMs that produce memory for consumers.  To be blunt, big whoop? The impression I've always gotten was Crucial largely serviced the small subset of the DIY market that built low-spec machines. The PC you built for your uncle who edits photos and needed something a little more specific that a generic Dell shitbox, and used purely JEDEC spec RAM.  Plus, are Micron's DRAM offerings particularly appealing to this market in the first place? For DDR4/5 Hynix and Samsung rule the roost.",hardware,2025-12-04 17:58:33,14
AMD,nsc1zhh,"Micron is still producing RAM ICs and consumer-facing companies will buy them.  Micron simply doesn't want to deal with the logistics of package production, PCB production, and whatever meager advertising budget that's left in the Crucial brand. Crucial hasn't been a name people put in high regard when it comes to enthusiast consumer RAM. Far more buzz about G.Skill, Corsair (yes I know it's a binning lotto with them), and Teamgroup, all of which buy ICs.",hardware,2025-12-04 23:36:53,3
AMD,ns9jlqj,"Nah, memory makers are not investing in new fabs because they know what’s coming, IA is about to pop. It will probably take the whole economy with it but the hobby survived worse shit than this.  For now pray that you won’t need anything new for a while and if you do try the used market, let’s hope scalpers won’t take over.",hardware,2025-12-04 16:01:14,64
AMD,nsarvxg,"> is the DIY PC culture at risk?   What a melodramatic title. Micron exiting direct to consumer by dissolving crucial isn't that important in the grand scheme of things. Micron NAND and DRAM is still going to make it's way to consumer DIY through commercial B2B sales to businesses like Kingston and Corsair, who will put it in RAM and SSDs sold to consumers. They are simply adopting the same business strategy as their competitors Samsung and Hynix. You can't buy a Samsung or Hynix branded RAM kit directly from a retailer, and nobody has ever seen that as a threat to PC DIY lol",hardware,2025-12-04 19:35:52,13
AMD,ns9hc1g,at risk no there's still many other brands but crucial was definitely one of the best. OEMs will just have to adapt their hardware to other makes now  yes AI is just the new NFTs and 3D Televisions where are they now? there's always a new fad and this one is being pushed by the us government to launder money as well as push a fad to profit before they get voted out. wont matter by then the biggest wealth transfer in history basically just happened with nvidia and by the time an alternative government can come in and investigate it'll be far too late to do much,hardware,2025-12-04 15:50:22,14
AMD,nsb54wt,"DIY enthausists online tend to trash Micron/Crucial DDR5 and all wanted Hynix dies, Hynix whom does not have direct to consumer DDR5 products...  More seriously as it's been mentioned Micron has likely wanted to exit the direct consumer market for some time. Remember they've shut their Lexar brand and sold it. They discontinued Ballistix. Now they're discontinuing Crucial.   Hynix doesn't have a consumer DDR5 division. They have a limited consumer SSD division. Samsung has a somewhat limited consumer DDR5 division (nothing that targets DIY enthuasists with ""heatpreaders""). They do have a large consumer electronics divison which includes SSDs however.",hardware,2025-12-04 20:43:12,3
AMD,ns9krvk,"AI Bubble will eventually pop, but at the same time it is inevitable that miniPCs will replace DIY PCs one day. It's just a question of when the technology is that PCs no longer makes any more sense.",hardware,2025-12-04 16:06:58,14
AMD,ns9xd0y,At least nowadays non-last gen hardware is more than enough for playing comfortably most games,hardware,2025-12-04 17:08:00,2
AMD,nscidvk,"I think you're reading too much into that one event. Remember that Micron/Crucial ended their Ballistix branding with DDR5 three years ago. They never had much in the way of offerings in the DDR5 generation, and what die they had were the worst of the three DRAM manufacturers. Micron chips work best around 5200 and can range up to 5600, but any kits above that rating are utilizing extremely binned chips at high voltages. I don't know of any reason why Micron couldn't redesign their DRAM to be competitive, but they clearly chose not to. Even Samsung's memory clocks higher, though it doesn't clock anywhere close to SK Hynix chips nor can reach their timings.  It's worth pointing out the people who had the most trouble with memory on their DDR5 builds were more often than not using Micron or Samsung based kits. I'm sure the heavy returns hurt their financials. But you should realize that Micron's current DDR5 offering was about to be made moot. Intel already clocks over 7000, and AMD is going to (farking finally) launch its 2nd gen DDR5 controller that should move the bar well above the 6000 sweet spot. Not even OEMs are going to be buying 5200-5600 kits regardless of price, meaning without a die redesign the only space left to offload chips would be servers and that's what Micron is now doing.  The AI bubble will pop, money isn't infinite and spending billions to make a few million every quarter isn't sustainable. There is no magic market that will materialize to fill in that gap disparity. Short of creating a true AI that can act as a people replacement the AI bubble is going to pop... AI won't truly go away, we're stuck with it just like we're stuck with the internet. But we are very much in a bubble and it's hard to imagine a scenario where the hundreds of billions being blown on datacenter deployments won't end up with a massive overbuild of DCs that will cause multiple bankruptcies in the future. I used to think there would always be enough demand elsewhere to keep former-AI DC's in use, but we're quickly building past that level.  As for breaking up NVIDIA, I'm not sure how that'd be feasible, they design a single base tier uArch and then scale it across all market segments. Forking the company into a consumer-only GPU division would probably just create another Radeon situation where the company keeps prices high to amortize its dev costs across the consumer market, instead of across all of its markets.  Micron isn't going anywhere, they're just focusing exclusively on servers and enterprise where the margins are largest and product support is easiest. The other day I read that Micron is now going to build a $9.6 billion dollar HBM fab in Japan. It's unfortunate but not unexpected, all three HBM vendors have been selling out their entire annual HBM production 12-18 months in advance since 2023. If other DRAM vendors still existed they would've already jumped ship to chase those silly numbers too.  Maybe at some point in the future when there's a single vendor for some piece of the computer hardware the market might be at risk. But it seems unlikely, because OEMs buy consumer level hardware and in order to supply OEMs they have to produce the same parts consumers would buy regardless. In Micron's case the DRAM they're producing isn't even good enough to meet OEM's needs, let alone consumers.  Edit: Will say I've been corrected, Micron does make a newer gen die for DDR5 that tops out at 6400 in 16/24Gb flavors, and it's even available under their own branding. But it's sure not common and still suffers from high timings.",hardware,2025-12-05 01:14:07,4
AMD,ns9q4hi,You will own nothing and billionaires will be happy,hardware,2025-12-04 16:32:52,4
AMD,nsap24t,I hope china does a thing,hardware,2025-12-04 19:21:41,2
AMD,nsbdmur,"The stuff DIY gets is downstream from server parts right? I don't think Intel and AMD will stop making socketed chips for desktops, AMDs chiplet strategy allows it to efficiently use its dies in all sorts of products. As long as businesses/OEMs need computers it makes business sense for them to offer consumers and ethusiasts products. Even for Nvidia the geforce business is 5-10%, that's worth keeping around even if its totally neglected for AI; its good money and pretty well cornered.  This dire ram situation may last a while but it will pass. Maybe we will see Chinese ram and flash production emerge to fill the gap.",hardware,2025-12-04 21:25:41,1
AMD,nsbfnfu,Nah. It may cost more(not as much as the 90s and early 2000 though) but it ain't going away there's still good money there.   I wouldn't mind if Apu someday became good enough that we didn't need a dgpu sure but that's a different issue,hardware,2025-12-04 21:35:44,1
AMD,nsedr4k,🎵Now we'll rely on the Chinese🎵 https://youtu.be/mTPZAflsxDI,hardware,2025-12-05 09:40:40,1
AMD,nsfjp58,"Oh NOOO  where will I get sticks of 16GB ddr5 5600–cl48 for $300?!?!,",hardware,2025-12-05 14:48:18,1
AMD,nsihfry,Moore's Law is Dead has a great perspective on this (25 min mark): https://www.youtube.com/live/g38VR-GJp7o?si=VWs0AzQDDRTBr9-5  Micron not selling ram to the public does NOT mean Micron will stop making ram for consumer pc's.  There is a difference between Micron making ram and Micron selling ram through a Micron retail channel.,hardware,2025-12-05 23:58:48,1
AMD,nsjzrxx,Keep in mind that Crucial is huge in OEM especially laptop RAM (SODIMM). Going forward it’s gonna be a Samsung / SK Hynix duopoly and you tell me if it’s better or worse.,hardware,2025-12-06 06:18:11,1
AMD,nsso6qn,The same fear has entered my mind. PC gaming would die if this happened or be bastardized into laptops only or streaming games.   Not something I'd appreciate,hardware,2025-12-07 17:58:34,1
AMD,nsuhp0h,"While crucial might be, consumer RAM isn't just for DIY desktops. It's for all consumer products, phones, tvs, laptops, your smart spy fridge, servers for smaller companies.",hardware,2025-12-07 23:25:23,1
AMD,nswxh0h,Micron supports many DIY OEMs that sell memory. Their chips will sll be sold to consumers.,hardware,2025-12-08 10:16:48,1
AMD,nsxn51p,No. Idiots just left the door open fir Chinese competition. In half a decade they Gona rue the day,hardware,2025-12-08 13:42:49,1
AMD,nsc91nx,"Trust me, the manufacturers would absolutely love for you to be forced to buy a McComputer from whoever leads the market. There is no world in which these fuckers want you being able to do your own thing.",hardware,2025-12-05 00:18:41,1
AMD,nscto3a,"This is somewhat self correcting though not without a lot of pain. My theory is that OpenAI wants to price new entrants out of their market (witness Google's ""We have no moat, and neither does OpenAI"" paper, this is them trying to build a moat), and that is why they're buying so much RAM.  It's self correcting because if they price out new startups they'll eventually price out AI consumers as well.",hardware,2025-12-05 02:20:40,1
AMD,nsbtiwq,"It sucks that Crucial/Micron are being this greedy but then that's what happens when you spend decades fostering an economy based on greed and contempt for the working classes.  Once the AI bubble bursts this will all become moot anyway, it'll muck up the economy pretty badly and I expect prices to come down very quickly.  As for Micron, like someone else said I think exiting consumer memory has been on the cards for a long time now and this has just given them the perfect excuse to do it, AI bubble or not.  I can't see the loss of Crucial affecting me that much personally, I can't remember the last time I had that brand of RAM in my PC. Maybe DDR2 if i'm remembering correctly, I think it was the yellow stuff.",hardware,2025-12-04 22:48:11,0
AMD,nsg0wf8,"No,someone else will fill the gap in the market",hardware,2025-12-05 16:15:36,0
AMD,nsl3j4r,"Memory, water, electricity, farmland.      Ai is starting to compete for resources with its creators.",hardware,2025-12-06 12:44:20,0
AMD,nsls1pc,"The market will simply be outsourced to China. Future DIY PCs will have ""Made in China"" written all over them. Silicon Valley will become another rust belt once the AI bubble pops.",hardware,2025-12-06 15:20:32,0
AMD,ns9j8l4,"I dont understand the freakout.  We’re just gonna get more VRAM in gpu’s.  Ram might be more expensive. Guess it’ll affect budget gamers, but i didnt get into pc gaming for budget builds.",hardware,2025-12-04 15:59:29,-22
AMD,ns9glbt,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-04 15:46:46,-7
AMD,nsartxf,Especially know that SK Hynix is apparently upping their DRAM production from 20k wafers to 190k wafers per month till the end of 2026 and Samsung is upgrading a fab to produce more DDR5/GDDR7     Next year will be hard but after that we will see normal pricing again,hardware,2025-12-04 19:35:36,76
AMD,nsc1dkf,"The biggest red flag was the transition from ddr4->ddr5 when they just jettisoned a reputable subbrand like Ballistix and replaced it with a lazy ""Pro Overclocking"" line with mediocre timings and a bland black heatsink.  Crucial Ballistix was probably one of if not the most trusted ddr4 choice for gamers and had a sterling reputation.",hardware,2025-12-04 23:33:17,25
AMD,nsaj6x8,"Thank you for a non doom-and-gloom take, it's bad enough with the shortage already.",hardware,2025-12-04 18:52:56,38
AMD,nsnqg0e,"\> Even then, we had the perception that Crucial was like 1% of their business and an afterthought.     I figured the reason Micron made the Crucial brand in the first place, was because before Crucial existed, lots of places sold ""micron ram"" that wasn't actually made by micron - it just had micron chips lol. It was certainly a thing at computer shows with the sketchy computer shops from City of Industry lol.",hardware,2025-12-06 21:38:15,1
AMD,nswxoa5,">Crucial finally closing is just a business efficiency decision, and they can focus more on B2B so that other companies can sell consumer products.  Exactly. All those chips will end up in Kingston and other consumer products and nothing will really change.",hardware,2025-12-08 10:18:49,1
AMD,nsbl9tz,CXMT is making DDR5 now (has been for about a year) but on a larger node.  I’m not seeing them add enough capacity to really put a dent in this pile of nonsense though.,hardware,2025-12-04 22:03:52,22
AMD,nsbgy0q,"Exactly what I thought.  The Chinese always exploit these fuckups,  and once the AI meme is over and micron tries to crawl back it'll be too late cause china will have its market share.   Then the typical bitching and crying will start. Same shit every time",hardware,2025-12-04 21:42:10,47
AMD,nsdjfkk,username matches,hardware,2025-12-05 05:04:27,4
AMD,ns9qisa,"There's also DDR4 RAM which can be made on old fabs, and things like 8-channel to increase bandwidth.    For GPUs, I think purpose-built AI chips will knock out stuff like Nvidia... I think they should have done so by now. The Euclyd Craftwerk project looked promising, mostly due to the names involved, and they promised a 100x increase in power efficiency with 3kw chips. There's no need for GPUs at all really. However we might still be stalled out fab-wise because the issue is cost of wafers, availability and buying capacity in advance...",hardware,2025-12-04 16:34:48,16
AMD,nsditn1,"> APUs make a lot more sense than low-end dGPU products. ...  I would just add that APUs is just part of the long ongoing trend of the CPU absorbing other components that existed elsewhere, like the math coprocessor,  the north/south-bridge, the memory controller,  having USB ports, and so on.  As the transistor budget increases, it becomes practical and eventually cheaper to do so.",hardware,2025-12-05 05:00:00,8
AMD,nsrxqgi,Problem is they will become big enough to impact economy that they will get their ass saved first. Same story,hardware,2025-12-07 15:43:11,1
AMD,nswz834,Which sucks because if we learned anything from past optimizations what this actually mean is just cut features like physics out wholesale.,hardware,2025-12-08 10:34:29,1
AMD,nsauj07,Samsung DDR4 and DDR5 was shit except for a singular revision they managed to strike gold on and could never replicate. Micron DDR4 was consistently good to excellent in the Crucial Ballistix line. It's Micron DDR5 that has never taken off.,hardware,2025-12-04 19:49:15,3
AMD,nsbqgdm,Crucial recently released 6400mt cl32 kits,hardware,2025-12-04 22:31:16,1
AMD,nsjpm4c,> will continue to supply 3rd-party OEMs that produce memory for consumers  [At least those with firm contracts](https://www.reddit.com/r/hardware/comments/1pdydlh/sandisk_and_samsung_delay_nand_shipments/).,hardware,2025-12-06 04:52:55,1
AMD,nsjpy9v,> Micron is still producing RAM ICs and consumer-facing companies will buy them.  [Maybe](https://www.reddit.com/r/hardware/comments/1pdydlh/sandisk_and_samsung_delay_nand_shipments/).,hardware,2025-12-06 04:55:32,1
AMD,ns9z10u,PC gaming is honestly a case study in resiliency. It’s survived so much shit that would kill other products and yet here we are with even console makers considering just building standard PCs,hardware,2025-12-04 17:16:09,51
AMD,nsacp1j,"Micron is literally building 6 fabs right now in the US, with the first one set to complete in 2027",hardware,2025-12-04 18:22:03,33
AMD,ns9mwr5,"Good news, if the economy collapses the market will be flooded with cheap RAM chips...   Bad news is you may be more interested in saving your money for a bag of potato chips to eat by that point though... since your pension pot probably was invested in AI 😅",hardware,2025-12-04 16:17:15,29
AMD,nsas7ii,[https://www.computerbase.de/news/arbeitsspeicher/speicherproduktion-faehrt-hoch-sk-hynix-und-samsung-entdecken-dram-neu-und-setzen-nicht-alles-auf-hbm.95324/](https://www.computerbase.de/news/arbeitsspeicher/speicherproduktion-faehrt-hoch-sk-hynix-und-samsung-entdecken-dram-neu-und-setzen-nicht-alles-auf-hbm.95324/),hardware,2025-12-04 19:37:29,1
AMD,nsalcqo,"This is the best take and course of action. It will pop. And the hardware that these companies are gobbling up the most of will see the biggest drops in prices specifically when AI bubble does pop.  I would follow the advice outlined above, but also add that—if you are interested in things that benefit most from the same hardware being bought up now, like a running a homelab, it would be smart to start saving up some money now so that you can take advantage of when those prices do dip on used parts companies need to sell for Pennie’s on the dollar when AI is no longer nearly as profitable. It’s smart to save up some money at any point for hardware though period, given that you don’t know when something might break lol.",hardware,2025-12-04 19:03:19,-3
AMD,nsbcyqk,Yep. This would be the equivalent of NV saying they're not going to make founders edition cards any more.   It doesn't mean NV are not selling GPUs. It means they're concentrating exclusively on selling GPUs to companies who will sell cards to customers.,hardware,2025-12-04 21:22:19,8
AMD,nseol3m,"> What a melodramatic title.  The OP is slop. I clocked it right away and checked their comments, they've explicitly posted AI comments and been called out for slop submissions before (they claimed it was their browser's ""built-in grammar correction"", as all clankers do).",hardware,2025-12-05 11:23:21,5
AMD,ns9ibr2,"I recently wanted to get a 2TB SSD with DRAM cache, but it is selling at 100% premium. Even older stocks are sold at these prices. This is hoarding and no one is taking any action against this. Now I have decided to wait.",hardware,2025-12-04 15:55:08,6
AMD,nsdt523,You're sorely mistaken if you think AI is the new NFT and crypto lol,hardware,2025-12-05 06:23:32,0
AMD,nsbr4kz,Klevv,hardware,2025-12-04 22:34:59,0
AMD,ns9rf9n,"Yea, just like how the laptop and then the tablet and then the smart phone all killed the PC.",hardware,2025-12-04 16:39:11,47
AMD,ns9p030,Could you explain why you think that?,hardware,2025-12-04 16:27:22,15
AMD,nsap4kv,"Nah, fuck that.  I don't want that locked down garbage.  Crapdragon with soldered everything and locked down firmware 🤮.",hardware,2025-12-04 19:22:02,8
AMD,nsahhfi,I heard this exact same thing when tablets started getting big a decade ago.,hardware,2025-12-04 18:44:51,7
AMD,ns9qx49,"I believe that as long as there are users who need specific resources, there will always be plug-and-play components for them to build what they need.   The real question is how much are they going to charge for those components. As bad as other shortages and supply squeezes have been, companies have never gone as far as to say, ""Fuck the average customer! We doing B2B now!"" at least openly. With Micron more or less doing just that, it really opens the floodgates for other companies to more openly flout the consumer market in favor of business contracts. Is the average PC gamer, HomeLab type, or even a hoarder, going to throw down 3x the regular price and some of the logistical headaches just for regular components. A few will, but others will decide to either stick with what they have, jury-rig a solution with older parts, or give up after a while.",hardware,2025-12-04 16:36:44,6
AMD,nsa7esm,">  it is inevitable that miniPCs will replace DIY PCs one day. It's just a question of when the technology is that PCs no longer makes any more sense.  What are you basing this prediction on?  Mini-pcs can be great in certain applications but being able to tailor a system to a specific need also has value  I mean, even a lot of mini-pcs today have changeable ram and storage  Its easier to be able to change components rather than having to produce 50 different SKUs  Some customers might need a lot of ram and CPU power, others might need a lot of storage but with low CPU wattage  I don't see why having the ability to pick and choose components would go away",hardware,2025-12-04 17:56:50,4
AMD,ns9x9am,When the AI bubble pops we are headed for a global depression. AI fuckery is the only thing propping up the market right now.,hardware,2025-12-04 17:07:29,2
AMD,nsx01fl,"> inevitable that miniPCs will replace DIY PCs one day  Over my dead, cold screwdriver.",hardware,2025-12-08 10:42:24,1
AMD,ns9l3de,"You're not getting more VRAM in GPUs if NVIDIA says no. Considering their track history of gimping GPUs by their VRAM, that is the most likely scenario.",hardware,2025-12-04 16:08:30,13
AMD,ns9k2c0,It's because you'll be paying high end prices for low-mid range components.,hardware,2025-12-04 16:03:29,17
AMD,ns9gxbi,This is not a build help or tech support. It is general discussion about a recent news on CNBC tech.,hardware,2025-12-04 15:48:24,6
AMD,nsc0v6g,How do you just jump that much in production with little notice?,hardware,2025-12-04 23:30:17,17
AMD,nse826h,"Holy shit, this is the most hopeful news I've read all year. Hopefully we can get normal prices again by end of next year and this doesn't stretch to 2027/8.",hardware,2025-12-05 08:45:34,0
AMD,nsf8enj,"This is the new normal pricing, sorry bud",hardware,2025-12-05 13:44:44,-6
AMD,nsgxqbe,"Micron's DDR5 isn't very good (and is probably the worst among the 3 major manufacturers), whereas their DDR4 was quite good. So it makes sense they dropped their performance DDR5 lineup, since they couldn't compete with Hynix DDR5 in clocks/latencies.",hardware,2025-12-05 18:55:11,11
AMD,nsed2it,"It just hit me that Crucial has been pretty low key for DDR5, when they were the rage during the late DDR4 era for good value and high performance. (Mostly because I don't follow the overlooking scene bit still I never saw Micron chips mentioned)",hardware,2025-12-05 09:33:45,2
AMD,nse80im,"It's the other way around. The ddr4 price crunch was caused by Chinese makers, who had been selling at a loss to get market share, being told by the government to pivot to HBM. Their below-cost selling had forced other makers out of the ddr4 market early. Unclear if it's worth restarting production lines for ddr4 now.",hardware,2025-12-05 08:45:08,19
AMD,nsc1ioo,"and these Chinese Dram/nand do not have fear when AI ""actually"" bubble; leaving with over supply issue to deal with. Their Gov probably got them covered, if that happen all their gov need is to force domestic Dram/nand consumption to buy only from Chinese companies.  Thats when those Cartel suddenly found themselves with a lot of inventory waiting to be clear because suddenly the Chinese stop buying from them.",hardware,2025-12-04 23:34:08,9
AMD,ns9st9s,"The problem with ASICs is that AI software folks don't know which math is the right math. GPUs are not the best at any particular computation, but they are highly flexible compute. Eventually, ASICs will be the endgame.  Whether AI is buying ASICs or GPUs doesn't really matter, though. Leading edge wafer supply is more or less fixed for years. A new big buyer steps in and the only way it can possibly work is for the value end of the market to simply not get supplied.",hardware,2025-12-04 16:45:55,17
AMD,nsecbs4,"Ddr4 is honestly far enough for most applications. If it can be made in fabs that haven’t been upgraded yet that’s great. But how many of these older fabs still exist, does anyone know?",hardware,2025-12-05 09:26:09,2
AMD,nswyo74,"> I think they should have done so by now.  Did you mean to say if they were going to do so, they would have done by now? Because so far the only one that comes even close is Googles TPU, and thats for internal use only.",hardware,2025-12-08 10:28:54,1
AMD,nsdr4lu,"Maybe if there were faster memory options for APUs, it doesn't take much for a DGPU to surpass an APU option, and then you're left with a cut down CPU.",hardware,2025-12-05 06:06:00,3
AMD,nsbkv8m,>Micron DDR4 was consistently good to excellent in the Crucial Ballistix line.  I can't remember ever hearing much about Micron for DDR4. It was either B-die or Hynix.,hardware,2025-12-04 22:01:46,3
AMD,nsa99ne,"Well, one major factor helping the PC Gaming market is the name itself containing the term PC. A computer is useful for much more than just gaming, so that usefulness props up slumps in the market, but this AI bubble is going to be the worst test yet.",hardware,2025-12-04 18:05:46,36
AMD,nsbn1xk,"Consoles were already increasing in price, who knows how expensive they will get now.. Its really not an issue about PC vs console rivalry or benefit for one of the other now, we'll see increased prices and less interest in both probably. Who knows what that will do to game development, maybe some more tweaking and optimization to make things work on lower specs than initially expected... but it will also hurt sales potential and investment.",hardware,2025-12-04 22:13:03,3
AMD,nsaqxj1,"Lol right? This guy is so laughably wrong I feel like my head is going to explode. ""Memory makers are not investing in new fabs"" he says while I look out the window at my state's largest construction project ever, which happens to be a memory fab. Maybe he missed the literally hundreds of billions of dollars that chip manufacturers are spending right now.   Hynix is spending $500B on four new fabs, first one finished in 2027. Micron building 6 new fabs, costing somewhere around $200B. Samsung is doing some joint venture for new fab spaces with Hynix, and they are building their new P5 fab. This all after many years of almost no global investment into new fab space.",hardware,2025-12-04 19:31:01,26
AMD,ns9yd20,"except not exactly.   yes some ddr5 is being used, but alot of these companies are actually selling the wafers to enterprise before they become ddr5.   The idea is they will become custom HBM3 implementations  will not just be a flood of ddr5 in a couple years",hardware,2025-12-04 17:12:54,17
AMD,nsbpvn7,I mean they kinda did,hardware,2025-12-04 22:28:10,3
AMD,ns9uodb,"Just look at what Strix Halo can do, and while it is expensive right now imagine if that technology becomes available for entry level costs.",hardware,2025-12-04 16:54:52,-2
AMD,nsakwow,"the entire ""system"" for the latest iphone is about the size of the camera island  this is an entire modern 6 core CPU (battery powered, singlethread on par with ryzen 9950X), 12GB LPDDR5X (but could likely be doubled in the same size), powerful GPU close to an RTX3050 (yes, it has ray tracing), a 5G modem + BT/wifi, fast SSD  you could turn this into an imac and a lot of people would be none the wiser, it's plenty powerful for many years to come. it would even have better cooling and power!  no need for a big case, power supply, large PCBs with long traces, expandability connectors and all the potential issues that come with it (just use USB-C)  the idea of docking a small mobile device has been around for a long time and we're honestly technologically largely ready for it. Samsung has packed software which presents a desktop environment and allows data/app access between the two interfaces. I've also seen a laptop-like ""dock"". Slide the smartphone in, it uses the battery + screen + keyboard/mouse.  Back it all up continously in cloud accounts and when it's stolen, get a new device, log in and get access to all of your stuff.  Seriously, for 95% of users this would be fine. Documents, web surfing, photos, content consumption, generally light workloads...  Yeah sure if you want to keep track of 8TB of data, like to handle large workloads with powerful hardware on your ergonomically designed custom environment, have a lot of peripherals, like to geek around... not great. But the vast majority is fine with it.",hardware,2025-12-04 19:01:06,0
AMD,nsx0frg,Niche hobbies always overpay in order to fund the hobbymaker companies existence. Its always a question of price.,hardware,2025-12-08 10:46:16,1
AMD,nsaleo7,"Phones are a fixed set of SKUs, people use them for years and exchange it. Most people buy ""a computer"" and switch it out after some years, without ever opening it.  Apple has been selling macbooks/mac minis with fixed configurations for years now. Buy what you expect to need. You can swap out the battery when it's worn out, otherwise you upgrade to a newer variant anyway.",hardware,2025-12-04 19:03:35,-1
AMD,nsa1mim,"""The market"" at this point is a small select group syphoning up the collective wealth and assets, material and immaterial, of the entirety of human society for themselves.",hardware,2025-12-04 17:28:44,3
AMD,ns9m9p1,But i got money,hardware,2025-12-04 16:14:10,-12
AMD,ns9u8dh,There's already a 500 comment thread discussion on it...,hardware,2025-12-04 16:52:43,4
AMD,nsc5dzf,DRAM makers have actually been underproducing RAM since 2023. They overproduced from 2020-2022 during Covid and after Covid demand for RAM fell and RAM prices crashed. So much so that they were losing money with how cheap the prices were. So they underproduced from 2023 all the way to now. So a lot of that extra production is just going back to pre 2023 levels but there is definitely extra investments there.,hardware,2025-12-04 23:57:07,65
AMD,nsc10uh,"They don't, that jump will be happening throughout the next year. 190k is the output at the end of 2026",hardware,2025-12-04 23:31:13,15
AMD,nsg4kb1,"They aren’t ramping up total production that much, they are ramping up the latest 1c node that much.  Which they were likely planning on doing anyways.",hardware,2025-12-05 16:33:26,6
AMD,nsf8hgi,It's not  Plenty of scenarios that disprove your claim,hardware,2025-12-05 13:45:10,7
AMD,nsgy8zk,"Hynix is the clear leader for DDR5 in terms of clocks/timings (and the only one that can achieve the 6000 CL30 spec recommended by AMD), so there's basically no interest in Micron RAM among overclockers.",hardware,2025-12-05 18:57:42,7
AMD,nsi18e4,"Micron developed DDR5 but completely botched it--SK Hynix makes performance by ignoring DDR5 timing paradigms established by Micron. Micron's DDR5 just doesn't perform well at all. It's only on the server side with very low clock speed that they're making sales at all, anyway.  TBH, with memory performance and timing increasing, I'm not sure how much longer that will be true anyway.",hardware,2025-12-05 22:22:14,2
AMD,nscedfu,"I mean, that's what the US can do too. In terms of banning the sale of Chinese memory in US, But it's unlikely",hardware,2025-12-05 00:49:31,4
AMD,nse3xgj,? I fail to see your your point. Are you bummed out that APUs like the one in the steam deck exists? Just don't build with an APU if you plan to have a dGPU...,hardware,2025-12-05 08:04:08,2
AMD,nsbqmlk,Micron e dies are the poor mans b die  B die can do tight timings and high clocks   E die can do moderate timings and clocks   Hynix for high clocks but crappy timings,hardware,2025-12-04 22:32:13,2
AMD,nsaswj1,The converse is also true: you will always be able to play games on general-purpose PCs. You might not be able to afford a 5090 but Nethack will run anywhere.,hardware,2025-12-04 19:40:59,19
AMD,nsl5vw2,So dramatic lol,hardware,2025-12-06 13:02:30,1
AMD,nsbaeb5,"Insane misinfo, I agree. It's like they forgot the CHIPS act ever happened. On a more positive note I'm genuinely impressed by the level of investment in semiconductor manufacturing, the US might actually come back and become a powerhouse in that again.  My hopium is on supply gaining above increased demand from AI. If anything will drive down price it'll be that, and me (the idiot) decided on the worst time to upgrade my DDR4 system to DDR5.",hardware,2025-12-04 21:09:31,9
AMD,nsbuqro,"I think they are interpreting “Memory makers are sticking to their currently planned CapEx for fabs” as “memory makers aren’t investing in new fabs at all”.  Fabs are major projects, not spun up overnight and by the time one started now starts making RAM it’ll be 2029.",hardware,2025-12-04 22:54:58,6
AMD,ns9z6c9,Yey so it'd not even as good as the end of the crypto crisis! Another mark against the AI bubble.,hardware,2025-12-04 17:16:52,10
AMD,ns9x6c8,"Strix Halo is a massive chip though, it's larger than the chip used in a 5070.",hardware,2025-12-04 17:07:05,19
AMD,nsa5br7,"People have been predicting the ""death"" of the desktop for decades, it never happens. AI will be nothing more than just another bump on the road like cryptomining was.  Add in the fact that the games industry shows no signs of ever correcting their behaviour of heaping ever more expensive rendering techniques on the slop they churn out, only further underlines the need for the desktop.",hardware,2025-12-04 17:46:55,8
AMD,ns9xuow,"If RAM prices stay elevated, systems like that will get even more reasonable comparatively. Since strix halo is sharing RAM across CPU and GPU, that is fewer RAM chips needed to achieve the same results vs separate DDR and GDDR.",hardware,2025-12-04 17:10:25,2
AMD,nsx04ee,>Just look at what Strix Halo can do  Cost 3 times its processing power should? no thanks.,hardware,2025-12-08 10:43:12,1
AMD,nse95gy,"> Most people buy ""a computer"" and switch it out after some years, without ever opening it.  Are you taking into account workstations   For companies like Dell and Lenovo, being able to switch components to provide different offerings to their clients still seems pretty useful",hardware,2025-12-05 08:56:09,2
AMD,nsx0krz,Did you really compare average PC builder to average phone user?,hardware,2025-12-08 10:47:37,1
AMD,ns9mo2a,"Sure, you're free to throw it at bonfires, casinos and hookers as well if you want to. Doesn't make the pc market wither any slower.",hardware,2025-12-04 16:16:06,6
AMD,nsey8pc,"From the articles I'm seeing, the dram production SK hynix is creating is for HBM and other data center memory chips, not standard DDR5 ones.    So that extra production won't replace crucials gear.",hardware,2025-12-05 12:39:59,10
AMD,nswxrsz,that 1c node so far represents only small portion of their total production as well. And yes will likely replace older nodes over time.,hardware,2025-12-08 10:19:49,1
AMD,nskjwqs,Heavy bot farms trying to anchor pricing for consumers lol.  People just need to remember they can just not buy the new thing.  In the old days if a company tried to gouge many customers wouldnt come back afterwards,hardware,2025-12-06 09:37:54,2
AMD,nsf9mol,Can you explain? I'm still waiting on a 50 series GPU that's as good as the previous Gen's 70 series for less than 200 like how it used to be,hardware,2025-12-05 13:51:42,1
AMD,nswyy0z,Except you cant build with a dGPU now instead of APU if you want the equivalent performance. Theres no 5030.,hardware,2025-12-08 10:31:41,1
AMD,nsqtwca,"Oh yeah, for sure, my original comment is just oozing drama.",hardware,2025-12-07 11:07:22,1
AMD,nsbcrbh,"I understand why everyone is hoping for an oversupply scenario so they can get their parts on the cheap, but the industry is so much healthier if they can just accurately forecast demand and keep prices where they ought to be. The pendulum of oversupply and shortages hurts us all in the long run because it leads to labor issues following layoffs, inappropriate customer stockpiling leading to waste and other supply issues, over protective business practices that deprioritize risky tech innovation, etc.   But yeah, it is great that we are building up so much silicon infrastructure here. I personally believe that everyone has been wanting to build in the US for a long time, but they were waiting for a handout because it would have sucked to open your $50B fab the day before the government would have just dropped 10% of that in your lap. The CHIPS act opened the floodgates for the industry that had been desperately hoping for that opportunity.",hardware,2025-12-04 21:21:17,9
AMD,nsby4og,"For what it's worth, even that isn't true. Micron recently announced an extra new fab at their Boise site in addition to the new fab that they are already making. I personally believe that is because the NY fabs they are trying to build have taken far too long to get through the red tape phase and they want to make all the hbm NOW.",hardware,2025-12-04 23:14:19,3
AMD,nsa6d6y,What does that matter if the final form factor that the chip is integrated into is smaller than 99% of BYOPCs? Chip size has minimal relevance here.,hardware,2025-12-04 17:51:52,-5
AMD,nsa6qqw,But 10 years from now? Will it be as massive?,hardware,2025-12-04 17:53:40,-2
AMD,nsa7yaf,"Yeah, but if nobody can afford to build the PCs to be able to play those games, then what?",hardware,2025-12-04 17:59:24,0
AMD,nsge84c,They come with half the RAM soldered in and one slot to add whatever you want. At least that's what they have been doing for years now,hardware,2025-12-05 17:20:57,1
AMD,nsx2yt2,I'm talking about the market. Most PCs don't have upgrades,hardware,2025-12-08 11:10:34,1
AMD,ns9n7lm,"So whats thes future? If we’re gonna get great pc performance (8k,500 fps,max settings), i honestly dont really care how we get there",hardware,2025-12-04 16:18:42,-6
AMD,nsfafuk,"And when was that? I checked till the 970 and there was no 50 card that was even close to beating a 70 card from the previous generation 😂  Even then this would be just one example, you think this is the first shortage that has happened?",hardware,2025-12-05 13:56:20,4
AMD,nswxyhj,You are waiting for impossible.,hardware,2025-12-08 10:21:38,1
AMD,nsaidmx,It matters for cost.,hardware,2025-12-04 18:49:04,14
AMD,nsx07lt,10 years from now Strix Halo will be the equivalent performance of you now having a 950.,hardware,2025-12-08 10:44:04,1
AMD,nsbpzyu,Then geforce now,hardware,2025-12-04 22:28:48,1
AMD,nszjfxw,"Then we just move back to prefabs. Prefabs have already come back to being recommended for a while now, after years of telling people to avoid prefabs like the plague.",hardware,2025-12-08 19:30:36,1
AMD,nsbkf1b,"Then nothing. It's a blip, it'll be over and forgotten about just like with crypto.",hardware,2025-12-04 21:59:27,0
AMD,nsxzdho,"they specifically said ""DIY PCs"". As in PCs that were built by the owner. Those do often have upgrades.",hardware,2025-12-08 14:53:21,1
AMD,ns9wifq,We will never get to 8k 500fps in modern AAA games. As graphics cards get better games will become more demanding,hardware,2025-12-04 17:03:49,4
AMD,ns9nlj1,You can get that for the low low price of $5999*   *VRAM not included. 10 GB of VRAM is $599.   Oh don't forget 64 GB of RAM for an accessible $1999.,hardware,2025-12-04 16:20:36,2
AMD,nskzdo4,"Ooft, yeah I got my numbers mixed up   The 480($500) is worse than the 1050($100), there was a 6 year gap between them   So, in 2022 we should have seen a 50 card that was as good as or better than the 1080 for at most 150   Instead it's 2025 (9 years later), the 5050 is what we've got which is $250",hardware,2025-12-06 12:09:25,0
AMD,nsak15u,"Sure, but not to the extent you and the other OP were insinuating. These are premium products with premium margins for OEMs. If more competition comes to the SoC market, price points will come down.",hardware,2025-12-04 18:56:56,0
AMD,nsbn0a2,"Yet crypto is still a thing, in some fashion. Granted it's not as big as it once was, however, it's still a thing.",hardware,2025-12-04 22:12:49,1
AMD,nsx0ud4,"I think you are missing his point entirely. If hes so rich he does not care, he wont care about these prices either.",hardware,2025-12-08 10:50:14,1
AMD,nskzzob,"1. Inflation is a thing, you can't simply compare numbers and say 'But it was like that before'. There's also more that we need to add to that calculation, like buying power.  2. TSMC is at the moment the only viable company that can produce these cards, they have a 40-50% profit while being simultaneously being sold out as everyone wants to produce their stuff with them. Obviously that affects NVIDIA's pricing too because they can only produce a limited amount while paying a lot for that too.   AI on top of that is causing even more issues because of the margin they shift the limited supply towards server GPU's which means even less space for lower pricing.  The 30 series had really good pricing because it was fabbed with Samsung instead, so hope that Samsung and Intel finally step up their game to be real competition",hardware,2025-12-06 12:14:48,2
AMD,nswy3i4,Its always the 1000 series. You take one complete anomaly and expect this to apply to market as a whole when in reality this has never happened before or after.,hardware,2025-12-08 10:23:00,1
AMD,nsdtcxv,"Die size is the single biggest factor driving up chip prices, what are you on about?",hardware,2025-12-05 06:25:28,2
AMD,nsbnzvy,"Crypto is still a thing yes, but its impact on the GPU market was a blip. It'll be the same with AI.",hardware,2025-12-04 22:18:02,1
AMD,npgwoyv,"TLDW:   Model used: XFX Swift Gaming RX 9600 8GB (Dual fan, OEM only)     A) 7 game average (1080P/1440P Low/Medium):      The RX 9060 8GB:      - Offers similar performance to the RTX 5060 8GB      - Is 13-14% slower than the RX 9060 XT 8GB      - Is 23-26% faster than the RX 7600 8GB              B) 7 game average (1080P/1440P High/Ultra):      The RX 9060 8GB:      - Is slightly faster than the RTX 5060 8GB (~4-6%)            - Is 13-14% slower than the RX 9060 XT 8GB      - Is 26-29% faster than the RX 7600 8GB",hardware,2025-11-18 10:11:25,45
AMD,nph3mth,This should have been in the Steam Machine.,hardware,2025-11-18 11:19:04,76
AMD,npgziik,Feels like this is what the 9060XT 8GB probably always should have been.,hardware,2025-11-18 10:39:49,19
AMD,nph9blw,"NGL for 200 usd thus would be best budget gpu, would shit on b570",hardware,2025-11-18 12:05:12,14
AMD,npgy0i1,misread as 960 and it bought me back to the college day when the debate was RX 380 vs GTX 960,hardware,2025-11-18 10:24:48,5
AMD,npnlm1v,This is very shallow. Few games and results. I'll wait for a real test.,hardware,2025-11-19 11:56:47,0
AMD,nphv79e,You forgot the most important bit:  * **OEM-only product**,hardware,2025-11-18 14:17:25,48
AMD,npjdkfb,how is it 13-14 percent slower than the 9060 xt and 5 percent faster than the 5060 when the 5060 is only about 5 percent slower than the 9060 xt to begin with.  [https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219](https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219)  edit: nvm I see only a 7 game sample size so this data is not really useful,hardware,2025-11-18 18:46:47,2
AMD,npheci3,"Depends on price. Word on the street is that AMD gave Valve a reaaally good deal on 7600 dies.   Essentially it appears that AMD produced way too many Navi 33 dies, using an older node (6nm) to make them cheaper and more plentiful, expecting to put a lot of them in laptops. Apparently, OEMs preferred the more expensive (and more efficient) 4060 instead, and that left AMD with lots of Navi33 (RX 7600) dies on their hands.  Talking to AMD Valve probably saw an opportunity to take this excess capacity for cheap, and AMD saw an opportunity to recoup some cost, rather than risk throwing them away. I can imagine Valve getting those RX 7600 cards for maybe half the price of what a 9060XT would be (depending on memory prices).",hardware,2025-11-18 12:41:03,49
AMD,nphf55x,"Probably would have cost $100 more, but I think it would have been worth it. Almost PS5 Pro performance would have looked better. Plus better upscaling.",hardware,2025-11-18 12:46:22,8
AMD,nphv1nx,"All is forgiven if the steam machine is under $500. Anything more, and yeah, i agree.",hardware,2025-11-18 14:16:36,12
AMD,nphfypk,"Would have made a lot more sense.  Would have been first of it's kind though, no ODM with existing designs to produce the board, AMD clearly do not want this chip in anything other than add in boards etc.  It's strange since it seems like it would be a good chip for laptops, mini PCs etc.",hardware,2025-11-18 12:51:41,4
AMD,nphatq8,"Exactly, the Machine could have been ever so slightly bigger or fan slightly noisier to accommodate +20W TDP.",hardware,2025-11-18 12:15:56,2
AMD,npiikvj,"It probably will be in the OEM ones, maybe Lenovo makes one.",hardware,2025-11-18 16:16:32,1
AMD,npha935,AMD should just make a 9060 16GB variant,hardware,2025-11-18 12:11:45,8
AMD,nph0q95,Amds focus on high bin output this gen made even bad batches selleable quantity. Shame amd never allocated much production for this gen for AI.,hardware,2025-11-18 10:51:38,-2
AMD,nphn6ra,Yup $199 shud be perfect,hardware,2025-11-18 13:34:32,8
AMD,nphytqk,Back in the good old days...,hardware,2025-11-18 14:36:27,3
AMD,npiiiig,An OEM steam machine with phone of these would be sweet but probably expensive.,hardware,2025-11-18 16:16:13,11
AMD,npmkb70,Thx added.,hardware,2025-11-19 05:55:30,2
AMD,nphivh9,"It's Navi 33, but with 28CUs it's more like an [RX 7600M](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7600m.html#amd_support_product_spec) than a [desktop RX 7600](https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7600.html)",hardware,2025-11-18 13:10:06,26
AMD,nphm87p,So the new Steam Machine is a PC McRib?,hardware,2025-11-18 13:29:12,35
AMD,nphiydn,"Someone on this platform tried to convince me that valve is willing to spend crazy r&d money on AMD to get a custom GPU solution akin to strix halo for the steam machine.  I told em valve is smart financially and will never do that and will just do the same exact thing as the steamdeck, pick an existing chip that AMD needs to dump off at an exceptional price. The 7600 system that's leaked will be what you see and what you get.  Well well it's almost as if that particular redditor does not have any business experience at all and valve is a multi billion dollar for multiple reasons including being financially smart.",hardware,2025-11-18 13:10:34,36
AMD,nphq8pw,"The issue is that 3060 performance with outdated upscaling, severe lack of machine learning capabilities meaning no FSR4, No FSR Redstone and very outdated RT capabilities is not exactly a great look for what will sell as a brand new machine in the year 2026.  It really needs to be priced very low to be competitive.",hardware,2025-11-18 13:51:01,20
AMD,npivu2r,"i dont think its full fat navi 33   that is 32 CUs, this thing is 28 CUs  so its rejects on top of navi 33 rofl, IE 7600M/7400 class.  now, if that is fulfilled with actual salvage or cut down from fully functional that is another question.",hardware,2025-11-18 17:21:15,3
AMD,nphmxb5,Made the most sense,hardware,2025-11-18 13:33:05,2
AMD,nphvvgx,Sure if its a really good price it makes sense..,hardware,2025-11-18 14:20:58,2
AMD,nplndx9,"And for this, I expect Steam Machine to be in a reasonable price range. It’s the same for both Machine and Frame, there are very clear signs of compromises for cost purposes that Valve didn’t take for something like Index. The whole product was planned with cost in mind and while I don’t expect a subsidized price, I don’t think it’s going to be too expensive.",hardware,2025-11-19 02:09:10,2
AMD,npjey5g,I too watch MLID 😆,hardware,2025-11-18 18:53:25,1
AMD,npkihpg,"They're still selling 6600 new. And for some reason, 7600 never dropped to 180$ like 6600 did now almost 2.5 years ago...",hardware,2025-11-18 22:11:24,1
AMD,npn8p69,of course they did. They would have to throw away the 7600m dies otherwise because noone wanted them.,hardware,2025-11-19 09:56:58,1
AMD,npi4twa,Eh. I'm betting on $600 for the entry level version and $750 for the 2TB version.,hardware,2025-11-18 15:08:21,9
AMD,npi1fzx,"How is this even possible. The individual components today cost more than $500, not to mention the 2TB option will be easily more expensive than it was when the Steam Machine was first announced.",hardware,2025-11-18 14:50:28,-4
AMD,nplgv05,The fact that they’re using salvaged Navis means they’re selling jack shit 7600m dies. Which is to nobody’s surprise given how few laptops exist with 7600m. Oems do not want amd dgpus on their laptops.,hardware,2025-11-19 01:30:22,3
AMD,nphel2a,This would make the SM $100 extra and for sure the CPU would have been a bottleneck for this card.,hardware,2025-11-18 12:42:41,1
AMD,nphu99v,Why?   This is a 1080p card.   8GB is sufficient for its performance.,hardware,2025-11-18 14:12:23,7
AMD,npio2rf,think thats why they did the rx 7600M,hardware,2025-11-18 16:43:12,7
AMD,npjgqzl,"That's true. OEMs actually want this at least a little, unlike the old 7600m",hardware,2025-11-18 19:02:11,6
AMD,npho41z,"Exactly. AMD apparently printed lots of mobile 7600 dies, and sold jack shit. So now they are trying to get rid of them",hardware,2025-11-18 13:39:36,22
AMD,nphr48b,"Yep, appears to just be a 7600M with +20W TDP.",hardware,2025-11-18 13:55:40,15
AMD,npi6dqv,Also AMD still doesn't have any mobile RDNA4 parts,hardware,2025-11-18 15:16:21,6
AMD,npiits3,RX 7400 too.,hardware,2025-11-18 16:17:44,2
AMD,npi0d5n,"McRib with surge pricing, yes",hardware,2025-11-18 14:44:41,19
AMD,nphtxcu,They never said it used an APU.,hardware,2025-11-18 14:10:38,-8
AMD,npiis04,Kek Strix Halo is expensive AF. And if would be RDNA3(.5),hardware,2025-11-18 16:17:29,9
AMD,nphqsud,"I saw an interesting video yesterday where the guy tested a Steam Machine-like config (6 cores, RX7600) to see how it performed and yeah... not good when RT involved at advertised 4k, Indiana Jones was unplayable.  Even non-RT loads required very low settings.  New games are going to really test that hardware, I think RT is a total non starter on it as well.",hardware,2025-11-18 13:53:59,19
AMD,npjhzc5,Exactly.  I wouldnt consider a 3060 useable without dlss working wonders.  Especially at 4k where you need dlss performance or ultra performance.  Trying to do that without a proper upscaler is going to produce hideous results.,hardware,2025-11-18 19:08:18,2
AMD,nphtq7s,"$1000 also isn't a great look for a console killer. Between Sony releasing the PS5 Pro at a really steep price ($750/€800), both Microsoft jacking up the Xbox Series price (X for $650) and Sony doing the same (PS5 for $550) and Nintendo charging quite a bit more for the Switch 2 ($450) I'd say there's an opening here for a Steam Machine to enter the console market.  If Valve can release something that can compete with these consoles, at a similar price, with a similar feature set, but with also all the advantages of being a computer, then it's definitely worth it. Going from $1000 to $700 will open a huge market for Valve. Going to $600 would be the nail in the coffin for Xbox.   No online fees, lower game prices, your game library will never expire, infinite game catalogue, and you can use it as your main PC as well, for the same price as a normal console... Not a bad sales pitch.  BTW, lots of people are using a Steam Deck as their main console. If millions are happy with Deck performance, this is more than enough. If you want better, build your own, unlike with classic consoles, you can easily do it now.",hardware,2025-11-18 14:09:35,0
AMD,npji68j,I doubt there are enough defective ones to be all salvaged. It'll be like the consoles where they're cut back to ensure as many dies can be used as possible,hardware,2025-11-18 19:09:14,7
AMD,npndw4g,"I'm hoping $600, unless RAM prices fuck it all up and it becomes $700-800",hardware,2025-11-19 10:48:48,1
AMD,npiepin,Valve isn’t buying individual components at retail price.,hardware,2025-11-18 15:57:42,9
AMD,npimr8n,More and more games demand RT and even software RT is vram heavy. Doom the dark ages can run much better on the 7600xt compared to its 8gb version even though they're the same card with only mild clock differences,hardware,2025-11-18 16:36:48,3
AMD,nphvqbz,"If the 9060XT can utilize 16GB, so can a 9060. The performance difference is imperceptible without a framerate counter.",hardware,2025-11-18 14:20:14,2
AMD,npi41fp,"Plus, much easier to integrate into a smaller form factor than shrinking down a desktop GPU. The Steam machine is basically a screen-less laptop hosting a Ryzen 7640 (without iGPU) plus an RX 7600M, but with a huge-ass heatsink and fan to properly allow for higher clocks.",hardware,2025-11-18 15:04:12,13
AMD,npiht8y,It's honestly hilarious. AMD will have zero FSR4 compatible mobile product at all until 2027 without hacky aftermarket solutions. Mind you laptop sales began to outpace desktop about two decades ago.,hardware,2025-11-18 16:12:47,7
AMD,npmu9g3,Remember KFConsole,hardware,2025-11-19 07:27:43,3
AMD,npk21pv,"Some people are just that delusional, best case scenario AMD gives valve extremely faulty strix halo chips that are cut down by more than 50%. That individual straight up said it will be fully custom.",hardware,2025-11-18 20:49:10,7
AMD,npii1iu,"Except it's not even a desktop 7600, it's a mobile 7600m. Another 4CU cut.",hardware,2025-11-18 16:13:55,7
AMD,npjiqjm,"I think this thing makes no sense at all.  Especially since you re not just buying a new low end gpu to put into an old pc for cheap to give it some new life.  No , you re buying cpu, gpu, ram, ssd, mobo, case, power supply  Thats a ton of money.  And the performance wont justify the price at all.",hardware,2025-11-18 19:12:02,2
AMD,npjhoio,"The 4k claim is really weird. On old games, sure. But am 8GB 7600 is a 1080p GPU, which isn't even that bad on a TV, especially since upscaling 1080p content is kind of a TV's bread and butter",hardware,2025-11-18 19:06:48,1
AMD,npnkqvm,"599 is ideal but I’d tolerate 699. Speaking of the RAM shortage, I get reminded of how Valve launched Steam Deck during the worst of COVID shortage, so I am somewhat hopeful",hardware,2025-11-19 11:49:55,1
AMD,npkcmmp,"Cool, but individual components involve free assembly by you, steam machine needs assembling and to make money.",hardware,2025-11-18 21:41:42,-1
AMD,npi7vxo,"Slapping the chip from the RX 9060/XT onto a laptop style motherboard would not be technically any more difficult than using the 7600M chip. The 7600M is itself just a slightly cutdown, underclocked desktop RX 7600 chip.  Both chips are physically pretty much the same size (200 mm2 give or take a few) and the desktop versions have very similar power usage.   The only reason to use the RX 7600 over the RX 9060 chip in this application is cost.",hardware,2025-11-18 15:23:58,15
AMD,npin135,"Yeah he mentioned that... result will be worse. 4K60 is going to be a very limited thing, marketing beware etc but I hope people are prepared for a little/lot of choppiness.",hardware,2025-11-18 16:38:08,11
AMD,nplg6z6,"Because of frame gen and upscaling, they can say it’s any output resolution any output frame rate and it’s technically not lying. It’s a useless piece of info without details.",hardware,2025-11-19 01:26:20,3
AMD,npkwtoj,"The steam machine doesn’t need to make money. The steam machine needs to get people to play PC games, which steam has an effective monopoly on. If Valve sells the steam machine at cost they’ll make money.",hardware,2025-11-18 23:31:41,2
AMD,npiat7e,"I agree, but it's still more work than just slapping the existing laptop board that's already engineered with the proper footprint, connections, inputs and outputs in there, because AFAIK there's no RDNA4 laptop parts at all and the most we have are the RDNA3.5 GPU in the PS5 Pro and the AI APUs",hardware,2025-11-18 15:38:31,5
AMD,npinvbn,Any corporate mention of resolution and framerate is completely meaningless now because it only refers the output. It could be achieved by upscaling from 540p and with 4x frame gen (amd doesn't use it atm) and it still wouldn't be technically lying.,hardware,2025-11-18 16:42:12,5
AMD,npll15b,Frame gen and upscaling can't solve a lack of VRAM in many games either though (frame gen makes it even worse). Unless they're talking about FSR1 lol,hardware,2025-11-19 01:55:20,1
AMD,npqdb29,"Yes it does, it can't be loss leader because otherwise people will just buy it for office use. They said it themselves.",hardware,2025-11-19 20:53:44,0
AMD,npqumrv,"Who is buying a steamOS machine for office use? Which office is going to bother buying this instead of any of the hundreds or thousands of offerings from T1 SIs that actually provide support and service? This thing doesn’t even use standard components, are you even going to get official drivers on Windows?",hardware,2025-11-19 22:21:52,3
AMD,npt4qet,"They promoted the steam machine as being able to install other OS. If they don't support it then I suppose at least it would be really funny for how much people glazed them for it.   There will be companies buying it if it's cost effective due to valve eating costs. Hell, even if they don't, valve just have no control over it's userbase, it could have a really low acquisition rate from non steam users, just like steam deck, so all that loss leading would be for nothing.  If you are right then it's a pc, that you can't upgrade, that can't play hugely popular games like league of legends, GTA, battlefield or valorant. Device designed not for casuals and not for enthusiast. Just people who are okay with paying a lot of money to play old, locked in time, singleplayer games on TV. It has to support windows.",hardware,2025-11-20 07:30:05,-1
AMD,nptbykt,"No sizable company is buying a ton of computers to replace the OS on them, especially not for this level of hardware. No normal user is buying a Steam Machine running SteamOS from Steam because they really want a Linux desktop.  The problem with charging a PC price instead of a console price is that it’s just worse than an actual gaming PC. Even with all the advances Valve has made to Linux gaming you still can’t play a lot of popular multiplayer titles, and hardware wise it’s essentially a 5-year old console. There’s just no justification to pay more than $500-600 for one of these.",hardware,2025-11-20 08:43:03,3
AMD,nrti3rj,"RTX 5070 is now 11th of all GPUs with its 2.28% share.  Top 10 Monthly gains:  1. NVIDIA GeForce RTX 5070: 2.23% (**+0.35%**) 2. NVIDIA GeForce RTX 5060: 1.62% (**+0.32%**) 3. NVIDIA GeForce RTX 5060 Ti: 1.20% (**+0.21%**) 4. NVIDIA GeForce RTX 5070 Ti: 1.14% (**+0.19%**) 5. NVIDIA GeForce RTX 5080: 1.01% (**+0.15%**) 6. NVIDIA GeForce RTX 4060 Laptop GPU: 4.44% (**+0.14%**) 7. AMD Radeon RX 7800 XT: 0.88% (**+0.13%**) 8. NVIDIA Graphics Device: 1.16% (**+0.12%**) 9. NVIDIA GeForce RTX 4050 Laptop GPU: 1.54% (**+0.07%**) 10. AMD Radeon RX 7600 XT: 0.46% (**+0.06%**)  No 5050 in the charts just yet. The 50 series card, ordered by share:  1. NVIDIA GeForce RTX 5070: 2.23% (**+0.35%**)  2. NVIDIA GeForce RTX 5060: 1.62% (**+0.32%**)  3. NVIDIA GeForce RTX 5060 Ti: 1.20% (**+0.21%**)  4. NVIDIA GeForce RTX 5070 Ti: 1.14% (**+0.19%**)  5. NVIDIA GeForce RTX 5080: 1.01% (**+0.15%**)  6. NVIDIA GeForce RTX 5060 Laptop GPU: 0.91% (**0.00%**)  7. NVIDIA GeForce RTX 5090: 0.36% (**+0.03%**)  8. NVIDIA GeForce RTX 5070 Ti Laptop GPU: 0.26% (**+0.06%**)",hardware,2025-12-02 02:31:12,49
AMD,nruhg3s,"I'm surprised to see RTX 5080 so high up already. It's above 4080, 4080S and even 4090.",hardware,2025-12-02 06:41:45,13
AMD,nruq8da,Props to the peeps still rocking functional DX9 GPUs.,hardware,2025-12-02 08:04:51,14
AMD,nrtqjn4,">Interestingly, AMD’s RDNA 4 is still missing from the survey results.  As is the Secure Boot/TPM metrics that Valve started collecting.",hardware,2025-12-02 03:21:31,32
AMD,nrtluxo,"Here before the tide of ""why no survey on my amd pc, I'm not saying anything just curious ;)"".  How hard is to accept that Nvidia is just that dominant in the consumer space.   By that same logic the current popularity of ryzen cpus in the survey is a sign of AMD bribing Steam and not that their products are just that much better.",hardware,2025-12-02 02:52:53,106
AMD,nruttc1,"Why are there multiple entries that just say ""AMD Radeon(TM) Graphics"" or ""AMD Radeon Graphics"" ?",hardware,2025-12-02 08:41:16,10
AMD,nrvxqdk,"almost 10% of user running 8GB or less of **system** RAM in the year 2025, may god have mercy on their souls.",hardware,2025-12-02 14:06:40,9
AMD,nrys6j3,"So when people talk about how Valve is out of touch with pc specs when announcing the Steam Box, just remember that they have access to the majority of the system specs from all Steam users. They're not gonna just wing it and guess.",hardware,2025-12-02 22:30:22,4
AMD,nrtz732,5070 was a disappointing upgrade from 4070 or 3080 tier cards but is pretty good value for people new to PC gaming or coming from weaker cards. The 5060 is pretty wack with 8GB of VRAM but it's all a lot of people are willing to spend on a GPU.,hardware,2025-12-02 04:18:03,11
AMD,ns7yw9p,"given johnpeddie research showing AMD market share falling further to 7%, im not surprised RDNA4 still isnt showing up.",hardware,2025-12-04 09:40:35,2
AMD,nrwqj0w,">Interestingly, AMD’s RDNA 4 is still missing from the survey results.  I just checked and at least my 9070XT is reported as ""AMD Radeon(TM) Graphics"" so that would explain that.",hardware,2025-12-02 16:35:15,5
AMD,nrv5svk,"> The RTX 5070 remains the leading GPU in the Blackwell lineup, holding 2.28% of all surveyed systems.  Huh, so so some people have begrudgingly accepted Jensen's ""4090 performance for $549"" after all.",hardware,2025-12-02 10:41:55,-5
AMD,nrwukax,this is opt in right? ive not had the question in years after rejecting it back in the day,hardware,2025-12-02 16:54:24,-2
AMD,nrufv9u,"This 5070 is desktop + mobile?   I assume mobile play a huge part of it. 5070 mobile is pretty ""unlderwhelming"" with only 8GB vram but the price is also not bad. Many buying mobile probably also opt 5070 since the price diff is as small as $150 on a $1200-$1500 laptop.",hardware,2025-12-02 06:27:35,-4
AMD,nrtjmnn,"interesting, back during thanksgiving my nvidia gaming laptop ran the hardware survey once again, but my AMD main gaming rig hasn't ran the survey in what feels like years  at this rate speaking from my personal experience, I think they have a ""bug"" with registering AMD hardware.",hardware,2025-12-02 02:39:56,-45
AMD,nrth6d2,"Can't wait to see ""AMD Radeon(TM) Graphics"" reach 5%+ Steam market share in 2 yrs",hardware,2025-12-02 02:26:09,-26
AMD,nrtksxh,"AMD CPUs (42.61%) have gained another 0.52% share while Intel (57.30%) has lost the same percentage.  For RX 9000 GPUs, If you sort only by Linux:  * AMD Radeon 9070/XT/GRE: 1.46% (**+0.27%**) * AMD Radeon 9060 XT: 0.56% (**+0.15%**)  If you sort only by Windows DX12 Systems:  * AMD Radeon 9070: 0.13% (**+0.02%**) * AMD Radeon 9070 GRE: 0.03% (**+0.01%**)  They do not appear in the overall GPU percentages likely due to them being lower that 0.15%.",hardware,2025-12-02 02:46:42,52
AMD,nru80nx,>NVIDIA Graphics Device: 1.16%    What is this? DGX Spark,hardware,2025-12-02 05:22:14,13
AMD,nrtrq3o,No way ppl bought more 7800xt than 9070xt....,hardware,2025-12-02 03:28:52,-9
AMD,nrut8k0,to be fair the non-super 4080 was dogshit value,hardware,2025-12-02 08:35:12,14
AMD,nrv0hd1,Blows dust of my 2011 laptop to fire it up to find some old files only to be presented with the steam survey 🤭,hardware,2025-12-02 09:49:37,13
AMD,nryr0an,"There are people with purpose-built PCs for playing older games (e.g /r/retrobattlestations/), but a large amount of them shouldn't be appearing in this survey since it looks like Valve isn't tracking WinXP & older anymore or the PCs are no longer connected to the internet. (Such is the case with my own WinXP machine)",hardware,2025-12-02 22:24:16,3
AMD,nrtpj7v,Everything is a conspiracy to AMD bros.,hardware,2025-12-02 03:15:12,89
AMD,nruymzh,"Taking the lead on Steam hardware survey takes years and years. AMD CPUs are consistently blowing Intel's out of the water since 2022 and still Intel is in the lead. Taking that into consideration, simply stop losing market share on GPUs should be considered a success for AMD this generation.",hardware,2025-12-02 09:30:44,21
AMD,nrtshen,"It's a pure echo chamber full of gaslighting people there on r/Radeon I have even noticed that they often downvote any criticism about AMD Radeon there especially with the ongoing driver issue on their part.  Yes, AMD Radeon is currently suffering with [multiple driver issues](https://www.reddit.com/r/radeon/comments/1pasyy1/do_we_think_amd_is_aware_of_the_huge_amount_of/) and there are plenty of user posts there about it, they just downvote it to oblivion and gaslight people who has issues into thinking that it isn't AMD's fault, and that it is users fault.  Instead of helping the users and voicing out the issue Radeon currently is experiencing with driver issue, they still [defend](https://www.reddit.com/r/radeon/comments/1pat6x8/comment/nrlmbmj/?context=3) them and gaslight people, no wonder why Radeon Team Group got used to not releasing quick fixes as quick as Nvidia which just recently released [Hotfix driver](https://www.reddit.com/r/nvidia/comments/1p31xf3/the_hotfix_driver_is_amazing/) that solved the issues that Windows Update has brought up recently.",hardware,2025-12-02 03:33:47,29
AMD,nru4hnw,Intel is still leading in the surveys btw,hardware,2025-12-02 04:55:38,4
AMD,nrutp84,"I'm a 9070XT owner and hater, I couldn't really care less, but I did not get any survey on my desktop yet.   I really don't have good things to say about my 9070XT, my playthrough of Alan wake 2 was ruined because of stuttering that affects AMD cards.",hardware,2025-12-02 08:40:04,1
AMD,nrufnoh,Anyone else feels that they encountered more nvidia bros acting as the victims of unreasonable amd bros than the thing they complain about?  Like this post implies that they are completely delusional and deny the survey as fake or something and I genuinely never seen it.  But here we have this guy and the top replies to that feel like they are salty because they live under amd bros thumb and really want to talk about it.,hardware,2025-12-02 06:25:44,-5
AMD,nrts9yj,No their logic is how is anyone still using intel cpus in their computers? Marketshare should be 80% amd 20% intel,hardware,2025-12-02 03:32:25,-7
AMD,nruf0zi,Nvidia is like the only company left still care about gaming unlike AMD/Intel chasing that AI money.,hardware,2025-12-02 06:20:19,-5
AMD,ns1bbqe,I mean someone above who got the survey said their 9000 series (AMD) GPU was not detected.,hardware,2025-12-03 08:43:51,-1
AMD,nruymwt,"Different versions of integrated AMD GPUs, most from laptops.",hardware,2025-12-02 09:30:43,22
AMD,nrvb5sn,"There's also RDNA2 based extremely basic 2CU iGPUs in all Zen 4 and 5 Ryzen CPUs apart from the f skus. I think mine is just called some sort of Radeon graphics anyway, no specific name to it. It's literally supposed to be basic display adapter iGPU, so businesses can use them or you can get your PC going before buying a GPU, or temporarily sell a GPU and still have a PC for web browsing and desktop tasks.  One of them might be those, not sure but it'd make sense with Ryzen's marketshare.",hardware,2025-12-02 11:30:29,11
AMD,nrw2y3b,I got the survey yesterday and it reported my 9070 XT as exactly that. I tried to see if I could edit it but could only click accept.,hardware,2025-12-02 14:36:20,1
AMD,nrw6snk,Steam isn't just for AAA games. There are many casual and older games that run just fine on garbage hardware.,hardware,2025-12-02 14:57:22,10
AMD,ns81iye,This makes it even crazier how out of touch they are. Half the systems in this survey are already more powerful than the steambox.,hardware,2025-12-04 10:06:28,2
AMD,nrvhafg,A lot of hardware reviewers obviously struggle to see this perspective. If you had a 1070/1080 or even something as fast as a 2080Ti then the 5070 is a very obvious and decent upgrade.,hardware,2025-12-02 12:20:08,12
AMD,nru459h,"I'm a new RTX 5070 owner, coming from the 7-year-old RTX 2070.  Gen on gen upgrade hasn't been very impressive with Nvidia recently (RTX 30 series was the last impressive one), but their DLSS has really added a lot of life into older cards like the RTX 20 series, so we could hold on to our cards longer without upgrading.  My 2070 could honestly still play a lot of demanding games at 1440p 60 FPS with DLSS 4 (and with settings turned down), and it still looked great for how old the card was.",hardware,2025-12-02 04:53:06,18
AMD,nru8hwd,5060 is just a Blackwell 3070 and the 3070 is still a potent card today. That and it bodies the PS5 at everything except VRAM.,hardware,2025-12-02 05:26:00,9
AMD,nrvh1bk,"Or just evaluated it to be the best performing card at its price point, which is correct. YouTubers and redditors be damned.   I saw them at £460 this week, you can't beat that.",hardware,2025-12-02 12:18:10,13
AMD,nrvaxav,Average buyer didn't even know or never believed the marketing. 5070 is good if you're coming from turing or 3070 sub.,hardware,2025-12-02 11:28:26,16
AMD,nrv7m08,"The 5070 is pretty good value tbf. It's a little weaker than a 9070, but also sells well below MSRP, which results in similar price/perf. It's also more common in prebuilts. It's also a really big step over the 5060 Ti 16GB (which sells at MSRP) for only a small price difference.  The uninformed buyer gets a 9060XT, 5060, 5060Ti or 5070 prebuilt. As a result the 5070 gets decent market share.  Even as a pc builder myself, the 5070 is appealing in the Swedish market, and if I were in the market for a GPU right now, I'd definitely consider it. It's about 1500 sek/160$ cheaper than the 9070 right now, or 2000SEK/215$ cheaper than a 9070 XT. (5190 SEK vs \~6700 SEK vs \~7200 SEK, or \~550 USD vs \~710 USD vs \~765 USD)",hardware,2025-12-02 10:58:51,10
AMD,nrvbtcy,"5070s a good card and frame gen pulls its weight depending on games. It was never a bad GPU, just poorly marketed. People KNOW they're not getting a 4090, but I think it is still a viable upgrade for many people for $550 ($450 or less for black Friday). Seeing that the average is a 3060, you're essentially doubling performance or more.  My biggest gripe with the GPU was the 12GB (should have been 16GB). Games like Spiderman 2 hit that limit with ray tracing even at 1080p without DLSS. I felt like I was paying a lot for a card that was obsolete out of the box. Otherwise it was crushing everything I was throwing at it.",hardware,2025-12-02 11:36:07,6
AMD,ns81o9q,if you rejected it then it will remember your rejection and wont query you again. Same if you agreed. This should remain consistent until you reinstall steam.,hardware,2025-12-04 10:07:54,2
AMD,nrvhwky,"5070 mobile is a different chip and reports to software differently. It is ""Nvidia Geforce RTX 5070 Laptop GPU""  It's not present in this list because presumably it has proven to be quite unpopular. You may as well just get a 5060 or even a 5050 mobile laptop.",hardware,2025-12-02 12:24:43,10
AMD,nrtnlet,"Doesn't really explain the CPU numbers though, does it? For all the legitimate statistical conversations you could have about how Valve goes about sampling for these surveys, they've very clearly been able to chart the success of Ryzen.",hardware,2025-12-02 03:03:17,38
AMD,nru696m,Redditor discovers what random sampling is.,hardware,2025-12-02 05:08:46,26
AMD,nrtqq6l,LOL,hardware,2025-12-02 03:22:37,19
AMD,nrvbj9e,"My PC July 2023, 2024 and 2025 have gotten the survey as the same RX 6950 XT and Ryzen 7600X build the entire time. It's just the survey being random with what accounts get the survey and how they get it, me seemingly having a predictable schedule while others are completely random.",hardware,2025-12-02 11:33:43,3
AMD,nru57gf,"It's weird, I rarely use my Ally or laptop but definitely seem to see a hardware survey when booting them up more often then I ever do on my main PC. I figure it's just them getting a data point on what seems like a newly connected computer or something",hardware,2025-12-02 05:00:58,2
AMD,ns81ria,if you agreed to the survey steam tends to remmeber that and dont show you the popup again.,hardware,2025-12-04 10:08:47,1
AMD,nrthwql,"AMD Radeon(TM) Graphics increased 0.01 compared to october and AMD Radeon Graphics actually decreased 0.013, surely at some point you'll see how that conspiracy theory does not make sense.",hardware,2025-12-02 02:30:07,27
AMD,nru0jfy,You'll see another generic amd Radeon entry in steam stats and another rename of their gpu lineup in two years.,hardware,2025-12-02 04:27:19,7
AMD,ns8232q,integrated graphics were always a popular choice for prebuilds.,hardware,2025-12-04 10:11:54,2
AMD,nruaxbc,Glad the 9070GRE was not canceled,hardware,2025-12-02 05:45:29,10
AMD,nru0n98,Quite suspicious that 9070 **XT** doesn't show up on Windows DX12 systems at all. Are we to believe that it sold less than the China-only GRE?,hardware,2025-12-02 04:28:03,16
AMD,nrw0nrl,My guess is Nvidia cards from people who for whatever reason do not have proper drivers installed.,hardware,2025-12-02 14:23:24,15
AMD,nruuc0f,No way it's DGX Spark. Just launched and it's $4000.,hardware,2025-12-02 08:46:36,15
AMD,nrwbf5z,FX5200 gang reporting for duty.,hardware,2025-12-02 15:21:46,6
AMD,ns7z2kc,this is people with Nvidia GPU who havent installed the correct driver so it shows up as generic graphics device,hardware,2025-12-04 09:42:21,3
AMD,nrtsgt3,"There's a time when 7800xt can be bought near $400, iirc 430 or 440. That's probably the reason why it had decent ownership share.",hardware,2025-12-02 03:33:40,39
AMD,nrtsbrw,Clearance sales for Rx7000 (which we should be running out slowly) and high prices for Rx9000 and lower availability until a few months back likely heavily damaged the initial sales.,hardware,2025-12-02 03:32:45,16
AMD,nruawom,People who already have 7800xt are not upgrading to 9070 xt,hardware,2025-12-02 05:45:21,20
AMD,nru4m41,"You couldn't get a 9070xt at MSRP a week after launch when MSRP stock dried up, up to maybe a month ago. The launch price was a lie. It was supposed to be $150 cheaper than a 5070ti, when it was more like only $80 cheaper most of the time. And the 7800 XT was for sale for 3x as long. Like 8 months vs 24 months.",hardware,2025-12-02 04:56:33,12
AMD,nru2noy,Still a good enough card.,hardware,2025-12-02 04:42:27,5
AMD,nruqa1i,"People are simply in denial. 5080 released on 30th of January 2025, 7800XT did so on 6th of September 2023, casual 17 months earlier, but they are gaining market share at almost the same rate. Totally normal, nothing to see here.  Month | 5080  | 7800XT ---|---|----|---- 2025-02 | ≤0.14 | ≤0.14 2025-03 | 0.20 | ≤0.14 2025-04| 0.38 | 0.28 2025-05 | 0.47| 0.37 2025-06 | 0.57 | 0.48 2025-07 | 0.65 | 0.56 2025-08| 0.74| 0.64 2025-09 | 0.84 | 0.72 2025-10 | 0.86| 0.75 2025-11 | 1.01| 0.88",hardware,2025-12-02 08:05:19,-13
AMD,nrvcqjr,"The 5080 is also ultra dogshit value if you wanna go that route, especially compared to the 5070ti. Basically, there is no experience the 5080 offers you that the 5070ti won't for cheaper. Only the 5090 will do that, such as high refresh rate path tracing without framegen.  The 5080 is just as bad as the 4080 was, especially after how good the 3080 was.",hardware,2025-12-02 11:43:46,17
AMD,nrz4yty,> no longer connected to the internet  I hope they aren't,hardware,2025-12-02 23:40:40,5
AMD,ns1g156,I wish I could get a steam survey on my Windows 3.11 machine :(,hardware,2025-12-03 09:31:21,1
AMD,ns801y9,"steam no longer works on XP and older operating systems. This is because steam is basically a wrapper for a browser and the browser stopped supporting these OS, so steam was forced to drop support too.",hardware,2025-12-04 09:52:02,1
AMD,nru3nrp,nvidia has had driver issues all year.  Not sure why you're acting like team green is roses,hardware,2025-12-02 04:49:40,13
AMD,nru2foz,Not sure which driver update did it. But I lost all game history in the Adrenalin control panel.  I'll move back to Nvidia for my next card.,hardware,2025-12-02 04:40:51,4
AMD,nruul04,"I mean it takes one second to check Amazon or any other major retailer, after which you will see that the 9000 series is selling really well compared to prior generations. 9000 series GPUs are straight up the most popular for DIY systems in several countries, especially outside the US, where Nvidia prices are especially high.  You could also look at AMD gaming revenue in order to make the same conclusion. 9000 series is selling well. It's not Nvidia numbers, but the cards are certainly moving in decent volume compared to the RDNA 3 flop.  Another anecdote is that system integrators seem to be putting more AMD cards in their systems than they used to, likely because the 9060XT is better value than 5060 Ti. Despite that, RDNA 4 numbers on steam are still low.  You can't look at the 7600XT growing from 0.2% to 0.8% THIS YEAR with the 9060XT or 9070/XT not even showing up on the damn list and tell me the data is representative. You genuinely believe that the slow, overpriced 7600XT that is more expensive than a 9060XT has sold better than the entire 9000 series this year?",hardware,2025-12-02 08:49:08,-5
AMD,nrvc0eh,"The most damning thing that I ever encountered after buying my 9800X3D and having so many issues with it is the /r/AMDHelp subreddit. It's a subreddit with almost 200k subs that is very active. There is no such subreddit for Intel or Nvidia, because you guessed it, it just works.",hardware,2025-12-02 11:37:44,-1
AMD,nru5n24,"It lost 9 percentage points over the last year. It's at 56.4% right now.  At this rate, it'll take less than a year for Ryzen to capture a simple majority of the Steam Install base. Nova lake isn't expected till late 2026.  Potentially, Panther lake could stop the bleeding in laptops (especially since AMD is on a refresh cycle for mobile next year), but I don't see anything changing anytime soon in DIY Desktop at least.",hardware,2025-12-02 05:04:14,32
AMD,ns80rtt,">Like this post implies that they are completely delusional and deny the survey as fake or something and I genuinely never seen it.  Maybe you need to open your eyes first, because this happens in every survey. Especially if you go to AMD subreddit for example.",hardware,2025-12-04 09:59:09,2
AMD,nruj1ji,"Just check the August survey to see what I mean, my complaint and your response are ironically also cliches by now.  Before you accuse me of being an ""nvidia bro"", I have bought exclusively AMD for the past 10 years and one of the worse products AMD ever slapped its name on because of hype, so yeah I'm gonna complain about AMD until I've gotten my money's worth.    Also anyone else feels that they encountered more amd bros acting as the victims of unreasonable nvidia bros than the thing they complain about?  Like this reply implies that they are completely delusional and I genuinely never seen it.  But here we have this guy that feel like they are salty because they live under nvidia bros thumb and really want to talk about it.",hardware,2025-12-02 06:56:04,2
AMD,nru09uf,Marketshare is not Install Base.  Steam HW survey tracks the Install Base. Not everyone upgrades their hardware frequently.,hardware,2025-12-02 04:25:27,39
AMD,nruog1w,Pre-builts and laptops. Which are by far the largest part of the market.,hardware,2025-12-02 07:47:33,7
AMD,nru70zm,Because reddit and tech echo chambers dont reflect reality. Also intel are still very big in the laptop space which is a sizeable share thats you can see based on how many rtx mobile gpus users there are,hardware,2025-12-02 05:14:32,24
AMD,nrv8eab,"Anyone buying a 7600 for the price of 14600K with the idea of buying a 3X more expensive CPU in the future because of X3D is only a hypothetical, popular  online",hardware,2025-12-02 11:06:08,5
AMD,nrtxc4n,"Core 200 is quite good, even if its not as good as x3d at 1080p gaming (high resolutions its a lot smaller of a difference), and a lot cheaper.  Most people dont upgrade that often as well",hardware,2025-12-02 04:05:28,10
AMD,ns80wd1,For budget builds Intel is best bang for buck right now due to Intel CPUs being much cheaper.,hardware,2025-12-04 10:00:23,1
AMD,nrtytzk,Look at the Linux numbers. It’s overwhelmingly amd.   I do legit think there is a bug in collection on windows.  Not just amd cards but also nvidia newer cards due to a bug many of us think is happening on amd CPUs with an apu enabled. A lot of people are favoring x3d for gaming builds and the data looks weird there.   I have two desktops: a windows box with 265k and 6900xt and a Linux box with a ryzen 7900 and 9060xt. My wife has a 5900x with 7900xt and a 3700x with a nvidia 960 on Linux.   Steam last took surveys from my systems around June. That was before I got the 265k and 9060xt. (Had a 14700k in windows and arc a750 in Linux) my wife hasn’t had a survey since before she got the 7900xt last year.   I expect nvidia to win as well as Intel on cpu due to Internet cafes in Asia along with trends there on purchases but I still feel like amd and nvidia should have more share on their newer gpus.   Sometimes I wish valve did more breakdowns like what the specs are on systems with current gen CPUs and gpus. (Most popular current builds)  also builds by region.  I except a slam dunk for Intel and nvidia in Asia.  I would expect stronger market for amd in Germany and the US in part because of the Linux numbers,hardware,2025-12-02 04:15:34,-12
AMD,nruz0t9,"Doesn't explain why it's so generic. On the Intel side, it says that it is integrated graphics and even what generation it is.",hardware,2025-12-02 09:34:41,-4
AMD,nsacbvp,"And half aren't, mathematically. They aren't aiming for BIS hardware",hardware,2025-12-04 18:20:19,3
AMD,nru0xxq,"They don't collect specific CPU models though. Just check the Cores/Freq and GenuineIntel/AuthenticAMD string.  I'm quite sure the vendor share in GPUs is reliable in the same way. Specific model distribution though, WTF is the difference between Radeon (TM) Graphics and Radeon Graphics?",hardware,2025-12-02 04:30:09,-6
AMD,nrv25lv,I'm not assuming anything but in 5 years my gaming laptop has had 3 surveys and my desktop has had 0,hardware,2025-12-02 10:06:22,-1
AMD,nrulww5,I wonder if he's referring to the Steam Machine which could register under a generic AMD graphics part. But imo it won't sell that much given what we know of the pricing,hardware,2025-12-02 07:22:59,4
AMD,nru844x,Idk but China does have a lot of people and PC gaming is huge there.,hardware,2025-12-02 05:23:00,45
AMD,nrv9yw9,1.4 billion people live in China.,hardware,2025-12-02 11:20:06,19
AMD,nrupw9p,"As is usual for you you are literally making up something to fit the data.   Is it not superbly incredible to you how those ""clearance sales"" affected only 7800XT, which went from not showing up so ≤0.14% in march to 0.88% now? For comparison 7700XT went from 0.22% in march to 0.26%. 7900XTX was at 0.49% in march, it's at 0.50% now.  Casual clearance sales selling 6x more GPUs that was sold before ""clearance sales"". Totally normal.",hardware,2025-12-02 08:01:33,-12
AMD,nrur6kn,"So what's the conspiracy? That all 9000 GPUs get counted as 7800XT, even though they're showing up fine in all the other categories?   And even if that's true, they'd still be selling absolutely _awful_",hardware,2025-12-02 08:14:16,18
AMD,nrxjpq0,as someone who moved to a 5080 I held off buying for like 3 gens looked to see what was at the top of the stack so I can not upgrade 3 gens again looked at the 5090 price and said nope. 5080 it is.,hardware,2025-12-02 18:53:54,3
AMD,nrwadmh,"I mostly agree, but the 4080 was way worse value compared to the 4070 ti, considering it was $1200 at launch, whereas the 5080 is ""merely"" $1000",hardware,2025-12-02 15:16:27,3
AMD,nrugcnz,"That's not how surveying works. With random sampling, you would only need a couple thousand respondents to get 99% confidence representations of a population of millions.",hardware,2025-12-02 06:31:54,17
AMD,nruijs2,Skipped basic statistics class didn't you?,hardware,2025-12-02 06:51:35,13
AMD,nru4mpr,"The point is, when Nvidia had the driver issue, i didn't see many people on r/nvidia gaslighting the person who had the issue making them think that it's ""Their Fault"", and the issue was rightfully criticized and even got popular to the point that Nvidia had to release a fix for it as quick as possible resulting with the Hotfix driver that fixed most of the issue.  That isn't the case with r/radeon, most users are instead gaslighting the user who has problems and is defending AMD Radeon like it's their parents honour being at the stake instead of rightfully voicing out the problems and calling out AMD Radeon for it like what Nvidia users did with Nvidia.",hardware,2025-12-02 04:56:41,39
AMD,nrureot,That's the thing: When Nvidia has driver issues it's a scandal and over the news because it's that rare    For AMD it's just expected after years of it.,hardware,2025-12-02 08:16:32,18
AMD,nrudwg0,"AMD has had a terrible reputation for literally decades.  I stopped using AMD in 2013 because I had been dealing with driver issues since 2006 (it might have been ATI still). And here we’re are almost 20 years later and it’s the same story.  I haven’t had any issues with my nvidia cards, ever, but even it was “all year” that would be a relative blip.",hardware,2025-12-02 06:10:34,-3
AMD,ns8073r,Nvidia driver issues were never as bad as AMD driver issues. Just remmeber that last year AMD drivers would have gotten you VAC banned.,hardware,2025-12-04 09:53:28,1
AMD,nrvp0mz,"My 3080 recently became very unstable so I was seeking a replacement, and I very seriously considered a 9070XT. Microcenter is selling them for $580 and that is a tempting price for sure. In the end I picked a 5070 Ti, found one at MSRP and I am quite happy with it, no regrets. It would have always bothered me that I couldn't use DLSS, that was honestly the main thing. But yea, a history of mediocre drivers was another factor. Nvidia's feature set is just better, more more complete and more mature. If I were running Linux AMD would be a no-brainer though. But I'm not, for now... holding on to W10 as long as I freaking can.",hardware,2025-12-02 13:14:15,2
AMD,nruxhte,"The steam hardware survey represent an installbase, even if RDNA4 was selling well compared to RDNA3 it install rate on gamers  is not as good as RDNA3, i doubt Valve and Steam have any reason to portray RDNA4 on a negative light when they run RDNA2 GPU's and Ryzen CPU'S on the steamdeck and will run a full system on the steam machine.   There is no need to look for conspiracy theories, people may have given the series 9000 a chance but there clearly a lack of every day system users actively playing on them to show on the steam hardware survey compared to RDNA3 or RDNA2 users.   This is also not something new, back when RDNA3 launch RDNA2 cards also increased in terms of installbase which clearly shows that the average AMD GPU user looks for extreme value and will look for retailers clearing their inventory from these old cards over the newer architecture.",hardware,2025-12-02 09:18:55,19
AMD,ns80fwo,"imagine thinking amazon lets you know sale numbers :D you do realize that the top sellers list there is algorythm to increase buying from you and does not represent any real data, yes?",hardware,2025-12-04 09:55:53,3
AMD,nrvgk1a,"I think that's naive, Nvidia have had big driver problems this year.  The 50 series in general has been a real departure from the usual Nvidia experience for many. I think there's a toxic mix of Windows 24H2/25H2 and crappy Nvidia drivers involved though.",hardware,2025-12-02 12:14:28,4
AMD,nrvaq53,"Yeah Intel losing 9% to Ryzen in a single year is absolutely dire, you can just see that line in the graph keep dipping in favour of AMD. It suggests new CPU marketshare that Ryzen is absolutely curbstomping Intel's offerings for Intel to still be bleeding so quickly even as the install base begins to approach 50/50. Ryzen is DEFINITELY going to get a majority sometime 2026, no way it isn't happening especially if Zen 6 comes that apparently will be on a new node with 12 core CCDs it'll be a good generational upgrade itself, Intel really won't be able to catch up, maybe keep up from falling behind worse but yeah AMD has the leadership right now in new CPUs. Zen 5 is already significantly faster in gaming and Zen 6 is going to extend it well over Zen 5, vs Zen 4 to 5 where it was Zen 5% and Intel Core Ultra flopping with less gaming perf and still less efficiency along with then recent problems with previous generations made Intel miss their opportunity hard.",hardware,2025-12-02 11:26:42,9
AMD,nru8v7y,"for laptops I really hope AMD’s Strix Halo succeeds, the 395+ is all I’ve ever wanted in a laptop with the powerful 8060S gpu",hardware,2025-12-02 05:28:58,-2
AMD,nrx5rv7,>Just check the August survey to see what I mean  [This?](https://www.reddit.com/r/pcmasterrace/comments/1n6b3dq/steam_hardware_software_survey_august_2025/)  What there should explain your take?  Also why being vague with what you bought?,hardware,2025-12-02 17:48:29,-1
AMD,nru3e9o,"Yes, but by region, it's not going to show up like we see here.  China is big on nvidia + intel.  They don't run other stuff.  A few other asian countries tend to skew that way too.    Linux numbers are going to be enthusiasts, but also more US and Western European countries, which are more likely to run AMD hardware. (also a bit of south america and russia on the OS side, but I don't think valve is in russia)        My open source OS downloads tend to come from the US, Germany, UK, Spain, Canada, and Brazil the most.",hardware,2025-12-02 04:47:45,1
AMD,nrtyy4y,It also just doesn't really matter unless you have a 5090 or want to play competitive games at incredibly high framerates. Tons of people with high end GPUs are still on 5900x/5700x3d/5800x3d.,hardware,2025-12-02 04:16:20,7
AMD,nru3jel,"At 4k, it's rather similar to the 14700k I had previously for gaming, but much faster for compiling software.",hardware,2025-12-02 04:48:48,4
AMD,nrultol,"Last time I checked, the Intel 285K was just as much slower at 4K as at 1080p in EU5...",hardware,2025-12-02 07:22:08,1
AMD,nru1ox3,>Look at the Linux numbers. It’s overwhelmingly amd.  Filtering by Linux only:  AMD AMD Custom GPU 0405 13.57%  AMD Radeon Graphics (RADV VANGOGH) 12.46%  This is Steam Deck. More than a quarter of the Linux Steam userbase is basically tied to the AMD hardware by Valve themselves.,hardware,2025-12-02 04:35:28,18
AMD,nru7gv8,The line of thinking that there must be a bug because the numbers dont line up with preconceived numbers pulled out of amd fanbase asses is just insanity lol. And the same people will eat up some German retailers sales numbers like its the holy scripture,hardware,2025-12-02 05:17:56,8
AMD,nruodcz,I assume it's because the linux install base will be biased towards people who also build their own PCs. Whereas the windows base will include loads of off the shelf laptops which Intel still leads in CPU marketshare.,hardware,2025-12-02 07:46:50,1
AMD,nrvhjl3,"That is AMD's choice, it's just how they present their card to whatever API Valve are probing it with.",hardware,2025-12-02 12:22:03,24
AMD,ns8199e,"Its up to the driver developer to make sure the GPU shows up with unique name. Often this is not done and is shown with a generic name. Steam therefore cannot know which iGPU it is. Intel has been pretty good at identifying theirs for a while now, but in the past (say 10 years ago) almost all of them showed up as generic intel graphics too.",hardware,2025-12-04 10:03:52,2
AMD,nsbehfm,"I dont know what BIS hardware is, but if they are releasing a new product thats already worse than half of the depreciating stock then they are releasing a product thats obsolete on arrival.",hardware,2025-12-04 21:29:56,1
AMD,ns81v1u,"different iGPUs with different names in driver, where driver maker did not bother to report exact model to the directx API.",hardware,2025-12-04 10:09:45,1
AMD,ns821k3,"in over a decade i had survey on desktop 3 times, on laptop 2 times and on my server machine (where steam exists purely for social features) 1 time. Sampling seems fine. But half of my reports are from machines that dont actually play games.",hardware,2025-12-04 10:11:29,1
AMD,ns824xs,"steam deck reports as separate GPU device, i dont see why steam machines cant.",hardware,2025-12-04 10:12:24,2
AMD,nrurxhy,"It also affected the 7600XT. It went from 0.2% in June to 0.46% in November.  Idk if you lived under a rock but from March to July there was a massive shortage in new GPUs. GPUs like the 9070 and the 9070XT, which were the successors to the, you guessed it!!, 7800XT. Which means that people either paid out of their ass for and AMD GPU, or went for the new 5070 at over $600 or just settle for a 16GB 7800XT with ""close enough"" performance for $500 or below.  Even when these 9070 cards came back in stock, they did not hit MSRP until November. So imagine the whole market, starving for cards, gulping down whatever is available. And imagine the RX7800XT sitting at prime prices on the shelf, waiting to be picked up.  The only other AMD card having this sort of a climb was the 7600XT. Which let me remind you again, was replaced by the 9060XT and the 9060, both of which arrived late and slightly over MSRP. Which means at $300, you could buy a 16GB 7600XT or an 8GB 9060, with the 9060XT sitting between $360 and $400.  Does it make sense now???",hardware,2025-12-02 08:21:49,7
AMD,nrv90je,Common sense?,hardware,2025-12-02 11:11:38,1
AMD,nrv0k44,"If you go back to these threads that get posted every month, this exact user always tries his best to disparage this survey and attempt to say that AMD GPUs are way more popular than they actually are. Just mark him on RES or something and move on.",hardware,2025-12-02 09:50:24,11
AMD,nrusjtj,"> So what's the conspiracy?  There is no conspiracy. It's a bug. But calling this a conspiracy makes others look dumb, so people like you can't help themselves. >  > That all 9000 GPUs get counted as 7800XT, even though they're showing up fine in all the other categories?  No, 7800XT was not counted correctly before, and it's rise has nothing to do with 9000 series.  9070XT literally does not show up right now, just like 7800XT did not show up before april.  > And even if that's true, they'd still be selling absolutely _awful_   Sure, but there is a bit of a difference between selling awfully and 9070XT literally having 0 cards sold, which is the case according to survey.",hardware,2025-12-02 08:28:05,-4
AMD,nrv88cx,"That's because for a long time, threads about driver issues in the Nvidia subreddit where straight up deleted by the mods, and the users were redirected to the Nvidia forums.",hardware,2025-12-02 11:04:39,8
AMD,nrvp3py,Is it?,hardware,2025-12-02 13:14:47,-1
AMD,nrwdmmm,"On windows, amd has had issues in the past. The opposite is true on Linux. Amd has the good drivers and nvidia has a history of terrible drivers. On FreeBSD, nvidia is the only company to offer official drivers.   Depending on your os choice, it colors your whole view of the companies and their drivers.   Anyone being pragmatic will tell you that amd and nvidia have had some bad driver releases on windows this year at least once.",hardware,2025-12-02 15:32:48,-6
AMD,ns8za7q,If you used their anti-lag feature when new for the first month.  It wasn't broken. Game companies considered it cheating.  There's a difference.,hardware,2025-12-04 14:15:32,0
AMD,nrw9xlq,> holding on to W10 as long as I freaking can  cough *massgrave.dev* cough sorry had an awful tickle recently,hardware,2025-12-02 15:14:09,1
AMD,nrv5u0b,"Again.  The survey shows higher increase for the 7600XT than the entire 9000 series combined from march until now. It also shows the 7800 XT share increasing more in november alone (+0.13%)  than any 9000 series card over its entire lifetime (none crack 0.15%). It does not track with sales data whatsoever.  AMD representation is prebuilts was always very low, but has vastly increased due to the undeniably great value of the 9060XT. The 7600XT was a very poor seller, especially in prebuilts, which tend to go for cheaper options, since it was much more expensive than the 7600 and only offered extra VRAM.  Even more absurdly. The representation of the 7800XT in the Steam survey in has grown more in November alone (+0.13%) than the overall market share of any single 9000 series card. (none crack the 0.15% number to make it onto the list). Do you genuinely believe the 7800 XT sold more in november alone than any 9000 series card has since release?  It does not compute. It probably just registers the integrated graphics on the CPU or nothing at all for the GPU. I suspect that Valve just rejects DxDiag data from GPUs that have not been added to the list. I suspect they just haven't gotten around to adding the RDNA 4 cards there due to the change in naming scheme, while the 5000 series was added before release, since the naming scheme was consistent.  It clearly shows decent market share on Linux, which is admittedly likely to be inflated due to Nvidia drivers being pretty bad on Linux. Since Linux doesn't use DxDiag, valve probably has a different pipeline for handling Linux data in the hardware survey, which is why these cards show up there and not on Windows.",hardware,2025-12-02 10:42:14,-3
AMD,nrvgplb,"Yeah, and where is the Nvidia help subreddit? /r/Nvidiahelp has 1.5k subs and hasn't been active for 9 years, despite Nvidia holding 90% of the GPU market.",hardware,2025-12-02 12:15:41,9
AMD,nru9gtz,It's a shame theres...1 super expensive HP workstation laptop with it.,hardware,2025-12-02 05:33:42,12
AMD,ns80o4b,It wont. Strix Halo simply too expensive.,hardware,2025-12-04 09:58:09,1
AMD,ns80yzc,Or want to play any sim/builder/strategy game where its always CPU bottlenecks.,hardware,2025-12-04 10:01:05,1
AMD,nru2znz,"Those two are, but there's a lot more there.  AMD hardware works the best with linux right now.  No p vs e core hassle.  Best GPU driver support.  (Intel arc is also solid)",hardware,2025-12-02 04:44:51,0
AMD,nrwf2am,"I’ve personally seen it detect only my apu on an amd system. It’s not based on bias on what I think the numbers should be but observation.   As far as retailers, I’m not just basing on mind factory.  I know people in retail in the US and they have been struggling to sell nvidia cards all year.  Amd sell out in a week or less after they get shipments.",hardware,2025-12-02 15:39:53,-1
AMD,nrwdvug,There is a bias toward enthusiasts but also regions.  China is more likely to pirate windows and use the number one vendor historically in a space.,hardware,2025-12-02 15:34:04,2
AMD,ns813y8,Linux install base is ~30% steam deck numbers. They dont get a choice.,hardware,2025-12-04 10:02:27,2
AMD,ns8brhe,Yeah that’s my issue. I have steam on my laptop at work in the background mostly to chat. Meanwhile my desktop which I game on has yet to have a survey. I’m not questioning the sample but it’s odd to me.,hardware,2025-12-04 11:39:18,1
AMD,nrutyhr,"> It also affected the 7600XT. It went from 0.2% in June to 0.46% in November.  Because 7600XT did not show up correctly, it also has impossible rise like 7800XT. It's rise also started later, again disproving your ""clearance sales"" theory.  > Idk if you lived under a rock but from March to July there was a massive shortage in new GPUs. GPUs like the 9070 and the 9070XT, which were the successors to the, you guessed it!!, 7800XT. Which means that people either paid out of their ass for and AMD GPU, or went for the new 5070 at over $600 or just settle for a 16GB 7800XT with ""close enough"" performance for $500 or below. >  > Even when these 9070 cards came back in stock, they did not hit MSRP until November. So imagine the whole market, starving for cards, gulping down whatever is available. And imagine the RX7800XT sitting at prime prices on the shelf, waiting to be picked up >  > Does it make sense now???  I just gave you couple examples that completely demolishes this your theory, you literally ignore that and give me more made up nonsense? Why did 7700XT share did not change at all if you are correct?  Also, the shortages started all the way back in December of 2024. Making you - again - wrong. Shortages started earlier, 7800XT rise started later. Your made up theory literally does not stand up to any scrutiny.",hardware,2025-12-02 08:42:46,-7
AMD,nrv85xm,I don't know if they're right but there's something weird going on isn't there? No 90 series cards at all and 8.6% of all GPUs are DX8 or below while less than 1% are DX9 or 10?,hardware,2025-12-02 11:04:01,-4
AMD,nrv3xf0,"> If you go back to these threads that get posted every month, this exact user always tries his best to disparage this survey and attempt to say that AMD GPUs are way more popular than they actually are. Just mark him on RES or something and move on.  I have never said that AMD cards are way more popular than steam shows. Only few AMD cards from only last two generations are not counted correctly.  People every month claim steam survey is perfect, so I ask them to explain the inconsistencies I present. Yet all I get are downvotes with zero viable arguments.",hardware,2025-12-02 10:23:47,-5
AMD,nrusqla,">As is usual for you you are literally making up something to fit the data.  This you?  >No, 7800XT was not counted correctly before, and it's rise has nothing to do with 9000 series.  Just to add, if there are counting issues, they are corrected for the month. So you will see a big jump/drop in a single month rather than a consistent increase over 9 months like it is for the 7800XT.  You can see these type of corrections, for example, in the case of sudden influx of Chinese users somewhere around April (i think) this year, which got corrected the next month. They did not gradual decrease it, and it was just 1 month where it sharply rose and immediately came back to normal levels.",hardware,2025-12-02 08:30:02,11
AMD,nruwzf9,"Wait, you seriously don't realize that only cards above a threshold show up? Or the fact that they literally do show up under narrower filters?",hardware,2025-12-02 09:13:43,8
AMD,nrwrc34,Okay? I'm sure the 0.1% of Linux gamers really care.,hardware,2025-12-02 16:39:04,8
AMD,nsbc1mi,Game companies considered it cheating because it was injecting code into games memory. Instead of doing it in driver like Nvidia did (and amd did after the shitshow).,hardware,2025-12-04 21:17:44,2
AMD,nrxwe8f,"There are sections for generic IGPU's for AMD which are AMD Radeon(TM) Graphics that increased 0.01 compared to october and AMD Radeon Graphics actually decreased 0.013.  Surely at some point you'll see how that conspiracy theory does not make sense, especially considering as the other redditor said that AMD gaming revenue is below where they were in 2022 & 2023.   Look at the 3rd quarter report for AMD in 2023 it Gaming segment revenue was $1.5 billion, higher than what is it currently.   There is no need to attack Valve or playdown RDNA3 GPU's success.",hardware,2025-12-02 19:55:25,5
AMD,nrw3i60,"Not sure but I know fine well Reddit is a terrible place for tech support these days. Feels like most general PC hardware and tech support discussion has been pretty well suppressed around here.  If you wanted help with an Nvidia product their forum is far better place to find it, seems very active.",hardware,2025-12-02 14:39:26,1
AMD,nrywrwh,There's a strange Asus laptop tablet thing with it too.,hardware,2025-12-02 22:54:42,2
AMD,nsa5awm,"Fair enough, those games make me feel like I should be getting paid.",hardware,2025-12-04 17:46:48,1
AMD,nru4k9a,"I agree that in general, Linux users are powerusers and are more likely to look for an AMD system rather than use the (default) Intel+Nvidia laptop that more people would be using.  >Intel arc is also solid  I learnt about this when Linus Torvalds himself now uses an Arc GPU in the LTT video.",hardware,2025-12-02 04:56:10,2
AMD,nruv969,">Because 7600XT did not show up correctly, it also has impossible rise like 7800XT. It's rise also started later, again disproving your ""clearance sales"" theory.  They both rose because of clearance. You can even see that they increased a lot as soon as their successors released at higher than MSRP with limited stock.  >I just gave you couple examples that completely demolishes this your theory, you literally ignore that and give me more made up nonsense? Why did 7700XT share did not change at all if you are correct?  How did you ""completely demolish"" my theory? You just said that the 7700xt and 7900xt haven't kept up with the other two (7800XT and 7600XT). How does that disprove anything? In fact you can say that these two do not have direct successors and are in kind of a no man's land - 7700XT at 9060XT (5060ti price) prices and 7900XT/XTX at higher than 9070/XT (5070Ti price). Basically both at Nvidia card prices, while the 7800XT and 7600XT both with **16GB of VRAM** sat at very attractive price points.  >Also, the shortages started all the way back in December of 2024. Making you - again - wrong. Shortages started earlier, 7800XT rise started later. Your made up theory literally does not stand up to any scrutiny.  Idk about this December 2024 shortage, but i do know that RTX 40 series production had been stopped around that time.   You can see that all GPUs rose in price (including the 7800XT) all the way till March, when the new GPUs actually dropped in stores (for a day or two). After which they (7800xt and 7600xt) were pretty nicely priced, while all new gen cards sat way above MSRP.",hardware,2025-12-02 08:55:58,7
AMD,nrv8uxy,"There definitely are issues and certainly some 9000 series cards are probably grouped up under ""AMD Radeon Graphics"" or ""AMD Radeon(TM) Graphics"", possibly due to driver issues or maybe iGPU problems. The reality is that none of those cards probably top the RX6600 or the 7800XT.  I'm not surprised that there are that many olds cards on the steam hardware survey at all to be honest when looking at directX versions.",hardware,2025-12-02 11:10:16,2
AMD,nrv563b,"I'd rather not have another conversation with you regarding sampling bias. No one claims that the survey is perfect, we know they have issues with drivers sometimes or that they don't poll a representative sample. We see this when we see generic Nvidia/AMD representation on the survey or when suddenly Traditional Chinese shoots up in percentage one month before going back down again.  You're boxing an imaginary opponent each month and somehow bait people to actually respond to you seriously.",hardware,2025-12-02 10:35:52,9
AMD,nrv88uj,"> Just to add, if there are counting issues, they are corrected for the month.  Not if, for example, 7800XT is correctly detected only with drivers from certain point onwards.  7900XT literally never showed up in steam survey under most granular directx data. It's almost 3 years since it came out, when this ""monthly"" issue will correct itself?",hardware,2025-12-02 11:04:46,1
AMD,nrv67ul,I do think the survey combines data from several months. I wouldn't be surprised if it combines running results from like 12 months.,hardware,2025-12-02 10:45:54,1
AMD,nrv25df,"> Or the fact that they literally do show up under narrower filters?  Why are you lying when it's so easy to disprove? 9070XT does not show up period.   > Wait, you seriously don't realize that only cards above a threshold show up? Or the fact that they literally do show up under narrower filters?  I was the one who literally last month explained to you how survey works, because you did not know this yourself. Just hilarious https://www.reddit.com/r/hardware/comments/1omcjjg/steam_hardware_software_survey_october_2025/nmq0ed2/",hardware,2025-12-02 10:06:18,2
AMD,nrv62pv,It literally shows the 7800 XT increasing almost enough to break the threshold in November alone (+0.13%). Threshold is 0.15%.  Do you genuinely believe the 7800XT sold more in november than any 9000 series card over its entire lifetime?,hardware,2025-12-02 10:44:34,-3
AMD,nrws81r,There is major growth in Linux. Many mainstream gamer tech tubers have covered Linux for gaming this year.   It’s not 2004,hardware,2025-12-02 16:43:16,-7
AMD,nsbcmgx,Yes but that wasn’t a driver bug,hardware,2025-12-04 21:20:36,0
AMD,nsbe9cs,"To each his own i suppose. To me its fun. In fact whenever i play an action game my mind keeps thinking ""i wish this was a builder game so i could manage this city im walking in""",hardware,2025-12-04 21:28:49,1
AMD,nrv14yx,"> They both rose because of clearance.  Yet none of other RDNA3 cards did. Not even in the slightest. >  > You can even see that they increased a lot as soon as their successors released at higher than MSRP with limited stock.  Do you even comprehend what ""a lot"" means in this context? This would have to be the most massive sell off of apocalyptic proportions. During march of 2025 7800XT went from selling less than <0.15% during 17 prior months to selling at least as much in one month to get to 0.28% in april, according to steam.  > How did you ""completely demolish"" my theory? You just said that the 7700xt and 7900xt haven't kept up with the other two (7800XT and 7600XT). How does that disprove anything?  Because such a thing is absolutely improbable. If you were not so invested in making the same excuses for months now you would see this too.  Your whole theory absolutely crashes also because those ""clearance sales"" you are talking about seem to never end. By late May nvidia had their supply mostly under control, by the end of summer that happened with RDNA4 too. If your theory had any ground these sales should have slowed down dramatically multiple months ago. Yet 7800XT literally kept pace with 5080 this year. Just an absolute joke.  > while the 7800XT and 7600XT both with 16GB of VRAM sat at very attractive price points.  No they didn't. 7600XT was always overpriced, and since 5060/9060XT/5060Ti release it's just terrible value, yet you want to convince me people started buying them exactly at the worst time, LOL. Regular 7600 that came out long before 7600XT does not show up at all BTW, despite usually being 50% cheaper than XT version and not having proper competition, totally normal. 7800XT was not great value during that time either, 5060TI 16GB is priced closely and generally even cheaper than 7800XT, for 10% more performance very few people will pick last gen AMD card.   > After which they (7800xt and 7600xt) were pretty nicely priced, while all new gen cards sat way above MSRP.  But 7700XT was not? 7600 was not? How many times are you going to circle around?",hardware,2025-12-02 09:56:11,-8
AMD,ns7zesg,"if you dont install the driver you get a generic graphics device. see the NVIDIA Graphics Device: 1.16% in the list as well. iGPUs are also counted as generic graphics quite often, i think only Intel shows proper names in the survey. for iGPUs this is a decision made by driver makers.",hardware,2025-12-04 09:45:40,1
AMD,nrvdf4k,">I'm not surprised that there are that many olds cards on the steam hardware survey at all to be honest when looking at directX versions.  Those aren't old cards, they're ancient, they're cards from over 20 years ago. And as I pointed out, nothing in between, no DX9 at all and less than 1% DX10/11. So something is wrong, it's not going to be world changing but it could make a material difference to some of the stats.",hardware,2025-12-02 11:49:21,0
AMD,nrv7iu1,"> I'd rather not have another conversation with you regarding sampling bias. No one claims that the survey is perfect, we know they have issues with drivers sometimes or that they don't poll a representative sample. We see this when we see generic Nvidia/AMD representation on the survey or when suddenly Traditional Chinese shoots up in percentage one month before going back down again.  Do you know that I don't talk about sampling at all in these topics? Slight monthly variations are irrelevant to the questions I'm asking.  > You're boxing an imaginary opponent each month and somehow bait people to actually respond to you seriously.  You literally said two completely made up things about me. If anything you are talking about your own imaginary projection of me, and then judge me based on it.",hardware,2025-12-02 10:58:01,-4
AMD,nrw16pt,"... yea? With how overpriced and low stock they've been, sounds entirely plausible.",hardware,2025-12-02 14:26:23,8
AMD,nsbf3lj,That was an issue caused by a driver. That it was intentional rather than a bug is even more damning.,hardware,2025-12-04 21:32:59,2
AMD,ns7zw0n,"9070XT shows up if you filter for linux only. Its there, its just not large enough.",hardware,2025-12-04 09:50:25,1
AMD,nqwji9j,"It's astronomically worse on Optane. On Intel, the issue can be somewhat alleviated by disabling C-states completely - Intel have a lot of documentation recommending this for Optane drives.  The other part of the equation is Windows 11 Virtualisation defences - both user level and deep system level need to be disabled to claw performance back. I have been battling this on my 900p's and 905p's for some time, but I did manage to claw back most of the performance on a 12900K system.  I am disheartened to see that AMD is even worse.",hardware,2025-11-26 16:33:54,36
AMD,nqw0gjo,"This has always been the way of things. The problem with Ryzen's chiplet design is that latency between cores, IO die, and devices is unusually poor, which you'll see the effects of in latency-sensitive tasks, which includes your rapid random IO to and from SSDs.  In terms of a fix, you can probably claw back some performance by disabling some power saving features and clocking IF as high as it will safely go, but it's an inherently poor architecture for that sort of workload, and you can't really tweak and configure your way around that.",hardware,2025-11-26 14:58:31,67
AMD,nqyjn3q,i think it's related to ryzen design which uses serializer between compute and io die which is slower than unserialized connections.  [https://youtu.be/maH6KZ0YkXU?t=468](https://youtu.be/maH6KZ0YkXU?t=468),hardware,2025-11-26 22:34:22,6
AMD,nqx77t6,"AMDs Chiplet approach with their infinity fabric is fundamentally flawed when it comes to high end I/O.  I don’t know why no one wants to talk about it but this has been an issue for ages now.  Intel on the other hand has put a lot of work into optimizing the I/O performance.  With something like optane SSDs with cherry picked benchmarks, there is a huge gap between them. With spdk intel managed to get higher IOPS on two CPU cores than a 128 core dual socket could achieve with all its cores.",hardware,2025-11-26 18:29:39,21
AMD,nqv48pp,"degraded? please go back to the article and read it  >""Running on our AMD platform, we find our test subject exceeds PNY's up to a sequential read performance quote of 14,900 MB/s. Impressive.""  while it does lose a bit of performance in some cases - it gains performance in others, which probably indicates the test platform that validated those speeds was probably not Ryzen 7000/9000.  >Our Intel platform falls just short on random read throughput and is well short on random write throughput. However, our AMD platform exceeds factory spec for random read, but again falls short with random writes.  and probably not Intel Ultra 200 series, as that platform also fails.",hardware,2025-11-26 11:27:38,33
AMD,nqv5zii,"It could be a temperature thing; they say that they ""employ an M.2 AIC for testing on [their] Intel Core Ultra 9 285K platform"". TweakTown doesn't seem to report temperature either.",hardware,2025-11-26 11:42:43,8
AMD,nqw43f7,"Given the fact than the Intel test system is an Arrow Lake, they most likely hitted THIS: https://www.tomshardware.com/pc-components/cpus/intel-arrow-lake-processors-bottleneck-pcie-5-0-nvme-ssds-by-16-percent-limiting-peak-speeds-to-12gb-s-instead-of-14gb-s",hardware,2025-11-26 15:17:47,3
AMD,nrizzvu,"Looks like another case of AMD’s PCIe and firmware tuning lagging behind a bit sometimes a BIOS update or tweaking power settings helps, but it’s usually just platform differences showing up in synthetic tests.",hardware,2025-11-30 11:58:40,1
AMD,nqxzj1a,I thought my Optane 905p was a lot faster in 4k random read (~250 MB/s to ~400 MB/s) because the benchmarking app was maxed on single core workload when I upgraded from 2700x to 12700KF. I guess it wasn't that...,hardware,2025-11-26 20:51:37,7
AMD,nqyhi9s,Wait really? I have a 905P as a boot drive on my Threadripper system. What changes should I make for more perf?,hardware,2025-11-26 22:22:37,2
AMD,nqwy08i,"It's amazing how hardware design can hide these latency deficiencies. It's why people were genuinely suprised when Intel scaled well with DDR5, and AMD didn't.  The average person assumes all things are created equal.",hardware,2025-11-26 17:45:19,32
AMD,nqx9crr,"Latency between the IO die and the chiplets, and the IO die to the IO would affect sequential storage workloads more, or equally compared to random storage workloads. This explanation is straight up untrue.   Besides the point, the 50ns interchip latency caused by the SERDES interface is such a tiny fraction of the total latency to get to storage, that any actual effect it has would not come up in numbers like this.   Correlation is not causation.",hardware,2025-11-26 18:39:53,22
AMD,nqw6il8,But most users will never see the difference.,hardware,2025-11-26 15:29:59,28
AMD,nqyk45r,it's the serializer thing   [https://youtu.be/maH6KZ0YkXU?t=468](https://youtu.be/maH6KZ0YkXU?t=468)  hence i guess the ssd will perform better with amd laptop apu or strix halo,hardware,2025-11-26 22:37:00,2
AMD,ns1d39a,The infinity fabric may have solved the issues they had before but boy is it horrible for internal latencies. Heres hoping the rumours of them fixing it for Zen6 is true.,hardware,2025-12-03 09:01:24,2
AMD,nqwwwq1,"Yep, Zen 6 is rumored to be getting a new IO die, which should help here.",hardware,2025-11-26 17:39:47,1
AMD,nqynnz2,Any chance you could be the one to talk about it? If you don't have hardware on hand maybe you could write up a blog describing the issue and asking for people to run a test? I'm sure someone would pick that up for the views,hardware,2025-11-26 22:57:03,2
AMD,nqv7c1a,"The sequential scores are normal. Meanwhile, it’s the random scores that AMD is significantly lower than Intel.",hardware,2025-11-26 11:53:43,50
AMD,nqv4euq,"Look at the summary, all benchmarks, including non synthetitcs:  Intel platform: 26 862    AMD platform: 19 747  Some synthetitcs are good, but on AMD it falls behind in real usage benchmarks.  I don't know why you downvote me for saying this. I don't like this situation, but downvoting will not solve the issue.",hardware,2025-11-26 11:29:08,14
AMD,nr3uz47,Maybe take your own advice pal,hardware,2025-11-27 20:07:42,1
AMD,ns1da9g,sequential scores are irrelevant for SSDs after your data existed there for a bit and wear leveling kicks in. The only time sequential read/write matters is when you have fresh data on fresh drive without anything else done to it yet.,hardware,2025-12-03 09:03:22,1
AMD,nqv9fq0,"Would be great to see benchmarks focused on this issue, with same coolers, memory etc.",hardware,2025-11-26 12:10:32,5
AMD,nqxgvze,Users who actually need 16 cores likely will. That’s a lot of data to keep feeding into a high core cpu.,hardware,2025-11-26 19:16:13,9
AMD,nqv8f2l,"The sequential scores are normal. Meanwhile, it’s the random scores that AMD is significantly lower than Intel.  For example, my Kingston Renegade G5 only got about 80/250 MB/s on the CDM RND4K Q1T1 test in my AM5 rig, which is 1/3 lower than the score on LGA1700. And yes, I’m sure my 9800X3D and X670E are both capable of PCIE Gen5, and the SSD was in the right slot. The temperature was also always under 50 C.",hardware,2025-11-26 12:02:26,33
AMD,nqwcu67,"Here's something that'll probably make you even less happy: [https://www.reddit.com/r/intel/comments/1gdib7e/a\_regression\_that\_most\_reviewers\_missed\_loading/](https://www.reddit.com/r/intel/comments/1gdib7e/a_regression_that_most_reviewers_missed_loading/)  The test you linked used Arrow Lake, the 4K random performance (which is what truly matters for loading times of most stuff you run) is significantly worse on Arrow Lake than Raptor Lake.",hardware,2025-11-26 16:01:14,16
AMD,nqv6df4,"In ‘real world usage’ it will feel fast on both platforms. If you look at the charts, AMD wins some and Intel wins some.  I don’t think there is anything to be done because the platforms are designed differently in how the PCI-E port connects to RAM.",hardware,2025-11-26 11:45:55,-7
AMD,nqv7u1h,I didnt :),hardware,2025-11-26 11:57:43,-8
AMD,nqyyz8g,They can improve it with better packaging and stuff like glass substrates. Hopefully completely with advanced silicon to silicon bonding especially as node improvements slow,hardware,2025-11-27 00:05:57,1
AMD,nqxfdos,"Yep, and same SSD problem was reported with Arrow lake because it switched to tiles. Chiplets are pretty much a failure for consumer products. But they are necessary for servers so AMD and Intel won't stop. We can only hope the companies making monolithic cpus with much better performance (qualcomm, apple) can force AMD to make an actual consumer focused product for consumers, not just rebadged server scraps.",hardware,2025-11-26 19:08:45,-4
AMD,nqv93vz,"This should get higher attention, benchmarks shows the issue but no one has any explanation for this. As much as I like my AMD I'd be even more happy if it doesn't bottleneck disk performance.",hardware,2025-11-26 12:07:57,17
AMD,nqwd1bk,"Yeah, I hope AMD is working on this issue, would be nice to know if they work on it .",hardware,2025-11-26 16:02:14,2
AMD,nqynvpu,"They're necessary not just for servers, but to keep costs of consumer chips down as each new node costs substantially more than the last.",hardware,2025-11-26 22:58:17,7
AMD,nqvcnrq,"I'm not sure why I only see people talk about it now, but it's been always the case, ever since first gen Ryzen. Random read has been higher on Intel platforms. Subjectively, they also are more trouble-free. My Zen 4 PC keeps running into weird IO bottlenecks where over time transfers slow to a crawl between two fast SSDs if I also download files at the same time. It never happened on my old Kaby Lake media PC. My guess is Intel did more work and is quite a bit more polished with IO.",hardware,2025-11-26 12:34:52,19
AMD,nqwo5kf,"If you really want higher disk performance you should use Linux and an appropriate filesystem anyway.  I also presume any big difference between Intel and AMD would go away, since I don't think it's a hardware problem at all that causes this delta.",hardware,2025-11-26 16:56:21,-1
AMD,nqwwpf3,"There is nothing to be done for current and previous generations, since this is HW limitation.  Strix Halo APU is using newer chiplet design, version of which should be used on next generation AMD CPUs. Someone testing SSD performance with such system would provide a glimpse into how performance could change in the future.  Intel will also improve their design with upcoming CPU generation, so if there were any slowdowns due to architecture they potentially will see uplift too.",hardware,2025-11-26 17:38:46,2
AMD,nqypu7b,Smaller chips —> higher yields etc.!,hardware,2025-11-26 23:09:59,1
AMD,nr8blfe,Qualcomm is making consumer only chips that are cheap too. Snapdragon x plus was cheaper than lunar lake while still matching in battery life,hardware,2025-11-28 15:42:09,1
AMD,nqvfylf,"The problem that transferring data between SSD halted is probably due to AM5 only has PCIE 4x4 connecting the chipset and CPU. Assuming you have one SSD connecting directly to CPU and one connecting to the chipset, if both SSD are PCIE 4x4, it would easily eat up all the bandwidth.",hardware,2025-11-26 12:57:53,15
AMD,nqwojzt,"If this is software issue, there is software fix. It is also something worth to check.",hardware,2025-11-26 16:58:17,4
AMD,nqwsbsl,"Thanks, I suspected this to be the case too, but the behavior is surprisingly undesirable if this were the case. Things are all good until too much data is transferred at a time, where the transfer speeds tank to literally 100-200MB/s between two PCIe 4x4 drives.   This can be easily induced with multiple data transfers happening at the time, but it's not limited to those scenarios. I can plug in a SATA drive to have the third disk drive transferring files to either of the existing two, or start large downloads writing to one of my disks, and the system will be paralyzed with sub 100MB/s transfer speeds across all devices combined.  My mobo's front drive slot is actually PCIe 5 connected to the CPU, and the secondary one is PCIe 4 connected to the chipset, so the hardware should theoretically easily accept faster drives.  While seeking support, I was told it is expected behavior due to IO bottlenecks that aren't restricted by pure bandwidth, but the logic queueing the requests that can get overwhelmed.  The same drives worked far more consistently on a Kaby Lake PCIe 3 mobo.",hardware,2025-11-26 17:16:59,3
AMD,nqwaovt,"Would you see that ""over time"" though? I'd have thought you'd see it pretty much immediately.",hardware,2025-11-26 15:50:48,1
AMD,nqwrpf1,"Yeah, but likely at the kernel level. Or maybe firmware, or an interaction between both.",hardware,2025-11-26 17:13:53,4
AMD,nr5owd8,At some point you quickly saturate the DRAM buffering the QLC as well.  Its 12000KB per second... for the first .01 second.,hardware,2025-11-28 03:05:24,3
AMD,nqwb13b,I’m not exactly sure what “over time” means here.,hardware,2025-11-26 15:52:29,2
AMD,nr0zs5w,"I'm quoting from the post you replied to. To me, ""over time"" implies initial transfer performance is great, but then tails off.  I can't see a PCIe bottleneck causing that kind of behaviour. Thinking about it, I wonder if they're seeing SLC caches being exhausted if the copies are large enough?",hardware,2025-11-27 09:08:47,3
AMD,nr2x53q,"Thanks for inquiring! By ""over time"" I mean that my transfers can be neat multiple GB per second, and then fall. But the most consistent way of reproducing the issue is by adding additional IO operations (say, also copy to a SATA SSD, or initiate a couple of file downloads) the speeds fall to a fraction of that. They get massacred when I use Opera and enable parallel downloading (which basically downloads each file in bits in parallell), where I can get sub 100MB/s transfers across all disks.",hardware,2025-11-27 17:06:50,2
AMD,nnfo7h1,"Two ""leaked"" Strix Halo products, both with 8060S iGPU but 12C24T (392) or 8C16T (388).",hardware,2025-11-06 15:48:08,76
AMD,nng1lrn,"Finally, locking the best igpu to monster chips always felt like a waste, most who needs a 16 core cpu is going to want bigger than a 8060s anyway",hardware,2025-11-06 16:51:37,20
AMD,nnfyerl,Now put it in a actual laptop,hardware,2025-11-06 16:36:19,28
AMD,nnfsjte,"388 sounds really nice but kinda too late. Feels like Medusa Halo with RDNA5 will be the ""real"" one to go with.",hardware,2025-11-06 16:08:27,35
AMD,nngxb2l,"When AMD makes a next gen version of this with FSR4 or higher, then we'll have something special.",hardware,2025-11-06 19:22:59,8
AMD,nng2n1a,AMD Ryzen AI Max+ 391 and a half.,hardware,2025-11-06 16:56:33,4
AMD,nngsi1v,"Excited about the 388 i would love an 8/16 cpu for lighter things like gaming and general computing with the 8060, right now you basically have to pay for 64-128gb ai machines and not everyone wants that",hardware,2025-11-06 18:59:30,2
AMD,nniqcmg,I'm just hoping this leads to a mini PC with this chip in it so I can replace my ancient gaming desktop with an all in one machine. The current Strix Halo line is everything I've ever wanted in an all-in-one machine but is too overkill and expensive for what it's worth. A cut down variant with the same iGPU would be sweet.,hardware,2025-11-07 01:07:27,2
AMD,nnq9kar,"ALRIGHT, will more laptop manufacturers finally start using Strix Halo?  really really really want more than 8GB of vRAM for video editing and game dev, but I cant afford RTX 5070Ti laptop prices....",hardware,2025-11-08 06:46:14,2
AMD,nnfs3wu,How many days of driver support can we expect?,hardware,2025-11-06 16:06:22,7
AMD,nng7z7l,Hopefully these would be affordable and priced in accordance to competition (4060/5060 laptops). But knowing how the world is these will be sold at exorbitant prices to generate AI slop.,hardware,2025-11-06 17:22:23,1
AMD,nnfo0a9,"Hello work-school-account! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 15:47:12,1
AMD,nngfc5e,When do you think consumer RDNA 5 gaming cards will come out? Maybe around June?,hardware,2025-11-06 17:57:41,1
AMD,nnfonzs,That's what we need!!!!  Perfect for handhelds,hardware,2025-11-06 15:50:18,51
AMD,nnkun5o,I'm praying that Minisforum somehow shoves these on an ITX motherboard.,hardware,2025-11-07 11:23:54,2
AMD,nnjgzvw,"they should really do a APU silicon with only quad core + GPU equivalent of RX580 performance. (not a cut down chip but actually small chip). That handheld will be a monster.  \*note we had i7-7700K able to push GTX1080/GTX1080Ti, so quad core is actually more than enough.",hardware,2025-11-07 03:51:41,7
AMD,nng47qh,"This will still be a big, expensive processor.  8C CPU CCD is pretty small.",hardware,2025-11-06 17:04:07,14
AMD,nnfz2ma,That's an OEM's job.,hardware,2025-11-06 16:39:29,12
AMD,nnfwu99,"Think that's a year out, at least",hardware,2025-11-06 16:28:44,22
AMD,nnfyyh8,Medusa Halo won't see the light of day before H1 2027.,hardware,2025-11-06 16:38:56,7
AMD,nnfvgho,Is rdna 5 confirmed?,hardware,2025-11-06 16:22:09,3
AMD,nnqpufs,"Keep an eye out at CES next year, if there are any more strix halo laptops imminent then they will be announced there or at the very least AMD will talk about strix halo. If it doesn't get mentioned then the means most OEMs are holding out for rDNA5, RTX 6000 or medusa halo in 2027.",hardware,2025-11-08 09:31:31,1
AMD,nngun7g,2 year cadence. 2027,hardware,2025-11-06 19:09:54,3
AMD,nnfrbk2,Assuming that they're actually available and affordable.,hardware,2025-11-06 16:02:41,36
AMD,nnko7ge,"not really. it's still gonna drain the battery way too fast. slower than the 395, but not by a lot.",hardware,2025-11-07 10:24:19,5
AMD,nni7e5v,"These will be great (but expensive) for the GPD, AYANEO, or OneXPlayer devices. I doubt that we'll see these in a mass market ASUS, Lenovo, or MSI handheld unless we get a ""Z"" series version of Strix Halo. I am anticipating something like a ""Z2 Ultimate"" if AMD stockpiles enough yields with defective+disabled NPUs or something. Maybe there will be yields that are only stable up to 45w with reduced clocks that can find a home in more conventional handheld designs. The Phawx has proven that any TDP over 25W will offer massive gains over the existing Z2E.",hardware,2025-11-06 23:16:45,4
AMD,nngjm9g,"Not sure about handhelds. I think these processors are intended for NUCs, so maybe they don't work very well with a battery. Perhaps for a ""Steam Console""? I'm not sure either. I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV, see if it's any good.",hardware,2025-11-06 18:17:59,3
AMD,nnv8p4o,I'm still happy with the performance I get out of HX370 @ 5w TDP,hardware,2025-11-09 02:24:33,1
AMD,nolpn70,"Right, because you totally want a 60W APU in a handheld.",hardware,2025-11-13 08:44:20,1
AMD,nnfsovn,"Why? There's no need for a 12-core CPU in a ROG Ally, even 8 is overkill.",hardware,2025-11-06 16:09:06,-1
AMD,nng7s73,The onus is still on AMD to sufficiently supply them. That’s been a consistent issue for them versus intel.,hardware,2025-11-06 17:21:26,27
AMD,no3eqlv,OEM cannot do it if AMD does not ship any units.,hardware,2025-11-10 12:15:14,1
AMD,nngelza,"And if typical AMD release patterns hold true, only a few, very expensive,  SKUs will be available in 2027. I'm not banking on buying a Medusa Halo product that fits my budget until late 2028 at the earliest. Don't sleep on this expanded Strix Halo availability. These should be priced to compete with Panther Lake in 2026. I'm personally hoping to see gaming notebooks like the Asus TUF A14 updated to use the 388 chip",hardware,2025-11-06 17:54:17,18
AMD,nnkwofi,"In time for the market crash and prices returning somewhat normal, hopefully",hardware,2025-11-07 11:41:18,2
AMD,nnfyf51,I thought it would be released in the first half of 2026?  Edit: Oh lol seems like i was wrong lmao,hardware,2025-11-06 16:36:22,2
AMD,nnqxwar,"well, my current 7 year old laptop's hinge ([MSI GF63 8RC](https://www.notebookcheck.net/MSI-GF63-8RC-i5-8300H-GTX-1050-Laptop-Review.340606.0.html)) is torn in half, and its battery is completely dead  so I am absolutely in need of a new laptop in short notice, the latest new tech I can wait for is prolly Intel Panther Lake >!as the laptop itself has to release before [Arknights Endfield](https://www.youtube.com/watch?v=1XpQz8k1NwE) comes out in Q1 2026!<",hardware,2025-11-08 10:54:50,1
AMD,nngjqs0,ADHD meds with or without ADHD?,hardware,2025-11-06 18:18:34,2
AMD,nng47rh,"If we're being honest, they'll still be up-priced to sell to AI-bros instead of being tiny gaming boxes like we (here) want.",hardware,2025-11-06 17:04:07,20
AMD,nnv9ykh,GPD WIN MAX 5 does better with both 395 and 385  https://gpdstore.net/product/gpd-win-5/,hardware,2025-11-09 02:32:57,1
AMD,nnvaazw,">  I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV  If anyone has done that for reviews, it would be https://www.youtube.com/@ThePhawx/videos",hardware,2025-11-09 02:35:12,2
AMD,nnhi426,>  so maybe they don't work very well with a battery  I don't see why not though? Don't they have awesome performance at very low power?,hardware,2025-11-06 21:05:54,2
AMD,nnhsreb,"Well, that’s exactly my use case with beelink gtr9. I’m using it as a tv gaming console running bazzite (and also as an llm server).   Gaming performance for 8060s - it’s much better than I expected. About every modern fighting game runs in 4k at stable 60, though upscaling is usually required for that. It also megabonks perfectly.   So I have high hopes for these new chips, given that they use the exact gpus. It now all boils to price and form factors.   My gt9 is undervolted and slightly overclocked, and runs at about sustained 140 watts.   Given that I live in Ukraine and lately experience blackouts, so during those I put it on balanced profile (sustained 80watts) and run off ecoflow together with monitor (instead of tv when life is gucci). This balanced setup all together is consuming around 137 watts when megabonking at 4k.",hardware,2025-11-06 21:57:40,1
AMD,nnfw596,They are talking about the 8 core version,hardware,2025-11-06 16:25:26,37
AMD,nngcj44,"That, and a complete unwillingness to invest in engineering support for laptop OEMs.",hardware,2025-11-06 17:44:32,11
AMD,nngda20,"Since there's a bunch of noname companies releasing products with Strix Halo,  what make you think there's not sufficient supply?  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",hardware,2025-11-06 17:47:59,2
AMD,no4cib3,"Videocardz covered 6-10 devices that use Strix Halo, that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship sufficient units. Both ASUS and HP's laptops are in stock all around the world.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",hardware,2025-11-10 15:35:11,1
AMD,no4bk1f,"Videocardz covered 6-10 devices that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship any units.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",hardware,2025-11-10 15:30:25,1
AMD,nnfzbti,RDNA5 won't till 2027,hardware,2025-11-06 16:40:41,13
AMD,nng22ps,"Nah zen6 launches in H1 26 in servers, year end/early '27 for desktops/laptops. Rdna5 is probably early '27. From the most recent rumors I've seen atleast",hardware,2025-11-06 16:53:52,8
AMD,nnr0mk0,"Probably keep a lookout during black Friday then as there might be a sale on a 5070 ti or 5080 laptop that brings it down to your budget. Even if CES 2026 brings the perfect laptop, there's no guarantee it will be buyable before May 2026 based on previous years.  Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.",hardware,2025-11-08 11:22:07,1
AMD,nnh7qgd,Take a guess.,hardware,2025-11-06 20:14:13,3
AMD,nnj1xx4,"It's just a big, expensive architecture.",hardware,2025-11-07 02:17:39,3
AMD,nngodkr,No CUDA and RAM getting more and more expensive won't make them super desirable for AI.,hardware,2025-11-06 18:40:14,7
AMD,no3ehqd,They cost too much to make to be tiny gaming boxes.,hardware,2025-11-10 12:13:18,1
AMD,nniq5gn,Who tf uses a handheld for AI lmfao,hardware,2025-11-07 01:06:15,-1
AMD,nnfxgwk,Even 8 cores is excessive in a handheld. You'd get more with a stronger GPU,hardware,2025-11-06 16:31:45,-4
AMD,nng22hs,8 cores x86 cores takes up too much power in a handheld unless it’s some 4 big core 4 small core thing,hardware,2025-11-06 16:53:51,-3
AMD,nng3wxq,8 cores is still too many for a handled. 6 is ideal,hardware,2025-11-06 17:02:40,-7
AMD,nng8444,Surely it would still be a 100W+ chip when pushing gaming loads though?,hardware,2025-11-06 17:23:03,-6
AMD,nnger22,"One of those “no name” companies (GPD, the oldest and most established) has literally said AMD doesn’t give enough supply, lmao.  https://videocardz.com/newz/gpd-accuses-amd-of-breaching-contract-by-not-supplying-enough-ryzen-7-7840u-apus-on-time  Secondly, AMD being able to supply handhelds that sell several thousand units is a drop in the bucket compared to the millions major OEM laptops need to use. The fact that AMD supplies them in very small numbers, and not OEMS at capacity is proof of my point, not a refutation of it, lol. ￼",hardware,2025-11-06 17:54:57,19
AMD,nngukrm,"These SoCs are very interesting not just for AI stuff. The NPUs have a lot of potential to dramatically improve energy efficiency and perf for a lot of other tasks as well, its just that the tools are still in their infancy so we haven't managed to develop many applications on them yet.",hardware,2025-11-06 19:09:34,3
AMD,nnh4bg5,Would love to see more handhelds use them for the performance per watt at lower tdp.,hardware,2025-11-06 19:57:12,1
AMD,nnqbc5e,">The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.  for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB",hardware,2025-11-08 07:03:19,1
AMD,no4g99f,So a total of 4 laptops? Two of which are from OEMs i never even heard of?,hardware,2025-11-10 15:53:37,1
AMD,nng3o0d,That would be very early for Zen 6 based on AMD's typical timelines.  Nobody is really moving that fast anymore outside the smartphone chip space.,hardware,2025-11-06 17:01:28,3
AMD,nnrbfzj,">Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.  but since the [5050](https://www.youtube.com/watch?v=ihJRJryQXTE) & [5060](https://www.youtube.com/watch?v=-VM_H7soWBQ) doesnt seem to have much of a performance boost compared to RTX 4060 & 4070, are older laptops with RTX 4060 & 4070 still viable picks if they're cheaper?  The main thing is that I want a sub 2kg laptop with dedicated GPU and 32GB RAM, so the main laptops I'm eyeing rn are either:  * 2023 ROG Zephyrus G14 * 2024 HP Omen Transcend 14  but their new RTX 4060 versions are still hovering around 1300 USD in my country, way above my budget.  So ofc I'm hoping that their price substantially goes down either during Black Friday or 12/12 sale, but do you think that they're even still worth pursuing or should I just focus on 2025 & upcoming laptops? (My third pick that fits my requirements is the new Legion 5, but that one's even more expensive atm)  Also technically there are three used laptop stores trying to sell a used RTX 4070 Omens to me at 1300 USD, but no way used stores are going to give any substantial Black Friday discounts",hardware,2025-11-08 12:54:16,2
AMD,nnis0md,#1 usecase for these in AI is running medium sized MoE models for local LLMs.  And for that they offer a pretty compelling solution. You basically have to spend twice as much to do it with Apple. At which point you're not using CUDA either.  ROCm works and inference engines like llama.cpp also support Vulkan compute AI.  It's basically the most power efficient and most cost efficient solution on the market for local LLMs.,hardware,2025-11-07 01:17:39,7
AMD,nniqrl3,"Well, mobile ai, these things don’t use sodim ram (or ram sticks in general) and the speeds of ddr5x aren’t used for most CPU’s. This very well could be also targeting mobile ai ( specifically more ai/gaming where the 395+ was targeting development and simulations as well). Also ai doesn’t need cuda, rocm exists and it’s quite good, any downfalls of amd is on a architectural level, but if you’re doing local ai on the go then you don’t really need cuda if that makes sense.",hardware,2025-11-07 01:10:00,2
AMD,nnfyjb8,This is the strongest gpu they have available,hardware,2025-11-06 16:36:56,33
AMD,nng2x5i,"Yes, but this is the version with the best GPU with the least cores, so it becomes the best for handhelds",hardware,2025-11-06 16:57:53,11
AMD,nngwxna,"From a power point of view disabled-by-binning cores would be exactly the same as if you disabled them in the OS, or just didn't have enough work for the OS to schedule threads on them.  Any real savings would require a new die (and there may well be possible savings there, less complex busses, easier routing, fewer ports on sram blocks etc - but I suspect they might be relatively small), but this isn't that.  The *only* advantage this would have over the ""full fat"" die is cost. And that still depends on yields and how much supply of these binned dies they actually get.",hardware,2025-11-06 19:21:09,4
AMD,nng4794,"Yes, but this is version with the best GPU with the lower amount of cores. So the best possible in terms of performance for a handheld",hardware,2025-11-06 17:04:03,9
AMD,nng95e9,You can set the TDP to 15W actually,hardware,2025-11-06 17:28:04,11
AMD,nngm2zc,Its a new product. Seems they made it for AI but found other very interested into it including nvidia partnering with Intel to counter these products. So I imagine 5heyre taking it more serious now that they're releasing these,hardware,2025-11-06 18:29:30,4
AMD,nngfqas,"My friend, we were talking about Strix Halo supply, not 7840U from 2 years ago.",hardware,2025-11-06 17:59:32,-4
AMD,nnqcf1u,Haha. You fell for the integrated NPU trap.,hardware,2025-11-08 07:14:01,1
AMD,nnqby1j,Give me examples where for the same price you can get a Strix Halo laptop vs   5060/5070 ones.,hardware,2025-11-08 07:09:20,1
AMD,no4hq94,"Copy paste of one of my comments why OEMs are not in a hurry to use strix halo in laptops.  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",hardware,2025-11-10 16:00:44,1
AMD,nng4b4n,"Mi450x is shipping in Q3 with zen6, and zen6 server chips are already in partner hands from latest earnings call",hardware,2025-11-06 17:04:34,7
AMD,nnre7nr,"Unless the extra efficiency of the new CPUs are a factor for you then you aren't missing much getting a 2023 or 2024 laptop and yes the 4060 and 4070 are still plenty viable, the 5060 usually almost matches the 4070 but you're going to have a similar experience most of the time between the 4060, 4070, 5060 and 5070 most of the time. So yes, if you can get a 4060 cheaper then go for it.  It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.",hardware,2025-11-08 13:13:25,1
AMD,nnfznhx,"Even then I'd just rather not have the cores. You're GOU limited anyways, so all they're doing is eating a bit of power and hurting your battery life. I've got a Z1X Ally and I'm generally hapoy with it but the usability on battery is easily its biggest weakness",hardware,2025-11-06 16:42:14,-12
AMD,nnh8398,Well i was saying this wouldn’t be enough anyway. I agree some handheld specific die would need to be made.,hardware,2025-11-06 20:15:59,-1
AMD,nngp9q1,Sure but the performance will be dogshit.  It's a nominal 55W chip and the 395 boosts to 120W to deliver the 4060-ish performance it is capable of.,hardware,2025-11-06 18:44:23,-2
AMD,nngkupf,You invoked “no-name” OEM’s being supplied at all by AMD as proof of them not having supply problems. I linked a very infamous instance of an OEM publicly speaking about AMD’s issues in keeping with supplying even these small boutique OEMS.,hardware,2025-11-06 18:23:45,11
AMD,nngrst9,"Strix Halo sounds significantly more niche than 7840U ever was, tbh.",hardware,2025-11-06 18:56:12,5
AMD,nnsxlvm,"Except I know that those NPUs are actually AIEv2 tiles, making this effectively an integrated CGRA :)",hardware,2025-11-08 18:20:47,1
AMD,nnqeynv,"in my country (Indonesia), if you wanted a sub 2kg laptop with more than 8GB VRAM, [your only option is either paying 52 million Rupiah for the RTX 5070Ti Zephyrus G14, or ""just"" 32.5 million Rupiah for the Flow Z13](https://cdn.discordapp.com/attachments/1315401041369366590/1436621115454652557/Zephyrus_G14_vs_Flow_Z13.png?ex=6910452d&is=690ef3ad&hm=d1c48964f898f82129b6acc9496b6f6db52c764812d0cfad8b9f2411a2c74826).",hardware,2025-11-08 07:39:45,1
AMD,nngen4p,"Yeah, but since Zen 6 is coming early in Epyc that doesn't mean Ryzen will come just as fast.",hardware,2025-11-06 17:54:26,2
AMD,nnw4sif,"> It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.     I'm from Indonesia!     The deal with our laptop market here is that we basically get the US's MSRP, but not their insane discounts (we ain't never getting a Legion 5 for $1000 unlike the US)     and the used market here is just extremely sparse for anything from 2023 forwards",hardware,2025-11-09 06:26:41,2
AMD,nng2wmz,Yes but dropping 2 cores is not going to cause a massive price cut or battery life improvement,hardware,2025-11-06 16:57:49,21
AMD,nnh5ap6,"The 8060S is about three times as fast as the 780M in the Z1E. The only IGPUs you can get that are faster than the 780M are the Intel Arc 140T, which comes in CPUs ranging between 14 and 16 cores, and the 8060S, which launched in 16-core parts and is now being made available in 12- and 8-core versions. If you want a good IGPU, you're getting at least 8 cores.  You can always disable them in the OS or UEFI if you're really dead-set on it. At that point, all you're losing relative to AMD engineering an entire new chip is some space on your motherboard.",hardware,2025-11-06 20:02:01,4
AMD,nngrdht,"[It actually doesn't scale nearly that well with power](https://youtu.be/vMGX35mzsWg?si=NIM7ZYvjsFLFElL9), there's simply not enough memory bandwidth to feed the GPU effectively.",hardware,2025-11-06 18:54:12,9
AMD,nnvw8zt,Those NPUs are actually integrated time machines.,hardware,2025-11-09 05:13:12,1
AMD,nnqktla,"I find that impossible to believe that's the norm. Your proof must be a great case of cherry-picking because I've been following Strix Halo and the competition very closely.  Here's examples where Strix Halo is more expensive, and much slower to a 5070 Ti:  [https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ](https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ)  [https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS](https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS)  In EU, the Stix Halo 395 cheapest laptops is 500-700 euro more expensive than 5060 laptops, 300-500 euro more than 5070, and 600-700 more that 5070 Ti. And that's for Z13 **TABLET**, not a laptop.  So my case stands. Strix Halo is a great niche product, but it doesn't make sense gaming laptops because the tech it uses is expensive and has a big die, thus making it uncompetitive. It's amazing for AI and gaming handhelds, thus we don't see many laptop options.",hardware,2025-11-08 08:39:20,1
AMD,nnqx5ue,"I mean I did say ""**sub 2kg**"", because thats what I actually want  I know stuff like the MSI Vector 16 or ROG Strix G16 are available with RTX 5070Ti for cheaper than the Flow, but those laptops are heavy bricks",hardware,2025-11-08 10:47:26,1
AMD,nnr4l64,"You actually said ""**for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB**"" and the discussion was about **laptops,** not tablets. Stop moving the goalpost and learn how to have a logical conversation.",hardware,2025-11-08 11:58:55,2
AMD,nt7v3cl,Ngl it’d be easier giving a budget range and the country you’re in vs this very specific comparison that I somewhat doubt is giving the most value you can find  Also what games you’re trying to play potentially bc different parts have different outcomes,buildapc,2025-12-10 01:52:12,1
AMD,nt7vf4q,"If you are only looking for pure gaming performance and want to maximize your FPS, go with the R5 5500X3D.",buildapc,2025-12-10 01:54:08,2
AMD,nt84b94,X3d all day.,buildapc,2025-12-10 02:46:38,1
AMD,nt7r819,"don’t get a 5060 get a 9060xt… there are so many reviews on this man idk why people still think thst the 5060 is a good card. If your solely focused on gaming, get a ryzen 5 7600 for am5 or if you really want to do am4 get the 5500x3d",buildapc,2025-12-10 01:29:03,1
AMD,nt7r5yz,"https://gamersnexus.net/cpus/am4-lives-amd-ryzen-5-5500x3d-cpu-review-benchmarks  The 5500X3D occasionally beats the **14900K.**  If you stream or occasionally do professional work like rendering or video editing then the 14400F can be a better choice, but for pure gaming, X3D chips tend to perform at least one generation ahead of non-X3D chips in their product line.  One asterisk is the motherboard though, if the 5500X3D uses a 300- or 400-series motherboard with PCI-E 3.0, the RTX 5060 could be hurting big time in modern games. The 14400F almost certainly uses PCI-E 4.0 or 5.0 which will be better for that GPU.",buildapc,2025-12-10 01:28:43,0
AMD,nt7rtcn,"It's obvious from their wording that these are 2 prebuilts they are considering, so the 5060 is the only option.",buildapc,2025-12-10 01:32:40,2
AMD,nt7uwwr,"Then find a different prebuilt, not that complicated tbh",buildapc,2025-12-10 01:51:10,-1
AMD,nt83psx,"It's always the ""just do this"" people who tend to think the least.  What makes you think they haven't?",buildapc,2025-12-10 02:43:10,4
AMD,nseekuz,"Yup! AMD is super efficient and in turn they are cooler, plus air coolers are really good too. AIOs are just for looks at this point tbh.",buildapc,2025-12-05 09:48:54,27
AMD,nselewz,"very few people NEED water cooling, and 7800x3D is a long way from that need category.    90% of AIO cooling out there is for the cool factor, and if that makes them happy, awesome.  But dont get sucked into thinking its a necessity for most.",buildapc,2025-12-05 10:54:24,14
AMD,nsef9r8,"I have the 7800X3D and i’m not a fan of watercooling so I went with Noctua NH-D15 and the case is Fractal Pop XL (silent black solid, not sure how many versions are) + 2x extra fans on top.  I usually get around 62-65C on both cpu and gpu in heavy load. In the worst case I have seen 70C but the room temp was above 25c and my rig is very close to the heater.  I’m not sure how a mid tower will handle the heat.",buildapc,2025-12-05 09:55:36,7
AMD,nsefrjv,"The major difference between an air cooler and an AIO cooler is where the heat is dumped.    Assuming you have typical case fans for the Meshy 3 to move the hot air out of the case, you will be fine.",buildapc,2025-12-05 10:00:23,4
AMD,nsex36j,"Of course, the thing uses like 70W in gaming...  It's nothing",buildapc,2025-12-05 12:31:42,4
AMD,nsedsos,"Yes it is enough, I had exact cpu and cooler before. No issues. And that cooler actually performs very well.  Just checked the link and looks like that one is 140, I was using with 120 so even better.",buildapc,2025-12-05 09:41:07,3
AMD,nsee0p5,I have this setup in an SFF case at the moment and works fine.,buildapc,2025-12-05 09:43:18,2
AMD,nselkhm,Got a 9800x3d with the peerless under the cpu stress test it makes out at like 63C with a little bit of ubdervolting,buildapc,2025-12-05 10:55:51,2
AMD,nsencur,"Yep, I have the Phantom Spirit, which is almost identical, and it works well. My idle temps hover in the 30-42°C range, non hotspot under load sits around 48-55°C, and my hotspot in games, caps at around 75-80°C at full load. Synthetic benchmarks can have the hotspot reaching 82-84C°.",buildapc,2025-12-05 11:12:24,2
AMD,nsejaqw,"I have a watercooler on it But air cooler is completely fine.  Is u have a case with some fans, u really dont need a watercooler tbh.",buildapc,2025-12-05 10:34:30,1
AMD,nsen2yt,"Built a new PC last week with a 9800X3D and a Phantom Spirit 120 EVO cooler in a Lian LI 207 case.    Currently I'm browsing, CPU fan running at 25% and case fans at 20%. Temperature is 40 degrees Celcius ...   When playing games it really stays cool as well.   So yes this is more than enough.",buildapc,2025-12-05 11:09:53,1
AMD,nsfrllq,Absolutely . I just switched my 7800x3d from an aio back to air cooling (NH-D15 G2) and honestly it runs just as cool if not cooler and it’s almost silent.,buildapc,2025-12-05 15:29:40,1
AMD,nsg10oj,I aircool a 7950x3d with Notcus D15s. Yes air is good enough.,buildapc,2025-12-05 16:16:11,1
AMD,nsg4tai,"Made my new build last week with 7800x3D and Thermalright Peerless Assassin 120 SE, works perfectly fine together",buildapc,2025-12-05 16:34:39,1
AMD,nsh1i0e,"Mine is air cooled, no problems at all.",buildapc,2025-12-05 19:13:45,1
AMD,nsjkjth,My AIO got me 20 degrees cheaper temps so no.,buildapc,2025-12-06 04:14:21,1
AMD,nsgoyih,Some AIOs are cheaper than high end air coolers and are easier to fit in some cases.,buildapc,2025-12-05 18:12:36,1
AMD,nsehrwz,"So yeah as I stated earlier I'm looking at the Meshify 3 case   [Meshify 3 Case](https://www.amazon.de/-/en/dp/B0CS3T22P8/?coliid=ICI4O54M5DZOY&colid=3H6AYEYF16QIN&ref_=list_c_wl_lv_ov_lig_dp_it&th=1)   Which I believe comes with 3 front fans preinstalled (I dont think they come with any top or rear fans), would I need to add top and rear fans and if so how many?  Thanks.",buildapc,2025-12-05 10:19:46,1
AMD,nseepek,Thanks last time I built a PC was before the liquid cooling was a thing so air coolers all the way for me!,buildapc,2025-12-05 09:50:07,1
AMD,nshfvzx,You shouldnt need an AIO at all with a CPU that barely hits 80watts under a full synthetic load.   $30 Peerless Assassin is more than enough for the 7800X3D.,buildapc,2025-12-05 20:28:33,2
AMD,nsem0h9,"No, you don't. All that air coming in will get pushed out by the pressure just fine unless you block the flow with something.  I use a Phantom Spirit Evo in a Meshify 3 with a 9800X3D and it's basically impossible to get it hot enough to even need to think about it. And that's with the front case fans turned down from stock.  Do note that it doesn't have a fine dust filter on the front intakes.",buildapc,2025-12-05 11:00:03,1
AMD,nserp21,Should be fine.,buildapc,2025-12-05 11:49:59,1
AMD,nsefhe7,"Just make sure it fits your PC case, they are called tower coolers so some cases might struggle with it.",buildapc,2025-12-05 09:57:39,2
AMD,nrv9gjy,I would go for AM5 because its future proof. I wouldn't invest in a dead platform like AM4 because you have essentially no upgrade path besides the 5800X3D.,buildapc,2025-12-02 11:15:39,6
AMD,nrv9phq,Am5 unless you already have ddr4 and it makes sense.,buildapc,2025-12-02 11:17:50,3
AMD,nrv9pqr,"Honestly yeah its more for ddr5 but the price for ddr4 is on the rise to so just get the future proof, being ddr5",buildapc,2025-12-02 11:17:53,1
AMD,nrva5ib,"You won't find a 5700X3D at a reasonable price. Since you don't have an AM4 platform already it makes even less sense.  How fast is that 32GB of RAM?  I can hardly believe I'm saying it but in many markets the 14600K is actually great value and performs very decently with DDR4.  DDR5 is just so stupendously expensive it hardly makes sense to get it in terms of value if you already have 32GB of decent DDR4. AM4 is just as dead as LGA 1700, and the 14600K performs pretty great, even with DDR4 better than a 5700X3D in most games.   Make sure to get a decent DDR4 board, make sure it's on the latest bios, and you should be done for around 300 EUR/$300. Less than the cost of JUST 32GB of DDR5.  [PCPartPicker Part List](https://nl.pcpartpicker.com/list/yMF6DZ)  Type|Item|Price :----|:----|:---- **CPU** | [Intel Core i5-14600K 3.5 GHz 14-Core Processor](https://nl.pcpartpicker.com/product/jXFmP6/intel-core-i5-14600k-35-ghz-14-core-processor-bx8071514600k) | €199.90 @ Amazon Netherlands  **Motherboard** | [ASRock B760 Pro RS/D4 ATX LGA1700 Motherboard](https://nl.pcpartpicker.com/product/pcZ9TW/asrock-b760-pro-rsd4-atx-lga1700-motherboard-b760-pro-rsd4) | €108.72 @ Amazon Netherlands   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **€308.62**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-02 12:23 CET+0100 |  No wifi/bluetooth though.",buildapc,2025-12-02 11:21:44,1
AMD,nrvabr9,"I feel your pain - AM5 is better option, but current RAM prices are insane.",buildapc,2025-12-02 11:23:14,1
AMD,nrybkoc,i'd suggest intel core ultra platform  you can pair your 9070xt with a 265 variant (265f/k) and a b860 mobo,buildapc,2025-12-02 21:09:22,1
AMD,nrv9nkq,Where do you live and what are your preferences? ATX or M-ATX? Black/ white? Do you have the right mounting Brackets for AMD?,buildapc,2025-12-02 11:17:22,1
AMD,nrybro1,there's no such thing as future proof,buildapc,2025-12-02 21:10:19,1
AMD,nrvach3,"i already have a 32GB DDR4, but either way i would probably repurpose the old part into a separate PC, so i will need another 16GB DDR4 stick if i go AM4",buildapc,2025-12-02 11:23:25,1
AMD,nrvaasp,"i already have a 32GB DDR4, but either way i would probably repurpose the old part into a separate PC, so i will need another 16GB DDR4 stick even if i go AM4",buildapc,2025-12-02 11:23:01,1
AMD,nrvaw0b,"it's 32GB (2X16GB)DDR4 3200MHz CORSAIR VENGEANCE LPX C16W    isn't 13th and 14th gen intel has some performance issues? i don't really mindwifi / bluetooth as i already got a dongle.  yeah 5700x3d is expensive, it's almost at the cost of 7500f and B650m combined, but B550m mobo is so cheap now i could get it for like $60",buildapc,2025-12-02 11:28:07,1
AMD,nrvc5sd,"yeah, and my 9070XT is quite a panic buy too. but now i feel my system is quite slow against that monster (lots of 100% cpu usage)",buildapc,2025-12-02 11:39:00,1
AMD,nrva2wl,"i live in indonesia, im rocking a white asus ap201 now and wants to keep it, so i'm pretty much locked on mATX/mini ITX. right now my cooler is stock for 10400f, but looking to upgrade to arctic liquid freezer iii pro 360.",buildapc,2025-12-02 11:21:06,2
AMD,nryc7se,Yes absolutely there is. The AM5 platform is supported until 2027. Thats the definition of future proof.,buildapc,2025-12-02 21:12:29,1
AMD,nrvbsil,"If ddr5 prices did not skyrocketed, it would be a hard pass on am4, but as it is. Check pricing and calculate.",buildapc,2025-12-02 11:35:55,2
AMD,nrvbd3e,That's the point. Yeah youre going to spend more but instead of having 2 or 3 more years of am4 you'll get the ridiculously long life still left in AM5 so you'll only need to upgrade gpu's and cpu's.,buildapc,2025-12-02 11:32:13,1
AMD,nrvbf4s,"That's very solid DDR4.  > isn't 13th and 14th gen intel has some performance issues?  Not performance but degradation, mostly happening to their 700 and 900 CPUs, but it's been fixed with bios updates.  I'd look at pricing of the 14600K and a decent B660/B760/Z690/Z690 board and see if that's cheaper than the 5700X3D plus B550M. Either way I'd stay on DDR4 for now.  > i don't really mindwifi / bluetooth as i already got a dongle.  On board wifi/bluetooth is usually a bit better. If you find a board with it included for a small premium I'd consider it. :)",buildapc,2025-12-02 11:32:43,1
AMD,nrvaf6l,"Okay, good to know. What is your budget?",buildapc,2025-12-02 11:24:05,1
AMD,nryee9u,"your future has only 2 years?...  i just won't care much. i'd buy whatever the latest gen and after several years if one day i feel my pc start to struggle and fail to serve my purposes i'll build a new one. i never ""upgrade"". i don't like for example pairing a cpu of year 2028 with a gpu of year 2024 and a ssd of year 2020",buildapc,2025-12-02 21:23:00,0
AMD,nrvcxwf,as it is right now i could only buy the 16GB DDR5 which i think quite small -- my current system took over 20GB of ram under load,buildapc,2025-12-02 11:45:25,1
AMD,nrvcrxl,Thanks for the info! i'll look into it.   my desktop already has a lan cable connected to it so i don't really need wireless haha and i use a bt dongle with a usb hub extension so it's near my sofa. i don't really trust the onboard bt  as i can't extend the range easily,buildapc,2025-12-02 11:44:04,1
AMD,nrvbpba,"the budget is quite flexible, i just want something that does not bottleneck my 9070xt. i have spare pc with aging 3770 + 6600XT AND an Nvidia 5060 which i'm not sure what to with -.-'. upgrading 10400f seems to unlock it so i can use it with either 6600xt or 5060.  Previously i choose intel because of hackintosh support, now with apple silicon out and seeing how good AM4 is for gaming i'm quite regretting that decision -.-'",buildapc,2025-12-02 11:35:11,1
AMD,nryewdj,Then tell me which platform is the most future proof,buildapc,2025-12-02 21:25:24,1
AMD,nrvdbxj,What are you using it for? Gaming or do you do art or video editing?,buildapc,2025-12-02 11:48:38,1
AMD,nrve2xm,"If I were you, I'd sell your spare parts, keep the 9070XT and use the money for upgrading your PC. Here's the [list](https://pcpartpicker.com/list/cB9GWc)",buildapc,2025-12-02 11:54:44,1
AMD,nrve85r,What PSU do you use?,buildapc,2025-12-02 11:55:54,1
AMD,nryfp8w,there's no such thing as future proof  a lot can happen. we can't predict the future  and i simply don't care because i never upgrade. i always build new pcs,buildapc,2025-12-02 21:29:13,0
AMD,nryg84b,But you're no OP so your opinion is irrelevant,buildapc,2025-12-02 21:31:41,1
AMD,nrygmfg,"op didn't mention future proof. it's your concern maybe, not necessarily his",buildapc,2025-12-02 21:33:34,0
AMD,nt05uh2,Are you still planning to use DDR4?,buildapc,2025-12-08 21:21:49,2
AMD,nt06j5h,">cpu intel i7 14700kf or i7 12700kf \[mainly cause of temps and wattage\]  Why not i5 14600K instead of 12th gen?  In case you don't want to buy DDR5 because of insane prices, LGA 1700 can be a good pick.",buildapc,2025-12-08 21:25:09,1
AMD,nt072s6,"For DDR4, 14600k (updated bios) > 12700k > AM4 X3D > rest of AM4.  For DDR5, AM5 X3D >>>> Ultra 7 265k (updated bios, tuned with fast ram) or 14600k (updated bios) or none X3D AM5s. AM5 X3D is decently far ahead and the other 3 trade blows depending on the configuration. I would personally avoid 14700k and 14900k even with updated bois.",buildapc,2025-12-08 21:27:49,1
AMD,nt068x5,yeah ive got a ddr4 kit lying around 64gb and ram prices rn ill eventually upgrade to a dr5 motherbord tho,buildapc,2025-12-08 21:23:45,1
AMD,nt07gc8,is it better than the other ones?,buildapc,2025-12-08 21:29:39,1
AMD,nt0809h,so im hearing i should go ddr4 intel i5-14600 then eventually ddr5 amd am5,buildapc,2025-12-08 21:32:22,1
AMD,nt06g2x,"Using DDR4 on AMD CPUs would require going back to AM4 (Ryzen 5000 and earlier). If you're building a new PC, I would look at Intel 14600K instead.",buildapc,2025-12-08 21:24:44,4
AMD,nt06spl,"Not worth it to change motherboard just because of the RAM, in that case I would pick a newer CPU with new mobo+ram too.",buildapc,2025-12-08 21:26:26,0
AMD,nt08720,"Games do not benefit from core count, but from bigger cache memory and better architecture, so i7 doesn't mean it is better than i5. Although 14th gen is just overclocked 13th gen, it is still a newer generation compared to 12th gen and i5 14600K easily beats i7 12700K in gaming.  i7 14700K is just a bit better compared to i5 14600K, but also more expensive, temps will be higher, more power hungry and.",buildapc,2025-12-08 21:33:20,1
AMD,nt0743z,the i-5 14600k? also you are right i forgot amd doesnt do ddr4 and ddr5 on the same cpu \[i may be tempted to wait a few for ram prices to cool down and ill get a ddr5 kit,buildapc,2025-12-08 21:27:59,1
AMD,nt09aox,ddr5 is just to expensive rn and i have 64gb of ddr4 on me,buildapc,2025-12-08 21:38:49,3
AMD,nt08nby,ohh i was just like bigger number = betterbut that makes alot of sense,buildapc,2025-12-08 21:35:35,1
AMD,nt07ptu,"Yes, the 14600K. It performs similarly to the 14700K in games and is easier to cool.",buildapc,2025-12-08 21:30:56,1
AMD,nt0c9ug,"Yes, 6 core Ryzen 5 5600 for example can beat previous gen 16 core Ryzen 3950x in gaming because of quite better architecture. Of course this is not the case in productivity tasks, which can utilize multi core.  Also noticed from other commends you mentioning AM5, honesty i5 14600K is still a very good CPU, I don't see you needing to replace it any time soon unless you are aiming for the best possible performance with Ryzen 7 9800x3D or upcoming 11800x3D (or whatever number will the next gen be).  i5 14600K + RTX 5070 is a very good combo. The CPU will handle even stronger GPUs without bottleneck.",buildapc,2025-12-08 21:53:38,2
AMD,nt08dtn,k ill go with that one for now and go amd when i get ddr5,buildapc,2025-12-08 21:34:16,1
AMD,nrwjw9o,"The Intel build is two generations old, the AMD build is current gen.  You'd have to be insane to go for the Intel option.",buildapc,2025-12-02 16:03:25,22
AMD,nrwhxlz,amd,buildapc,2025-12-02 15:53:54,16
AMD,nrwi355,Amd all the way,buildapc,2025-12-02 15:54:38,10
AMD,nrwlges,"Unless the Intel build is somehow like 40% cheaper due to some crazy sale or something, AMD is the option. It's significantly newer stuff.",buildapc,2025-12-02 16:10:56,7
AMD,nrwlm41,OMG.  I just built an Am5 9000 series for not much money.  It's so good. Don't even bother with Intel.,buildapc,2025-12-02 16:11:42,5
AMD,nrwogtr,"Interesting, Build 1 should be significantly cheaper due to DDR4 vs DDR5. Same price, AM5 all day here.",buildapc,2025-12-02 16:25:21,3
AMD,nrwjecy,AM5 platform of course.. I'd think the Intel setup would be much cheaper because of what it is,buildapc,2025-12-02 16:00:58,2
AMD,nrwofn1,"AMD.  AM5 will support at CPUs to at least Zen 6.  That platform has a long life.  If you go with Intel, no upgrade path to future CPUs - you’re stuck with 14th gen as the only upgrade path.  I made the mistake of building my last rig using Intel, thinking I can always upgrade to the best 11th gen but new CPUs in 12th, 13th, 14th gen are even cheaper than equivalent 11th gen retail with updated features and the used market are charging near retail for old tech.",buildapc,2025-12-02 16:25:11,2
AMD,nrwi5rc,"Second build because it gets you on AM5, both have shit GPUs.",buildapc,2025-12-02 15:54:59,4
AMD,nrwjd8r,If they are the same price AMD platform for sure.,buildapc,2025-12-02 16:00:49,2
AMD,nrwjwv4,"If you can, AMD because it's a newer system and will upgrade better. The next generation of AMD processors will likely be a significant leap above current (30-40% after IPC, boost and other optimizations) and will hopefully be a drop in replacement. Buy once so you don't have to buy again.",buildapc,2025-12-02 16:03:30,1
AMD,nrxbe5i,Amd for this particular scenario.,buildapc,2025-12-02 18:14:43,1
AMD,nrxlimr,"The 13600 is a fine cpu but at equal price, the 9600X is the better value.   I would expect there to be a minimum $125 or so difference (intel costing less) due to DDR4. You are being overcharged somewhere.   Also, stay away from Asrock AM5 boards. There is something strange going on which probably doesn't effect the 9600x but you can never be sure.",buildapc,2025-12-02 19:02:27,1
AMD,nrxlmst,AMD,buildapc,2025-12-02 19:03:01,1
AMD,nrx0sae,AMD build but swap out the ASRock motherboard. I’ve read that they cause issues with Ryzen CPUs,buildapc,2025-12-02 17:24:27,1
AMD,nrwj79q,If this is for gaming go with 7800X3D or 9800X3D CPU.,buildapc,2025-12-02 16:00:01,-1
AMD,nrwnwsj,The AMD one is fine but I would change that GPU for one from Nvidia,buildapc,2025-12-02 16:22:42,-2
AMD,nrwosvq,"Yeah but cooler, mobo, and cpu are more expensive so it kind of evens out.",buildapc,2025-12-02 16:26:58,1
AMD,nrwidss,Planning to upgrade as soon as I can from that gpu.,buildapc,2025-12-02 15:56:03,2
AMD,nrx1178,"I have heard too, but I'm pretty sure that is with the X3D COUs.  Also I plan to get the build in a month or two, and I'm hoping by that time that the issue is fixed.",buildapc,2025-12-02 17:25:38,1
AMD,nrxria6,Are you 13yo?,buildapc,2025-12-02 19:31:38,2
AMD,nrx4fgt,If you already have the GPU roll with it until it's not enough. If you're buying that GPU as a holdover just don't. Get at least like a 9060xt 16gb.,buildapc,2025-12-02 17:42:04,2
AMD,nryu477,"A month or two.. are you *certain* prices won't change, that you'll still be able to get the systems as configured? I would be very surprised, even astonished if that was the case.",buildapc,2025-12-02 22:40:31,1
AMD,nryq2sj,"As far as I can tell, this is a rumor based on a few anecdotes anyway. Nobody has been able to recreate the issue in a controlled environment.",buildapc,2025-12-02 22:19:31,0
AMD,ns1jou8,well just have to wait and see.,buildapc,2025-12-03 10:07:40,1
AMD,ns2c5hd,"Of course! If you absolutely can’t buy now, then you can’t. I was just hoping you had some flexibility there with your budget, so that you could move faster and grab that deal, is all.",buildapc,2025-12-03 13:44:22,1
AMD,ns2g6pp,"Well, all the flexibility in my budget has already been 'flexxed' to its max, and I really can't buy now, as I'm not buying myself, my parents are getting it for me, and having a very flexible budget there is tough.",buildapc,2025-12-03 14:07:29,1
AMD,ns2h47b,"Understood. It’s just unfortunate timing is all. For a lot of people, I’m sure.",buildapc,2025-12-03 14:12:44,1
AMD,nt7vg0u,Yes. Get a 5800xt. Or a used 5800x/5700x. It's your best option unless you want to pay premium for a Zen 3 X3D chip.,buildapc,2025-12-10 01:54:17,3
AMD,nt7w7i6,5700x is a substantial upgrade from 3700x.  Of course 5700x3D is better in games but twice the price.,buildapc,2025-12-10 01:58:48,2
AMD,nt84rxl,"I have a similarly powered build. Not exactly matching, but roughly similar weight class, anyway.  3600XT CPU  5700XT GPU  32GB 3600mHz RAM  2TB 980 Pro  Honestly, I might do what you're thinking of and upgrade to a maxed out AM4 and wait for AM6 or 7. I don't want to wait for RAM prices to go down to build a whole new computer. I think you should go ahead and upgrade your current rig.",buildapc,2025-12-10 02:49:20,2
AMD,nt7xeqb,"5800x + 9070 will be your best budget bet...but that 550W psu is stretching it a little. Also check if its modular and actually has the correct cables for a new card.  I run a 5700x3d / 5070ti (3x8pin power) on a 750W just fine so you might be ok, but check with the GPU specifications.  If you upgrade your PSU & budget, your options will stretch your AM4's longevity somewhat.",buildapc,2025-12-10 02:05:55,1
AMD,nt8nmjw,"RTX 5060 Ti (16GB VRAM ideally). This is non-negotiable for your LLM goal. Ryzen 7 5700X is likely your best bet if you want to avoid the X3D price/cache-only focus but still want a huge performance jump. If you pair the RTX 5060 Ti with the Ryzen 7 5700X, you have a much better shot at running the whole rig safely on your 550W PSU.",buildapc,2025-12-10 04:54:04,1
AMD,nt89dw1,https://pcpartpicker.com/product/QtnXsY/asus-b650e-max-gaming-wifi-w-atx-am5-motherboard-b650e-max-gaming-wifi-w  This is good value,buildapc,2025-12-10 03:17:25,1
AMD,nt8a3cx,What physical size do you want?  What features do you want?  What budget do you have?,buildapc,2025-12-10 03:21:54,1
AMD,nt8ghxb,I went with the gigabyte b650 eagle ax. No issues.,buildapc,2025-12-10 04:03:40,1
AMD,nt8ak71,"I'll take a look, thanks!",buildapc,2025-12-10 03:24:52,1
AMD,nt8owh9,"That is amazingly good value from NewEgg! Wish I was building again, rn. (But then again... No I don't lol)",buildapc,2025-12-10 05:03:35,1
AMD,nt8qke5,I have the B850 version of this board and it’s pretty solid. as far as I can tell the B650E and B850 versions of this board are identical. definitely a good deal,buildapc,2025-12-10 05:16:14,1
AMD,nt8aj9y,My budget would probably be around 200-250. My case I feel like is a pretty decent size but I don't need something massive. And features would probably have to be a decent amount of USB slots since I used a lot of those on my current mobo as well as ease of use in the BIOS since I know I'll probably have to update that,buildapc,2025-12-10 03:24:43,1
AMD,nt3s8i1,Are you having trouble running anything? You’re over thinking this imo,buildapc,2025-12-09 12:56:19,1
AMD,nt3v3b3,"I have similar system except the GPU, which is a 6800XT (>5% difference XD) and the RAM, which is 3600MHz for me, I'm waiting out AM6 and the next gen cards, had no problem with any game (even new ones) at 1440p tho I was forced to the use of FSR, speaking of FSR, I still have hope that we get a better version than 3.1 for older cards too.....",buildapc,2025-12-09 13:15:02,1
AMD,nt3wjf3,I think you will be fine for a while yet.,buildapc,2025-12-09 13:23:55,1
AMD,nt3svru,"Not really, more thinking how long it will last with newer games since it feels like with each new game release the spec requirements are higher and higher. Like when will I have to update or change to ddr5.",buildapc,2025-12-09 13:00:40,1
AMD,nt3zzu2,"Depends entirely on what games you want to play in the future, how well they are optimized, and how low you're willing to drop your settings. If you don't demand the very best, then you'll be fine for years.   Also you can't upgrade just to DDR5, it will require a new CPU and motherboard.",buildapc,2025-12-09 13:44:34,1
AMD,nrs6tyy,You aren't actually replacing a GPU.  You are installing one into a system that doesn't have a GPU.,buildapc,2025-12-01 21:59:05,4
AMD,nrs62ct,The 5600GT is a CPU. The RTX 3060 is a GPU.  What exactly are you trying to do?  You can link photos from a hosting site like imgur.,buildapc,2025-12-01 21:55:08,1
AMD,nrs6c6q,"I'm aware but looking at my specs there is no graphics card listed anywhere, all it says is AMD Ryzen 5600gt with Radeon Graphics. And looking inside my PC the areas where you plug into the graphics card are directly connected to the motherboard rather than a graphics card",buildapc,2025-12-01 21:56:32,1
AMD,nrs6zh3,"The 5600GT has integrated graphics.  All you need to do is install the new GPU, connect your monitor to it, then install the Nvidia drivers.",buildapc,2025-12-01 21:59:52,6
AMD,nrs7c3y,"The CPU has a shitty little GPU inside of it. If you want a GPU, you need to make sure your PSU can power it properly.",buildapc,2025-12-01 22:01:44,2
AMD,nrs8685,That's good to know but the monitor connection thing connects directly into the motherboard. Is that gonna be an issue?,buildapc,2025-12-01 22:06:10,1
AMD,nrs8d4b,> monitor connection thing  The video port? Like HDMI or Displayport?   You'll use the ports on the RTX 3060 instead.,buildapc,2025-12-01 22:07:11,2
AMD,nrs8g99,"Once you install your new GPU, you'll want to use the monitor connection ports on the GPU, not the motherboard's.",buildapc,2025-12-01 22:07:39,2
AMD,nrsobxr,"Plug your display cable into the GPU once installed, then download drivers from nvidia's website.",buildapc,2025-12-01 23:36:51,1
AMD,nspj5pi,"I just upgraded my 3600 to a 5700x and I'm a happy camper. I paired it with a 7700xt, but that was before the 9060/70xt launched or I would have gotten that one. Similar performance though.",buildapc,2025-12-07 04:12:52,2
AMD,nsojt2m,"These were my 2 options as well, I decided to go with the 5060 ti, yes the 5060 ti is usually a bit more expensive for just a bit more performance but in my opinion I feel like the 5060 will be more reliable in the future, with the dlss etc. both are great cards, and I think because your on the am4 the difference between each one won’t really matter, just buy the one which is the best price at the moment",buildapc,2025-12-07 00:27:14,1
AMD,nsoju7a,"Get a 5700x3d put one in my brothers PC it's a beast, and the b580 is quite competitive if your rocking pcie 4.0",buildapc,2025-12-07 00:27:24,1
AMD,nsp5y2s,RDNA 4 is better than DLSS at the moment so take that what you will if you want to play at high resolutions.,buildapc,2025-12-07 02:46:12,0
AMD,nsole9e,"atm the difference in prices between 5060 ti (450€ + ) and rx 9060XT (387-400€)    I also really enjoy the nvidia interface, control panel and im generally used to nvidia but they're expensive as f...",buildapc,2025-12-07 00:36:31,1
AMD,nsokpqi,issue with that cpu is that its either 400+€ or out of stock 98% of the time   i'v been on 4+ websites and checked the notification if it comes back in stock  \^that was 3 months ago,buildapc,2025-12-07 00:32:28,3
AMD,nsoqj8k,That cpu is no longer budgetable.. Use to be.. But that it shot up like a rocket  It's more than twice the price of a 5700x,buildapc,2025-12-07 01:08:29,2
AMD,nsoqoiw,"> I also really enjoy the nvidia interface, control panel  lol thats like the 1 place where nvidia gets trounced",buildapc,2025-12-07 01:09:25,2
AMD,nsomfqz,"If that’s the case then the 5060ti may be the better option for you, the GPU might be with you for a long time so if you ever decide to upgrade to am5 take that into account as well. Paying a bit more for the 5060ti might not sound as bad in the long term",buildapc,2025-12-07 00:42:50,1
AMD,nsop47y,Sorry! That stinks,buildapc,2025-12-07 00:59:26,1
AMD,nspswn5,"I'm sorry, guess I'm out of the game of things. I bought it pre tarrifs on AliExpress for cheap. Runs great wish x3d was cheaper currently.",buildapc,2025-12-07 05:20:03,1
AMD,nsq74p0,"Personally, neither. WoW appears to be sensitive to L3 cache amount (esp seeing how much improvement it has with X3D chips), so you want to avoid 5500 and 8400F as they both have halved L3 cache vs. other chips like 5600 and 7500F.  Try to get a PC with 7500F / 7600(X) / 9600(X) in it.",buildapc,2025-12-07 07:21:09,9
AMD,nsq7pfb,"However much I'm not a fan of the 8000 series it is a good value   Go for the 8400f build though I would encourage you to spend a little more and get the 9060xt 16gb since it will hold up slot better in the long term.  Ram is stupid expensive right now.   Although the 8400f especially will benefit from better ram, i think if it's a big enough saving, go for some cheap 5200-5600mhz cl36-40 instead.",buildapc,2025-12-07 07:26:44,1
AMD,nsq80wj,I'm guessing the 7700 with 32gb ddr5 would be much better?  I'm helping my little brother get a new gaming computer and he said he can wait another pay period which would increase his budget by $200,buildapc,2025-12-07 07:29:50,1
AMD,nsq88h6,"7700 is a fine choice too, but if you can stretch the budget a little more, I'd focus more on getting a PC with CPU I listed above, and upgrade the GPU to something like RX 9060 XT 16 GB.",buildapc,2025-12-07 07:31:54,2
AMD,nss4ece,"Since he seems a bit price sensitive, I'd look at the 7600x instead which is often the same price as the 7500f but a little faster.  There's kind of a large jump in games due to the cache, then it's smaller due to the clock frequency.        The extra cores won't help WoW much unless he's doing a lot of stuff in the background.",buildapc,2025-12-07 16:18:01,2
AMD,nsq8sf1,"The computer I initially told him to get actually had the components listed below, but was around $350 out his budget. The specs are pretty much what you advised. I'll go back to pushing him towards that one! Thank you!   GeForce 9060XT 16gb 32gb DDR5 6000mhz Ryzen 5 9600X 1tb SSD",buildapc,2025-12-07 07:37:19,2
AMD,nsqg2qp,"But going straight to a 7800x3D shouldn't actually be that expensive - at least here for me it only costs €60 more than the 7700 (current offers included, otherwise €90-110 - then it would be smarter and more economical to get it straight away if that's the case :)  Just as an option, unfortunately it's different in every country and it's not worth it :)",buildapc,2025-12-07 08:48:54,2
AMD,nss8uma,"I'd say about 95% of what he plays on pc is World of Warcraft. Everything else he prefers on the Xbox. He also almost exclusively plays Classic over retail, if that makes much of a difference. He does raid, but not often.  His current budget is $850, but said he could make it $1050 on his first February check.  Thanks for the advice! I'll mark the 7600x on the possible list.  So for WoW the preference is a faster over a CPU with more cores? Is 16gb vs 32gb very important in WoW?",buildapc,2025-12-07 16:41:05,1
AMD,nsq9gl0,"Yeah, getting Radeon RX 9060 XT 16 GB / GeForce RTX 5060 Ti 16 GB will also be able to play more demanding (esp VRAM heavy) games if he wants to play them, perfect for 1080p gaming (and even some 1440p gaming).  His last goal afterward would be (if he still plays WoW in about 2\~3 years time) to upgrade his CPU to an final AM5 X3D chip about 2\~3 years later-- and this will provide insane performance uplift for WoW specifically.",buildapc,2025-12-07 07:44:01,1
AMD,nssbski,"If they only play one game, it can make a difference as you can optimize a build just for that game.  Some games are more CPU demanding vs GPU demanding.  WoW is fairly CPU and cache sensitive, so you're better off shifting your budget from GPU to CPU on it.  As a good rule of thumb, the older the game the fewer cores are needed.  A quick search shows WoW uses up to 4 cores so you still have 2 cores mostly doing nothing or background tasks.  The 7700x would still be faster, it's just a minor bump and the money isn't worth it.  As far as I can tell you can get by with an 8GB card if all he likes to play WoW with low settings, but 16GB would be better if he likes high settings and definitely will age better if the PC will be used for many years into the future.",buildapc,2025-12-07 16:56:13,1
AMD,nsqb14j,"Sweet, that's what we will shoot for! I thank you very much for the advice. :) !2  He buys a new pre-built every 2-3 years, so won't have to worry about the last part. He gives the old one to our youngest brother every time.",buildapc,2025-12-07 07:59:09,2
AMD,nsuc2fo,"Just get the cheapest 9060 XT 16gb model. The difference between the 2 is at most 10%. They are also both about 50% faster than a 3060, can't go wrong.",buildapc,2025-12-07 22:53:20,1
AMD,nsuid22,that is not true there are plenty of scenarios where the 5060 ti is 30+ percent faster than the 9060 xt and plenty of situations where the 9060 xt is 20+ percent ahead of the 5060 ti for example in arc raiders the 5060 ti is 40+ percent faster than the 9060 xt and is AC shadows the 9060 xt is 20+ percent faster than the 5060 ti,buildapc,2025-12-07 23:29:11,1
AMD,nsunnr1,"There are always outliers, but a 40% gap seems impossible. I tried to find information about this, but I was unable. Can you share your source?",buildapc,2025-12-07 23:59:54,1
AMD,nsuufjz,[https://imgur.com/a/sSyheHq](https://imgur.com/a/sSyheHq)  42 percent  or maybe cs2 where there is a 70+ percent gap  [https://imgur.com/a/hDftBbW](https://imgur.com/a/hDftBbW)  or another 42ish percent gap in elden ring  [https://imgur.com/a/JsZAGac](https://imgur.com/a/JsZAGac)  also ue4 titles in general are really rough on rdna 4 like take this random ue4 game  [https://imgur.com/a/XRCGkSN](https://imgur.com/a/XRCGkSN) the 5060 ti is 40 percent faster  or another ue4 title stellar blade [https://imgur.com/a/J8JqzMY](https://imgur.com/a/J8JqzMY)  the 5060 ti is another 35+ percent faster  there are always lots of outliers,buildapc,2025-12-08 00:37:54,1
AMD,nt3l5wp,"Long explanation:  1.1. First I downloaded Adrenalin from official site (25.11.1) + GPU Tweak III from Asus. At first there was no problem that I could identife (I'm no pro user and learning everything by myself like many of us) Only that my fans in GPU Tweak III were stuck on 30% and there was nothing I could do. Later Installed MSI afterburner and problem gone. First game that I Opened was Dispatch. (Not the best game for testing but... my old pc has issues with UE4 for some reason). Everything worked fine, like what there is to not work in prerendered ""game"". Nontheless it is information. CPU GPU Usage was almost none if we compare stats with freststarted pc.  1.2. Outer wild was working fine.   2.0. Then there was minecraft (neoforge 1.20.4) with distant horizons and shaders (separated)  2.1. Minecraft with distant horizons and 60 fps lock showed me was working fine. 32 chunks + 100 Distant Horizons + Max CPU usage (5/5) + Quality of render (5/5). While it was redering, CPU temps were between 60-80C. When i stoped flying like a manic and started walking temps dropped to 40-55C. No Crashes, No Lags.  2.2. Minecraft with shaders (without distant horizons) - Complementary Shaders Unbound, all maxed out + 32 chunks. There were some lag spikes but only while rendering those chunks ('cause it's minecraft) but after that there were no trouble. Stable 60 fps, temp between 40-50C with no active fans.   3.0 After that windows decided that it is time to make some shitty unnecessary updates (still figuring how to turn them off, would've like a helping hand in that matter too).    After the ""update"" :  3.1. Adrenalin wasn't working at all  3.2. GPU Tweak (was still using it in this moment) was closing itself every 5 minutes or so.   3.3. So I did what every ""wanabe pc master race"" would do. I used my knowledge of task manager, device Manager and Programs and Features. Closed/Deleted manualy everything that was connected to drivers, reboot, reinstall. Everything ""worked"" again, Tweak was still stack at 30% fan but adrenalin worked fine. At that moment I should say that when I opened device manager I saw that several drivers was in yellow ! triangle. 1 PCI controller + AMD Streaming Audio Device + Other devices(Unkown Device + network controller). After drivers reinstallation those errors in device manager were still there. I thing other devices is xbox 360 wireless adapter but for now I cannot confirm it. (For now after everything I did later that day, PCI controller gone, but other still there)   4. Tried opening blender for the first time and it crashed with OpenGL 4.6 error. Open Adrenalin, setting, and saw that after reistallation i didn't have anything related to OpenGl, ""-"" all that i saw. Nice. Another reinstallation then. Reboot. Now blender works.",buildapc,2025-12-09 12:04:26,1
AMD,nt3l77v,"5.0. Next step was Space Marine 2. Downloaded, boot it up and it runs like shit. Even menu lags. Went to setting, checked GPU that the game was using: ""Radeon Graphics"". So, the problem was that the game could boot up, but i didn't saw $981,68 GPU (Yeah, pricing in my country is shit, almost $1k for GPU and there no better deal, only downgrading). The frustration was building up, but okay, let's try fixing it.  Several hours of deleting/reistalling drivers, googling, serfing in reddit, reading different opinions and experiences with same builds and how other were shitting on amd drivers, in that moment downloaded MSI Afterburner and installed older version of adrenalin (25.9.2). Tried booting up Space Marine 2. The Joy, the hope when I saw that game started smoothly, different tabs of adrenalin software poped up while booting, something related to FSR 4, ""Press ALT+R for more"" yada yada. Didn't even saw start menu, crashed. The frustration was building up. Too salt the wound, for some reason MSI Afterburner now didn't show me any GPU data and was not registering anything except the fact that there is GPU. No fan controll, no power usage %, nothing.  5.1. Installed newest drivers (25.11.1) - MSI Afterburner work again, Space marine 2 still wasn't seeing GPU.  5.2. Deactivated intergrated graphics, SM2 now won't boot up at all, crash from the start.  5.3. First time using DDU to uninstall drivers, using it without safe mode but deactivated connection to network so windows wouldn't download any drivers on it's own. Reboot. Installed Adrenalin (25.9.1). Reboot. Reinstalled MSI Afterburner. Reboot. Warhammer still crashes while suggesting me to report the issue. Msi Afterburner working for some reason. Boot up SM2, ALT+R for adrenalin menu, disabling everything modern AI setting and other stuff and forcing game to run at 60 FPS. Crashes. Googling. Finding out about power limit in MSI afterburner, Changing Power Limit from 0 to -30%. Boot up the game. IT'S RUNS. IT'S WORKING. Turned everything to ultra, CPU 60-70C, GPU 50-55C, 60 FPS STABLE. No lag spikes, no artifacts, nothing. I DID IT, YES.  Tried game. 5 mins, changes audio language to english. Crash. Reboot. Everything works, 1 hour playing not for the enjoyment, but for testing. Exit. Changed Power limit back to 0%, everything still works. wtf. It was not booting up before with 0% power usage, but now it is. Small Victory, but not the happy one. Sitting in old drivers with everything that cards offers in OFF state. But I need to test something more.   6. Booting up RDR2, not even saw black screen, it didn't start at all. Long story short, DDU with safe boot didn't help, older versions (25.3.1 didn't boot even adrenalin, 25.5.1 rdr2 didn't work, 25.6.1 didn't work)  7. Cyberpunk 2077, same, Crash without booting.  8.0. Then I thought, and downloaded PEAK, steam asked what I prefer, vulkan, dx11, dx12.  8.1. VUlkan work fine  8.2. DX11 Works fine  8.3. DX12 Game crashes instantly.  The frustration now was at PEAK (get it?)   And there I was, whole weekend wasted on troubleshooting and all I get is the reason why nothing works, but not how to fix it.   No human shall endure that kind of torture in my opinion, made some comments in different posts that suggests using DDU again but nah, this will be waste of time.",buildapc,2025-12-09 12:04:43,1
AMD,nt3l9q9,"What's next? 1. Next step for me is install clear windows and before everything istall GPU drivers while offline. 2. If that will not work, someone suggested tweaking some setting in BIOS, but from what I saw there is no EXPO ON, no PBO on, NO any other option on that seems shady. 3. If that will not work, someone suggested updating Motherboard BIOS, which I don't know how to do correctly and i'm shitting my pants thinking about it. +It sounds so stupid for me 'cause i never in my 25+ years of gaming updated bios on old pc, and now i need to updated it on PC part that not even 1 year old? Motherboard is newer than any part from the build. CPU 2023, GPU Q1 2025, Motherboard Mid2025. And you wanna say that issue is that manufacturer didn't give a shit about optimization and tests and now it's user problem to make everything work together? in 2018 I just builded pc and starter playing, but now in AI age I have feeling that half the work (if not more) done by AI and no one checks for anything.  While I didn't solved my problem for now, I still have hopes. Plus I hope someone will find my troubled joureny helpfull for themselfs, or help me, ehehe. In worst case I can return PC and refund some details like GPU and Motherboard to change them to others parts, thx for $100 i left specially for case like this. (in case of GPU: rx 9070 xt -> rtx 5070 ti, wich will cost me $200 more +newer case 'cause 5070ti is 2 cm bigger)   I'll keep here posting about everything, and once again, any help helps. Thx.   Q/A:  Is it pre-build?  Short answer no. Long: I bought details in my retail and it was build by worker of said retail, BUT NOT TESTED IN ANY SOFTWARE, it is not pre-build in that term. I could've build it myself, but i rather leave $100 for $2k build than trying to convine stuff of said retail that it's not me that broke pc part but rather it was already broken (happend before several times + worked there and know their policy). So they build it, run to BIOS and i took it home.   60hz monitor?  In future planning to take 144 hz 2k monitor, but now i'm out of cash.   Reason for upgrading?  Well, always wanted to do 3D, even in 2018 took rx570 for redering, but I was scammed and render wasn't working with that radeon GPU at all and with everything that happend in life I kinda left the idea. But now to much money was thrown away, so I don't have plans in backing up, i Would rather buy 5070ti and give away $200 more than sitting with halfworking piece of garbage.   SSD 512gb?  As i Said, I can buy more later, i'm already in debt with this build.",buildapc,2025-12-09 12:05:17,1
AMD,nswc4a3,"Definitley the 3600. Bigger upgrade path, and the multithreading is infinitley useful. I had a 9600k and it was heavily bottlenecking my 2060 super (even OCd to 5 ghz",buildapc,2025-12-08 06:40:59,2
AMD,nswbert,I would go for the ryzen because of am4 upgrade potential,buildapc,2025-12-08 06:34:23,1
AMD,nszzrso,What's a good and reasonably priced upgrade for the 3600 now using the same motherboard?,buildapc,2025-12-08 20:51:50,1
AMD,nt00d6c,"You mean what the upgrade path would be? It depends on your workload. If its simply needing more cores, you can go up to a 3700x, or a 3800x, or for the best of that platform a 5800x3d. Am4 gives you compatibility up to 5000 series AMD",buildapc,2025-12-08 20:54:46,2
AMD,nt3wlq7,"I know the 5700x3d & 5800x3d are the best for am4, but the 5600x, 5700x, & 5800x are pretty good alternatives. I would put the extra money into a gpu as it will be the bottleneck in 99% of systems. Try to get 2x16 3200MHZ ram for the ddr4 ram if you can, but 2x8gb 3200MHZ ram will work fine for 98% of current games",buildapc,2025-12-09 13:24:18,1
AMD,nsl0ffm,"I gonna say, your pc is alright for 1080p for height end gaming and some 1440p games",buildapc,2025-12-06 12:18:36,2
AMD,nskq1fx,That's defiantly a PC.,buildapc,2025-12-06 10:40:45,1
AMD,nskqa8k,Sensible build in today's economy imo.  This is basically the build I would do right now if I wasn't trying to wait for AM6.,buildapc,2025-12-06 10:43:15,1
AMD,nskqc7t,"I'd avoid ASRock still, they have been killing 9000(X3D) CPUs and while it's allegedly been fixed (several times now) I wouldn't risk it, in case you ever want to upgrade your CPU. There should be plenty of choice within your budget. Should be fine with 7000 CPUs though.  Not familiar with that PSU but rest looks solid. Hope you got that RAM for a good price. :')",buildapc,2025-12-06 10:43:47,1
AMD,nsl7gob,"Using PcPartPicker can help you get a better overall view of your build and let others suggest changes, but besides that, without knowing your budget or what you are going to be using it for that's a fairly reasonable build.",buildapc,2025-12-06 13:13:39,1
AMD,nsll399,"Thanks, just what I was hoping for with this.",buildapc,2025-12-06 14:40:21,1
AMD,nsku4vy,defiantly :),buildapc,2025-12-06 11:20:55,1
AMD,nsku7ju,"Thanks, that's what I was going for and with a path for upgradability for the GPU and CPU in the future if required.",buildapc,2025-12-06 11:21:37,2
AMD,nsku3qu,"I got a good deal for the motherboard with type C and wifi and the reviews seemed alright I was hoping to upgrade the CPU to the 9000 series a few years later. Fingers crossed.     Regarding the PSU, it's a B- rated PSU as per this PSU tier list - [SPL's PSU Tier List](https://docs.google.com/spreadsheets/d/1akCHL7Vhzk_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/)     The RAM costed me $208.27. I wish I had pulled the trigger earlier so I would have got same spec RAM for $156. :/",buildapc,2025-12-06 11:20:38,1
AMD,nslkcfq,[https://pcpartpicker.com/list/GRLKyW](https://pcpartpicker.com/list/GRLKyW)  A good mix of gaming(strategy and sports games) + entertainment(1080p and above) + productivity.,buildapc,2025-12-06 14:35:53,1
AMD,nskuo4v,"> I was hoping to upgrade the CPU to the 9000 series a few years later. Fingers crossed.  Then just check by that time to see what the status of ASRock boards is by the time you are ready to upgrade to a 9000 or 11000 CPUs.  > The RAM costed me $208.27  That's a great deal in today's prices. A 32GB kit with those specs will cost you over $300 now.  Enjoy the performance, 7600 with 9060XT is a great combo.",buildapc,2025-12-06 11:26:01,2
AMD,nskwe9i,">Then just check by that time to see what the status of ASRock boards is by the time you are ready to upgrade to a 9000 or 11000 CPUs.  Will keep that in mind.  >That's a great deal in today's prices. A 32GB kit with those specs will cost you over $300 now.     So true, the one's I bought went out of stock too. Glad I did not wait further for it.     Thanks!",buildapc,2025-12-06 11:42:17,3
AMD,nrwxqry,Building a PC is getting worse and worse.,pcmasterrace,2025-12-02 17:09:48,450
AMD,nrwucey,"They are well aware that with current RAM prices, their sales over the next 1-2 years will drop. So imo they increase prices now and hope this somehow compensates for the inevitable price drops during those next few years. Or maybe that people now run an panic buy after the news.",pcmasterrace,2025-12-02 16:53:21,329
AMD,nrwwkcb,Damn anyone who didn’t build something this year kinda missed the mark for the next few at this rate,pcmasterrace,2025-12-02 17:04:04,583
AMD,nrwv6t0,Say hello to my little 5700x3d,pcmasterrace,2025-12-02 16:57:23,233
AMD,nrws1xt,At this point my only hope is becoming old enough to outgrow gaming,pcmasterrace,2025-12-02 16:42:28,557
AMD,nrx1c6s,AMD prices keep Ryzen.,pcmasterrace,2025-12-02 17:27:06,61
AMD,nrwv02k,Merry Christmas everyone lol,pcmasterrace,2025-12-02 16:56:30,79
AMD,nrwymft,There's been a massive surge of these rumoured price increases on lots of products to sort of force people into buying them up before the holiday season. Not saying these rumours arent true but its just strange so many products are projected to increase prices next month.,pcmasterrace,2025-12-02 17:14:00,34
AMD,nrwwxi5,Corpo cunts are fucking everything up,pcmasterrace,2025-12-02 17:05:51,160
AMD,nrwuxyf,Pretty sure my next upgrade will just be using my Steam Deck more.,pcmasterrace,2025-12-02 16:56:13,69
AMD,nrx0zeq,"AI destroying the economy, white collar jobs, and now pc gaming.  ![gif](giphy|HP7mtfNa1E4CEqNbNL|downsized)",pcmasterrace,2025-12-02 17:25:24,22
AMD,nrwu2bo,"Thanks, I hate it",pcmasterrace,2025-12-02 16:52:01,17
AMD,nrwzp4j,"Holy crap, I am so lucky I built my dream rig in September.",pcmasterrace,2025-12-02 17:19:12,30
AMD,nrx0tjf,So like what can we do to pop the bubble?,pcmasterrace,2025-12-02 17:24:38,13
AMD,nrx3hyq,"Oh, fuck off.",pcmasterrace,2025-12-02 17:37:32,11
AMD,nrx0mwr,Guess I'll be sticking with my 5800X3D until AM6.,pcmasterrace,2025-12-02 17:23:44,8
AMD,nrx5fi3,It's funny how different the reactions are here when AMD does it compared to Intel.,pcmasterrace,2025-12-02 17:46:51,8
AMD,nrx8tt8,"Fuck it, let’s let the industry crash then. We all likely own enough games and gaming devices (Steam Decks, gaming PCs/laptops, even SBC gaming devices like Retroid or Anbernic etc) to play our vast libraries of games. So let’s just stop spending money upgrading so we can play the newest AI drivel and just enjoy what we already have. Let’s play through our backlogs and even revisit retro games of our childhood, we’ve got enough to play.    When the industry as a whole begins to lose money because nobody wants to be gouged repeatedly on hardware, software, and subscriptions just to play the latest slop then they’ll reverse course.",pcmasterrace,2025-12-02 18:02:41,14
AMD,nrwxm1l,"Realy hope that the AI bubble pop heavily enough. But not yet, big Tech need to stack more unusable hardware.  If i could get a used cheap a Radeon AI PRO R9700 which is literaly a 9070 XT with 32gb of vram.   I would really consider it as a side upgrade of my 7900 XT.  The price increase is definitely coming as a result of TSMC wafer price that increased heavily as the demand as exploded equally to dram.  CPU-wise my 12600k will get the job done until the end of AM5 or early AM6. Who know i may end up with a intel x3d chips when i am going to realy need it.  CPU last way longer that before now. A signly price increase dont feel much of a issue for something that people may old into for a whole decade.  i put 50 buck a month on a saving account for PC hardware stuff as its my main hobbies, been a while now that its hard to justify any purchase. So cash is accumulating.",pcmasterrace,2025-12-02 17:09:10,21
AMD,nrwyzbk,I just want to say im so glad I upgraded to a 7800x3d and DDR5/Am5 a few months ago.,pcmasterrace,2025-12-02 17:15:44,9
AMD,nrwwahd,Dammit. Had a 9800x3d and 64gb of ram a couple of months ago and returned it.,pcmasterrace,2025-12-02 17:02:44,18
AMD,nrwvf97,Let’s raise prices on the rest of the PC components right now,pcmasterrace,2025-12-02 16:58:30,8
AMD,ns187h5,"Why tho? CPUs aren't impacted by the ram shortage what reason do they have to raise these too man. Like people aren't getting fucked enough with storage, GPUs, and ram all going up??",pcmasterrace,2025-12-03 08:12:59,4
AMD,nrwvll1,Well. Hope my computer still ticks along for a few more years,pcmasterrace,2025-12-02 16:59:20,7
AMD,nrx02vo,>This could just be a case of the company restoring regular pricing from brand- and distributor-level discounts for Black Friday and Cyber Monday  Literal nothingburger.,pcmasterrace,2025-12-02 17:21:02,7
AMD,nrwxck4,"""This could just be a case of the company restoring regular pricing from brand- and distributor-level discounts for Black Friday and Cyber Monday,""",pcmasterrace,2025-12-02 17:07:53,3
AMD,nrzq2cx,This is bs and totally artificial when it comes to supply,pcmasterrace,2025-12-03 01:41:37,3
AMD,ns05b9f,Crypto and Ai ruined everything we love.,pcmasterrace,2025-12-03 03:11:22,3
AMD,ns1ppjo,The rising prices of memory and SSDs spell trouble for tech enthusiasts in the future.,pcmasterrace,2025-12-03 11:04:23,3
AMD,nrwzmtn,"I'm so glad I upgraded to a 9800x3d, new board, and 64 gigs of ram back in beginning of October.  I am going to keep my 3080 10 gig until it dies.",pcmasterrace,2025-12-02 17:18:53,4
AMD,nrwuo5a,Worldwide???,pcmasterrace,2025-12-02 16:54:55,2
AMD,nrx004a,"I was worried I was making a mistake building my pc when the 50 series cards first came out and were expensive and hard to buy, turns out it was a great call.",pcmasterrace,2025-12-02 17:20:40,2
AMD,nrx0lt2,"Thankfully and also unfortunately I'm gaming less and less every day, I don't consider myself old but I'm getting tired more easily and I'd rather unwind watching something than gaming these days.   That all being said, I still love the dance before building a new rig, watching reviews, comparing prices, using PCPP to check that everything will fit, go to the subreddits to get opinions, pull the trigger, wait for the hardware to arrive, take my Saturday to put it together and fire it up for the first time is the closest I'll have to having a child.   But now every time I try to juggle the parts list and the prices it makes no damn sense to upgrade. I don't even really need to it's just my love for building rigs and passing down my old ones to my brother or other family members.   I was waiting for the end of the year to do it but with RAM prices having gone through the roof it makes no sense at all, like I was counting on getting a normal 6000Mhz CL30 kit, they were going for 120/140 bucks, now they're over 300 or more.   Heck I almost bought a 2x24GB 6000Mhz CL26 kit for 320 bucks three months ago to call it a day and I chickened out thinking they would get cheaper lol.   I remember this happened with DDR4 around 2018 or so, but the prices were still somewhat acceptable, like a good kit was around 90/110 and they went to 180/200. Expensive, but not deal breakers.   The industry needs a civil war + slap in the face, the suits need to calm the fuck down. Fucking shareholders and shit ruin everything.",pcmasterrace,2025-12-02 17:23:35,2
AMD,nrx0p9g,"My i9 13900K suits me well enough. Hell, I might not get the highest frame rate compared to X3D processors, but it handles shader compilation surprisingly well.",pcmasterrace,2025-12-02 17:24:03,2
AMD,nrx4t1l,Greed eventually wins,pcmasterrace,2025-12-02 17:43:52,2
AMD,nrx640z,Glad I upgraded from 7600x to 9800x3d a few months ago when I got the free 5080 from the nvidia gamescom giveaway (praise be to huang),pcmasterrace,2025-12-02 17:50:03,2
AMD,nrx9xlb,Watch me hold with my 10700k and 3080 running 3440x1440.  I will not blink first.,pcmasterrace,2025-12-02 18:07:54,2
AMD,nrxbvrh,my 5900x will have to fight for another year,pcmasterrace,2025-12-02 18:17:00,2
AMD,nrxkso9,And here I thought AMD was the good guys.   /s,pcmasterrace,2025-12-02 18:59:01,2
AMD,nrxx5f8,"5600x still handles any game I've ever played with 0 sweat btw. Say it with me guys: ""We do not need to buy the biggest and newest and most expensive hardware nowadays""",pcmasterrace,2025-12-02 19:59:06,2
AMD,nrxzhg0,I just bought a 5700x to replace a 1500x and I guess I'll have to make it last for 10 years more.,pcmasterrace,2025-12-02 20:10:29,2
AMD,nryntz4,Y’all I just bought a 9800x3d yesterday. I feel like I got in the last chopper out,pcmasterrace,2025-12-02 22:08:10,2
AMD,nrzaaf0,Good thing I just bought a 9800x3D,pcmasterrace,2025-12-03 00:11:43,2
AMD,nrzup5j,Fuck amd,pcmasterrace,2025-12-03 02:08:58,2
AMD,nrzxbjv,![gif](giphy|lwYxf0qKEjnoI|downsized),pcmasterrace,2025-12-03 02:24:12,2
AMD,ns0l7m5,Kinda glad I upgraded my gpu last year and the rest of my build 2 months ago,pcmasterrace,2025-12-03 04:56:38,2
AMD,ns0oge7,Ima probs just go full used for my next eventual build it seems,pcmasterrace,2025-12-03 05:20:48,2
AMD,nrx0lxv,just boycott it all!,pcmasterrace,2025-12-02 17:23:36,3
AMD,nrxteow,Lol fuck them. They have no excuse to raise prices. CPUs don’t contain NAND. The cache sizes are negligible compared to other products. This is a straight up cash grab.,pcmasterrace,2025-12-02 19:40:57,2
AMD,nrwz1qo,I will probably retire my ddr4 when ddr6 is released at this point.,pcmasterrace,2025-12-02 17:16:03,1
AMD,nrwzju5,At what point would it better to just buy a prebuilt?,pcmasterrace,2025-12-02 17:18:29,1
AMD,nrx0kze,Welp seems like I took the perfect moment to upgrade my PCs three weeks ago.  Looks like it's the right time to go through my 1400+ gamea on steam lol,pcmasterrace,2025-12-02 17:23:29,1
AMD,nrx0q7v,"Even though I’m 100% PCMR, I will no longer crap on people deciding to buy a console due to cost.",pcmasterrace,2025-12-02 17:24:11,1
AMD,nrx0wf3,"Welp, time to put away the idea of upgrading to AM5 for this century.",pcmasterrace,2025-12-02 17:25:00,1
AMD,nrx17fs,"Damn I was really hoping for only for another year or so on my i9 9900k. Looks a lot like until it dies, or I come into a large sum of disposable cash.",pcmasterrace,2025-12-02 17:26:27,1
AMD,nrx2b4a,Luckily got 7800x3d for 260 bucks one week ago.,pcmasterrace,2025-12-02 17:31:45,1
AMD,nrx2wbc,Got a 9800x3d at msrp early this year. I hope when I need to upgrade the works stops being so shitty,pcmasterrace,2025-12-02 17:34:38,1
AMD,nrx31b2,Just finished mine:))  9700x 280 euro 32gb ddr5 140 euro Second-hand 3070ti 320 euro. So... im fine for now.,pcmasterrace,2025-12-02 17:35:18,1
AMD,nrx31gi,"This is why I'm home-brewing my old consoles...     Everything getting expensive about this hobby, I'm afraid of the price of the steam machine too if this keeps up...",pcmasterrace,2025-12-02 17:35:20,1
AMD,nrx32ob,I am so glad I bought now and didn’t wait,pcmasterrace,2025-12-02 17:35:29,1
AMD,nrx347a,Sometimes it’s better to just get a prebuilt pc. It’s turning out to be that way again I think.,pcmasterrace,2025-12-02 17:35:41,1
AMD,nrx38as,Now the pc i am trying to sell for 1700 became a whole lot more worth,pcmasterrace,2025-12-02 17:36:14,1
AMD,nrx3drc,Hope Steam fixed their deal with amd and ram producers previously… otherwise xbox/playstation is on the menu again for many,pcmasterrace,2025-12-02 17:36:59,1
AMD,nrx3gao,I sold a bunch of Legos and other toys to raise funds for a pc. Guess that all was for nothing,pcmasterrace,2025-12-02 17:37:19,1
AMD,nrx43x0,Looks like my 7700K will last a little while longer😂,pcmasterrace,2025-12-02 17:40:30,1
AMD,nrx4cl2,Here i was thinking it was time to upgrade fx9590. Sigh that chip just keeps chugging along.,pcmasterrace,2025-12-02 17:41:40,1
AMD,nrx4fhq,"Between storage and RAM, the upgrade I got the other week has increased by £215. Genuinely never been so relieved to have ordered when I did",pcmasterrace,2025-12-02 17:42:04,1
AMD,nrx4goq,At this point just buy a PS5 and call it a day.,pcmasterrace,2025-12-02 17:42:13,1
AMD,nrx4tk0,"Dammit, big tech! Stop making me thankful for upgrading to AM5 a few months ago.",pcmasterrace,2025-12-02 17:43:56,1
AMD,nrx4zlb,"Was literally gonna upgrade my PC last month to AM5 but had to delay it, and then RAM prices exploded. And now, I was literally about to buy the Ryzen 9950x3d LAST NIGHT.   Hell naw I ain’t getting rug-pulled again. Gonna grab one literally right now. Wtf.",pcmasterrace,2025-12-02 17:44:45,1
AMD,nrx5dlr,"LMAO, gpu, ram, ssd and now this? What’s left? PSU, motherboard, case and cooler prices should be raised too. Why not. 💀",pcmasterrace,2025-12-02 17:46:36,1
AMD,nrx5ggl,Damn. Every day I feel more and more like I won the damn lottery by building a PC back in August/September.,pcmasterrace,2025-12-02 17:46:58,1
AMD,nrx5hw1,fuck this.,pcmasterrace,2025-12-02 17:47:10,1
AMD,nrx5v0r,might really be able to sell my pc for 5k soon lol,pcmasterrace,2025-12-02 17:48:53,1
AMD,nrx6lo8,"Consequences of their own acfions  AMD are now all in on AI, like Nvidia",pcmasterrace,2025-12-02 17:52:20,1
AMD,nrx6qkc,looks like gonna have to use am4 for a while longer ....,pcmasterrace,2025-12-02 17:52:58,1
AMD,nrx6xra,Of course I started working and saving money for a pc earlier this year. Fuck everyone,pcmasterrace,2025-12-02 17:53:53,1
AMD,nrx80ae,This is fine.,pcmasterrace,2025-12-02 17:58:51,1
AMD,nrx87x6,*Me on a ryzen 3600 and a 9 year old GTX 1070*  Guess I'll stop gaming then,pcmasterrace,2025-12-02 17:59:50,1
AMD,nrx8804,Wouldn't it be CRAZY if MacMinis and iMacs became the affordable alternatives?,pcmasterrace,2025-12-02 17:59:50,1
AMD,nrx9iph,"Guess the GPU shortage wasn't enough, now processors too?",pcmasterrace,2025-12-02 18:05:56,1
AMD,nrx9tqg,"Holy fucking shit, I was planning on upgrading my PC (CPU, motherboard and RAM) the next week or so. Why does all of this have to happen right as I was about to do it.",pcmasterrace,2025-12-02 18:07:23,1
AMD,nrx9yg1,Glad I made my pc on Sunday. Only extra I paid was for ram.,pcmasterrace,2025-12-02 18:08:01,1
AMD,nrxa5b3,Just ordered my new ryzen laptop yesterday!,pcmasterrace,2025-12-02 18:08:54,1
AMD,nrxabqz,"Getting a pre-built seems more reasonable with each passing day , lmao",pcmasterrace,2025-12-02 18:09:44,1
AMD,nrxanl9,"Glad I got me a 5950X last month. Gonna stick with AM5 for at least another 4 years, lol.",pcmasterrace,2025-12-02 18:11:16,1
AMD,nrxb3tp,Cloud gaming and consoles gonna pop off,pcmasterrace,2025-12-02 18:13:23,1
AMD,nrxbatr,So glad I upgraded to 9800x3D and 64gb of ram some months ago,pcmasterrace,2025-12-02 18:14:17,1
AMD,nrxbgj4,"Damn, right after I question myself about 7500f and 32Gb RAM 5200Mhz Cl40 to maybe it's not that bad after all",pcmasterrace,2025-12-02 18:15:02,1
AMD,nrxbr1i,"Well good thing my backlog is as big as it is, cause it looks like ill never upgrade the way things are going.",pcmasterrace,2025-12-02 18:16:24,1
AMD,nrxc7ww,The selfish and evil side of my personality wants this because I paid an extreme price for RAM last week and I want my decision to build to be justified. I was worried that GPU and processor prices would jump too and my old system was limping.  But mostly this sucks for everyone.,pcmasterrace,2025-12-02 18:18:34,1
AMD,nrxerj8,Good guy AMD.,pcmasterrace,2025-12-02 18:30:35,1
AMD,nrxfkap,"They will say this is thanks to Data Centers but we all know it's about profit.      The manufacturers of every tech part in the industry are racing to grab as much profit margin as possible, knowing that nobody is at the helm of the CFPB.       The oligopoly powers of the tech industry are all trying to capture as much wealth as they can, knowing there's nobody in their way.  Who does this benefit? Almost nobody.",pcmasterrace,2025-12-02 18:34:22,1
AMD,nrxfo7t,"Good thing I purchased my 9800X3D during Black Friday for ""only"" 599CAD",pcmasterrace,2025-12-02 18:34:53,1
AMD,nrxg4ql,"Yeah, so I’m never buying or upgrading a computer ever again so let’s see what I can do with 16gb of ram within the next century",pcmasterrace,2025-12-02 18:37:04,1
AMD,nrxh5r5,Yep just keep raising prices and then conveniently forget to lower them when it’s no longer necessary and then turn away all comments about lowering the prices. All to the inevitable future of PC gaming being completely inaccessible to the average joe.,pcmasterrace,2025-12-02 18:41:57,1
AMD,nrxhaqf,That faithful old 9700k is going to have to trundle on for another little while it seems.,pcmasterrace,2025-12-02 18:42:36,1
AMD,nrxiehq,This is great news.. For shareholders,pcmasterrace,2025-12-02 18:47:44,1
AMD,nrxjowm,Thank fuck I bought that Microcenter bundle a few months ago. I was overdue for an upgrade anyway.,pcmasterrace,2025-12-02 18:53:48,1
AMD,nrxjxwd,We are not escaping AM4/DDR4 until 2030,pcmasterrace,2025-12-02 18:54:59,1
AMD,nrxk6fp,"I turned my AM4 build into a AM4 mini itx build, with a lillte bump to performance. Ryzen 3700x to 5700x (now supporting SAM) and new mobo and PSU.   Debated jumping up to AM5 but Ram and CPU was too expensive then and I didn't really need it.   Should have bit the cost because would have saved a lot down the line. Oh well",pcmasterrace,2025-12-02 18:56:06,1
AMD,nrxlgi6,Just need intel to release something that actually beats ryzen. If they can effectively trade blows and be on a price parallel we will be OK. Amd is only doing this because intel isn't a current threat to sales.,pcmasterrace,2025-12-02 19:02:11,1
AMD,nrxlj57,Guess ill wait for am6 lmao,pcmasterrace,2025-12-02 19:02:31,1
AMD,nrxnocy,God damn. Makes me glad I did my new build in October.,pcmasterrace,2025-12-02 19:12:53,1
AMD,nrxog6w,I guess they are pushing some to go the console way... If they don't raise the prices again too.,pcmasterrace,2025-12-02 19:16:38,1
AMD,nrxpui9,Has anyone noticed the sudden influx of mini pcs. I wonder if the whole plan is to kill the diy consumer grade market and replace it with prebuilts non upgradeable mini pcs.,pcmasterrace,2025-12-02 19:23:29,1
AMD,nrxpya2,I justtttt finished building my pc last month. Insane last minute timing … damn,pcmasterrace,2025-12-02 19:24:00,1
AMD,nrxq872,"Just finished a new build last week, feel a little better only getting screwed on RAM now",pcmasterrace,2025-12-02 19:25:21,1
AMD,nrxql0t,"Lucky me, got my 7800x3d, 32GB ram and 5070ti in july and have 12TB of storage. Should be good for a while.",pcmasterrace,2025-12-02 19:27:05,1
AMD,nrxsvrj,"Oh noes, the prices have Ryzen",pcmasterrace,2025-12-02 19:38:24,1
AMD,nrxtb3o,3 year old laptop. Wanted to build nas and new computer guess I am going to have to wait until 2030,pcmasterrace,2025-12-02 19:40:28,1
AMD,nrxylxc,Reading through this apparently Intel is not an option,pcmasterrace,2025-12-02 20:06:12,1
AMD,nrxz9zp,I got so lucky I finally decided to build a PC this summer.,pcmasterrace,2025-12-02 20:09:29,1
AMD,nrxzfas,Me having bought my 9950x3d and 64gb of ddr5 in september:,pcmasterrace,2025-12-02 20:10:12,1
AMD,nry05md,Bread and circuses is all they had to do..,pcmasterrace,2025-12-02 20:13:48,1
AMD,nry4de6,Mah fucking ryzen 5 2600 realizing at this rate it's gonna have to outlive me  https://preview.redd.it/m4gryo4xpu4g1.png?width=1100&format=png&auto=webp&s=c192b7dba187c3a09f532d1a98036bf010301fe4,pcmasterrace,2025-12-02 20:34:28,1
AMD,nry4kzs,Good thing I got my R7 7700x when I did,pcmasterrace,2025-12-02 20:35:31,1
AMD,nryhmxw,Soooooo glad I upgraded last month instead of waiting. Sometimes it's a good thing to be impatient.,pcmasterrace,2025-12-02 21:38:21,1
AMD,nryiy2v,Intel please wake up,pcmasterrace,2025-12-02 21:44:33,1
AMD,nryju1t,"> 7This could just be a case of the company restoring regular pricing from brand- and distributor-level discounts for Black Friday and Cyber Monday, although we don't know the tune to which the prices are being increased just yet.",pcmasterrace,2025-12-02 21:48:47,1
AMD,nrypln1,"people kept telling me i was crazy pairing a 9800x3d with my 5070, i just wanted to be future proof in case something like this was gonna happen. hoping i wont need to upgrade for the next 5+ years 🙏",pcmasterrace,2025-12-02 22:17:04,1
AMD,nrypwtr,5800X3D still going strong,pcmasterrace,2025-12-02 22:18:40,1
AMD,nryxfvz,my Ryzen 5 7600 just died (heartbroken) in the middle of November while 3 months past warranty and I don't know if it's worth approaching Newegg 🙃,pcmasterrace,2025-12-02 22:58:19,1
AMD,nryys5u,https://preview.redd.it/8c8ad3fvgv4g1.jpeg?width=1179&format=pjpg&auto=webp&s=685cc6e4db0c5bdf9339e9a15acc07a2d1a335cc,pcmasterrace,2025-12-02 23:05:37,1
AMD,nrz0xyz,I am happy for all the so glad people but this is really bad..,pcmasterrace,2025-12-02 23:17:39,1
AMD,nrzcewo,![gif](giphy|7Eipor01ypMm3LeG4v|downsized),pcmasterrace,2025-12-03 00:23:34,1
AMD,nrzvji6,"I have a ""old"" AM4 Ryzen 7 5700x, 32GB RAM, 2TB SSD, RTX 5070Ti and a 27"" Dell Ultrasharp QHD screen.   I'm really happy with it. ANY game I play it's around 90/100 FPS with eye candy quality.   Hardware it's so powerfull these days that you don't need to pay hundreads of dollars for a setup to achieve really great image quality.",pcmasterrace,2025-12-03 02:13:50,1
AMD,nrzwwb5,"Makes me feel good about my upgrade earlier this year, thanks AMD and RAM pricing",pcmasterrace,2025-12-03 02:21:43,1
AMD,nrzxo2i,"Damn, made a huge dodge when I upgraded everything back in September, it seems. First I thought to wait for Nvidia Super series, looks like that has gone down the toilet. Now ,RAM, SSD's CPU all went to shit too.   This decade needs to stop existing.",pcmasterrace,2025-12-03 02:26:15,1
AMD,ns06wky,I built my very first PC in October and keep joking to my friends that it would have been even more expensive if I had waited for Black Friday. Looks like I was right.,pcmasterrace,2025-12-03 03:21:15,1
AMD,ns0vl5w,Man I was one of those people who upgraded every few years and enjoy building new PCs just for the fun of it. Gone are those days for me. My current PC is going to get some serious mileage. I'm one consumer that isn't giving any of these companies my money for a long time and I know I'm not the only one. Tired of these big greedy corporations squeezing all of us and the intervals keep getting shorter and shorter.,pcmasterrace,2025-12-03 06:17:57,1
AMD,ns2ij52,Now I'm glad I was able to pull the trigger on a US Black Friday deal from Newegg. 9800x3d + 32GB 6400 + Asus Crosshair X870E Apex + MSI AOI cooler for $799 and 20% cashback from PayPal (effectively making the deal $693 after sales tax). I don't think we'll ever see deals like this again with how the industry is trending.,pcmasterrace,2025-12-03 14:20:37,1
AMD,nrwxrxu,it is a scam. nothing else.,pcmasterrace,2025-12-02 17:09:57,1
AMD,nrx2mdh,"They're raising prices because sales of CPUs, motherboards, and other DIY PC components have plummeted in recent months due to DDR5 memory prices surging. This sales drop has forced AMD to lower their forward guidance for their CPU business, and the only short term prayer they have to combat this drop in revenue is to raise prices and push these parts up-segment.  Basically, gaming PCs are, in the near future, moving away from higher volume mass market to lower volume luxury and will be priced accordingly.",pcmasterrace,2025-12-02 17:33:17,0
AMD,nrx3v5q,Is anybody surprised Nope! This was in woodworks,pcmasterrace,2025-12-02 17:39:20,0
AMD,nrx3vgv,they were already expensive and now they are gonna rise it even more?  I guess I'll stay with my intel motherboard a bit more... at least until AM6.,pcmasterrace,2025-12-02 17:39:22,0
AMD,nrxpom5,"Maybe hot take or unpopular opinion, but PC parts are still historically cheaper than ever before. I remember my dad bringing home a Pentium 2 that was like $5000. And that wasn't even top of the line then. TVs and monitors are also like dirt cheap. I guess the pain point is everything else is so expensive that spending even these historically cheap prices still feels too much",pcmasterrace,2025-12-02 19:22:41,0
AMD,nrwtwhq,Here come the AMD apologists lmao,pcmasterrace,2025-12-02 16:51:15,-13
AMD,nrxqqxn,Something about top 10% spending 50%. More and more company are just focus on the small group for max profit..,pcmasterrace,2025-12-02 19:27:53,80
AMD,nrx4ecd,Enshittification claims all.,pcmasterrace,2025-12-02 17:41:56,110
AMD,nrx0ui3,"Or maybe they just don't drop price at all and keep it like that forever, just like how Nvidia did GPU.",pcmasterrace,2025-12-02 17:24:45,163
AMD,nrx53pe,to be fair anyone who's building a gaming PC and putting up with these ram prices won't bat an eye at an extra 10-15% CPU cost,pcmasterrace,2025-12-02 17:45:18,31
AMD,nrxp4nn,"I don’t think that’s the idea because CPU prices aren’t going to drop back down unless RAM prices do to. Anyone buying a new CPU has to also buy RAM, so CPUs aren’t going to sell much more just because they’re cheaper if RAM prices aren’t the bottleneck.  I think it’s more likely that they recognize that overall sales will go down due to RAM costs, and anyone looking to buy a CPU in the next few years will be less price sensitive since they’re willing to shell out for RAM. I’d imagine they’re doing this then to offset the lower volume.",pcmasterrace,2025-12-02 19:19:57,1
AMD,nrywd8y,"It helps compensate for lower sales volume. If you know demand for your product is relatively inelastic you can increase your prices in the face of market headwinds to make greater margin at reduced volume. It does leave you open to being undercut by your competitors, but AMD is so far ahead in CPU right now that they might feel they can take the risk.",pcmasterrace,2025-12-02 22:52:29,1
AMD,nrx9d1a,"Got my Ryzen 7 9700X with 32gb ram and Radeon 9060xt 16gb vram for about $1,000 plus tax. Glad I didn’t wait.",pcmasterrace,2025-12-02 18:05:11,91
AMD,nrxk078,"few? id say forever lmao, now that they know they can jack up the prices and blame it all on ai while also artificially reducing productions because people will always buy anyway, things are never going the way they were",pcmasterrace,2025-12-02 18:55:17,12
AMD,nrwxh93,PS5pro looking more tempting right now.,pcmasterrace,2025-12-02 17:08:31,108
AMD,nrxezah,i built my PC this year (august). i literally did it at the perfect time.,pcmasterrace,2025-12-02 18:31:36,5
AMD,nrxfoo6,"Just finished my build a few weeks ago slowly buying parts over the previous month and a half.  I got every single part under MSRP, including the 5080, except for the damn Ram.  But even then I avoided the worst of the price hike.  Paid $240 for 32 gb of 6000 CL30 beginning of November.  Same kit costs $480 now.  Prices are going to be nutty when the Ram spike and subsequent shortage settles and trickles down to GPUs, prebuilts, etc along with whatever tariff bullshit Trump is doing.",pcmasterrace,2025-12-02 18:34:57,2
AMD,nrxbr3a,As someone who’s been holding onto their build for too long and will need to upgrade soon I’m pretty scared. I may end up just needing to take a break from gaming (or at least new titles) if I can’t afford an upgrade.,pcmasterrace,2025-12-02 18:16:24,1
AMD,nrxm9ok,"I spent a lot on my system, but hopefully I won’t need to upgrade anything for at least the next 7 years.",pcmasterrace,2025-12-02 19:06:06,1
AMD,nrxvflr,"I was on the fence, I guess I will wait more 😭",pcmasterrace,2025-12-02 19:50:48,1
AMD,nrxx16r,Damn. I was thinking of a new build next year but guess I'll just have see if a new GPU is even worth it,pcmasterrace,2025-12-02 19:58:31,1
AMD,nrxy0yn,"When i saw that RAM prices were increasing and looking at my situation too, decided to finally go forward on buying everything for my first PC and holy did i do a good choice here. The RAM that i bought at 185 euros (not counting the 10 euros of shipping and yeah prices were already increasing, two months ago that RAM was at 130\~ ) nearly two weeks ago is now at 350 euros.  And now with CPU prices rising and GPU apparently raising too, definitly glad that i didn't wait any longer x)",pcmasterrace,2025-12-02 20:03:21,1
AMD,nrydxtm,"Yeah that's me. I was finally stating to buy parts, mobo arrived 2 weeks ago, I was waiting on the 9950X3D2 announcement and was about to buy ram, maybe in another 6mo instead of 3 I will have a working PC :')",pcmasterrace,2025-12-02 21:20:49,1
AMD,nrymqcj,I told my friend but he didn't listen,pcmasterrace,2025-12-02 22:02:42,1
AMD,nryoq8b,"I just did mine this summer, so glad I stopped procrastinating",pcmasterrace,2025-12-02 22:12:40,1
AMD,nrytfed,I bought a pre build a couple weeks ago realizing they hadn’t really priced in ram madness yet. Base system went up 250€ since and that’s before these cpu shenanigans.   Happy now I finally pulled the trigger and can sit it out years now,pcmasterrace,2025-12-02 22:36:53,1
AMD,nrz16qy,Im using my 1080 and 1050 builds. Wanted to upgrade for a while. My wife told me to go for it. Ive put it off for the next 4-5 years.    Not sure its the right call but there is no going back now,pcmasterrace,2025-12-02 23:19:02,1
AMD,nrz469g,Bought a 9800x3d and 32gb of ram early this year. Glad I didn't wait.,pcmasterrace,2025-12-02 23:36:04,1
AMD,nrzrwal,good year for me to upgrade to something modern,pcmasterrace,2025-12-03 01:52:37,1
AMD,nrzv41o,My 1070 is wheezing but it ain't stopping yet!,pcmasterrace,2025-12-03 02:11:21,1
AMD,ns1c08p,"Built mine maybe 2 weeks ago, really feels like I caught the last train out.",pcmasterrace,2025-12-03 08:50:41,1
AMD,nrxtv9m,I bought a computer during pre black week sales. Still haven't got it delivered.  Really hope I get it as it may be an extremely good deal on now compared to just one month ago..,pcmasterrace,2025-12-02 19:43:12,1
AMD,nrwxyqs,>5700x3d  ![gif](giphy|glvNGHmbZwgrKH4YYA),pcmasterrace,2025-12-02 17:10:52,53
AMD,nrxte9e,"5700X here, still fantastic CPUs. Looks like I'm going to using it for a while.",pcmasterrace,2025-12-02 19:40:53,8
AMD,nrx1tru,Just got mine today too!  https://i.redd.it/bq8bimtvst4g1.gif,pcmasterrace,2025-12-02 17:29:26,7
AMD,nrxorwt,Ali express gang,pcmasterrace,2025-12-02 19:18:13,3
AMD,nryu24x,Could finally emulate my beloved PS3 games after getting this one. 5700x3d gang,pcmasterrace,2025-12-02 22:40:13,3
AMD,nrwwi3n,Start doing drugs getting new components for your PC will be the least of your problem,pcmasterrace,2025-12-02 17:03:46,158
AMD,nrx09ti,"Homie, this. Over the past 1-2 years ive been able to evade new game fomo and get maybe 1 full priced game a year. Other than that. Ive been rocking emus on the steam deck or replaying old favs from 2010-2020 and finding games i missed.",pcmasterrace,2025-12-02 17:21:58,10
AMD,nrws9xc,"Have kids, no need to wait ::shrug::",pcmasterrace,2025-12-02 16:43:31,17
AMD,nrwy0q4,"Nightlife, holidays, eating out etc.... The only problem is that those activities require disposable income as well.",pcmasterrace,2025-12-02 17:11:08,4
AMD,nrx2g14,I picked RuneScape back up last year since RS2. It’s become my forever game now especially with all the content. Lovely not having to think about the requirement haha,pcmasterrace,2025-12-02 17:32:25,5
AMD,nrx0dkn,40 year old here. Hasn’t happened to me yet.,pcmasterrace,2025-12-02 17:22:28,8
AMD,nrwzfmb,"In fairness to us, modern games are so awful that it's easy to dislike gaming.",pcmasterrace,2025-12-02 17:17:55,31
AMD,nry3n67,Old guy here. Not possible.,pcmasterrace,2025-12-02 20:30:52,2
AMD,ns0bgtt,"As a middle aged fellow who enjoys gaming 4-5 nights a week for an hour or two, I wish you luck with that.",pcmasterrace,2025-12-03 03:49:53,2
AMD,nrx416w,Get married. Game to much so your wife divorces you. Feel bad. Give up gaming out of guilt!,pcmasterrace,2025-12-02 17:40:08,1
AMD,nrx51p5,Just hope whatever your rig is can last for the next 15 years LMAFO,pcmasterrace,2025-12-02 17:45:02,1
AMD,nrx6qbn,Well sounds like time to take up golf.,pcmasterrace,2025-12-02 17:52:57,1
AMD,nrx6uzl,"I'm torn between a 9070xt upgrade or just saying fuck it,  take apart and clean/re thermal pasting my 1070 and buying one of the dozen of amazing handhelds on the market.",pcmasterrace,2025-12-02 17:53:33,1
AMD,nrxartn,I'm nearly 40. Hasn't happened yet.,pcmasterrace,2025-12-02 18:11:50,1
AMD,nrxehlc,"If you get out, you’ll be back. They always come back.   I myself got out in 2012 but came back in 2020.",pcmasterrace,2025-12-02 18:29:17,1
AMD,nrxji5q,Starting corporate over 10 years ago killed almost every passion. I highly recommended it.,pcmasterrace,2025-12-02 18:52:54,1
AMD,nrxxdge,we can just play the 2D games or old 3D games. Soooo many games I haven't played.,pcmasterrace,2025-12-02 20:00:09,1
AMD,nryc7r8,"say what you will but the obsession with always playing the latest games is beyond me. So many great indie and retro games available that can run on 5+ year old specs. The only reason you need higher specs is how shitly optimised newer games are. Stop rewarding the AAA companies and start rewarding those that value you, your time and money.",pcmasterrace,2025-12-02 21:12:29,1
AMD,nrx2fjk,![gif](giphy|l0HluN8PywCl6Hckg),pcmasterrace,2025-12-02 17:32:21,23
AMD,nryfezy,![gif](giphy|GLbiGvv9qrpny),pcmasterrace,2025-12-02 21:27:50,1
AMD,nrx4xzp,"Companies increased stock before tariffs. Some have been eating the cost. Some forced suppliers to eat the cost. It’s all coming home to roost now. Combined with AI making more money for hardware sellers and vacuuming up all the production, it makes sense prices are going to start rising.",pcmasterrace,2025-12-02 17:44:32,21
AMD,nrxh72w,The issue is the prices rarely if ever come back down.,pcmasterrace,2025-12-02 18:42:07,1
AMD,nrx9s0o,"Goddamn corpos, gonks the lot of em.",pcmasterrace,2025-12-02 18:07:10,9
AMD,nrwy11c,"Don't forget all the douchebags running local Big Autocorrect models, and DEFINITELY don't forget all the primo douchebags who knowingly even use Big Autocorrect.   Fucking godsdamn cunts.",pcmasterrace,2025-12-02 17:11:10,42
AMD,nrx8ddv,Shareholders,pcmasterrace,2025-12-02 18:00:32,3
AMD,nrxpac4,As they always do.,pcmasterrace,2025-12-02 19:20:43,2
AMD,nrx5eyk,Just run your pc at 720p and it can keep up with stuff too,pcmasterrace,2025-12-02 17:46:47,18
AMD,nrx0haw,Amen,pcmasterrace,2025-12-02 17:22:59,4
AMD,nrx41es,Same...finished my rig in my flair in early October. Right before the massacre started. Glad I did.,pcmasterrace,2025-12-02 17:40:10,2
AMD,nrxeauh,"Same. I built a new PC after the BF6 beta.   5090 was $1999 msrp, 9800x3d + mobo combo sale, ram was cheap, 4k oled was on sale.   Should last me several years as long as my 5090 doesn't burn everything down",pcmasterrace,2025-12-02 18:28:23,1
AMD,nry2tfj,Gotta stop using AI and AI Accessories,pcmasterrace,2025-12-02 20:26:50,6
AMD,nrxuc6m,"Not yet, let it get a bit more bigger. The bigger they are, the harder they fall.",pcmasterrace,2025-12-02 19:45:29,1
AMD,nrx4w9o,"Nothing and I don’t believe the bubble will pop. It will lose air slowly and some new thing will take its place in the stock hype. Prices are not going to suddenly drop, you will have to hunt for deals.  Maybe in 3 decades where almost all the old guard are gone, the shit they left the younger generations will just be unsustainable and things will crash.",pcmasterrace,2025-12-02 17:44:18,0
AMD,nrydgcr,A lot of people that buy AMD do so because they are price conscious and feel like they are getting a better value. Of course price increases are going to get a more severe reaction.,pcmasterrace,2025-12-02 21:18:27,2
AMD,ns2trzm,"This statement only applies to individuals who have control, morals, and good taste in games. So, about 0.02% of this sub lmao.",pcmasterrace,2025-12-03 15:19:53,2
AMD,nrxhgk3,I need to start doing this for my pc budgeting.,pcmasterrace,2025-12-02 18:43:21,1
AMD,nrxijl7,"Why would you get that AI pro GPU?  Also you could get a 14600k, I saw there’s a solid sale doing on them rn",pcmasterrace,2025-12-02 18:48:24,1
AMD,ns0tgod,"February. I upgraded the main components, when 32GB of RAM only cost me $170 AUD 😎",pcmasterrace,2025-12-03 06:00:21,1
AMD,nrx1q15,Why,pcmasterrace,2025-12-02 17:28:56,8
AMD,nrx8ipd,Why do people need 64GB RAM? What games actually use all of it?,pcmasterrace,2025-12-02 18:01:14,-5
AMD,nrx2e4s,Did my upgrade in August 2025. 9800X3D and 96 GB ram 6000MHz CL30 and a Asus Rog X870E Crosshair Hero. Now the ram costs over $300 more,pcmasterrace,2025-12-02 17:32:09,3
AMD,nrx27oi,"Honestly, some SIs make it very tempting",pcmasterrace,2025-12-02 17:31:17,1
AMD,nrxnaae,"There's always one area that is cheaped out on but easily replaceable, most likely the max performance you would lose would be 5% plus (at least in EU) you get guaranteed warranty from the retailer. For a £500 saving at 5% less performance and guaranteed warranty i would say prebuilt seems the way forward. At least at 1440p for the next few years anyway",pcmasterrace,2025-12-02 19:10:59,1
AMD,nrydam9,The PS6 will be 1000 bucks for the base edition 😂,pcmasterrace,2025-12-02 21:17:41,1
AMD,nrxaxl5,They’re not going to make less money just because a small number of people decide to stop buying.  They’ll just pivot to AI or to enterprise-related buyers.  AMD doesn’t have chip fabs.  TSMC makes their 9000 series CPU’s for them.  I’m sure they can tell TSMC to manufacture something else.,pcmasterrace,2025-12-02 18:12:34,0
AMD,nrxw3uq,What ecosystem can sustain that though?,pcmasterrace,2025-12-02 19:54:02,27
AMD,ns1eunt,That doesn't work for processors. There will always be chips that don't test at 100% speed so they lower the price and sell them as a lesser model. It's not like they're just going to trash the ones that only function at 90%.,pcmasterrace,2025-12-03 09:19:22,1
AMD,nrx47u5,"No clue what you are talking about. GPUs have usually been the most expensive parts.  Why are people even downvoting this? A 7500F system went from 350-400$ to spending close to that ON THE RAM ALONE. So entry level AM5 can cost about $600 right now. People who can't afford this, will just not upgrade, i.e. not buy CPUs. Meaning: Sales go down.  GPUs have no direct relation to ANY other part prices, so I have no clue what they mean with their comment. You can upgrade your GPU as long as your PC has a PCIe slot and a good enough PSU. And even GPUs in full systems are barely affected because a $2k being 10% more expensive due to RAM is much less relevant than a 50% increase on your individual CPU upgrade.",pcmasterrace,2025-12-02 17:41:02,-57
AMD,nrx66ld,"Well, it reduces the amount of buyers. And fewer buyers -> more stock -> lower prices. So it goes up by 10-15% due to AMD and then down due to supply.",pcmasterrace,2025-12-02 17:50:23,13
AMD,nryzln9,"Well, that is what I meant: They raise prices to compensate for the expected losses in sales. Practically speaking, they are making the retailers pay for their losses in sales volume, because CPU pricing is directly related to the RAM pricing.  Sure, that might just be $20-30 out of $600 total for your upgrade, but you'll still see an overall effect on the purchases. For a single person, it might not matter much to spend 20 bucks more, but in terms of the whole market, even small average price increases will show up in sales.  And keep in mind - Intel might be down atm, but they still exists and compete - and they'll potentially release their new architecture next year. So this is not just AMD increasing prices in a vacuum. The Ultra 235 is already contesting with 9700X and 7700X, the Ultra 225 with the 9600X and 7600X.",pcmasterrace,2025-12-02 23:10:09,1
AMD,nrz2yo3,"AMD isn't actually that far ahead. Non-X3D CPUs are head-to-head with Intel, if not worse in terms of value. That's just something that people aren't really talking about that much.  Intel has flown under the radar since the whole 13th/14th gen disaster, but they are far from dead. Even Apple is apparently going to produce some of their M-chips with Intel's new production process. Of course, it is hard to make any guesses of what this will actually mean for Intel's competition with AMD, but I really don't think it is as clear-cut as many people on this sub assume it is.",pcmasterrace,2025-12-02 23:29:04,1
AMD,nryyyjh,"I was upset that I waited a week to get a bundle from Microcenter, and in that week they stopped including RAM in the bundles. Now I'm glad I scooped up everything when I did. 7800x3d, B650E-E, 32gb GSkill CL36, Lian Li V100 case, Peerless Assassin cooler for $650 (after catching MC on a $100 web pricing error).",pcmasterrace,2025-12-02 23:06:36,9
AMD,ns7gg4s,Same here. R5 7600 + RX 9070 + 32gb ram = 1050$ (tax included),pcmasterrace,2025-12-04 06:42:43,1
AMD,nrxppgd,Even with having to pay for PS plus the math is still on PlayStation's side unfortunately. Wild times.,pcmasterrace,2025-12-02 19:22:48,8
AMD,nrxe1gt,"r/Angryupvote  but seriously I cant argue with that, As crazy as it sounds a PS5 Pro is looking far more appealing at the moment.",pcmasterrace,2025-12-02 18:27:10,35
AMD,nry8urn,PS5MasterRace,pcmasterrace,2025-12-02 20:56:11,6
AMD,nrx0eht,Until they get FSR4 working on the PS5 Pro I would personally stat far away. There are so many poor implementations of PSSR that when they revert back to FSR 3.0 it looks better. And FSR 3.0 is miles off from FSR 4.0 and DLSS. Better off getting a mid-range PC than a PS5 Pro IMO.,pcmasterrace,2025-12-02 17:22:36,15
AMD,nrx58tp,You’ll be paying that difference overtime through overpriced multi-platform games and having to pay to use your own internet.,pcmasterrace,2025-12-02 17:45:58,4
AMD,nrxlpc2,"I’ll definitely be looking at consoles again next gen if this shit keeps up. My current PC will be just fine for anything coming out this gen, and then productivity stuff for the next decade. But it’s not worth these prices just to be able to play games.",pcmasterrace,2025-12-02 19:03:22,1
AMD,ns0e7tw,It would be funny if Sony announces a price hike next week. It's not like they are immune to tariffs or RAM price increases.,pcmasterrace,2025-12-03 04:07:53,1
AMD,ns0qtus,"I was telling my friend this earlier, if you simply want to play a game consoles are more appealing right now.  Building a pc is costly currently if you aren't using it as a multipurpose tool beyond gaming",pcmasterrace,2025-12-03 05:39:20,1
AMD,nrxwve9,It's not. Too limited. Censored.,pcmasterrace,2025-12-02 19:57:45,0
AMD,ns1203q,I did in July but with AM4 and DDR4. I'll probably sit out until AM6 and DDR6 lmao.,pcmasterrace,2025-12-03 07:13:52,1
AMD,nrxrzh8,"A year ago, I purchased a 32 GB memory upgrade for my PC that cost me $83, that kit cost $300 now.",pcmasterrace,2025-12-02 19:34:01,1
AMD,nrx212a,"Yeah, I'm hoping my 5700X3D gets me thorough this whole thing haha. I play in 1440p with a 9070.",pcmasterrace,2025-12-02 17:30:25,27
AMD,nrzn42f,"5600g reporting in, rx9070",pcmasterrace,2025-12-03 01:24:08,1
AMD,nrxde2c,Where did you get one still?,pcmasterrace,2025-12-02 18:24:05,3
AMD,nrz47vw,What CPU did you have before?,pcmasterrace,2025-12-02 23:36:20,2
AMD,nrx2ud3,I started taking edibles and now I just want to game more!  ![gif](giphy|dhPG6a6SagtQk),pcmasterrace,2025-12-02 17:34:22,41
AMD,nrx3lb1,"Another option. Sell drugs, like the guy on my corner, and afford all components.",pcmasterrace,2025-12-02 17:37:59,10
AMD,nrx404u,Or start selling them and getting caught will be more of a problem. (For legal reasons this is a joke.),pcmasterrace,2025-12-02 17:40:00,3
AMD,nrx6cyj,Can confirm. Was a drug addict for 15 years. Definitely didn't game.,pcmasterrace,2025-12-02 17:51:13,1
AMD,nrwxlqv,"pshoo, speak for yourself. Now I have to pay for multiple parts and games.",pcmasterrace,2025-12-02 17:09:07,7
AMD,nrx2k7i,These miserable chaps aren't having kids that's for sure,pcmasterrace,2025-12-02 17:32:59,2
AMD,nrxxllu,Even with inflated prices the PC would still be cheaper in the long run...,pcmasterrace,2025-12-02 20:01:15,1
AMD,nrx2cex,Yup. Gaming is still a cheap activity in spite of being expensive up front.   Any social activity costs money. A lot of money actually. Gaming is the cheaper option. For now.,pcmasterrace,2025-12-02 17:31:56,4
AMD,nrx1y6p,50 and same,pcmasterrace,2025-12-02 17:30:02,6
AMD,nrx5sdk,"There are good ones in the mix. Expedition 33, BF6, Hollow Knight... just that the vast majority really do suck ass thanks to corpos investing the bare minimum amount of time to get the minimum viable slop out of the door.",pcmasterrace,2025-12-02 17:48:33,14
AMD,nrx3w94,At least we got a couple decades of games to enjoy.  But there have been a handful of great new games coming out. I would look away from AAA more and look into indie stuff.,pcmasterrace,2025-12-02 17:39:28,2
AMD,nrx2tou,Nah. This year’s Expedition 33 is the best game I’ve ever played.,pcmasterrace,2025-12-02 17:34:16,6
AMD,nrx4wrq,Bold thing to say when this year was stacked with masterpieces.  We're better off just ignoring live service slop.,pcmasterrace,2025-12-02 17:44:22,5
AMD,nrx5ryj,Im so tried of this fucking narrative. No modern games are great. Maybe learn to play more than one game.   This is very much not true if you actually pay attention.   Expedition 33  Arc Raiders  Baaltro   Marvel Rivals,pcmasterrace,2025-12-02 17:48:30,4
AMD,nrx6nlg,Modern AAA games *that need high specs* are generally awful. There are a ton of great Indie and AA games coming out for less than the price of a AAA game.,pcmasterrace,2025-12-02 17:52:35,2
AMD,nrx20qk,"Havent played a brand new game in a while, nearly all sucks",pcmasterrace,2025-12-02 17:30:23,1
AMD,nrx7u9u,"Define ""modern games"".",pcmasterrace,2025-12-02 17:58:03,1
AMD,nry7qv4,skill issue ngl. lots of fun games have been coming out.,pcmasterrace,2025-12-02 20:50:52,1
AMD,nryjnb8,we just had an excellent year of gaming though? or are you not accounting for indie and AA games?,pcmasterrace,2025-12-02 21:47:54,1
AMD,nrxk780,Buy the 9070xt and build a used parts batocerra emulation machine with the 1070  lol,pcmasterrace,2025-12-02 18:56:12,1
AMD,nrx7bge,"“but, but, but…. I didn’t know the tariffs would affect *me!*”",pcmasterrace,2025-12-02 17:55:38,27
AMD,nrx5kgw,"All this cash and Apple and Google’s autocorrects are *still* fk’ing worthless. If a typo word has never been in a single dictionary across the entirety of all of human existence, then WHY would I *ever* want you to autocorrect to that???",pcmasterrace,2025-12-02 17:47:30,13
AMD,nrx7erz,"I like your comment. More so if you use AMD and Linux, like undefined driver support through Mesa...",pcmasterrace,2025-12-02 17:56:04,6
AMD,nrxis2z,"But then how would I play at 18K resolution?  I bought a 5090 because I had a bad case of lumbago but unfortunately 4K doesn’t work without FSR 5.0 and I still have the lumbago (worse now actually), so I’m left here holding the bag, playing games at 480p, as I soothe myself through masturbation.  But Big Pharma *won’t tell you that*.  I digress though.",pcmasterrace,2025-12-02 18:49:30,1
AMD,nry5bh7,lowering the resolution doesn't help at all if the CPU is the bottleneck,pcmasterrace,2025-12-02 20:39:07,1
AMD,nrx7onc,In May!,pcmasterrace,2025-12-02 17:57:21,1
AMD,ns0ij5e,I say keep using all the free services and just don't pay them. Bankrupt them through the electric bill.,pcmasterrace,2025-12-03 04:37:30,5
AMD,nryfa8x,"Well after the success of Ryzen, AMD charged a premium for their good CPU's while Intel was struggling to produce decent chips.  Basically AMD follows most of Intel's playbook when they are successful, but people here treat AMD like a cult and they can do no wrong. I just think it is kind of funny.",pcmasterrace,2025-12-02 21:27:13,2
AMD,nrx2lw6,Already had a 5800x3d and couldn’t justify the $800 platform upgrade at the time.,pcmasterrace,2025-12-02 17:33:13,4
AMD,nrxccf4,"It's a niche but enormous builds in Satisfactory can outgrow 32GB and even 48GB of RAM, and that's without a web browser or anything open.  The game is fairly well optimised, there are just a few of us crazy people who want to place a million plus dynamic objects in the world to build super cool stuff *because* it can handle it - but you need a beefy CPU, and past a point you need more RAM. The game gets +40% FPS from RAM and interconnect overclocking on my 9950x3d and even scales to 2ccx.",pcmasterrace,2025-12-02 18:19:09,3
AMD,nrye8zd,Flight simmers,pcmasterrace,2025-12-02 21:22:18,1
AMD,nrx76if,"Same back in march, ram kit for 64gb CL30 was $199.99 at microcenter, when I hit my buy again button on the order page the same kit is now $600 lol",pcmasterrace,2025-12-02 17:55:00,2
AMD,nry2191,"The economy currently, wondering where the breakpoint is also. In general lot of hobbies also. See Star Citizen or any f2p game, prolly a few thousand players responsible for 90% of revenue",pcmasterrace,2025-12-02 20:22:59,29
AMD,ns1p8nf,"It works for ram currently there is only a shortage for 96/128Gbit chips, but the entire stack got expensive.. and for gpu 5090 are just rtx6000 with few defects.",pcmasterrace,2025-12-03 11:00:11,1
AMD,nrx756x,"Idk, what did a 1080ti cost on release? I feel like inflation aside, the 5080 costs more on release.  My sources: absolutely none",pcmasterrace,2025-12-02 17:54:50,25
AMD,nrzsg7o,"Sorry, I should have been more specific that I was talking about market share, not technology. AMD is quite far ahead in desktop x86 market share. That might change with Nova Lake, but that's not out yet so I can see why AMD might want to increase their margin now and potentially be more aggressive if NL turns out to be strong.",pcmasterrace,2025-12-03 01:55:56,1
AMD,nryuhyc,"Being in the PCMR sub usually means you know what a good deal is, even knowing the downsides of console gaming compared to PC, a good price is a good price. And also a rarity these days",pcmasterrace,2025-12-02 22:42:32,3
AMD,nrx3b4j,"If you don't have a PS5 already, the PS5 pros have been on decent sales lately and will likely be the go to since PC parts are currently killing any semblance of a budget build.",pcmasterrace,2025-12-02 17:36:37,29
AMD,nryhjos,mid-range PC is getting stupidly expensive though lmao,pcmasterrace,2025-12-02 21:37:56,1
AMD,nrx3dt3,You guys are good till am6. Stay the course . Save enough for in the meantime for a 4k monitor cuz that's where all the new hardware shines.,pcmasterrace,2025-12-02 17:36:59,14
AMD,nrydme3,"Heck, I have a 5700X and a 9070 XT, and it's glorious. I play MSFS at 4K High/Ultra and the 5700X is still not maxed out at 60% usage.",pcmasterrace,2025-12-02 21:19:16,4
AMD,nrxi17b,A local dealer had it in (limited) stock so I bought one. Gonna be skipping AM5 after all.,pcmasterrace,2025-12-02 18:46:01,2
AMD,ns0m7y1,Ryzen 2200g,pcmasterrace,2025-12-03 05:04:04,1
AMD,nrxlkhh,"Get a load of this guy, he thinks weed qualifies as a real drug 😂",pcmasterrace,2025-12-02 19:02:42,15
AMD,nrx1prv,"During the pandemic I built a PC for me, my wife, and my youngest (rest were already moved out). Man that was a pricey time. Two 3080Ti's, a 3090, all new everything for both of them and only thing I didn't replace was my monitor. I think I dropped close to $10k all said & done. I guess that doesn't sound like much compared to building equivalent today. I'd spend $5k just in equivalent gpu's and another $2k in RAM",pcmasterrace,2025-12-02 17:28:54,3
AMD,nrxydo2,Not if your kids take care of you when your old....,pcmasterrace,2025-12-02 20:05:05,1
AMD,nrx300g,59 and same,pcmasterrace,2025-12-02 17:35:08,8
AMD,nrx6u93,"BF6 is alright, but cannot be excluded do to it's heavy monetization and commonly encountered bugs. A lot of people have mentioned Expedition 33. Ive honestly never heard of it before today.",pcmasterrace,2025-12-02 17:53:27,-9
AMD,nrx64mx,"People do this shit with movies as well and its the dumbest shit ever. They will say movies are dead, but only see like 1 movie every 2 years.   These people complain, but they also refuse to try new games and movies.",pcmasterrace,2025-12-02 17:50:08,16
AMD,nrx66p6,"People do this shit with movies as well and its the dumbest shit ever. They will say movies are dead, but only see like 1 movie every 2 years.  These people complain, but they also refuse to try new games and movies.",pcmasterrace,2025-12-02 17:50:24,1
AMD,nrx6lau,"You are correct; there are plenty of modern indy and small dev games that are great. These games, however, don't typically require newer hardware (as they are either well-optimized or aren't computationally/graphically intensive). AAA games have been the biggest offendor for microtransaction, unoptimized, and AI slop.",pcmasterrace,2025-12-02 17:52:17,-1
AMD,nryxdob,"Dafuq. I think this is a luxury in every sense of the word. My 1080 GOAT is running cyberpunk, Hogwarts, horizon, etc. on high quality with enough fps to clutch warzone dubs.  Idk what game needs high specs. Obviously you can get unreal 4k ultra textures out of it with a high end spec, but neeeeed?",pcmasterrace,2025-12-02 22:57:58,1
AMD,nrx5tyg,This is very much not true if you actually pay attention.   Expedition 33  Arc Raiders  Baaltro   Marvel Rivals,pcmasterrace,2025-12-02 17:48:45,5
AMD,ns0vnv2,they don’t even know you exist man,pcmasterrace,2025-12-03 06:18:35,2
AMD,ns1nce0,>I say keep using all the free services  They make money out of your use one way or another.,pcmasterrace,2025-12-03 10:43:04,2
AMD,nrx4hcp,"Well now, just the ram is that much! You probably would have felt more justified now keeping it!",pcmasterrace,2025-12-02 17:42:19,6
AMD,nrx6an5,Now it’s time to decide whether you buy those same components for 2x literally rn or for 5x next year.,pcmasterrace,2025-12-02 17:50:55,3
AMD,nrx3jce,That is a cheap upgrade for a lot of capability.   In 2020 it was about that much to go from a 3570k DDR3 system to 3600 AM4+NVMe. Just under $800 IIRC.,pcmasterrace,2025-12-02 17:37:43,1
AMD,nrx4fht,I feel that. Shame though,pcmasterrace,2025-12-02 17:42:04,1
AMD,nrx9aa5,When I check on Amazon.se mem kit is $1587,pcmasterrace,2025-12-02 18:04:50,1
AMD,nryrdik,Go on with your talking points about “perpetual growth”.,pcmasterrace,2025-12-02 22:26:12,0
AMD,nryoycr,"so, basicly the anime industry model .. just pure focus on the whales",pcmasterrace,2025-12-02 22:13:48,18
AMD,ns1rx2f,"That's cool, but I specifically said processors.",pcmasterrace,2025-12-03 11:23:41,1
AMD,nrxax5h,"The msrp of the 1080Ti in 2017 was $699.  $699 in 2017 is equivalent in purchasing power to about $923.87 today, an increase of $224.87 over 8 years. The msrp of the 5080 is $999.  Median income has however only increased by 3% since 2017. So relatively speaking GPU prices have increased even more disproportionately.  Also it should be pointed out that nvidia has since shifted the model stack a tier down. The 1080 Ti was the same die as the Titan. The 3080 was on the same die as the 3090. The 80 series was *very much* closer in performance to the halo card.   Now the 5080 is more comparable to what a 1070 was back in 2017 or a 3070 in 2020. Those launched for $379 (1070) and $499 (3070).",pcmasterrace,2025-12-02 18:12:31,29
AMD,nrxdmqu,"The 1080TI compares to the 5090, not the 5080 in terms of where in the stack it fits.",pcmasterrace,2025-12-02 18:25:14,16
AMD,nrxhz9i,²⁰¹⁷699=²⁰²⁵926.20 with inflation. and the cheapest I see on Amazon now is 999.  Sadly not all 5080 are the same.  1500 for Asus tuf 5080.  Asus rog astral are 1700 and 1800 for white.  You are right.,pcmasterrace,2025-12-02 18:45:46,2
AMD,nrxghqv,"[Here ](https://gamersnexus.net/u/styles/large/public/inline-images/inflation_prices_7_full-4x_foolhardy_Remacri.png.webp)you got a source. As you can see, besides the flagship performance, pricing isn't actually that bad. And realistically, the flagship is of no real relevance for the typical gamer (and isn't really always comparable because flagship performance is really just what Nvidia choose it to be due to their monopoly on the high-end).  This is offtopic, though. Not even sure why I get downvoted for pointing that out.   GPU pricing is barely affected by other parts because it is the most expensive compent in like 90% of cases. If you buy a $2k build, $100-200 more sucks, but that isn't gonna change much about you having $2k to spend.  CPUs, on the other hand, are directly affected by RAM and mainboard pricing - and if RAM suddenly costs $100-200 more, then your CPU upgrade costs that much more. Especially for budget builds, that is easily 30-50% of the price.",pcmasterrace,2025-12-02 18:38:47,-1
AMD,ns4h7m6,Fair point.,pcmasterrace,2025-12-03 20:03:56,1
AMD,nrxgo1z,"Just remember the extra cost of the online subscription if you plan to play online. 10$/month for the 5 years or so you might use the console is 600$. Steam games are also usually cheaper. Not to mention greymarket keys etc.    I know cash flow is not the same as money now, but it's worth remembering total cost for those on a budget.",pcmasterrace,2025-12-02 18:39:37,6
AMD,nry0xcc,"I reckon further than that tbh. CPUs aren't doing anything radically different and games rarely make use of the ones we have, so ya long live 8c/16t  Although with the landscape changing over the next decade with AI bullshit and Nvidia clearly pushing to be lease-first, I worry about stagnation more than anything.",pcmasterrace,2025-12-02 20:17:35,3
AMD,nrxva9f,"According to the federal government, it’s just as dangerous as heroin, LSD, and ecstasy. But according to my state, it’s something I can just pick up on my way home from work 😂",pcmasterrace,2025-12-02 19:50:05,7
AMD,nrxre0q,"Depends where he lives. In Texas, he'll be beaten with a brick for gummy possession.",pcmasterrace,2025-12-02 19:31:02,6
AMD,ns0bju8,Who said they were weed edibles? 😂,pcmasterrace,2025-12-03 03:50:25,2
AMD,nry7ztu,They don't even go to school and take care of me...   Which is nice :),pcmasterrace,2025-12-02 20:52:03,1
AMD,nrxvj1e,Father Time here and Same,pcmasterrace,2025-12-02 19:51:15,4
AMD,nrxau6t,"If you've never *heard* of Expedition 33 it's time to diversify sites where you get news from and communities you engage in.   Kinda wild. I can understand not playing it, but not hearing of it seems impossible even if someone was attempting to somehow avoid it.",pcmasterrace,2025-12-02 18:12:08,10
AMD,nrx77ge,"Damn, you've missed out on probably the greatest gaming success of the year. Hell, they were nominated for pretty much every award possible in the game awards.",pcmasterrace,2025-12-02 17:55:08,1
AMD,nrxnaxm,I had a friend like this. He'd refuse to stop playing the same garbage AAA yearly slop releases and then complain that gaming was a waste of time.,pcmasterrace,2025-12-02 19:11:04,1
AMD,nrx72pe,"Battlefield 6 is amazing even if its not my genre and is triple A. If we go outside of PCs, mario kart and donkey kong had amazing games and are triple A. Ghost of Yōtei is triple A  Its okay to enjoy things.",pcmasterrace,2025-12-02 17:54:31,1
AMD,nrx6whl,"Balatro, at least, can run on a potato. Arc Raiders is also really well optimized.",pcmasterrace,2025-12-02 17:53:44,3
AMD,nrxqvde,"Well i didn't say ""ALL""",pcmasterrace,2025-12-02 19:28:29,1
AMD,nryjw7y,"Don't forget KCD2, Silksong, Hades 2",pcmasterrace,2025-12-02 21:49:05,1
AMD,nrx6c9a,2 of those are debatable,pcmasterrace,2025-12-02 17:51:07,0
AMD,nrx9b8s,Only Balatro on that list.,pcmasterrace,2025-12-02 18:04:57,-5
AMD,nrynprl,Haha for real. Especially if this is the outlook for a couple years.,pcmasterrace,2025-12-02 22:07:35,1
AMD,nryvxzh,Yeah.. subscriptions for the rest :(,pcmasterrace,2025-12-02 22:50:13,11
AMD,ns0qw6i,">The 1080 Ti was the same die as the Titan. The 3080 was on the same die as the 3090. The 80 series was *very much* closer in performance to the halo card.   Now the 5080 is more comparable to what a 1070 was back in 2017 or a 1070 in 2020.   Yeah, we're getting some real shrinkflation from Nvidia's newer cards. The 80 class is effectively extinct. The 80 Ti used to provide performance within spitting distance of the current gen 90/Titan class cards for a fraction of the price, and the regular 80s weren't far behind. There is no card in the 50 series lineup that matches either description.  The 5080 is definitely the worst 80 class card of all time. It's not meaningfully better than the 5070 Ti, and to my knowledge it's the only 80 class card that does not beat or even match the performance of the previous generation's 90/Titan class.  It is more accurate to call it a 70 class, but shit, 70 class cards as recent as 4070 Super traded blows with the 3090. You can't even say that about the 5080 and the 4090.",pcmasterrace,2025-12-03 05:39:50,3
AMD,nryvk6h,"And then you look at the 1080 which is $808 MSRP with inflation but for only 10-15% performance increase instead of 50% and compare that to the 5070 Ti at $749 MSRP with 30% performance increase (over the respective 70 models). So 50 series is way better value here.  With the 5080 you also compared a release card of the 50 series to a card that released a year after the 10 series came out originally. So 1080 Ti pricing should be closer to the (now delayed) 5080/5070Ti Super, if anything.  You need to look at the overall picture. Don't just pick the data that fits your conclusion. The overall picture is: Not much has changed. Nvidia specifically has some more agency over pricing due to their AI quasi-monopoly, but for gaming, it really isn't that bad atm.",pcmasterrace,2025-12-02 22:48:11,0
AMD,nrxo2hg,"Thing is 1080ti was on the titan die, so it was a downscaled flagship, part of the reason it's remained relevant for so long.",pcmasterrace,2025-12-02 19:14:47,4
AMD,nrxhnbg,"Yeah, there is a difference but it's not nearly as big as I thought. Another reply did the math too so it checks out",pcmasterrace,2025-12-02 18:44:14,1
AMD,nryivxb,"Those arguments are so off the mark but always circlejerked in these subreddits lmao  the 80$ a year (which isnt needed for a lot of the most popular online games anyway if that's your angle) also gets you 24-36 games a year added to  your account.  ""on a budget"" that is an excellent deal and you're not factoring in the savings you can get via physical media which is always so lost on PC only people",pcmasterrace,2025-12-02 21:44:16,4
AMD,nrxw9g1,Can't question the money proposition there. I just think it depends on what your use case/needs are. I cancelled my PS subscription because I stopped using my PS5 for online games. All my online play is PC and I almost use my PS5 exclusively for single player and local/couch co-op type games,pcmasterrace,2025-12-02 19:54:47,2
AMD,nry2tgu,"I hate the Steam it’s cheaper argument, on PS you can literally get a latest release physically then trade it for another latest release, you can play basically for free, +- few dolars, otherwise yea, paying for online sucks",pcmasterrace,2025-12-02 20:26:50,2
AMD,nrywpgg,"As a long time gooner, gamer and heavy drug coomer. I will say weed is probably one of the most powerful drugs ever. Nothing else has made me taste like I'm eating a literal tangerine just from taking it. Also, idk if it's just the concentration ""these days"" but with heavy consistent use you can develop withdrawal symptoms from quitting. Irritability, emotional instability, depression and even suicidal tendencies. Some research suggests it will lower your testosterone.  The others are obviously worse because as direct uppers/downers they are almost immediately addictive and deplete dopamine reserves instantly. And cut with nasty stuff due to the massive criminal enterprises behind them. With the fentanyl trend, I wouldn't touch of any of it nowadays.  LSD and mushrooms can can make you go crazy either by activating dormant bi-polar or schizophrenic genes (if you have them) or simply some people's reaction to entering a drug induced trip to mars.  My honest opinion is the safest drugs are hallucinogens, IF you can do it in a safe environment, acquire it safely, and don't have those potential gene activators.  Happy smurfing kiddos.  ![gif](giphy|VgXsj63vX4aOEQn6kL|downsized)",pcmasterrace,2025-12-02 22:54:20,1
AMD,nrywv0f,"God forbid they're brown, they may be sent to a foreign prison camp on top of it.",pcmasterrace,2025-12-02 22:55:10,2
AMD,nrxe8uh,It's a wild take to say that after saying all new games suck lol. Buddy doesn't even keep up with new game launches,pcmasterrace,2025-12-02 18:28:07,7
AMD,nrxd0ll,Yes im playing it with r3100 and rx5500xt 8gb (this conbo was the budget beast of 2019-2020) and its playable and quality isnt that bad (all low but i could go medium with fsr upscaling),pcmasterrace,2025-12-02 18:22:18,1
AMD,nrymcir,Exactly. People that say there are no new good games don't play enough games,pcmasterrace,2025-12-02 22:00:48,2
AMD,nrx6h9q,Not really. God forbid people enjoy different games than you.,pcmasterrace,2025-12-02 17:51:46,2
AMD,nrx9h0k,Again please recognize that people may enjoy different things than you.,pcmasterrace,2025-12-02 18:05:43,3
AMD,ns1ro2b,It is interesting they called the 5070Ti a 5070 considering it is a lot closer to a 5080 than a 5070.,pcmasterrace,2025-12-03 11:21:33,2
AMD,ns25dsa,"Funny that you write that. Performance-wise, we got  1070 -> 1080 = 10%  2070 -> 2080 = 20%  3070 -> 3080 = 25%  4070 -> 4080 = 45%  5070 -> 5080 = 50%  Going by inflation, the 70 series are fairly close in terms of pricing to what it used to be. And 80 series increased only by around 10% or so. So if you claim that the 70 is good, then 80 just got better. Even with the 5080 being bad value compared to the 5070 Ti.",pcmasterrace,2025-12-03 13:03:29,0
AMD,nrxrikk,So are 3070 Ti and 4070 Ti. But not the 5080 and 2080. So that doesn't really mean much at all on its own. 10 series in general was insane value.,pcmasterrace,2025-12-02 19:31:40,0
AMD,nryr70m,trading in a 70$ game for 15$ of credit doesn't come close to equaling just getting that 70$ game for 30$.    Not remotely close.,pcmasterrace,2025-12-02 22:25:15,-2
AMD,nrxnvod,"That is how it is whenever people say ""they dont make good games/movies/tv"" anymore.   You see this exact same thought in movie discussions and the last time they even went to the theater was 2019.",pcmasterrace,2025-12-02 19:13:52,2
AMD,nrx6ye2,"Right, thus why they're debatable",pcmasterrace,2025-12-02 17:53:58,-4
AMD,ns3u8u4,"And it's funny you write that, because it doesn't invalidate or address a single point I made.  The 5080 is the only 80 class card to not beat or even match the previous generation's 90/Titan class. Fact.  There is no longer a product offering that provides performance within spitting distance of the 90/Titan class card. Fact.  >So if you claim that the 70 is good, then 80 just got better.  I never claimed that at all, don't put words in my mouth.  The 50 series has some of the worst stagnation we've ever seen from a new generation. You bringing up the 5070 stat is meaningless, especially when the 5070 is only about as good as the last generation 4070 Super. It fails to even beat the 4070 Ti, when every other 70 class card before it was at least able to soundly beat the 70 Ti of the last generation.  Taking all of this into account, it's clear the utter lack of generational uplift on the 5070 is what's responsible for the 5080's growing lead. Insinuating that this is instead the result of 5080 being good value or providing good generational uplift rather than the 5070's failure to improve is completely disingenuous.  And we see this all across the 50 series' product lineup. The 5060 Ti 16 GB can't even beat the 4070, when the 3060 Ti was beating the 2080 Super. The 5060 can't even beat the 4060 Ti 8 GB, when the 3060 was beating the 2070. The 5070 Ti can at best match the 4080, when the 4070 Ti was beating the 3090.  And all this is corroborated by the [diminishing relative core count](https://gamersnexus.net/u/styles/large/public/inline-images/cuda_percent_7_full-4x_foolhardy_Remacri.png.webp) to the flagship we see in these newer generations. The current 5080 has a relative core count that is comparable to what the 70 class cards of the past offered.  If we look at the 30 series (the last generation to provide generational improvements across the board, and not just for select cards), we see how bad it currently is.  The 5080 offers 50% of the 5090's core count for 50% of the price. Seems reasonable, but it becomes a shit value proposition when you consider that the 3070 offered **56%** of the 3090's core count for only **33%** of the price. For a more direct relative price comparison, the 3080 offered **83%** of the 3090's cores for **53%** of the price.  The 5070 offers 28% of the 5090's cores for 27.5% of the price. Again, seems reasonable until you consider the price to relative core ratio of the 3070 I just mentioned. To rub salt into the wound, spending **26.6%** of the 3090's MSRP would have gotten you a 3060 Ti with **46.3%** of the cores.  By relative core count, the 5080 should be a 70 class. And this translates over to performance, too. The 5080 is only 66% as fast as the 5090, while the 3070 was 70% as fast as the 3090 was. What used to be the 80 class is extinct, there's no denying it.  I really don't understand why you're going to bat so hard for Nvidia here. Unless you literally work for their PR department and are paid to do this, you gain absolutely nothing trying to justify and rationalize the diminishing returns and increasing shrinkflation we've been seeing from the consumer cards. It's baffling behaviour to fight so hard on the behalf of a corporation that doesn't even know you exist.",pcmasterrace,2025-12-03 18:14:31,2
AMD,nrx7r5b,"No. You can say a game is good even if you dont personally like it.   I dont give a shit about expedition 33, but itss a good game. I dont care about donkey kong but its a good game. I dont care about battlefield 6 but its a good game. I dont care about silk song but its a good game.   Like if you go to these communities you can recognize it scratches their itch. That is how you know its good even if you are not the audience.",pcmasterrace,2025-12-02 17:57:40,5
AMD,ns4g8rj,">I never claimed that at all, don't put words in my mouth.  That is why there is an IF. How am I supposed to know that you don't think a 70 class card trading blows with the prior 90 model is not good? Why even write that as a seemingly positive example then? That is just misleading.  >The 5080 is the only 80 class card to not beat or even match the previous generation's 90/Titan class. Fact.  Which means little because Nvidia is the one setting the comparison for flagship. They can release a 90 series card that is just a refresh and there is nothing you can do about it cause they are a quasi-monopoly in the high-end. Thus, flagship performance is a really bad baseline to work with. What you should set as a baseline, is the main consumer cards, i.e. 60 and 70 models (maybe add 80 for the high-end consumer), that set the baseline for hardware prerequisites - and this baseline IS actually relevant for gaming.  >The 50 series has some of the worst stagnation we've ever seen from a new generation. You bringing up the 5070 stat is meaningless, especially when the 5070 is only about as good as the last generation 4070 Super. It fails to even beat the 4070 Ti, when every other 70 class card before it was at least able to soundly beat the 70 Ti of the last generation.  Which also means nothing because we are talking about the prices for the consumer. Whether the market stagnates or not is IRRELEVANT to the level of pricing.  >And all this is corroborated by the [diminishing relative core count](https://gamersnexus.net/u/styles/large/public/inline-images/cuda_percent_7_full-4x_foolhardy_Remacri.png.webp) to the flagship we see in these newer generations. The current 5080 has a relative core count that is comparable to what the 70 class cards of the past offered.  Which is even less relevant. It does not matter. Practically NOONE buys GPUs for relative core count - consumers buy it for performance. Whether Nvidia saves more money by using smaller chips or not - as long as you get the performance you need for a certain price, that is a non-argument.  >I really don't understand why you're going to bat so hard for Nvidia here. Unless you literally work for their PR department and are paid to do this, you gain absolutely nothing trying to justify and rationalize the diminishing returns and increasing shrinkflation we've been seeing from the consumer cards. It's baffling behaviour to fight so hard on the behalf of a corporation that doesn't even know you exist.  I am not - if you think about it, you are actually the one ""batting for Nvidia"". You are the one trying to state that Nvidia used to be much better - And I am telling you: No, they weren't. That is just nostalgia glasses and a confound of the overall economic situation. The GPU market is as bad/good as usual, and pricing is as bad/good as usual.",pcmasterrace,2025-12-03 19:59:14,0
AMD,nrx7w5c,"Sorry, I won't be giving praise to a game that proudly uses AI",pcmasterrace,2025-12-02 17:58:18,-5
AMD,ns4z2t6,">Why even write that as a seemingly positive example then? That is just misleading.  Because we're discussing the stagnation of the 50 series here, buddy.  >They can release a 90 series card that is just a refresh  And this is nothing more than a meaningless hypothetical. The 90 class cards are the only ones that have been consistently getting the generation to generation performance improvements, and the fact is that the rest of the lineup hasn't followed suit. Sure, if the 6090 has a performance uplift as marginal as the one from the 3090 to the 3090 Ti then your argument may have a leg to stand on. But that has not happened. Well, it did for the 5070, but that's another topic.  The 80 class used to provide similar die sizes and core counts as the 90/Titan class. It no longer does. No matter what mental gymnastics you'd like to do to justify this, you cannot deny this.  >What you should set as a baseline, is the main consumer cards, i.e. 60 and 70 models (maybe add 80 for the high-end consumer), that set the baseline for hardware prerequisites  And given the shrinking relative core counts and die sizes, it's clear that Nvidia can set these mid range tiers just as arbitrarily as they set ""the comparison for flagship"" as you say. You bring up the hypothetical that they can release a next gen 90 series card that is just a refresh. I bring up the reality that they *actually did that* for the 5070. Tell me then, why would the ""main consumer cards"" serve as a better baseline when the exact thing you claimed might happen to the 90 series actually happened to the 70 series?  If anything, your ""5070 -> 5080 = 50%"" is even more invalid than making comparisons to the flagship class, because the 70 class is a stagnant product and what your comparison is really saying is that the 5080 is only 50% faster than a 70 class card from the last generation.  >Which also means nothing because we are talking about the prices for the consumer. Whether the market stagnates or not is IRRELEVANT to the level of pricing.  It does mean something, because you are completely ignoring the aspect of value here. The 5070 being $50 less than the 4070 Super is not a particularly big win when I could have bought the 4070 Super at launch and got an entire year of usage out of it before the 5070 even came out and provided no performance uplift.  Hell, by your logic, they could have released the 5070 at $600 (making it a literal rebadge of the 4070 Super) and you would have been fine with that, and fine with comparing it to a 5080 because ""stagnation is irrelevant to pricing"". Never mind the shitty value proposition that would be.  You used to be able to spend less than a grand (even adjusted for inflation) on a 3080 and get something that was pretty much just as good as the best GPU in the world at the time. *That's* a meaningful metric for value, not comparing a 5080 to a lower tier graphics card that isn't even better than its predecessor.  >Practically NOONE buys GPUs for relative core count - consumers buy it for performance.  Well, good thing I brought up performance in my comment then! Did you have trouble seeing it? I can make it bigger so you don't miss it this time.  # The 5080 is only 66% as fast as the 5090 (at 50% the price), while the 3070 was 70% as fast as the 3090 was (at 33% the price).  That's a pretty good argument for the shitty value of the current 80 class, no?  >Whether Nvidia saves more money by using smaller chips or not - as long as you get the performance you need for a certain price, that is a non-argument.  This only matters if we can get better performance out of shrinking dies. The 4070 Super had a much smaller die than the 3090 and matched it in performance. In that case, sure, they got the same performance out of less silicon, and could justify the diminishing relative core count.  But saying it's a non argument no matter what as long as ""you get the performance you need"" is an awful argument. That is just settling for mediocrity and stagnation. By your logic, the 6070, and 7070, and 8070 could come out, be exactly the same as the 5070 for the same price, and it'd be a perfectly fine product because ""you get the performance you need"".  >that Nvidia used to be much better - And I am telling you: No, they weren't.  And how do you justify this? The hard fact is that you used to be able to buy a 70 class card that would match the flagship card of the previous generation. Or an 80 class card that would soundly beat it. Each new generation (save for a couple exceptions) offered incredible value in that sense. This one does not. You literally cannot ignore the diminishing generational improvements we have been seeing.  I'm not saying Nvidia was some great company before, but when we literally do not see the generational improvements they used to provide, how can you sit here and claim that ""they weren't better"" before? Nostalgia glasses do not tint the relative performance charts on TechPowerUp.",pcmasterrace,2025-12-03 21:30:05,1
AMD,nrx961o,"cool that doesnt mean its not a good game. There is not debate. Its objectively a good game.  You can say its unethical, but that is different than saying its a bad game.",pcmasterrace,2025-12-02 18:04:16,4
AMD,nsjv7ld,"Ryzen 5500 plus rx 6600 is a decent budget 1080p pc.   Idk about it lasting 5 years, maybe if you mostly play indie/old games.     It could play current AAA games 1080p medium settings at least for now.   Edit: if you can stretch your budget to an rtx 3060 12gb that'd give you a lot more future proofing since it has more vram and can use dlss.",pcmasterrace,2025-12-06 05:38:03,1
AMD,nsmynfj,That card is a beauty. Also I need to train in the art of cable management bc you all make me look at my pc with shame sometimes,pcmasterrace,2025-12-06 19:05:42,6
AMD,nsp4lab,I like your color choices,pcmasterrace,2025-12-07 02:37:34,4
AMD,nsn76w6,sheeeeesh it looks sick,pcmasterrace,2025-12-06 19:50:34,2
AMD,nsn8ke2,only 10 fans? you need more. pump those numbers up those are rookie numbers.,pcmasterrace,2025-12-06 19:57:55,2
AMD,nspn0sa,"What fans are these? Love the color scheme, super unique.",pcmasterrace,2025-12-07 04:38:57,2
AMD,nsri8mw,This makes me want nicer looking fans,pcmasterrace,2025-12-07 14:16:32,2
AMD,nsnwg6i,"Me, also with a 9070 XT but with only two (140mm) front fans, and a single (120mm) rear exhaust (in a Fractal North) with comparable GPU temps:  ![gif](giphy|l4FGq9MV1TZXDcn4Y)",pcmasterrace,2025-12-06 22:11:08,0
AMD,nss0jm4,Thanks man all fans are Corsair LX120. Except the front and side fans are LX120-R,pcmasterrace,2025-12-07 15:57:52,2
AMD,nso89xz,Hope he's running something heavy. Or maybe his house is just hot.,pcmasterrace,2025-12-06 23:19:20,1
AMD,nsphooi,"Im always confused about why so many people get so worked up about the amount of fans someone puts in a build.. I did mine solely on cosmetics. In this instance, each fan is running at bare minimum of 600rpm and the room is set at 70°F. So you can use that info for comparison to confirm that your cooling system is better than mine.",pcmasterrace,2025-12-07 04:03:25,3
AMD,nsqtvmd,https://preview.redd.it/stqjl004lr5g1.jpeg?width=1383&format=pjpg&auto=webp&s=730e893772961999c4bda29a1850bde44fc6848d  15 fans here. (gpu has changed) but with this fan config my pc has never gone above 70c even in 4K 144 FPS :),pcmasterrace,2025-12-07 11:07:10,2
AMD,nsrzzxw,Looks sick man! My temps reach about 80c at 2k 220fps in BF6. I've thought about frame capping at 180fps or so to see where my temp would be after that,pcmasterrace,2025-12-07 15:55:00,1
AMD,nssp5sj,i honestly cant see a difference above 144 so i just cap it to that and is been pretty cool since then,pcmasterrace,2025-12-07 18:03:18,2
AMD,nsh03xn,"Try timespy and steel nomad, maybe even port royal, month get some more records",pcmasterrace,2025-12-05 19:06:49,3
AMD,nsh705h,I'd imagine it's because you're in a very small pool of people who paired a 60ti tier class card with a 78x3d. It's an odd combo.,pcmasterrace,2025-12-05 19:41:53,3
AMD,nsgsf0r,Iirc if you force rebar it pushes up scores even higher but rebar can make certain games perform worse,pcmasterrace,2025-12-05 18:29:08,0
AMD,nsh0czh,Mabey in 20 min,pcmasterrace,2025-12-05 19:08:03,1
AMD,nsh89g2,https://preview.redd.it/lya2zeu7wf5g1.png?width=886&format=png&auto=webp&s=8812aa1b78a09b0bf2020823523573d4b5fefaff  Still 17th place for 5060ti 16 GB but jeah there are only 290 other benchmarks with my CPU and GPU,pcmasterrace,2025-12-05 19:48:29,2
AMD,nsiw47x,"This is it. My mate is on top of the 5070ti and 11900KF leaderboards for some benches, because it's a very unique combination.  The fact that he has the 3dmark achievement of a legendary score for his hardware and I don't is annoying me to no end lol. I'm going to have a fresh crack at it in winter time as I'm armed with a bit more knowledge on the 50 series capabilities.",pcmasterrace,2025-12-06 01:30:30,1
AMD,nsgtrge,Already did that but thanks,pcmasterrace,2025-12-05 18:35:43,4
AMD,nsiwfck,Best to overclock in winter time if you're not doing it already.,pcmasterrace,2025-12-06 01:32:32,1
AMD,nsgxr0b,You using a fancy setup for this or just tweaking afterburner etc?,pcmasterrace,2025-12-05 18:55:16,1
AMD,nsgyk55,Nah this is my Setup 😂😂  https://preview.redd.it/tkobchynnf5g1.jpeg?width=2304&format=pjpg&auto=webp&s=acb1baa4d684ac6e211d0edef067cf7e2ed96941,pcmasterrace,2025-12-05 18:59:13,1
AMD,nsh14j3,"Lol well whatever works, you don't have to deal with crazy power draw so it works out really well",pcmasterrace,2025-12-05 19:11:52,1
AMD,nsvxp36,Your cable management messy,pcmasterrace,2025-12-08 04:41:56,1
AMD,nrwwnum,Pc gamers going to feel even more pain in the wallet.  Glad I'm sorted GPU wise.,pcmasterrace,2025-12-02 17:04:33,84
AMD,nrx0aht,Grab the 9060XT 16GB at $350 and wait till this BS blows over in 2-3 years.,pcmasterrace,2025-12-02 17:22:03,30
AMD,nrwue0a,"To everyone that was waiting for cards to come down in price, ouch.  Good thing I already finished my build.",pcmasterrace,2025-12-02 16:53:34,35
AMD,nrxbh8j,"That's far less than I expected, but I believe that's just the price increase charged to their board partners.  The actual street price from distributors is likely going to be much, much higher.",pcmasterrace,2025-12-02 18:15:08,6
AMD,nrx7u0d,Thank God I just bought my 9070xt.,pcmasterrace,2025-12-02 17:58:01,9
AMD,nrxtbb8,I got my 9070 XT recently so I'm doing good for now. My cpu is gonna be next but I'm not planning on being able to do that anytime soon sadly. I play at 3440 x 1440 so my 10th gen i7 isn't a bottleneck in anyway shape or form but it's still something I'm def worried about as time goes on.,pcmasterrace,2025-12-02 19:40:30,3
AMD,nry6e0a,Just bought a 9060 XT 16GB for 319€. Now i'm even more happy with it. 🙂,pcmasterrace,2025-12-02 20:44:19,3
AMD,nrxja3j,And add another $50 from retailers on top of that.,pcmasterrace,2025-12-02 18:51:52,5
AMD,nryaiha,"I already grabbed a 9060xt 16gb for $330, I’m not mad at this news",pcmasterrace,2025-12-02 21:04:15,2
AMD,nrxzqqt,Sucks for anyone that doesnt have a pc      My card is a few years old at this point: 6900xt. But honestly I have no drive to upgrade and I am hoping I can just rock this for another 5 years if i'm being honest. I feel like we are already past the diminishing returns part of graphical fidelity. And frankly a lot of games im into can be ran on hardware much weaker than mine.,pcmasterrace,2025-12-02 20:11:45,1
AMD,nry3d1q,"...Kid that sold me the PNY 5090 OC ARGB (used for under $2k) im using wanted to buy it back, after i fixed his botched thermal pad/paste job; unregistered even got the reciept.  PTM7950/TG-PP10 and thermals are EXCELLENT, warranty gtg.  Yea, sorry kiddo, thats a HARD no from me.   RAM is in everything nowadays; prices on GPUs, phones, RAM itself is likely going to double...",pcmasterrace,2025-12-02 20:29:29,1
AMD,nrycmz9,Stares at my 4080S lovingly.,pcmasterrace,2025-12-02 21:14:31,1
AMD,nryqvk5,Another re post of story 2 weeks ina row now,pcmasterrace,2025-12-02 22:23:35,1
AMD,nrz1vsw,"Upgraded as well to a 9070 since a price hike as just a matter of time. Got a good deal on it too, €499. Lowest price I've seen for it.",pcmasterrace,2025-12-02 23:22:56,1
AMD,ns0eyog,"Luckily, I bought an opened box 9070XT 16GB yesterday for $777cad ($556USD)",pcmasterrace,2025-12-03 04:12:51,1
AMD,ns0hzh3,"Well that doesn’t seem that bad, I was afraid it’d be far far worse. Maybe there’s hope for me yet and I can eventually retire my old 5700xt.",pcmasterrace,2025-12-03 04:33:37,1
AMD,ns0ptp3,Already bought a 3070 ti. I don't have to care about this.,pcmasterrace,2025-12-03 05:31:24,1
AMD,nrxeamh,So it begins.,pcmasterrace,2025-12-02 18:28:21,1
AMD,nrxj1rr,so the final price will be much higher than $50,pcmasterrace,2025-12-02 18:50:45,1
AMD,nrx4c1i,Snagged a 9070 XT for $580 over Black Friday.,pcmasterrace,2025-12-02 17:41:36,0
AMD,nrx201o,"Read that as raising memories of the cards initially, because having only 8gb vram on a modern card should be a crime.",pcmasterrace,2025-12-02 17:30:17,0
AMD,nrx3abd,vram and dram prices now work like crypto prices just wait the ram chip on my 1st gen raspberry pi will be worth as much as a used car soon,pcmasterrace,2025-12-02 17:36:30,0
AMD,nrz445e,"Welp, time to switch to nvidia! Only thing i care about is price to performance and for my country 40 bucks is allot considering prices are higher than in america",pcmasterrace,2025-12-02 23:35:43,0
AMD,nrwzm03,Daily reminder to undervolt your cards,pcmasterrace,2025-12-02 17:18:46,-5
AMD,nrxhpgq,Yup I just bought a 5080 a couple days ago to make sure I avoid all of this garbage and am set for a minimum another 3 years.,pcmasterrace,2025-12-02 18:44:30,16
AMD,nry5u5e,Same here,pcmasterrace,2025-12-02 20:41:38,1
AMD,ns9vxqb,"Instead of investing in gold or stocks, invest in RAM and VRAM",pcmasterrace,2025-12-04 17:00:58,1
AMD,nrx6ggp,"Last time I thought about building my own PC was during the crypto boom, I put it off and I started thinking about it again recently... And now this.   Its fine. My 1650 surely has a few more years left in it.",pcmasterrace,2025-12-02 17:51:40,3
AMD,nrzpo9q,Yeah I think prices seem pretty good right now. 16 GB cards should have some good life in them for a lot of games.,pcmasterrace,2025-12-03 01:39:18,1
AMD,nrys9zx,imagine thinking this will blow over and prices just wont keep increasing every single generation because gamers have just accepted paying more for everything every 2 years.,pcmasterrace,2025-12-02 22:30:52,0
AMD,nrxctxu,"Ive been waiting for a 5070 super because 12 GB is kind of meh. Then I saw that Nvidia is telling board partners to source their own ram now, so I jumped on a $480 5070. I have a feeling this is just the beginning and prices are going to get outrageous. I was feeling kind of bad about spending that much, then I looked back on my MC account and saw i bought my 3070 for 800 fucking dollars in 2021 during the crypto boom. I have a feeling we might see those prices in a few months.",pcmasterrace,2025-12-02 18:21:26,7
AMD,nrxhvcn,Yup. Glad I grabbed my 5080 when I did. A couple days ago. I don’t wanna deal with this bullshit.,pcmasterrace,2025-12-02 18:45:16,1
AMD,ns0mjcp,"Same but with the non xt , i was gonna get the xt but im off grid so i like the lower power if the non xt",pcmasterrace,2025-12-03 05:06:26,1
AMD,nrxqklf,Ikr!,pcmasterrace,2025-12-02 19:27:02,0
AMD,nrxqhwg,Plus tax,pcmasterrace,2025-12-02 19:26:40,2
AMD,ns3ohdl,I’m in the same boat as you got 2 6900xts and 2 6800s and I feel the run just about anything I want maybe this will finally make developers optimize there games better when people can’t run there shitty game because they can’t get a new Gpu to run it.,pcmasterrace,2025-12-03 17:47:04,1
AMD,nrxqvw3,I still have my 6800xt midnight black edition and 9070xt sapphire nitro,pcmasterrace,2025-12-02 19:28:33,7
AMD,nrz440t,Cries in Rx 550,pcmasterrace,2025-12-02 23:35:42,1
AMD,nrylcjq,If a 5080 doesnt set you up for the next 10 years we are cooked,pcmasterrace,2025-12-02 21:56:01,1
AMD,nry1jik,So you're just gonna stand there and complain instead of buying the current fair prices when you know you want an upgrade?,pcmasterrace,2025-12-02 20:20:36,-7
AMD,nrxffkt,So please stop thinking about pc forever and find a new hobby.  You would do us all a favor,pcmasterrace,2025-12-02 18:33:45,-19
AMD,nrykr62,I picked up my 9070xt a couple months ago and just got a 9060xt for my tv gaming box.   Half temped to pick up a B580 as a cheap standby card if this all gets stupid expensive and something breaks.,pcmasterrace,2025-12-02 21:53:09,2
AMD,nryvybk,Lmao I hope it sets me up that long!,pcmasterrace,2025-12-02 22:50:16,2
AMD,nryx27j,10 years?!?!? Two gens..three gens if you push it,pcmasterrace,2025-12-02 22:56:15,2
AMD,nry5lpw,"Yeah, I'm going to function like a normal human being, look at the price of something, decide its not worth the amount of money and I'm happy enough with what I've got.  And, like most humans do, complain about how expensive everything is lmao.",pcmasterrace,2025-12-02 20:40:29,7
AMD,nryr2w5,Yea tough times,pcmasterrace,2025-12-02 22:24:39,1
AMD,nrzxb1a,"It's about standards and expectations, and also the games you play. People have this weird fetish over pre-set graphics settings like 'Ultra' when by being more selective it's possible to get perfectly adequate gaming experiences at lower settings. Not to mention the vast difference in playing in 1080p @60hz versus 4k at 165...",pcmasterrace,2025-12-03 02:24:07,2
AMD,ns3c5tt,I dont mean 1440p ultra for 10 years lol,pcmasterrace,2025-12-03 16:47:43,1
AMD,nry7h8g,If you're happy with it that's different. Out of curiosity what do you play?,pcmasterrace,2025-12-02 20:49:36,-8
AMD,nrzxj3e,But then you get reddit crying over optimization...10 years is horseshit always has been always will be  By this logic the thing should last forever...but when you're running games at 35 fps on low..the card is done and has been for a couple years,pcmasterrace,2025-12-03 02:25:27,-1
AMD,nrycoyw,"Dude, go away. Mind your own business.",pcmasterrace,2025-12-02 21:14:47,4
AMD,nrzyp4q,"The trend is for gpus to last longer now than they did ten years ago. We are reaching the point where we don't need higher res and we don't need more frames. A 1080ti is what, 8 years old at least? They are literally releasing xx50's and xx60's today that barely outperform it in pure performance...  And we should _all_ cry about optimisation, _regardles_ of the performance of our cards.",pcmasterrace,2025-12-03 02:32:19,0
AMD,nryjsvr,"Lmao you guys are weird as hell. They complain, I say why, they say they're happy. I say what games you play, now some random tells me go away.   A 1650 can't even play unreal engine 4 games at good enough quality, obviously I'm interested in what games that piece of shit card can even manage aside from Minecraft.",pcmasterrace,2025-12-02 21:48:38,-1
AMD,ns006g6,"So you are trying to make your perceived ""trend"" of GPUs last 10 years by means of crying..lol have fun with that..  If you honestly think any of these cards are going to last 10 years with shifting A.I. tech then you're high...they may last less time if anything",pcmasterrace,2025-12-03 02:40:51,0
AMD,nrzc05y,I've played ex33 fine on my pc.  Played a ton of bg3 and it ran just fine.,pcmasterrace,2025-12-03 00:21:19,4
AMD,ns0pgg8,"Yeah this is such a weird thing to think. That's basically saying you want a 980ti to still be good these days. Complete bonkers but I guess according to reddit it's fine since all new games are trash, fun is forbidden und you have to be a patient gamer waiting 10 years to play anything",pcmasterrace,2025-12-03 05:28:31,2
AMD,ns00ef3,Who is crying?  Are you an adult or is there no point engaging in this further.  Please tell me how 'shifting AI' tech is going to stop my 3080 or someone else's 5080 from being able to play games.,pcmasterrace,2025-12-03 02:42:09,0
AMD,nrzhb4t,"bg3 is cpu bound that makes sense, but how did you run ex33? Videos show it as sub 30fps in doors on 1080p low with 85% tsr",pcmasterrace,2025-12-03 00:50:30,-1
AMD,ns02uvi,Just buy a console ffs lol,pcmasterrace,2025-12-03 02:56:30,0
AMD,nrzhwjn,"Just turn down settings and be okay with a game not always running at a perfect 60 fps, its not like I used some black magic to make it work.   It wasn't the smoothest gaming experience I've ever had, but it was more then playable.",pcmasterrace,2025-12-03 00:53:53,2
AMD,nsir4s8,With the ram prices?? I have a i7 8700 I want to upgrade so bad but the ram / cpu / mobo is around 1k alone,pcmasterrace,2025-12-06 00:58:22,11
AMD,nsisi1s,"That’s a big fat upgrade, welcome to the X3D party!",pcmasterrace,2025-12-06 01:07:03,6
AMD,nsjdj9x,I’m going from a 9400f to a 5950x 🤩 I don’t need to train my damn cached memory,pcmasterrace,2025-12-06 03:24:32,3
AMD,nsirbcq,Welcome :),pcmasterrace,2025-12-06 00:59:30,5
AMD,nsiy6kj,"Nice! Good upgrade, you’ll enjoy it",pcmasterrace,2025-12-06 01:44:07,2
AMD,nsj4m0o,"that is one hell of a jump, have fun!",pcmasterrace,2025-12-06 02:25:57,2
AMD,nsj5c1r,Enjoy bro !! X3Ds are the best !,pcmasterrace,2025-12-06 02:30:33,2
AMD,nsj96ws,Enjoy it!!,pcmasterrace,2025-12-06 02:55:32,2
AMD,nsjehc9,That's a wild jump your gonna love it,pcmasterrace,2025-12-06 03:31:01,2
AMD,nsjhr8v,nice upgrade you got... hope you enjoy your new brain. (cpu),pcmasterrace,2025-12-06 03:53:51,2
AMD,nsk84tr,Its the most wonderful time of the year... with x3d,pcmasterrace,2025-12-06 07:37:58,2
AMD,nsmbloj,"Congrats on your new parts man! Hope you have a blast with your new upgrades - unfortunately I’m at a point where I kinda feel burnt out from games and never finishing games, I need to ease back into them  You’ll have an amazing time with your upgrades!",pcmasterrace,2025-12-06 17:05:52,2
AMD,nsittqs,Congrats bro! How much did it cost you?,pcmasterrace,2025-12-06 01:15:34,1
AMD,nsivvrn,QHD to OLED NICE BRO THAT NOT HOW THAT WORKS,pcmasterrace,2025-12-06 01:28:59,1
AMD,nsixq3f,what the biggest improvement that you have notice since jumping from a i5 9400 to ryzen 7 78003d?,pcmasterrace,2025-12-06 01:41:07,1
AMD,nsirhdq,Thankfully with FB Marketplace/eBay I have gotten a lot of good deals! Nothing I have bought is new parts haha. Yea RAM is crazy but I dont see the prices coming down within the next 4 years and I also am VERY ready to upgrade,pcmasterrace,2025-12-06 01:00:33,3
AMD,nsj31qe,Ya its crazy how much even 64gb of RAM costs,pcmasterrace,2025-12-06 02:15:45,1
AMD,nsisryr,Thank you!! Im very excited to get everything together!,pcmasterrace,2025-12-06 01:08:48,2
AMD,nsjdqi0,"What a blessing fr. I am so excited, this CPU is going to melt my face compared to the old one and same with yours haha!",pcmasterrace,2025-12-06 03:25:55,1
AMD,nsirp3h,Aye!! Thank you again!!,pcmasterrace,2025-12-06 01:01:55,3
AMD,nsjdrdo,Thank you!!,pcmasterrace,2025-12-06 03:26:05,1
AMD,nsmw19f,Thank you very much! I know that feeling all too well. I think this upgrade will help with that ahah.,pcmasterrace,2025-12-06 18:52:21,2
AMD,nsiu1sv,"$300! Was happy to pay that price, no taxes, no shipping",pcmasterrace,2025-12-06 01:17:02,1
AMD,nsiw02k,"So if I had a QHD monitor and bought an OLED monitor, thats not how that works?",pcmasterrace,2025-12-06 01:29:47,1
AMD,nsiz3vf,Never seen people so excited to build PCs before 😂,pcmasterrace,2025-12-06 01:50:10,1
AMD,nsj0t96,"I have not had the chance to use it just yet sadly. Build will probably be dont late December, but will give an update once I do use it!",pcmasterrace,2025-12-06 02:01:16,1
AMD,nsj22u1,Went from a i7 7700k to a 7800x3d - its only +100 better than the old one (plus it can do 3d)  I didnt notice much difference,pcmasterrace,2025-12-06 02:09:26,1
AMD,nsisqnt,Yeah might just “upgrade” to AM4 and thug it out until it cools down I have 32Gb DDR4 hah,pcmasterrace,2025-12-06 01:08:34,2
AMD,nsmw5ag,Hope you have a blast with it my friend :),pcmasterrace,2025-12-06 18:52:55,1
AMD,nsj5e01,Really good price!,pcmasterrace,2025-12-06 02:30:54,2
AMD,nsixli6,"No, QHD is the resolution = 1440p. Oled is the panel technology. That can support many resolutions, mostly 1440p but also 4k  Your ""qhd"" monitor was probably an ips, va or tn monitor, these are other common panel types",pcmasterrace,2025-12-06 01:40:15,4
AMD,nsisxgg,Thats extremely fair😂 I had to talk myself into it for sure but if you convince yourself to spend too much on RAM everything else is easy to buy  ![gif](giphy|d3mlE7uhX8KFgEmY),pcmasterrace,2025-12-06 01:09:46,1
AMD,nsixzrg,Ahhhh I see! Thank you for the education and explanation!,pcmasterrace,2025-12-06 01:42:52,4
AMD,nsjnalc,Was about to comment this. Ur oled is most likely also 1440p qd-oled.,pcmasterrace,2025-12-06 04:35:03,1
AMD,nsjnl8s,It is 1440p:) but its 240hz instead of 120hz so thats an upgrade! True black is absolutely INSANE to see as well.,pcmasterrace,2025-12-06 04:37:19,2
AMD,nsk4v1w,Yeah. My phone is amoled so i love it lmao,pcmasterrace,2025-12-06 07:06:11,1
AMD,nskjifj,The real upgrade is the true black from the lighted one,pcmasterrace,2025-12-06 09:33:41,1
AMD,ns7tqyt,did You consider the ram prices?,pcmasterrace,2025-12-04 08:48:16,2
AMD,ns7wa5p,"My question was more about gaming experience. Is this a good setup, consider where I am coming from? Is dealing with AMD complicated in terms of drivers, reliability, etc?",pcmasterrace,2025-12-04 09:13:58,1
AMD,nt6q2rt,"Looks good, should be a nice 1440p build",pcmasterrace,2025-12-09 22:01:07,1
AMD,nscx1ix,"Microcenter guy doesn't know ball. Motherboard doesn't matter here, the 5500 doesn't have integrated graphics to display in the first place.",pcmasterrace,2025-12-05 02:40:09,8
AMD,nscxbbg,"Dammit. What's the cheapest/most basic CPU that has integrated graphics?    This is purely for a home media server/utorrent seed box, I need no horsepower. I just want something very low-power-consumption and cheap.",pcmasterrace,2025-12-05 02:41:42,2
AMD,nscykfe,"If I remember right without Googling (I am doing it anyways), it's any CPU that has the G designation. Example: 4400G, 4600G, 5400G, 5600G.",pcmasterrace,2025-12-05 02:49:04,3
AMD,nscyq09,Athlon 3000G or Athlon 200GE,pcmasterrace,2025-12-05 02:49:58,1
AMD,nsd56h3,"Dang, looks like they're a solid $100 more. At that price I could just stick my old GTX960 into the thing.",pcmasterrace,2025-12-05 03:28:57,1
AMD,nsd6uat,"You can get a 5600g for $100 or so on ebay. If you really don't need the beans, you can get a 2200g for more like $25.",pcmasterrace,2025-12-05 03:39:27,1
AMD,nsd7h61,"Thanks for the tip. I'm very wary of buying CPUs used, too much uncertainty around such a sensitive part.",pcmasterrace,2025-12-05 03:43:30,1
AMD,nsd7p9a,"You're not entirely wrong, but at the same time it is exceedingly rare for a CPU to fail. If there are bent pins, you get a refund and move on. Odds are it works totally fine, assuming the seller is reputable and not from China lol. There is some fake stuff out there. For $25 though, who cares lol.",pcmasterrace,2025-12-05 03:44:58,2
AMD,nsda81t,"Well, who knows if they can just pivot their chips to be packed into Epycs instead of Ryzens. It's the same curse/blessing when you have chiplet design, you could just scale it up for AI bros if you really wanted to.",pcmasterrace,2025-12-05 04:01:21,5
AMD,nsf3f6e,now what I will do with all those ryzen cpus I panic bought this week,pcmasterrace,2025-12-05 13:14:48,3
AMD,nsdfsqr,But RAM on the other hand…,pcmasterrace,2025-12-05 04:38:31,2
AMD,nsli1p6,"Do you have the rest of the specs? Like the motherboard, or the specific type of SSD? Just to make sure they aren’t cheaping out on an important component to save money.  In a vacuum though, this doesn’t seem too bad. Really good CPU and GPU, and everything else seems good for that class of PC. If you don’t understand enough about PCs to build one, and would prefer a prebuilt, I think this is a solid one.",pcmasterrace,2025-12-06 14:22:29,2
AMD,nslj6rm,"In this market? I'd take it. It's about a $200+/- difference building it yourself. The difference in price boils down to what the components actually are (Brand, CL rating, PSU rating, SSD gen, etc.)",pcmasterrace,2025-12-06 14:29:03,1
AMD,nsljet1,No motherboard? Hah. I’m sure there is as long as some low end for that hardware. It’s not bad price. If you pick those parts yourself does it add up to that price?,pcmasterrace,2025-12-06 14:30:21,1
AMD,nslpfuu,This is a really solid price in the current market with ram and storage prices. Unlikely you beat it building yourself.  edit: [This is about as cheap as I could get it without real compromise.](https://pcpartpicker.com/list/rfMKyW),pcmasterrace,2025-12-06 15:05:50,1
AMD,nslk405,"Unfortunately, I only know it's a Gigabyte B850 and DDR5-6000 on the ram.  Not sure what the Latency or brand is.",pcmasterrace,2025-12-06 14:34:30,1
AMD,nslkgh7,"Pics look like Gigabyte b850 gaming mobo, XFX gpu, & G.skill ram.",pcmasterrace,2025-12-06 14:36:33,1
AMD,nslp669,"Follow up question, if you don't mind.  Given the choice between 2 bundles at the same price which would you pick?  9800x3D/B650E  or  7800x3D/X870E  Both ASUS MoBo. I'm just now finding out about Micro Center and there's one in my city.",pcmasterrace,2025-12-06 15:04:22,1
AMD,nslkkqt,"I didn't realize I cut out the MoBo.  It's a Gigabyte B850 and and if I pick those parts myself to pre-build, it comes out about $200-300 more than this.",pcmasterrace,2025-12-06 14:37:16,2
AMD,nslssyf,"[This](https://pcpartpicker.com/list/NjXNfd) is actually the build I put together myself, opting for the 4TB, since I do video and audio editing. (My drum software is massive) and going for the OC GPU for gaming.",pcmasterrace,2025-12-06 15:24:52,1
AMD,nsm2klv,"Interesting. It's an easy question with a not so easy logic. Here's how I see it:  7800X3D/X870E: - A future proof board. - PCIE 5.0 support. - Up to 5 M.2 NVME SSD, two of which support Gen 5. - Tons of ports with USB 4.0. - Supports up to 4 SATAs at 6/gbs. - Up to 256gb of RAM. - Plenty Fan, AIO, and RGB connectors. - 7800X3D performance is not too far behind a 9800X3D, so it won't be a bottleneck anytime soon. Best performance/$ value.  9800X3D/B650E-E (TUF?): - A solid board. - PCIE 4.0 support. - Up to 3 M.2 NVME SSD, with primary supporting Gen 5. - Less ports with USB 3.0. - Supports up to 2 SATAs. - Up to 192gb of RAM. - Fans, AIO, and RGB connectors. - 9800X3D is the best CPU for gaming and some productivity at the moment. Will probably outlive some components.  These are all the important things (at least to me) that I remember from when I was doing some research for my current build. I ultimately went with a 9800X3D because of its performance and projected longevity, and a X870E because of how much features and future proofing it offers. But what really sold me on these is what I plan to do in the future. I'm currently running a digital storage and plex server with my older PC. In the future, my current build will be my next server, so having the option to expand the storage, memory, and connect more peripherals was a very attractive prospect.  Like I said, not an easy logic. To summarize it for you; buy the one that you think will satisfy your need and your expectations. But whichever one you choose, it's not a bad one.",pcmasterrace,2025-12-06 16:18:07,1
AMD,nsm1q8q,Right so this is a good deal regardless. Especially with prices of ram and soon to be ssd right now.,pcmasterrace,2025-12-06 16:13:33,1
AMD,nsbt9bf,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-04 22:46:41,1
AMD,nsctx3j,"Nothing much to be excited about, since Gorgon Point is just a Strix Point refresh.  You get slightly higher clocks but otherwise it's mostly the same.",AMD,2025-12-05 02:22:07,24
AMD,nscf8hg,"If these rumors are true, I am wondering why they haven't moved onto RNDA 4 based iGPs instead of the RDNA 3.5 they've been using for a while. Makes me wonder if they are just waiting for RDNA5/UDNA before updating their iGPs.",AMD,2025-12-05 00:54:41,11
AMD,nsemi1x,"Damn, AMD core stagnation is REAL  Quad core Ryzen 5 in 2026 is insane",AMD,2025-12-05 11:04:32,4
AMD,nscyc1i,Gorgon is a refresh of Strix. Not new silicon. It’s still RDNA3.5 for that reason alone.,AMD,2025-12-05 02:47:44,22
AMD,nsczqpf,"[3.5 was specifically made for mobile](https://www.pcgamer.com/hardware/graphics-cards/amds-tweaked-rdna-35-gpu-is-solely-focused-on-improving-mobile-gaming-performance/) and 4 as per leaks and rumors, didn’t have any plan to release on the mobile space (either scrapped alongside top dies or never was, don’t know which to believe)  A “good enough” strategy until they deem the next arch to be viable and feasible for mobile requirements.  Also, 3.5 is still new and these upcoming are refreshes.",AMD,2025-12-05 02:55:56,10
AMD,nscyhnt,I think your guess is right. I suspect they will update their entire line with UDNA when it releases,AMD,2025-12-05 02:48:38,3
AMD,nsdstls,"Remember, the main limit for an igpu is the memory bandwidth and latency since it uses ram. Strix halo has a wider bus thus higher bandwidth, but all the others use normal lpddr5x, for example in handhelds. It might be that a rdna4 igpu isn't really faster than a 3.5 one due to this aspect alone, even if it would have nice features like fsr4. They probably are waiting for ddr6 memory to increase the bandwidth and put stronger igpus in devices, or at least this is my theory.",AMD,2025-12-05 06:20:41,5
AMD,nsh49ar,This is just a refresh of the current lineup. It is heavily rumoured that Zen6 will feature more cores.,AMD,2025-12-05 19:27:40,0
AMD,nsvsfxm,Isn't Lunar Lake eating their iGPU lunch? I certainly hope their next arch is viable and feasible for mobile.,AMD,2025-12-08 04:06:29,1
AMD,nsebm0v,LPDDR5X 10667 is in mass production and matches the introductory data rate of LPDDR6 (if we ignore 1.5X channel/ECC overhead). They can brute force GB/s issue.   Also RDNA 3.5 has many of RAM BW saving tech as so we'll see how much they actually need to push this. See C&C breakdown and LLVM post u/Defeqel.  But it's an easy win for higher end configs for sure as they're prob BW starved rn.  AMD better have FSR4 INT8 ready by 2026 because RDNA 3.5 products are going nowhere. Even standard SoCs with Medusa Point (without dGPU die) will reuse RDNA 3.5 for another gen according to Kepler\_L2. No UDNA except for premium mobile SKUs (with seperate dGPU die bolted on) it seems u/SAUCEYOLOSWAG and u/Hero_The_Zero   But it's good enough for entry level as it doesn't need ML power of RDNA 4 or GFX13 and like u/J05A3 said it's new and it'll prob be a repeat of Vega 7nm iGPU except this time even more prolonged.,AMD,2025-12-05 09:19:00,5
AMD,nsdx4o6,"RDNA4 introduced new compression tech, which is why it gets away with rather small bandwidth, though perhaps 3.5 already has that",AMD,2025-12-05 06:59:19,4
AMD,nshvxoa,Hopefully   AMD has been on 6/8 core Ryzen 5/7 longer than Intel was on 4 cores lol,AMD,2025-12-05 21:53:13,5
AMD,nsocxj1,"did you forget that the x950 parts launched in 2019? sure the cheaper mainstream is still 8 cores, but Intel was selling 8 cores for $1000 until Zen. at least AMD isn't selling 16 cores for that still though I hope they increase mainstream to 32 soon.",AMD,2025-12-06 23:46:53,-2
AMD,nsofl9h,I did not. But did you forget that those were a different price class and basically HEDT and not really mainstream?  AMD has had the same core counts on their mainstream CPUs for almost 9 years LOL.,AMD,2025-12-07 00:02:36,5
AMD,nsokt3v,"they're on the mainstream platform without HEDT features. though it's proving that without competition from Intel, AMD will do the same thing as them. it's actually even longer if you consider Bulldozer an 8-core lol",AMD,2025-12-07 00:33:01,2
AMD,nsyvdrd,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-08 17:33:25,1
AMD,nsywdgg,DDR5-9800 in this economy?!,AMD,2025-12-08 17:38:18,606
AMD,nsz0nyo,As in $9800?  Sounds about right in this economy...,AMD,2025-12-08 17:59:14,162
AMD,nsyyj1n,Do not expect this to actually run in most cases. Either a reading error or a literal diamond,AMD,2025-12-08 17:48:49,92
AMD,nsz91vv,I think its not possible to live without both kidneys.,AMD,2025-12-08 18:39:55,7
AMD,nsyzh71,"5300 MHz base clock speed, and 9800 MT/s memory support. I call BS",AMD,2025-12-08 17:53:27,33
AMD,nt14s65,"Guys can we just calm down a bit... this is barely POSTed, possibly one time, and likely sampled to the moon.  We already had previous clickbait ""results"" around 9600MT/s...  [https://videocardz.com/newz/colorful-shows-off-igame-shadow-ddr5-on-ryzen-7-9800x3d-running-at-9600-mt-s-with-oc](https://videocardz.com/newz/colorful-shows-off-igame-shadow-ddr5-on-ryzen-7-9800x3d-running-at-9600-mt-s-with-oc)  [https://www.techpowerup.com/340048/colorful-reveals-ddr5-6400-ram-overclocked-to-9600-mt-s](https://www.techpowerup.com/340048/colorful-reveals-ddr5-6400-ram-overclocked-to-9600-mt-s)  Some of the... theories... in this thread are completely wild. Using strix halo IOD chiplets is just straight ignoring facts like besides throwing away 300mm2 of silicon they use a completely different physical interconnect.  I'll gladly post a retraction and eat a hat or 3 if any of this stuff comes true in my daily.",AMD,2025-12-09 00:33:29,6
AMD,nsz4bc7,Why not 9850 ddr5?,AMD,2025-12-08 18:16:56,10
AMD,nszisj7,![gif](giphy|5xjbWDIgEZSgM),AMD,2025-12-08 19:27:21,5
AMD,nsyzsnk,They testing a new I/O die...?,AMD,2025-12-08 17:54:59,10
AMD,nsz40t7,This is what I was hoping for. I imagine it's got an upgraded IO die with higher memory capabilities. Also goes hand in hand with the new boards coming out specifying higher speed ram capabilities.  I mean if I could afford ram.,AMD,2025-12-08 18:15:31,3
AMD,nt1ajkg,"A lot of people will bait the 9850x3d fomo, wait for zen 6, thats the real deal. 12 core, 1 ccd, if probably dead, next gen x3d, new Io controller for ram.",AMD,2025-12-09 01:07:46,2
AMD,nszk736,Can’t wait for this to cost $10k,AMD,2025-12-08 19:34:24,3
AMD,nszh9uq,damn if just there are no AI bullshit,AMD,2025-12-08 19:19:45,2
AMD,nszsfja,"I dont see what the crazy thing about this is? Apart from the RAM, running UCLK = MEMCLK/2 makes it 2450mhz which is high but with a silicon lottery and changing voltages absolutely doable",AMD,2025-12-08 20:15:24,1
AMD,nt2xwbn,Hear me. Out-what if the major change to the 50 is not frequency but the use of a new IOD as a test bed for zen6?,AMD,2025-12-09 08:20:42,1
AMD,nt3pw7f,CL28 6000 bis,AMD,2025-12-09 12:40:09,1
AMD,nt3x3mj,"AMD is really going hard on the Gaming-CPU front, it does not let Intel off the hook.  Interesting times, apart from the RAM prices, we are in right now.  Let’s see what 2026 brings.",AMD,2025-12-09 13:27:21,1
AMD,nt46yzt,Isn't 9950x3D > 9850x3D ?,AMD,2025-12-09 14:24:36,1
AMD,nt4ixi7,So its RAM worth like 3 grand?,AMD,2025-12-09 15:28:33,1
AMD,nt5dnd8,"These leaks are hilarious.    Current AM5 RAM already costs a kidney, but for these ones we'll have to give up a lung as well. :)",AMD,2025-12-09 17:56:45,1
AMD,nt6zoox,"Is this an AIO or an Air? Do I need to update Agesa to 5.6? Let's say I set the configuration to fclk2000.cl60, but I still can't find any information about bandwidth etc.",AMD,2025-12-09 22:51:16,1
AMD,nt7dj9i,When is this coming out ? I am building my son a PC and giving him my old 7800x3d and motherboard,AMD,2025-12-10 00:09:42,1
AMD,nt1ah99,people will spend 5x as much for 1 single frame then wonder why these tech companies are absolutely robbing us blind,AMD,2025-12-09 01:07:22,0
AMD,nt1dphs,"One day, that currently common DDR5-6000 will age badly like a DDR4-2666.",AMD,2025-12-09 01:26:58,0
AMD,nt1fqkj,what does it matter if people aren’t going to have the minimal conditions to pay the ridiculous price tags that ram chips have nowadays….,AMD,2025-12-09 01:39:07,0
AMD,nt3uv6j,How much are single 4GB sticks at this speed?  Asking for a friend.,AMD,2025-12-09 13:13:37,0
AMD,nt3x73k,So is that $2000?,AMD,2025-12-09 13:27:56,0
AMD,nt3x7ob,So it's 400 for the cpu and 2500 for the ram?,AMD,2025-12-09 13:28:02,0
AMD,nsz1w7t,Mt/s - mhz conversion error lol?,AMD,2025-12-08 18:05:11,-4
AMD,nsz1d45,"9800 isn't just the speed, it's also the price",AMD,2025-12-08 18:02:36,417
AMD,nsyytv0,Must be screenshots from Elon or Bezo's box,AMD,2025-12-08 17:50:17,42
AMD,nszg1rh,Next is DDR5 9800X3D,AMD,2025-12-08 19:13:40,17
AMD,nsz8w9z,Probably $9799.99.,AMD,2025-12-08 18:39:11,28
AMD,nszpizj,"Sam Altman: ""Hold up. That's our RAM according to section D, paragraph 2 - ""... The poors shall have none, because I've mon, hun""",AMD,2025-12-08 20:00:54,21
AMD,nszrf5r,tbf they didn't say how much,AMD,2025-12-08 20:10:23,3
AMD,nt6fegd,"Isn't it actually DDR5 9600 (4800 x 2)?  If so, that means the FCLK:MCLK:IMC speeds are (at a minimum):  1600:2400:4800  Although with the general rule that if you can support an FCLK of > 105% of the 2:3 ratio then it's worth doing.  So  2000:2400:4800  Is likely what they're running.",AMD,2025-12-09 21:08:58,1
AMD,nt09qbo,"They can do 8000 on the AI Max+ chips, but only because the CPU and RAM are soldered to the same module and it can’t be replaced.  Nobody is getting DDR5-9800 from a socketed chip (which we know that it is) or socketed RAM (not with standard DDR5, anyway).  Hell, we can’t even get 4 modules running reliably in sync.",AMD,2025-12-08 21:40:59,45
AMD,nt402p0,"This seems like yet another hoax, like the guy who was spoofing CPU ids on benchmarks which is where the 9950x3d2 rumors came from. He also spoofed 9850X3d benchmarks.  This is a BIOS screenshot, which is harder to spoof but not impossible (probably easier to just photoshop than actually doing some sort of HWID spoofing).  Also I am not familiar with this BIOS but the RAM speed reported at the bottom is 4800MHz, while above it's showing 9800MHz. Even if we assume one of the figures is actually MT/s and the other is MHz, 4800MHz is only equal to 9600MT/s",AMD,2025-12-09 13:45:02,3
AMD,nsz4ho9,"New I/O die, maybe with lessons learned with the AI Max chips.",AMD,2025-12-08 18:17:47,18
AMD,nsz67y5,"CPU base clock (terrible metric, to be honest) has nothing to do with RAM data transfer rate, but memory controllers do (that have nothing to do with base clock...).",AMD,2025-12-08 18:26:10,15
AMD,nsz6dib,"Yeah it still has the same memory controller, 8200 will be the most you can get without some serious tweaking or exoc.",AMD,2025-12-08 18:26:54,5
AMD,nt1p68m,"> strix halo IOD chiplets  Ya idk what people are smoking but that's not happening.   It wouldn't even fit on the package with the trace layout lmao  Right with you, this thing will be just another 14900ks. 5% when you're lucky, for a large amount of power increase.",AMD,2025-12-09 02:33:49,2
AMD,nsz49v3,That's my suspicion. The new motherboards coming out seem to be touting higher memory speeds as well and next gen Zen is still a long way away.,AMD,2025-12-08 18:16:44,3
AMD,nt0yl7c,"Nah, just someone having fun using LN2 and a single stick of memory.",AMD,2025-12-08 23:57:45,2
AMD,nsz4av7,"Most likely sounds like it, probably testing for ryzen 10,000",AMD,2025-12-08 18:16:52,2
AMD,nsz4fjt,JEDEC seems to have had speed profiles up to 8800 back in 2024.  https://www.techpowerup.com/forums/threads/jedec-updates-ddr5-specification-for-increased-security-against-rowhammer-attacks-new-ddr5-8800-reference-speed.321808/,AMD,2025-12-08 18:17:31,2
AMD,nt1okoh,"Wouldn't 2450 be the goldenest of golden chips? Maybe you can get some chips bench stable but 24/7 that sounds absolutely impossible. Iirc buildzoid said something about 2200 being the absolute max but it's been a while I checked back in on the topic.   My 9800X3D doesn't really like more than 2133 with voltages that the IMC still works or I get single bit errors hours into tests. Granted, dual rank kit so very temperature sensitive.   Annoying as hell but wcyd.",AMD,2025-12-09 02:30:19,1
AMD,nt3e5fe,"it uses totally different tech for the connection (InFO vs serdes), so, no",AMD,2025-12-09 11:04:15,1
AMD,nsz5mog,Per GiB....,AMD,2025-12-08 18:23:17,78
AMD,nszflk2,"It's over 9000  What? Over 9000, that's impossible.",AMD,2025-12-08 19:11:29,43
AMD,nt3edja,![gif](giphy|VIPfTy8y1Lc5iREYDS|downsized),AMD,2025-12-09 11:06:21,2
AMD,nszebpn,Elon / Bezo's shared CP box?,AMD,2025-12-08 19:05:10,9
AMD,nt4yrv5,Elon would just pay someone else to do it and then try to take the credit on a livestream.,AMD,2025-12-09 16:44:34,2
AMD,nszpl60,The price of an item in your cart has changed > refresh $9999,AMD,2025-12-08 20:01:12,17
AMD,nt0bzne,Zen 6 with cudimm support and a good 1dpc board might do 9000+,AMD,2025-12-08 21:52:12,17
AMD,nt5jwpk,"What..? You can run over 8000Mhz on 9000 series, probably even on 7000, but there is no point because you can't get mclock and uclock to run 1:1 at that speed.",AMD,2025-12-09 18:31:45,4
AMD,nt39tyr,"I learned the last part the hard way. Near the begining of ddr5, we built a PC for a client that used 4 modules of ram because it was never an issue before, it said it was supported on the MB specs and, well, it has 4 fucking slots in there which I will assume are usable like every other PC, otherwise they wouldn't be there.   Can't tell you the amount of shit that brought out. It worked well with 2 modules, somewhat reliable with 3 modules, but with 4 all hell broke lose. We escalated this everywhere up to gigabyte (the motherboard brand). Took forever, they returned the board with a very dry report ""motherboard repaired"" without actually telling us wtf they did. It wasn't ""fixed"", it had the exact same issue, i doubt they even did anything at all except wasting everyone's time. Meanwhile we read the endless torrent of complaints online about 4 modules. For commercial reasons we ended up replacing the motherboard for an Asus and decided to go with just 2 modules from now on.",AMD,2025-12-09 10:22:37,2
AMD,nt7vtsf,Apple’s m5 does 9600. I don’t imagine anything is beating that just yet. It’s all on the same module as well and Apple is very aggressive on speed recently. So I’m not saying it’s impossible but I really doubt it,AMD,2025-12-10 01:56:32,1
AMD,nt15vj8,"As far as I understand, from some very passionate people, the 4 RAM sticks not running well on Zen5 is an old wives' tale. I don't know enough to really understand it, but the person I spoke to seemed pretty sure its completely doable on just about all chips.",AMD,2025-12-09 00:39:46,-5
AMD,nsz8ixl,It's not going to get a new IOD. It's literally just a better binned 9800x3d,AMD,2025-12-08 18:37:23,57
AMD,nszcvi9,They update the I/O die shockingly infrequently. I am skeptical it’ll get an update this generation.  Delighted to be wrong here!,AMD,2025-12-08 18:58:04,26
AMD,nszs8g1,"This could be using the AI Max IOD, it would seem likely AMD has extra CCDs for that product line and they would probably enjoy pushing up volume incrementally before launching an entire high volume product stack to it.  If so, I would be very interested in the 9955X3D, dual VCache CPU with the new fabric.  I already have DDR5-8200 (48GB), would love to push it above 6000 😂",AMD,2025-12-08 20:14:25,2
AMD,nszkcx1,"It’s the exact same die, just a different stepping.",AMD,2025-12-08 19:35:12,1
AMD,nsze977,"A 9800X3D is an absolute furnace at 5.3 GHz locked clock speed, which base clock implies. We're easily talking 150W on the CCD, if not 200W",AMD,2025-12-08 19:04:49,7
AMD,nt28g8f,I dang if only RAM wasn’t so high. I’d be willing to get off my x870e Aorus master for a new gen. At that point I’d have a new server sooner and I could do an interesting mini itx build with my 5800x3D.,AMD,2025-12-09 04:34:20,1
AMD,nt25j1i,"You are confusing UCLK with FCLK.   It's the memory controller running over 2200MHz and not the fabric clock. Memory controller runs at 3000Mhz no problem with 6000Mhz RAM in 1:1 setting, so 2450Mhz in 2:1 ratio should be easy.",AMD,2025-12-09 04:14:13,2
AMD,nt3wat5,Per Hz,AMD,2025-12-09 13:22:26,5
AMD,nt18lp3,![gif](giphy|tPKoWQJk3cEbC),AMD,2025-12-09 00:55:59,17
AMD,nt47egk,I just put ECC memory into every system that I own. No issues with 4 sticks of ECC UDIMMs but I hear only horror stories about non-ECC DIMMs from coworkers and friends who have tried 4 sticks.,AMD,2025-12-09 14:27:02,2
AMD,nt17aot,"All of the documentation that I've seen haven't it is impossible, but it can be challenging depending on which motherboard, BIOS, and memory sticks, along with BIOS settings (particularly around voltages).  It can be done, yes, but it's not the plug in and go experience that it was on DDR4 unless you're running at lower speeds and higher voltages.",AMD,2025-12-09 00:48:07,14
AMD,nt47qcp,"It's due to the lack of ECC mostly. If you take the small performance hit of running ECC UDIMMs instead of non-ECC memory, you run into a lot fewer issues.",AMD,2025-12-09 14:28:54,2
AMD,nszdqyl,How bad is the I/O Die on Ryzen 9000/X3D chips?,AMD,2025-12-08 19:02:18,5
AMD,nt1rdmy,"it is just miss opportunity. Zen 4/Zen 5 sharing the same IO die but with the release of Zen 5 desktop, AMD could have updated I/O die to RDNA3.5 with double the CU to 4CU. It could also update the memory support upto 6400/7200/8000, solving zen 5 memory bandwidth constrain or improve any latency.  AMD literally rebrand 8000 series for Zen 4 without changing the spec. Whats the point of refresh without frequency bump and spec change? They could have updated the IO die for Zen 5 and Zen 4 refresh. (under Ryzen 8000 for zen 4)  AMD being the ""underdog"" in market share somehow manage to do worst than Intel desktop iGPU. Intel arrow lake iGPU is like twice as fast as Radeon 610m. Radeon 610M being only 2CU RDNA2 is so slow you can actually accidentally wake up dGPU with any slightly heavier task.",AMD,2025-12-09 02:46:38,0
AMD,nszqm6l,Recent postings on geekbench actually has it in the same stepping family.,AMD,2025-12-08 20:06:22,6
AMD,nszqxwy,"i mean, this is very likely running with test bench cooling and not normal cooling",AMD,2025-12-08 20:08:00,7
AMD,nt0yd0y,"It wasn't shown under which conditions these clockspeeds were achieved. Most RAM OC records are done using a single stick and LN2…  Also, the memory controller probably ran at 1:4",AMD,2025-12-08 23:56:26,2
AMD,nt18p0j,"I think thats accurate. I did find the comment from /u/rockstonicko   ""No, and there wasn't on AM4 either, just a whole lot of people who don't know how to tune RAM. (Which is understandable, RAM OC is a giant PITA even for those who enjoy it).  Yes, it's true that with 4 sticks you won't be reaching an unlinked ~4600MT/s on AM4, or ~8200MT/s on AM5.  But assuming your chip's IF is good, maxing out the FCLK with 4 sticks on both AM4 and AM5 is usually achievable with a few key tweaks and some trial and error (twrrd @ 3-4, correct Proc-ODT for your DRAM ICs, correct RTTs for your board, and finding the CLDO VDDP sweet spot of your IMC).  If you know the ins and outs of memory OC on both platforms, you will almost always be able to max out your linked FCLK limit before you run into your board/IMC MCLK limit with 4 sticks, at least assuming you didn't severely lose the IMC lottery (which is also a rarer occurrence than people think).""",AMD,2025-12-09 00:56:33,8
AMD,nszenpr,"Terrible.   Remember Zen 5%? That was because Zen 5 is seriously bandwidth constrained. X3D gets a bit of a break due to being able to bypass a lot of work with the larger cache, but it games with heavy RT workload the chips get crippled when they have to touch main memory.   Server Zen 5 doesn't have this issue due to being able to run up to like 12 channel RAM to bypass the issue.",AMD,2025-12-08 19:06:49,34
AMD,nsze949,"It's not bad - it's just bottlenecked for synthetics in terms of bandwidth, especially with single CCD parts like the 9800X3D.",AMD,2025-12-08 19:04:48,3
AMD,nsziyfk,"I mean I have a 9800x3d and no complaints.  I do have an Asrock MB so I had the first one for and replaced, but all is good now. I don’t think that’s an I/O die issue.  I run some 6400 memory and just turned on the xmp profile and it works fine. I didn’t mess with the fabric ratio at all. So no issues from my end, but I have limited experience with it.",AMD,2025-12-08 19:28:10,2
AMD,nszs0y1,If it's real at all. I'm not sure if the base clock shows the overclocked frequency if you override.,AMD,2025-12-08 20:13:24,2
AMD,nt7op0x,"Strange how Reddit groupthink works, eh?  Your first post and second post are saying the same (correct) thing in two different ways. One is downvoted, one is upvoted.  But, yeah, I stand by what I said. I've been responsible for getting 4 sticks running fast for a lot of people on both AM4 and AM5. And yes, it can be a PITA and requires persistence and trial and error, but the overwhelming majority of Ryzen IMCs can run 4 sticks at speed just fine if you do the work and know what values need changed.",AMD,2025-12-10 01:13:46,2
AMD,nt1ors6,I don't know what almost any of the acronyms are LMAO. I set my ram to expo 6000 and buy the lowest CL I can with 2 sticks,AMD,2025-12-09 02:31:28,4
AMD,nt2ivvq,"great, except nobody wants to run linked (1:1:1) FLCK on AM5  non-monolithic AM5 chips max out FCLK around 2200, if you run it synced thats DDR5-4400 which is slow as shit.",AMD,2025-12-09 06:00:13,1
AMD,nszivmd,"I remember back in 2024 during Zen 5 launch it offered a single-digit improvement over zen 4 in majority of game titles and the memory controller in Zen 5 was the same as Zen 4, not including 9000X3Ds which came after.  Only improvement was lower power consumption on Ryzen 5/7 and whatever issues zen 4 had was fixed on Zen 5",AMD,2025-12-08 19:27:46,8
AMD,nt0xfm0,"In games, a CPU is almost never bandwidth constrained. It's always latency. Many games perform the same when you go from dual-channel to single channel.  What causes slowdown in games with heavy RT is pushing a couple hundred megs of BVH data to the GPU over PCIe for each frame, again mostly due to latency. Titles not performing well in that regard won't run any better on a Threadripper or Epyc CPU.  Also, Zen 5 is actually quite good at feeding its CPU cores, even without the 3D V-Cache https://chipsandcheese.com/p/running-gaming-workloads-through",AMD,2025-12-08 23:51:07,6
AMD,nt3bv4l,">Remember Zen 5%? That was because Zen 5 is seriously bandwidth constrained  Not really.   >X3D gets a bit of a break due to being able to bypass a lot of work with the larger cache,   It performs relatively better because of the higher all core boost clocks in comparison to Zen 4X3D, vs how Zen 5 vs Zen 4 vanilla did.   >Server Zen 5 doesn't have this issue due to being able to run up to like 12 channel RAM to bypass the issue.  Even with all those memory channels, this wouldn't have helped memory bandwidth per core or per CCD because the pinch point is the CCD to IOD connection. GMI-wide on some server skus helps alleviate this issue, especially for specific workloads, however wholistically the difference is not large on average, because Zen 5 isn't significantly bandwidth constrained.",AMD,2025-12-09 10:42:40,1
AMD,nt02rqe,6400 in 2:1 is going to be pretty slow! Luckily you have that cache to mask it,AMD,2025-12-08 21:06:40,6
AMD,nt17lls,"I'm running 6400 1:1, if you need settings for reference here you have my configuration: [https://i.imgur.com/9RUa8bB.png](https://i.imgur.com/9RUa8bB.png)",AMD,2025-12-09 00:49:56,3
AMD,nszvh2e,"I mean, there's plenty of reasons to doubt it, but I don't think the cpu cooling is a ""problem""  some mobos for sure do show overclocked freq as base if you enter a fixed multiplier",AMD,2025-12-08 20:30:32,5
AMD,nt0yflg,It does.,AMD,2025-12-08 23:56:50,2
AMD,nt7sega,"Yeah its always interesting to see the collective thought process be swayed by some imaginary points. In any case though, I try my best to not pay attention to that.   While you're here though /u/Communist_UFO did comment this in response:  ""great, except nobody wants to run linked (1:1:1) FLCK on AM5  non-monolithic AM5 chips max out FCLK around 2200, if you run it synced thats DDR5-4400 which is slow as shit""  Which made me think that there might have been a typo in your original comment, or I didn't understand it entirely. I checked my own settings and my FLCK is running at 2000 while the RAM is at 3000 making it 2:3. Maybe you meant UCLK since I have that running at 3000 meaning its 1:1 with RAM speed.",AMD,2025-12-10 01:36:12,2
AMD,nt23026,"you're leaving a lot on the table, expo subtimings are really loose",AMD,2025-12-09 03:57:32,4
AMD,nt2c4ws,"Well yeah, I think the consensus is that for 99% of people thats more than adequate. I also just bought 6000 CL28 and set it to expo. But the original point was that one can get 4 RAM modules running reliably in sync, consistently. It just takes a bit more work than EXPO if you want higher speeds and lower timings.",AMD,2025-12-09 05:03:06,3
AMD,nt0xqfy,>it offered a single-digit improvement over zen 4  Memory latency remained the same. No surprise there.,AMD,2025-12-08 23:52:52,3
AMD,nt08np0,"Any idea what kind of impact it has?  I regret that I for the 6400 kit as I could’ve gotten a 6000 kit.   I don’t feel like manually adjusting my timings, or running and default speed, so I just run it in XMP mode. Definitely my fault for not researching this properly.  Especially after my Asrock presumably fried my CPU I’m reluctant to run anything “off label.”",AMD,2025-12-08 21:35:39,1
AMD,nt1kx5q,Thank you! I’ll check this out.,AMD,2025-12-09 02:09:52,1
AMD,nt7v14c,"I probably should've specified I was referring to AM4 specifically there, as AM4 is the 1:1:1 rule, not AM5.  On AM5 I should've specified I was referring to the common 2:1 ratio with 4 sticks, as there's no golden FCLK/MCLK ratio on AM5 once you're unlinked, the goal is simply the highest FCLK/MCLK you can achieve.  2000MHz/6000MT/s to 2100MHz/6200MT/s is where most IMCs and boards start to hit a firmer soft limit where going above requires exponentially more fine tuning. The 2200MHz/6400MT/s target with 4 sticks has a larger chance of being lottery limited, but in many cases is still also achievable.  Edit: My brain is mush. I meant 2:1 ratio, not 3:2. Fixed.",AMD,2025-12-10 01:51:50,2
AMD,nt27po4,I do the same. CL30-6000 and call it a day. I could spend the time to fine tune my timings like I did on AM3+. But my perf is good enough on my build that I don’t thinks it’s really worth the time at this point.,AMD,2025-12-09 04:29:15,5
AMD,nt4ic0j,That's why I bulldoze my way through with a 9950x3d and 5090 and CL26 ram,AMD,2025-12-09 15:25:35,3
AMD,nt248rw,"I have a Hynix A-die kit and I tried using Buildzoid’s “low effort” timing, and afterwards I couldn’t even get into BIOS. But then I also couldn’t push past PBO -10 without BSOD on Windows log-in.",AMD,2025-12-09 04:05:41,1
AMD,nt0b5ig,"I've not seen direct testing between these configurations, but common wisdom is that you need to reach at least 7600 or so in 2:1 before it starts to make sense vs running 1:1. Your memory controller clock is running barely over half of what it would be at 6000. You can force the memory controller to run in 1:1 mode, but there's a good chance it won't be stable depending on silicon lottery and what your memory kit is.  It may be also worth checking if your kit has any lower frequency XMP profiles, using the one you currently are and manually reduce the speed to 6000, or grabbing buildzoid's easy timings and punching those in for 6000.  Remember that XMP/EXPO are considered overclocking and are not officially supported configurations, which is kind of BS from AMD because of how much they push EXPO configs in their marketing.",AMD,2025-12-08 21:48:00,6
AMD,nt81p5h,"I see, that makes sense. Thank you for your input, I do appreciate it. Still getting used to all these RAM parameters, but its slowly starting to make more sense.",AMD,2025-12-10 02:31:16,1
AMD,nt4r29s,totally valid.,AMD,2025-12-09 16:08:01,1
AMD,nt1l1vr,"Thank you! I’ll look into it more.  Sadly I did not see a slightly slower profile, unfortunately, but I’ll check out some guides to correct it. Thanks!",AMD,2025-12-09 02:10:39,3
AMD,nsnidsa,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-06 20:53:17,1
AMD,nsnjyif,"PassMark, my favorite game.",AMD,2025-12-06 21:02:22,224
AMD,nsnnpyx,"The biggest factor of whether this will be loved or ignored *(regardless of price)* is we know that it's going to be higher binned chip, but for overclockers, if the memory controller on it is improved and people can push RAM overclocking into the 8600Mhz to 9000Mhz, then it will sell out amongst enthusiasts instantly.  I expect Zen 6 to be able to do 9000Mhz+ but if they can squeeze out some extra juice with the 9850x3d on current generation, then it'll get the enthusiast community excited until Zen 6 drops.",AMD,2025-12-06 21:23:22,164
AMD,nsnknlu,4.7% more for a lot more heat and power.,AMD,2025-12-06 21:06:22,137
AMD,nsnxz2f,Memory has become so unattainable that it sort of moots any interest in upgrading from AM4,AMD,2025-12-06 22:19:50,20
AMD,nsnm47a,Seems to be basically the same multi score you get with +200mhz maybe slightly higher single core because of 5.6ghz. If it costs more it will be meh. If it’s the same price then it will become a why not kinda upgrade over the 9800x3d but will likely also make the 9800x3d a better deal till its stock runs out at least,AMD,2025-12-06 21:14:29,16
AMD,nsnnyh1,"Waiting for Zen 6 and the 12 core CCD, gonna upgrade to that once it launches... 7700X can soldier on till then.",AMD,2025-12-06 21:24:39,26
AMD,nsnthjt,Sounds kinda pointless,AMD,2025-12-06 21:54:42,8
AMD,nso72fs,"I'll wait till am6, my 5800x3d and 7900xtx is fine. Especially with ram prices the way they are. The ram in my cart that I had since last year was less than $200 now its $700.",AMD,2025-12-06 23:12:15,7
AMD,nsob2eo,Thinking about upgrading my 5800X3D for one of these but I can't afford ram.  Going to be waiting another gen probably.,AMD,2025-12-06 23:35:52,3
AMD,nso06ok,So it's a 9800x3d with pbo on. Cool,AMD,2025-12-06 22:32:26,2
AMD,nsntxnr,"So like a normal 9800X3D, just mildly OC'd? Only real difference might be a better memory controller that could maybe handle 4 DIMMs of RAM easier/faster RAM, but seeing current RAM prices AMD might not even bother because who has the money for that sorta setup ATM?",AMD,2025-12-06 21:57:10,2
AMD,nsnyduv,Just wait for Zen 6 x3d...,AMD,2025-12-06 22:22:15,1
AMD,nso2rqf,I’m waiting for 10800X3D,AMD,2025-12-06 22:47:05,1
AMD,nso3rm6,"I genuinely wonder why they're releasing this chip now. Normally they do these type of things with GPUs on a refresh cycle, as they phase them out and have extras on the mature node. But here it just seems, unnecessary? Theres definitely a market for these top chips, but with the current RAM prices its harder to justify a premium OC product for a majority of consumers.",AMD,2025-12-06 22:52:49,1
AMD,nso448v,Make sense if its one ccd from 9950x3d.(and single core score is slightly lower)   So reduction from 5.7 to 5.6Ghz with reduced cache. Ofc i could be wrong but it would make sense to me.,AMD,2025-12-06 22:54:52,1
AMD,nso55g1,How would it compare to 9950 x3d tho,AMD,2025-12-06 23:00:54,1
AMD,nso5qx8,This is only gonna make the jump to Zen 6 looks less impressive I feel lol.,AMD,2025-12-06 23:04:28,1
AMD,nsoa64s,Damn I just got a 9800x3d too. Still haven’t build it yet tho its in its box. Im not sure if its worth returning and getting a 9850x3d for a 4.7% difference tho what do you think?,AMD,2025-12-06 23:30:34,1
AMD,nsoofs9,Meh,AMD,2025-12-07 00:55:12,1
AMD,nsoq3l2,Could have called it 9847X3D for clarity,AMD,2025-12-07 01:05:44,1
AMD,nsq0y7f,What an uplift.,AMD,2025-12-07 06:24:16,1
AMD,nsqor51,4% in \*what\* at \*what\* resolution?,AMD,2025-12-07 10:15:23,1
AMD,nsra6bv,"Im just interested in x3ds for their mmo games where they shine, im enjoying 5800x3d still, 1ghz more should be interresing upgrade over 5800x3d,if not ill wait 2027 and zen 6 x3d",AMD,2025-12-07 13:25:46,1
AMD,nssuzut,When is it such to get released?,AMD,2025-12-07 18:30:57,1
AMD,nsw7y8v,Games rather need optimization. I have a 9800x3d and some games are still not working flawless. Since ram is unavailable anyway I fear that this might be a rats tail that will hurt the whole market. Because why buy a MB and CPU if you cannot afford 32gb of ram?,AMD,2025-12-08 06:03:18,1
AMD,nswubqw,Will be great for cs2 with possible pbo +200 at 5.8 ghz. Will get one as soon as its available,AMD,2025-12-08 09:44:40,1
AMD,nsxghgj,"The majority of people commenting seem to be 9800X3D owners that are butt hurt that their CPU is no longer the absolute best. They all seem to understand that a 5% increase isn't going to make much of a difference on their setups, but are also sour about it at the same time. This product is not intended for people that already have a 9800X3D. Sure, some owners will swap them out. But the largest segment of buyers for this chip will be the massive 7800X3D/7600X3D/7700X/7600X user base, along with anyone willing to eat the RAM prices to move from AM4 to AM5. The 9850X3D will sell out quickly as long as it's in the ballpark of current pricing.",AMD,2025-12-08 12:59:47,1
AMD,nsyjxt7,Worth it,AMD,2025-12-08 16:37:06,1
AMD,nsz08q0,"AMD, above expectations again! You get 4.7% for only 0.5% improvement in the name!",AMD,2025-12-08 17:57:09,1
AMD,nt2uo90,great! Now i can finally build a new system.   Oh wait i cant because i cant afford RAM :-(,AMD,2025-12-09 07:48:46,1
AMD,nt7z9lx,Gotta see those 1% lows,AMD,2025-12-10 02:16:49,1
AMD,nsnyslu,I'm so whelmed,AMD,2025-12-06 22:24:32,0
AMD,nso1l87,But will it be 4.7% more expensive? /s,AMD,2025-12-06 22:40:21,1
AMD,nsnly4p,Cool,AMD,2025-12-06 21:13:33,0
AMD,nsnkkdp,And 4% !! Gosh imagine the seconds saved in a month,AMD,2025-12-06 21:05:52,70
AMD,nsoxroh,The single core score is what I learned when I started onto console emulation on PC.,AMD,2025-12-07 01:54:27,4
AMD,nsns6e0,"I have a feeling the IMC will stay the same, if AMD had to bin both the memory controller and the CPU cores so 6600 1:1 and/or 8600 1:2 would be possible and stable with the CPU running at 5.6+ GHz then they would only have a handful of 9850X3D processors to sell at a high price.  Imagine selling your 9800X3D (a good sample capable of doing 6400 1:1 CL28 with tight timings) and ending up with a 9850X3D stuck at 6000 MT/s in a 1:1 configuration.",AMD,2025-12-06 21:47:38,66
AMD,nsnyjon,"Zen 6 will be a huge generational improvement due to the elimination of the Infinity Fabric bottleneck and having ""sea of wires"" instead. We'll see large bandwidth and latency improvements.",AMD,2025-12-06 22:23:09,24
AMD,nsos4j6,"The memory controller will be exactly the same because the memory controller is in the iodie and they are not changing the iodie   On an 8 core amd cpu, 6000mt/s is already hitting the bandwidth limit on the fabric speed   For 8000mts ram to matter at all, you need multiple chiplets for ccd",AMD,2025-12-07 01:18:45,8
AMD,nsovde5,Lmao. No. They did not design a better IMC for this cash grab gamer brain chip.,AMD,2025-12-07 01:39:23,5
AMD,nsoxza9,I just wish for better memory controller that can easily support 4 sticks of ram instead of 2. It's been too many years of wait.,AMD,2025-12-07 01:55:46,2
AMD,nsrupy1,Why would they improve the memory controller on this when its getting revamped for Zen 6.  I don't really see the point for 1 chip.,AMD,2025-12-07 15:27:16,2
AMD,nsoi5u6,"Will people go for it? By the time this is released the high performance RAM may cost the same, if not more, than the CPU",AMD,2025-12-07 00:17:40,1
AMD,nso2t49,And likely not very noticeable to anyone using a GPU worse than a 4090/5090,AMD,2025-12-06 22:47:18,37
AMD,nso3h36,They seem to list it as the same power as the 9800X3D. I suspect this is just a better bin of zen5 chiplet,AMD,2025-12-06 22:51:08,8
AMD,nsonl6l,"Not necessarily, they will be just better binned chips meaning higher clocks at lower voltage. Depending on how crazy these are binned you could technically have chips that pull similar power at 5.6ghz as a bad 9800x3d at 5.2.  I'm guessing most of them will just be a minor power increase over your typical 9800x3d.",AMD,2025-12-07 00:49:52,0
AMD,nsqanad,If it means cheaper 9800X3D why not,AMD,2025-12-07 07:55:30,0
AMD,nsnv036,"3700 for me, but yeah.",AMD,2025-12-06 22:03:04,5
AMD,nsnyth1,"As someone with AM4, I'm just hoping the RAM prices comes back from the moon by the time Zen 6 is out. My 5800X is tired.",AMD,2025-12-06 22:24:40,3
AMD,nsnxw6o,"Rumors of zen7 also on am5, I'd love that too, I can wait. The 7700x is no slouch, even though it's not that much high end.",AMD,2025-12-06 22:19:23,1
AMD,nsqowaj,"Sometimes the silicon comes out way nicer than expected, that's why they introduce a higher model model to sell them as. They don't want to sell the 9800X3D's that perform close to 5% better as ""9800X3D's"". They will label them as a better model and sell them that way. If silicon performs worse than expected, they label and sell them as lower models.",AMD,2025-12-07 10:16:51,6
AMD,nsrtu6p,"I wouldn’t expect any memory controller difference. Even if a CPU had higher drive strength enabled, the memory chips on the DIMM would have to drive through the extra capacitive load of a second DIMM per channel being there.  The real solution for that problem is simply going threadripper with a second memory controller die. The problem with that is the drop in clock speeds ignoring cost of board and chip.",AMD,2025-12-07 15:22:31,2
AMD,nsrwdz4,Where is this idea coming from that there will be a better memory controller?,AMD,2025-12-07 15:36:05,1
AMD,nsrptlm,"So, like nine months+ more?  Unless AMD suddenly switches up their strategy this round, it will be the same as before and we will get the non X3D parts first.  People are less and less willing to buy those for any sort of gaming load.",AMD,2025-12-07 15:00:10,1
AMD,nsqpsus,"It has been cooking for a while. AMD has released refreshes or updates to their lineup before. It has been almost a year since the 9800X3D released in that time they have no doubt either stockpiled or improved the process so much that they can justify a whole new part. And since it wont be another year before next gen, a mid lifecycle refresh is a great excuse to jump back to MSRP and keep selling CPU's.",AMD,2025-12-07 10:26:07,3
AMD,nsoj1gy,They bury intel on the charts further,AMD,2025-12-07 00:22:46,1
AMD,nsonf25,"4.7% is nothing in terms of cpu, just keep it",AMD,2025-12-07 00:48:50,1
AMD,nsrwi9w,no,AMD,2025-12-07 15:36:43,1
AMD,nsoq7rm,… or 10260X3D,AMD,2025-12-07 01:06:29,2
AMD,nspvu5l,Because not everyone has a 7800x3d or 9800x3d some are still on am4 and want to upgrade this will entice them,AMD,2025-12-07 05:42:30,1
AMD,nsnqpty,and all the extra money spent on power makes it worthless,AMD,2025-12-06 21:39:44,21
AMD,nsoaacp,"Yep, I have the same feeling, which is why *(in that case)* it's going to be either ignored or hated.   Unless it comes at the same price as the current 9800X3D, and the 9800X3D drops in price slightly. Then it will be loved.",AMD,2025-12-06 23:31:16,16
AMD,nsoajcp,Wait I was supposed to get 6400 not 6000? Fml.,AMD,2025-12-06 23:32:44,11
AMD,nsois7k,"We're gonna get owned by RAM prices, though.",AMD,2025-12-07 00:21:16,17
AMD,nstcjq6,Has that been confirmed yet?,AMD,2025-12-07 19:55:05,2
AMD,nsob03v,"For the average Joe case you mentioned, barely any difference. The main gains would be waiting for Zen 6 *(next-gen CPU's on AM5 that should come by mid-to-end next year),* I would not waste a single $ on these if someone already has a 7800X3D or a 9800X3D.",AMD,2025-12-06 23:35:29,9
AMD,nsqrwya,"I ran like 20 modern games at 1440p after I got my 5080, paired with a 7800x3d. I was gpu limited in every single one. By a lot. I have a 9800x3d now, and my fps did not increase in any game.",AMD,2025-12-07 10:47:22,2
AMD,nso3ftu,Even then you most likely need to game at 720p low to see more fps.,AMD,2025-12-06 22:50:56,14
AMD,nsr3x5y,have you ever played world of warcraft? there's many more games like that.,AMD,2025-12-07 12:39:25,2
AMD,nso87z5,"tdp is only vaguely related to power draw. 9800x3d stock pulls 140w in all core loads and a 7800x3d pulls 75w, they both have the same TDP  https://skatterbencher.com/2023/04/05/skatterbencher-60-amd-ryzen-7-7800x3d-overclocked-to-5400-mhz/#AMD_Ryzen_7_7800X3D_Stock_Performance  https://skatterbencher.com/2024/11/06/skatterbencher-82-ryzen-7-9800x3d-overclocked-to-5750-mhz/#AMD_Ryzen_7_9800X3D_Stock_Performance  TDP IS NOT POWER DRAW  TDP IS NOT POWER DRAW  TDP IS NOT POWER DRAW",AMD,2025-12-06 23:19:02,17
AMD,nso6qn4,"This is not directed to you, but I am legitimately curious about where other people are getting the idea that this chip will run hotter?  I believe its rumored TDP (not representative of the actual power draw) is the same 120w, as the 9800X3D. It just seems to be better binned like you said.",AMD,2025-12-06 23:10:20,1
AMD,nsphi9p,"If you can afford RAM, them prices going up every week",AMD,2025-12-07 04:02:17,2
AMD,nsq94a9,"Same here. Bought upgrades for my wife's PC for her birthday just before prices launched (9600X, 32GB Crucial DDR5-6400 and an X870 Mobo since it was cheaper than the B650 I was originally looking at, all upgraded from an FX-8350 setup) but my PC will be staying on the 3700 for at least 1 more year.",AMD,2025-12-07 07:40:35,1
AMD,nso8zc2,I just have to say I feel like a King right now with a 5800x3D and 32 GB RAM.,AMD,2025-12-06 23:23:29,10
AMD,nsv4jln,That won't happen. Forecast is at least 2027 if not 2028 until we see DRAM come down in price. It's only going to get worse before Zen 6.,AMD,2025-12-08 01:39:17,1
AMD,nss6gq6,It's a confirmed fake rumour anyway.,AMD,2025-12-07 16:28:46,1
AMD,nsrqjb9,"I should have added, wait if you can, sorry :-D",AMD,2025-12-07 15:04:13,1
AMD,nsssobl,"Well as long as they price it right, I could see that being good. Like if they drop the 9800x3D price by like $75 and put this one on its launch MSRP.",AMD,2025-12-07 18:19:52,1
AMD,nspfvbo,"Especially in games, unless you play at 1080p for some reason.",AMD,2025-12-07 03:51:00,1
AMD,nsrb8vi,"With ram prices... is anybody not made of money on AM4 going to upgrade to AM5 now? AMD has pretty much just the existing AM5 userbase now to appeal to, and Intel 12-14th gen and that's really it.",AMD,2025-12-07 13:33:04,1
AMD,nsqw3ih,"If efficiency was your purpose, you'd not be getting these chips anyway.",AMD,2025-12-07 11:29:11,5
AMD,nsoioer,6400 requires a gold sample 9800X3D; most can only run 6000-6200.,AMD,2025-12-07 00:20:40,7
AMD,nsol4kc,Still AM5 right? So we'll just have to stick with our 6000 kits for a couple more years.,AMD,2025-12-07 00:34:54,16
AMD,nsq496u,"Thats why i kept my older DDR5 ram kits, i have 3 sets now, so im good for years.",AMD,2025-12-07 06:54:06,2
AMD,nsq2a74,Already bought 48gb 6400 last year for my 7500f to upgrade to zen6,AMD,2025-12-07 06:36:05,3
AMD,nt2swnv,will be pretty bad coming 2 years with ram prices,AMD,2025-12-09 07:30:56,1
AMD,nsrvbws,I wouldn't consider this with a 9800X3D. However due to clock speeds it looks to be a 20% difference from the 7800X3D so It would be a maybe for me on that chip. However all the big gains will be Zen 6.,AMD,2025-12-07 15:30:31,1
AMD,nsp1vcd,"I have nothing else to buy in 2026 though , might as well",AMD,2025-12-07 02:20:17,-5
AMD,nsqugyq,"That's why i keep my 7800X3D until Zen6, i'm also in 4K, so yeah gpu is the real limit here..",AMD,2025-12-07 11:13:05,1
AMD,nsu4xup,Try Tarkov and come back after you've pulled your hair out,AMD,2025-12-07 22:15:34,1
AMD,nso3rqb,"Nah, in CPU intensive games like battlefield6 it can be measured using normal resolutions.  It's very niche",AMD,2025-12-06 22:52:50,17
AMD,nsrraag,Yes and I know how bottlenecks work,AMD,2025-12-07 15:08:26,2
AMD,nsoe9qp,"TDP is ""thermal design power"" which refers to how much heat energy the cooling solution is expected to need to be able to exhaust in order for the device to run at manufactured spec.  The blanketed design of the x3d layer in the 7800x3d and earlier generations traps heat more strongly than in the 9800x3d where the x3d layer is on the bottom.   This trapping effect means that the older x3d cpus  cannot have too much power usage lest they burn, and that they need a stronger cooler than would typically be required for the amount of power they actually use lest they end up too hot and thermal throttle heavily.  I'm not saying the 9850x3d and 9800x3d are likely to use the same default power limits, but I am saying that their TDP are probably much more comparable than is the case as between the 9800x3d and 7800x3d.",AMD,2025-12-06 23:54:50,4
AMD,nsrauwt,"Because usually in tech, CPU or GPU, higher clock speeds mean more power draw to achieve them. I'm sure in gaming with single threaded tasks there's no issues pushing 5.6GHz, but it will likely mean more wattage vs the 9800X3D doing those tasks IF the CPU is being pushed hard to make use of the extra. Plus higher clock speed you to go the silicon's limit, usually the efficiency curve plummets to get each extra bit of clock speed. Not sure how much 5.2 to 5.6GHz is pushing it, but the highest clocked Zen 5 is already only at 5.7GHz with the marked specs.",AMD,2025-12-07 13:30:28,2
AMD,nstzkum,"I’m on a 5600x3d, similar gaming performance to the 5800x3d and I can’t see myself upgrading till at least AM6",AMD,2025-12-07 21:48:35,1
AMD,nssii30,"Right. My point is simply by understanding the underlying technology, such as rumor should never have become known unless someone could credibly explain how certain obstacles and technology limitations were overcome.  Personally, I have a flight sim using two systems and three GPUs. When originally bullt, the threadrippers didn’t have the clock speeds necessary to do what I needed for portions of code limited by single thread performance. I’m thinking hard about a strx50/wrx90 chip/board for next upgrade and collapsing two systems to one. Unfortunately the stupid DDR5 prices and overall cost of that jump are pushing it beyond budget right now. Getting rid of 1000-1500W of heat from one case is a solved problem for me and I already power off of 220V so even that’s not a concern.  The one x16 slot plus an X4 or x8+x8 +x4 are a bummer on AM4/AM5 because PCIe gen4 or gen5 100G Ethernet or 16/32g FC are costly, as are other solutions like PCIe fabric switches and the cabling.",AMD,2025-12-07 17:30:41,1
AMD,nsol67o,"I have built my first PC 5 days ago with a 9800X3D and 64GB 6400Mhz CL32 DDR5 ram and it runs stable with no issues in EXPO. Does that mean i have a ""gold sample""?",AMD,2025-12-07 00:35:10,9
AMD,nsr9lwb,"Yeah us on AM5 will be okay... it's the people who don't have DDR5 already who are screwed and probs won't find any worth in upgrading when it'd cost so much.  Wonder how AMD will handle things with Zen 6 actually, they really can't charge too much now can they? The main market to appeal to now for them will be those already on a DDR5 platform, most of which will be AM5 owners (with some Intel 12-14th gen in the mix) where all the CPUs are still competent, just some on entry level Zen 4/5/Intel may still want something faster. Any AM4 owners who may have wanted to jump in next gen are screwed, same with old Intel platforms, AMD can't count on them anymore when RAM makes the platform switch itself insanity so AMD needs to get people with Rzyen 7600/7700s, other non X3Ds and Intel 12-14th gen owners want to buy Zen 6.",AMD,2025-12-07 13:21:56,3
AMD,nsuko1i,"Haha, thats one of the games I didnt test. Could never get into it",AMD,2025-12-07 23:42:39,1
AMD,nsogcpa,"it's not just ""how much heat energy"" but the area it's being emitted from too. Also, it's not a standard formula that every manufacturer uses, one heatsink company uses a different formula to define their TDP compared to a different heatsink company, so they aren't even directly comparable.  For example, a 5 millimeter square piece of silicon using 120w needs a higher TDP cooler than a 10 millimeter square one also using 120w. Coupled with the TDP ""bins"" AMD wants to put their CPUs in (either 65w, 105w, 120w, nothing inbetween right now) you get these wide spread where you have two 120w TDP CPUs and one uses literally twice the power as the other and even goes above the TDP rating stock (9800x3d uses 140w, tdp is 120w), tdp is just about the general class of cooler you need and not much else",AMD,2025-12-07 00:07:05,5
AMD,nspf7u3,"No, if all you did is enable expo then you are running 2:1 and getting worse performance than if you had 6000 1:1",AMD,2025-12-07 03:46:40,20
AMD,nsooped,"yeah mine is also 24/7 stable (ran every test out there for days) 6400 1:1 CL30 with tight timings and great benchmarks, zero issues, zero crashes, nothing, nada, flawless for 12 months and counting.  my config: [https://i.imgur.com/zM9qfd7.png](https://i.imgur.com/zM9qfd7.png)",AMD,2025-12-07 00:56:52,3
AMD,nstpuse,"6400 1:1 at 1.245VSOC stable, on a 2dpc board, is indeed a golden sample",AMD,2025-12-07 21:00:59,2
AMD,nt0onux,If my 7800x3d can do that at 6000cl28 1:1 is that nothing special or just exceptional ram? It's also run 6400 1:1 reliably but at cl32,AMD,2025-12-08 23:00:10,1
AMD,nsx9r6k,"6600 is golden, 6400 is not that rare",AMD,2025-12-08 12:09:37,1
AMD,nsxvp24,"at 1.3vSOC sure, at 1.245??",AMD,2025-12-08 14:32:54,2
AMD,nsbt22y,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-04 22:45:35,1
AMD,nschsh6,Looks like it should be great for people with low end amd5 cpu to upgrade to. It won't be good for people with am4 cpus though considering 32GB of ddr5 would cost just as much as the CPU.,AMD,2025-12-05 01:10:23,124
AMD,nsbylmp,"My 9800x3d is chilling with a 61w PPT and -40mv undervolt, nearly stock perf in games and ~10% worse in absolute worst case fully maxed muticore stress tests.",AMD,2025-12-04 23:17:05,70
AMD,nsdya29,I wonder whether we'll see a 9700X3D with reduced clock and a 65-95W TDP like the 5700X3D,AMD,2025-12-05 07:09:50,15
AMD,nscqhi3,This is kind’ve a nothingburger for AM5 X3D owners.,AMD,2025-12-05 02:02:27,26
AMD,nsedoey,When is this coming to retail?,AMD,2025-12-05 09:39:55,3
AMD,nse8id5,Is this worth waiting for and returning 9800x3d?,AMD,2025-12-05 08:50:05,5
AMD,nsee878,If ram prices stay high then I buy two of these next year,AMD,2025-12-05 09:45:23,3
AMD,nscnqk1,no shot its only 120w. i'd bet my left nut on it,AMD,2025-12-05 01:46:04,-2
AMD,nsefb5u,"Boring, give me Zen 6 in 2026 plz, I want efficient X3D to upgrade from my 7600X, not clocked out the moon to make Australian summer more brutal.",AMD,2025-12-05 09:55:58,-1
AMD,nsenvp6,Ok i can put in the bin my 9800x3d lol,AMD,2025-12-05 11:17:08,-1
AMD,nscpbix,Yeah I'm over here sad the 5700x3d/5800x3d is gone. Oh well...,AMD,2025-12-05 01:55:28,53
AMD,nsjj7km,"If this is anything like most AMD's other super high-end, late-in-cycle CPUs, it will be in limited quantity for a limited amount of time. Once Zen6 comes out, this thing will disappear like the 5800X3D. I figure the same thing will happen with the 9950X3D2. Except neither of those are going to take a price dip before falling completely off the market, unlike the 5800X3D.   Intel always did the same thing. All Extreme editions and KS models were short term, limited production.",AMD,2025-12-06 04:04:27,2
AMD,nse1nh8,sounds like you hit a die lottery. you usually can't go past -30mv all core,AMD,2025-12-05 07:42:08,26
AMD,nselg5w,Meanwhile my 12900k PC over in the corner casually using 140 to 150w under a medium gaming load...  St least I can turn down the heater in winter I guess.  If ddr5 wasn't so stupid expensive I would switch. Well let's see in 2 or 3 years I guess. Thank you AI.,AMD,2025-12-05 10:54:45,6
AMD,nsc56s3,"For 99% of gaming, CPUs really don’t need much.",AMD,2025-12-04 23:55:54,25
AMD,nsk8r34,It’s not Mv. It’s cycles - which is between 3-5mv. So 3-5Mv * 40 is what your undervolt actually is,AMD,2025-12-06 07:44:08,1
AMD,nsfjomo,"That’s what I was hoping for my wife’s build, however I just might get myself a 9850X3D or splurge for a 9950X3D and put my 9800X3D in hers and run at the 65w tdp lvl.",AMD,2025-12-05 14:48:13,4
AMD,nscuswj,kind of*,AMD,2025-12-05 02:27:10,15
AMD,nsd5o4u,"Thanks for telling the other 9800X3D owners that they already know they don't care about a product that is a slight refresh of theirs.  What, do you want a medal for not caring?  Or you could perhaps recognize a large portion of the market isn't even on AM5 yet and these products still matter to them.",AMD,2025-12-05 03:32:05,-13
AMD,nsgawuo,Likely q1 26. However if it holds true zen 6 x3d will come out the same year. That’s not me saying not to get the 9850x3d only don’t if u have a 9800x3d. But if u like to do a lot of upgrades u might as well wait for zen 6 at this point.,AMD,2025-12-05 17:04:28,3
AMD,nseegpk,No,AMD,2025-12-05 09:47:45,18
AMD,nseq1kj,"I'm pretty sure no. As 9800x3D owner who was previously on 5950x, I won't notice slightly more single core perf. What would be a noticeably a boost would be having a rumored monolythic Zen6 12-core CPU. A bit more IPC and 50% more cores for few other use cases while no drawbacks of talking inter CCD.",AMD,2025-12-05 11:36:02,9
AMD,nsk0h0b,"It depends on what you consider ""worth it"".  You can sell 9800x3d and add a little to get 9850x3d if you really care about slightly more performance, maybe better OC to you're memory if you got a bad chip currently and a better resell quality later because this chip is simple newer.  I am interested in the fact how overclockable it would be, or not.. like, can it be run in 5.8 - 6.0 range without problems.",AMD,2025-12-06 06:24:38,1
AMD,nsgtsvd,The 9800x3D is already really efficient when undervolted without noticeable performance drop,AMD,2025-12-05 18:35:55,3
AMD,nsdgg18,Lel im here on 3900x and 6900xt,AMD,2025-12-05 04:43:06,16
AMD,nscq8i3,14700K works with ddr4……,AMD,2025-12-05 02:00:57,-17
AMD,nse3399,Mine can't go past -15 lol,AMD,2025-12-05 07:56:04,12
AMD,nskdj4v,"That's not exactly how it works though. The undervolt offset is relative to starting point, he could have more headroom for the exact opposite reason from a performance/watt perspective (chip running higher vcore values from the factory)  Undervolt values don't mean anything without the actual voltages, frequency, stability, and other contributing variables if we're keeping it real.  It can also mean what you suggest, but without more info it's impossible to conclude either way. -30mV means nothing in its own, a better chip out of the box running at -20mV could very well be outperforming it without having more info.",AMD,2025-12-06 08:31:57,4
AMD,nse95ab,"I have no idea how, but mine is stable with -45 and +125mhz. It's bonkers.",AMD,2025-12-05 08:56:06,2
AMD,nsd3lgm,And the other 1% is shader compilation and city skylines II sim,AMD,2025-12-05 03:19:03,29
AMD,nse1bre,The most popular games played on Steam benefit quite a bit from x3D processors.,AMD,2025-12-05 07:38:57,5
AMD,nscrckq,Depends on the game.  Simulations that run a lot of physics need powerful cpus,AMD,2025-12-05 02:07:35,13
AMD,nsfgu91,Is BF6 the 1%?,AMD,2025-12-05 14:32:25,2
AMD,nsiecrx,"Well we have world of warcraft, call of duty, battlefield, and arc. Just to name a few that benefit quite a bit from the x3d",AMD,2025-12-05 23:39:45,2
AMD,nsdyhnt,Kind of would of should of,AMD,2025-12-05 07:11:48,4
AMD,nt5j4vf,It means the same thing for all intensive purposes.,AMD,2025-12-09 18:26:58,1
AMD,nsdjm1v,"I think you're taking offense from nothing here, and this is a strangely rude response to it.",AMD,2025-12-05 05:05:47,15
AMD,nsjpvh3,"True but Zen 5 isn't a good enough generational upgrade yet, clocking it higher for another 5% that will result in higher power consumption in gaming loads isn't what I want either.  Really this refresh is a big load of nothing? Other than for show offs and tech channels that can swap in a slightly faster gaming CPU for GPU testing as well as a review content vid to tide over until Zen 6 which AMD has slated for 2026... same year this binned CPU is releasing. Doubt it'll have the value proposition in Zen 6's life to make sense either as the non X3D parts are always a lot cheaper and fill the price to performance role as is, and Zen 6 in leaks looks like a good one for uplifts cache or not unlike 5 that had a more bottlenecked design that benefited the cache parts more uplift wise.   I'm sure it'll sell though, it's why AMD is doing it, Zen 5 X3D is their golden milking machine that sells for MSRP even now, obviously business wise makes sense to release the golden bins that can clock higher for an extra 5% performance at a premium for those who want the best of the best NOW. AMD leads in performance, those chasing the best will take it if they don't want to hold for Zen 6.",AMD,2025-12-06 04:54:55,0
AMD,nse98hn,get the 5700x3d or 5800x3d no questions asked,AMD,2025-12-05 08:56:54,12
AMD,nscu7w3,...but still needs a new motherboard.,AMD,2025-12-05 02:23:50,9
AMD,nse38rs,"I'm using -20mv big core, the rest -30mv",AMD,2025-12-05 07:57:34,6
AMD,nsfe00y,My 7800x3d can’t go past -10. Lost the lottery by a large margin.,AMD,2025-12-05 14:16:36,5
AMD,nsf8z42,"i dont overclock it, just undervolt",AMD,2025-12-05 13:47:59,1
AMD,nsdlcnd,"Cities 1 with all DLC and a few mods brings my 9700x, 32gb of RAM, and 6750xt to its knees too lol  Especially the RAM though",AMD,2025-12-05 05:18:59,10
AMD,nselox4,World of Warcraft would like a word with you.,AMD,2025-12-05 10:57:01,6
AMD,nsd2qbp,"He meant Watts, not performance. As in, they don't consume many watts in gaming.",AMD,2025-12-05 03:13:43,7
AMD,nsfhmy3,I ran BF6 on a 9600x before I got a9800x3d— no difference in performance.,AMD,2025-12-05 14:36:54,-3
AMD,nsdzoy4,where's u/CouldWouldShouldBot when you need it...,AMD,2025-12-05 07:23:07,2
AMD,nsq5ynq,Zen 6 might be like another 5% boost in performance.  It seems like silicon chips are nearing the limit of what we can squeeze out of them unless we come up with new techniques.  Performance gains between generations has steadily been declining for awhile now,AMD,2025-12-07 07:10:00,3
AMD,nseswtq,Is it worth to go from 5700x to 5800x3d? Purely for gaming,AMD,2025-12-05 11:59:59,2
AMD,nscwmps,Those are cheap.  Asus AYW B760 is $90.,AMD,2025-12-05 02:37:44,-17
AMD,nshvmrq,"Same, but with my 9800X3D :( but it’s still something.",AMD,2025-12-05 21:51:36,1
AMD,nsja677,"Lmao no wonder my 7600x with an RTX 2060 barely gets above 35 fps.  But hey, I played Cities 1 with the same city and config for like 7 years on a i5 7300HQ laptop with a GTX 1050. I’m all good with that numbers lol",AMD,2025-12-06 03:01:58,3
AMD,nsdei87,Yeah for marvel Rivals my 5800hs runs better at 11watts than anything higher.,AMD,2025-12-05 04:29:33,6
AMD,nsfy4do,"Yeah, both are current tier CPUs. And your GPU is your bottleneck, maxed out.  On AM4, or similarly aged Intel CPUs, it is a different story. If somebody upgraded their GPU they will discover their CPU has to work hard on BF6.  My 5800X3D is still good but load of 80% when paired with a RX9070 is normal. Had I kept my Ryzen 3600, or maybe bought a 5600X, I would be CPU limited at 1440p Ultra.",AMD,2025-12-05 16:01:59,3
AMD,nsdzrra,:'(,AMD,2025-12-05 07:23:51,1
AMD,nseu3y5,"this is a really hard question tbh... rather not, save for AM6 first X3D",AMD,2025-12-05 12:09:24,8
AMD,nsfzaee,"Depends on your GPU - if you have something like 7600, no need.   9070 series? Yes definitely go for 5700X3D, sell your current CPU and you're golden for years.",AMD,2025-12-05 16:07:45,3
AMD,nsf6mij,"Depends of what type of games you play. In ""mainstream"" games it mostly won't matter.    In something like Kenshi, any map painting game from Paradox and any other CPU heavy game, the jump in performance will still be massive, up to 50% in some cases.",AMD,2025-12-05 13:34:21,2
AMD,nsgp5z6,"It's like upgrading to a 7700X/7800X (non-3D). If you can get an AM4 X3D CPU for cheap and are CPU limited in your games, go for it.  RAM prices are out of control (up to 3.2x compared to a couple months ago already) and will stay that way for a while - this currently makes full platform upgrades a very expensive decision.",AMD,2025-12-05 18:13:35,2
AMD,nsd51o9,I'm not even gonna check the vrms on a barebones entry level asus mobo. I think I know performance will already be impacted by a non negligible margin.,AMD,2025-12-05 03:28:05,9
AMD,nsfb4kv,"Ok I will do that, if ram prices ever drop again...",AMD,2025-12-05 14:00:17,4
AMD,nsg3pa1,"Got a 9070xt in September, I am tempted when I get my Xmas bonus to try find a 5800x3d but they seem quite expensive still on eBay",AMD,2025-12-05 16:29:15,1
AMD,nskwexb,"What would be a good option for me to upgrade to? I’ve got an i7-8700k and play cpu heavy games like kenshi, and warhammer. Upgrading to am5 is just too expensive but 5800x3d is still expensive outside of looking at like OfferUp or facebook marketplace. I can still play these games but when I do heavy modded runs it starts to struggle a bit",AMD,2025-12-06 11:42:27,2
AMD,nsfb8wu,"At the moment, mainly arc raiders, lil bit of cs2, Spiderman and soon Spiderman 2, BF6",AMD,2025-12-05 14:00:57,1
AMD,nsh04nc,"Thanks for the advice, I'm going to try snag a 5700x3d but they're still £285 on eBay which is abit wild, think I'd rather save that towards full platform upgrade, kinda worried ram prices will never drop though",AMD,2025-12-05 19:06:55,2
AMD,nsd9lan,No one was asking you to do fuck all.,AMD,2025-12-05 03:57:10,-12
AMD,nsg43zm,"No need, the 5700X3D is so close in gaming performance that the price difference is simply not worth it.",AMD,2025-12-05 16:31:14,5
AMD,nstzmx8,"Try 5700x3d or 5600x3d, you already have DDR4 memory from 8700k. Both Kenshi and Total War benefit massively from 3d cache. A friend of mine gave me his late game Kenshi savefile, and we tested it on 3 cpus: 12700k, 5800x3d and 9800x3d.  12700k had around 30 fps, 5800x3d around 100, and 9800x3d was close to 200 fps.",AMD,2025-12-07 21:48:53,1
AMD,nsfi9hy,"You'll get better 1% lows, and a bit better average framerate. But the upgrade won't be that big, 10-15% max on average.      BF6 shows the biggest jump - around 20%.  For comparison's sake, in something like Kenshi (CPU heavy and loves 3d cache) the performance jump will be close to 50%.  [https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/](https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/)  5800x : 83-111  5800x3d : 106-142",AMD,2025-12-05 14:40:24,1
AMD,nsh3ty6,"If you can make good use of that 5700x3d for another 2-3 years it'll be worth it.  RAM prices may drop after the AI bubble has burst, which will happen eventually. We're already in the ""faking revenue"" stage in order to pump the stock further. As soon as the first few large investors want out to secure their gains, they'll trigger a cascade and the whole house of cards collapses.",AMD,2025-12-05 19:25:29,1
AMD,nse4vnx,"He was saying the vrms are too poor, give reading comprehension a try",AMD,2025-12-05 08:13:29,10
AMD,nsg6zjg,"Brilliant, so I'll look for a 5700x3d and finally be happy for a few years. Thanks for the advice!",AMD,2025-12-05 16:45:10,2
AMD,nswl4ff,"based on that the 5700x is still there. u/shuffleyyy1992  id say only upgrade IF you found a buyer for your CPU, otherwise no",AMD,2025-12-08 08:08:34,2
AMD,nsfksrq,Yeah for those gains I don't see too much point tbh. I'll stick with it until probably am6 and ram prices come back down a little,AMD,2025-12-05 14:54:12,2
AMD,nsfkvqq,"And thank you very much for doing the research, I really appreciate it!",AMD,2025-12-05 14:54:38,2
AMD,nswp61m,I've got a buyer only if I can get new mono and ram - my mates buying my old parts whenever I upgrade but he won't want to pay me for a CPU he can't use. I'll take your advice and stick with 5700x until I can go am5/6. Thank you!,AMD,2025-12-08 08:50:27,2
AMD,nsbt593,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-04 22:46:04,1
AMD,nsc1uv4,"> neither source is affiliated with AMD  So just more speculation, then. Not sure I would call these people ""insiders"" when they have no more info than anybody else.",AMD,2025-12-04 23:36:08,38
AMD,nseh10r,"Why would AMD increase Ryzen 9000 CPU prices? They would rather stop making GPUs than reduce their CPU sales, considering the CPUs have much higher margin",AMD,2025-12-05 10:12:29,2
AMD,nsfiis4,I wouldn’t be surprised if they are reasoning prices on CPUs to help hedge against rising ram cost.,AMD,2025-12-05 14:41:50,1
AMD,nsc3khj,"There's legitimate no reason for a price hike.  There's a price hike in DRAM because nvidia is buying out all the allocation for their next gen AI GPUs. But 9000 series is built on *N4P whereas next gen AI GPUs will likely be built on N3P (Ruben) and N2P (Rubin Ultra, i.e. the ""SUPER"" refresh of AI GPUs). So there's no effect on AMD allocation as Nvidia have no need to buy out that node.*  AMD also likely have their allocation for N3P or N2P set for their next gen CPUs so unlikely to be hurt here either.  However, if rumours are true, all allocation for A16 has already been bought out by Nvidia, so there will be some competition down the line for sub N2P allocation between Apple, AMD and all of TSMC other high end customers.  We could even see CPUs stagnate for a few years over this.",AMD,2025-12-04 23:46:19,5
AMD,nscbtb4,"Does any corporation need a reason other than ""because shareholders demand ever-increasing returns?""  I hope you're right though.",AMD,2025-12-05 00:34:37,7
AMD,nse5hs3,You are thinking logically. Companies think only about profits.,AMD,2025-12-05 08:19:38,2
AMD,nscyouq,Except if they raise prices when there's stock on the shelves people will stop buying.   The only time people buy at hyped up prices is when stock is non existent or extremely limited.,AMD,2025-12-05 02:49:47,5
AMD,nsdbhs3,"In my opinion, AMD doesn't really need to raise the price for Ryzen 9XXX because-  1. It's been out for more than a year 2. Instinct is far more important for the stock than pure Ryzen revenue 3. Ryzen is now just a marketshare and mindshare play. 4. The more people buy into AM5, the more they'll be willing to upgrade to Zen 6 or 7 (if zen 7 is indeed on AM5), to divert people away from Nova Lake if Zen 6 is indeed a tad slower.",AMD,2025-12-05 04:09:43,1
AMD,nseh5zq,>Companies think only about profits  which is logical,AMD,2025-12-05 10:13:50,1
AMD,nshar0g,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-05 20:01:20,1
AMD,nshgrhh,"essentially a binned 9800X3D which gets you closer to the 6GHz mark  good luck selling it in this market, maybe if AM5 could support DDR4",AMD,2025-12-05 20:33:11,91
AMD,nshz0jw,"For reference, this is my geekbench with a 9800x3d at 5.6ghz and 8400mhz ram. https://browser.geekbench.com/v6/cpu/15407700",AMD,2025-12-05 22:09:58,8
AMD,nshiquj,"The people who buy these CPUs have the money already, they are way less impacted then those buying CPUs like 7600 or 7500F",AMD,2025-12-05 20:43:46,67
AMD,nsk2u49,It will sell fine that is only a problem for new builds.  This chip is an option for anyone with an AM5 board that is using something slower.  Those boards have been selling for 3 years already.,AMD,2025-12-06 06:46:57,4
AMD,nsicz8m,"> maybe if AM5 could support DDR4  I know Intel did something similar in the past, anyone here with enough knowledge to say if you could build a DDR4 AM5 board? If it's possible I'd expect China to start churning these out.",AMD,2025-12-05 23:31:10,1
AMD,nt5rsre,"It will sell like hotcakes in this market, dont kid yourself.",AMD,2025-12-09 19:12:40,1
AMD,nshojb5,"Doesnt binned usually mean cut down product that didnt meet the requirements, isnt this better than the 9800x3d?",AMD,2025-12-05 21:13:59,-2
AMD,nshjchg,"problem isn't in the money, its in the fact that you can't even get good kits as easy as you could month ago  this is also why AMD should be looking to restart 5000X3D production and expand to both 5900X3D and 5950X3D, sure not the latest and greatest but many people are not upgrading because of insane DDR5 pricing and would anyways appreciate 9700x performance but on affordable platform",AMD,2025-12-05 20:46:56,18
AMD,nsjpppx,"As someone who bought a 9800X3D, RAM prices are so crazy right now that literally one of the 2 32GB RAM kits I bought is now more expensiove than the processor itself. There's ""They have the money"" and then there's ""They have over double what the processor cost""",AMD,2025-12-06 04:53:41,2
AMD,nsikts3,Counter point - people like me who buy these already got the 9950x3d by now. No point in upgrading for 9800x3d owners either,AMD,2025-12-06 00:19:47,1
AMD,nsjmjw0,"The CPU's memory controller needs to support it too, you can't just drop any existing AM5 CPU into some hypothetical DDR4 motherboard.",AMD,2025-12-06 04:29:21,3
AMD,nshrqr8,"Binning specifically means a process in which chips are categorized depending on their performance in initial testing. Usually the best chiplets are reserved for top of the line products and companies stockpile them.  Though I suppose theoretically shitty chiplets left over for lower lines are also ""binned"", on enthusiast forums it just means top chips.",AMD,2025-12-05 21:30:51,10
AMD,nshp6gt,Binned = Better  🤦‍♂️ I stand corrected then!,AMD,2025-12-05 21:17:21,4
AMD,nsimxoq,Count me in to buy a 5900X3D,AMD,2025-12-06 00:32:35,8
AMD,nshzszx,"What about people like me, moving from 7xxx non 3XD parts and looking at either this, or continuing to wait for Zen 6?  I wouldn't have to change my memory at all.  This would be a drop in CPU replacement.  I worry it will be quite a while before the 10800X3D or whatever it is called for Zen6 comes out.  A lot of people on early AM5 chips like mine are going to feel the urge to upgrade depending on what the timeline looks like.",AMD,2025-12-05 22:14:19,7
AMD,nsih8nf,I'd buy a new 5000x series haha   5600x getting old and I might never upgrade again at this point,AMD,2025-12-05 23:57:34,2
AMD,nsj78lg,I think 5800x3d is already maximum for AM4,AMD,2025-12-06 02:42:48,1
AMD,nsybf1r,AMD to start memory production would be a cool headline about now...,AMD,2025-12-08 15:55:20,1
AMD,nshs7b5,AMD definitely should be leaning into AM4 while the memory crisis is happening.,AMD,2025-12-05 21:33:18,1
AMD,nsiro3g,"What do you think is gonna happen to DDR4? it's in a better state right now because the demand is way lower, and has far less manufacturing capacity for it, DDR4 will just be in line with DDR5 if that happened",AMD,2025-12-06 01:01:45,0
AMD,nshsjis,"Yeah i had mostly heard it with the 7600x3d/7500x3d being binned versions of the 7800x3d, and the same with the 5700x3d/5600x3d to the 5800x3d",AMD,2025-12-05 21:35:08,1
AMD,nshprgw,"Ah just looked it up, and it can mean both",AMD,2025-12-05 21:20:24,2
AMD,nsjzkpc,"As someone on a 7600X...hold for Zen 6, pretty sure a slide AMD did last month put Zen 6 as 2026. This higher binned Zen 5 X3D is coming early 2026, it's just milking the last of what they can of Zen 5 X3D with a more expensive sku they can charge a premium for before Zen 6 tanks the value of such a part.   Like the clock speed boost is only 400mhz, that's maybe 7% higher boost (that will mean higher power draw), probably about 5-6% faster gaming performance if that. It isn't really going to change Zen 5 from being a skip to worth it if the 9800X3D wasn't enough for you already.   My plan is Zen 6, you'll get more cores per CCD as per leaks, AMD has confirmed a node leap which means higher clock speeds and better power efficiency so Zen 6 will be a GOOD generational leap for us, it won't be Zen 5% again.",AMD,2025-12-06 06:16:21,4
AMD,nsi3ams,"people who are on AM5 are good as long as they have a DDR5 kit  issue is people who are still on AM4 literally don't have a reason to upgrade when platform cost has just gone up by $300 and improvement you get over 5800X3D is at most 20%  this is why AMD should be restarting zen 3 X3D production and expanding to 5900X3D and 5950X3D, their AM5 sales dropped by 50% FFS and that is not sustainable long term with complexity zen 5 X3D is having",AMD,2025-12-05 22:33:52,0
AMD,nsj1pza,"Lol I upgraded my 5600x PC from 16GB to 64GB of RAM while memory was super cheap. I don't think I'll be upgrading for a while either...  But I'd definitely think hard about a Zen 3 x3D SKU, if one ever came out with an acceptable price.",AMD,2025-12-06 02:07:09,2
AMD,nsk4adl,Correct they will never release a 5900X3D or a 5950X3D not going to see any new CPU's on AM4.,AMD,2025-12-06 07:00:38,1
AMD,nshurhu,"Nope, they are riding AI train together with other companies.",AMD,2025-12-05 21:46:59,19
AMD,nsid3t6,I think that product stack is already as full and as wide reaching as it could be.,AMD,2025-12-05 23:31:58,2
AMD,nsj30f2,"there is so much DDR4 that you cannot drain its supply on the market, its everywhere  and a lot of people are already on DDR4 so they won't need a memory upgrade unless they go for 32gb setup  it is all up to AMD to restart zen 3 3D production and release 2 new SKU's; 5950X3D and 5900X3D",AMD,2025-12-06 02:15:30,1
AMD,nsmsvdf,">The base 5800X already runs hot as genuine shit, it's even worse for the X3D version  hence the undervolting, i run my 5800X3D with -30 all core CO and corsair 240mm AiO transfered from 3800x and with undervolt i see at most 70c in heavy loads while in gaming it sits at happy 55c  >A potential 5900X3D would have major heating and latency issues. Not happening.   straight up misinformation, what is your source of this?  >They had prototypes of a 5900X3D and 5950X3D and they never released them for a reason.  reason why is because there was no real reason to make these CPU's back when 5000 series was still new  now with AM5 being hard to enter due to DDR5 pricing and many people still on AM4 i don't see why wouldn't AMD build these when AM5 saled got chopped in half in the last month",AMD,2025-12-06 18:36:08,1
AMD,nslpthi,In the industry we have terms like down bin and up bin. But usually we use down bin never really used up bin. Bin can be used in place of up bin.,AMD,2025-12-06 15:08:00,2
AMD,nsia8ta,Bro I’m on 3700x with 64GB ram. I was going to build a new system in September with a 9800x3d but didn’t and now I’m stuck. My pc runs fine though honestly so I could stick it out for another year or two if I need to.,AMD,2025-12-05 23:14:30,5
AMD,nsj8gq1,"It's ""everywhere"" because we've got DDR5 that is taking up a lot of the demand from people upgrading, directly people go to DDR4 that will be in the same situation.   Plus it's not ""everywhere"" many sticks are out of stock and they start at £150 which is double what they were a few months ago so little supply is happening too, imagine what would happen if demand went right up.",AMD,2025-12-06 02:50:45,0
AMD,nsjip2c,I've never regretted not going thrpugh with my purchade in november... literally a few weeks later prices went nuts,AMD,2025-12-06 04:00:39,1
AMD,nsm0g4e,"used market is so full of DDR4 that it flushed out a ton of DDR3 and DDR2 systems into the market  and significant amount of people have DDR4 based system these days, not everyone upgraded to AM5 or lga 1851",AMD,2025-12-06 16:06:37,1
AMD,nsmhxh1,"The used price for DDR4 32gb 3000mhz+ starts at £90 for the UK.... That's double the price than it used to be and was the same as DDR5 new prices....   Yes most people have DDR4 but there's still a huge market for people getting into pc gaming or home labs or home AI stuff.  It's currently costing that much when the DEMAND IS LOW, now imagine all those people who are wanting DDR5 now goes on to DDR4 instead.... That used market will be wiped out in a matter of weeks and the price will skyrocket.",AMD,2025-12-06 17:40:06,0
AMD,nsmrrkk,">The used price for DDR4 32gb 3000mhz+ starts at £90 for the UK.... That's double the price than it used to be and was the same as DDR5 new prices....  many people don't need 32gb ram and lets be real with X3D variants you don't need high speed DDR4 to have a great time  >Yes most people have DDR4 but there's still a huge market for people getting into pc gaming or home labs or home AI stuff.  entry level GPU's for 1080p gaming are quite cheap and you don't need latest and greatest to enjoy competitive titles on 1080p  avg. person doesn't care about homelab and AI, those are niche markets already covered by AM5 platform  >It's currently costing that much when the DEMAND IS LOW, now imagine all those people who are wanting DDR5 now goes on to DDR4 instead.... That used market will be wiped out in a matter of weeks and the price will skyrocket.  i got my G skill aegis 2x16gb 3200 c16 kit for 50€ brand new last year after production ramped down and demand was higher for DDR4 because a lot of people rushed to upgrade to 5700X3D  and not like used market is flooded with cheap DDR4, i can find 2x16gb DDR4 4000 kits as low as 100€ which is very cheap",AMD,2025-12-06 18:30:26,1
AMD,nsobbpj,"A lot of people do want 32gb, including myself, I would have gone for 64gb if this whole ram situation didn't happen. Games are starting to creep towards the 16gb limit and some games go over that.   Not everyone wants an entry level lots of people are getting 1440p 165hz monitors and up, yes 1080p is still the most common but that doesn't mean a lot of people aren't buying high end PCs aiming for high 1440p or 4k refresh rate.  I know the average person doesn't do home lab and AI but that is STILL adding to the demand of RAM which is why I mentioned it.  Congratulations you got your RAM new for €50 when there was little demand and plenty of supply, and now you are suggesting about spending double the amount of money on used RAM, although better quality.  You mentioned that fabs are ramping down DDR4 this would be to match the current and future demand, what happens when that demand goes up by 100-300%? No more stock available and prices sky rocket.   You are seriously underestimating the amount of computer components sold for brand new custom pc built and prebuilds.",AMD,2025-12-06 23:37:27,1
AMD,nt8fx08,Is the additional $30 from the steel legend rx 9070xt worth it?,AMD,2025-12-10 03:59:46,1
AMD,nt5ppny,This will probably be as low as gets. Buy it while you can.,AMD,2025-12-09 19:02:16,29
AMD,nt6af5s,"I got the RX 9060 XT Gaming OC 16GB for 324€ and this model has a 40€ cashback, so final price of 284€. This price was on amazon for just 1h before they changed again to their normal price of 390€. What a deal, hopefully amazon doesn't cancel.",AMD,2025-12-09 20:44:56,34
AMD,nt5t8s7,"With the cashback, I can snag a Radeon RX 9070 XT GAMING OC for just 499€, but isn't it one of the worse 9070 XTs out there?",AMD,2025-12-09 19:19:53,36
AMD,nt6xhne,I have the gaming oc kinda regret it hot spot temps where shit 95c at times so I risked my warranty and opened mine up lo and behold it's the older version of the card where it  seemed like someone went to town with the thermal gel.  Didn't care about warranty in the first place since I have plans to do a custom water cooled loop soon.  After replacing it with some thermal putty I had card won't go past 85c hotspot at 35% fan speed 2650mhz memory +10 PL -50mv which still isn't good.,AMD,2025-12-09 22:39:35,4
AMD,nt65joe,Found one new for £416 but it is asus prime tho,AMD,2025-12-09 20:20:55,2
AMD,nt6ur0x,"If I live in any of listed eligible territories, can I buy the card from German online store (I would have it delivered to my german mailbox provided to me by forwarding service (mailboxde.com)), or must I buy the card from the store that is in the same territory where I actually live?",AMD,2025-12-09 22:25:02,2
AMD,nt6utcc,Of course i see this after buying my pulse 9070xt for 583€,AMD,2025-12-09 22:25:22,2
AMD,nt5wbss,Did they fix their crappy melting thermal pads and start using ptm7950 like everyone else??,AMD,2025-12-09 19:35:11,4
AMD,nt7igz4,If i buy a prebuilt with all this stuff does the discount count?,AMD,2025-12-10 00:37:31,1
AMD,nt5yhqq,"Yeah great, it looks like you need to purchase motherboards to or cpus to be eligible for a cashback",AMD,2025-12-09 19:45:56,1
AMD,nt6my52,Gigabyte makes the loudest coolers,AMD,2025-12-09 21:45:53,1
AMD,nt6izax,"That's an insane deal, happy for you.",AMD,2025-12-09 21:26:23,8
AMD,nt5tfv7,For that price it's the best,AMD,2025-12-09 19:20:51,70
AMD,nt6egdq,"It currently has issues where the thermal pads can randomly come lose and slip out and the overall quality isn't that good  Igor's Lab reported on this and Gigabyte stopped providing him with samples, so his claims are probably true.  Honestly would skip that card because of this issue  https://www.igorslab.de/das-geheimnisvolle-server-grade-thermal-condiuctive-gel-von-gigabyte-entzaubert-mittelmaessiges-thermal-putty-und-eine-fragwuerdige-waermeleitpaste/  If they fixed it someone can update me, but my last knowledge was this issue",AMD,2025-12-09 21:04:21,14
AMD,nt6831x,Where from you buying with this price?,AMD,2025-12-09 20:33:29,2
AMD,nt699zw,They're all pretty much the same and perform pretty much the same.  Is it one of the ugliest and least desirable ? Maybe if you care about that.,AMD,2025-12-09 20:39:21,3
AMD,nt5tjsm,Better than Asrock,AMD,2025-12-09 19:21:24,6
AMD,nt6jd44,Is that the Gigabyte one?,AMD,2025-12-09 21:28:15,1
AMD,nt6ez8q,"Decently happy with my Prime card, 1100rpm fans at mid 80's hotspot temp, though that's obviously my own fan curve and an undervolt (-50)  Not loud at all but also not entirely silent.  That's in a NZXT H7 flow 2024 with every fan slot filled so it's not exactly struggling for cold air.",AMD,2025-12-09 21:06:55,2
AMD,nt6hyr5,UK? Where from?,AMD,2025-12-09 21:21:23,1
AMD,nt5z4r9,"Fix yes, ptm7950 no",AMD,2025-12-09 19:49:05,5
AMD,nt6atg1,"No, that is for extra cashback, you can just buy the GPU.",AMD,2025-12-09 20:46:53,2
AMD,nt6s796,also the cheapest,AMD,2025-12-09 22:11:49,3
AMD,nt6zsjt,"Afaik the default bios has an awful fan curve, but the quiet mode is much better. The cooler is decent. But they run it at higher watt out of the box.",AMD,2025-12-09 22:51:51,3
AMD,nt7m1sz,"Asus Prime 9060xt 16gb is \~320USD on black friday, i got one myself as my second gpu",AMD,2025-12-10 00:58:04,2
AMD,nt6ktw1,"Through my job, I can get it at MSRP without VAT",AMD,2025-12-09 21:35:27,1
AMD,nt6e7cv,"Friend has one and it's loud and hot.  2000 RPM fans at high 80's hotspot temp.  That's in a Fractal Define case so it's not the best at GPU cooling but still. (I think, he's not sure which one it is)",AMD,2025-12-09 21:03:08,2
AMD,nt6e24w,When did asrock become a toilet brand? Always been a budget brand but relatively solid,AMD,2025-12-09 21:02:26,11
AMD,nt6w1pr,Yup,AMD,2025-12-09 22:31:55,1
AMD,nt8fhq8,"Hotspot temp 80? That's not too bad.   Is this gpu same as nvidias 3070ti and 3080? Had those gpus and by undervolting i could squeeze more performance while keeping them cooler, crazy...",AMD,2025-12-10 03:56:55,1
AMD,nt8f0il,Its a seller on ebay. Will PM you,AMD,2025-12-10 03:53:42,1
AMD,nt6h8ya,"Are you sure? To me it looks like you gave to buy from column A and B to get cashback  Edit: You are right, my bad I read it wrong",AMD,2025-12-09 21:17:56,0
AMD,nt711ro,"It was a general observation regarding Gigabytes cards. I've just had so many examples of people cheaping out buying the ""Eagle"" edition and it was extremely loud in general. Lesson learned, pay the few bucks more and get a decent cooler.",AMD,2025-12-09 22:58:36,1
AMD,nt6twn9,"People are conflating the X3D chip issue with everything else. Also no idea what the guy saying it’s the same as Asus, they’ve been split since 2002 lmao",AMD,2025-12-09 22:20:37,12
AMD,nt6grhr,Well I dont know about their GPUs but their motherboards especially the x870 series have a higher chance of frying the 9000 series specifically 9800x3d and they cannot figure out the cause. The failure rate is much higher than other brands. Its such a sad situation as their motherboards on paper are best for value. Their PSUs are great though specifically the taichi and phantom ones as they provide over temperature protection for their 12vhpwr connections.,AMD,2025-12-09 21:15:36,1
AMD,nt6lax1,"Asrock is the company that is behind Asus.  Asus and Asrock are garbage quality one today. I have issues on Asrock board, Meanwhile the Gigabyte board one for much cheaper works just fine there when its about on ram usage (ram sticks itelf are fine, so this much a board issue)",AMD,2025-12-09 21:37:47,-8
AMD,nt8jied,"Yeah, undervolting works the exact same.",AMD,2025-12-10 04:24:22,1
AMD,nt75i79,I had a gigabyte 7970 back in the days that was fine but yeah I initially ordered 9070 XT from them but after researching the different cards I canceled the order. I went with the ASUS Prime instead.,AMD,2025-12-09 23:23:29,1
AMD,nt7bw6u,That’s what I thought it was lol,AMD,2025-12-10 00:00:19,3
AMD,nsive4c,Another strix Halo mini PC. Still only one laptop 🙄,AMD,2025-12-06 01:25:48,18
AMD,nshoi55,"There's already a billion of these mini PCs with this chip in it, who cares.",AMD,2025-12-05 21:13:49,17
AMD,nsi2s7x,A little late to the game…,AMD,2025-12-05 22:30:58,3
AMD,nsi327f,"What are you on?? The AI Max+ 395 is still in its early days. They’re pretty expensive to purchase. You’re looking at $2k+ with 128GB ram. With ram price going up, the prices for these will only go up. And I haven’t seen many laptops around either. Maybe 2-3 manufacturers including a gaming handheld.",AMD,2025-12-05 22:32:32,-5
AMD,nsivl4g,It's never not going to be expensive to purchase. It's a shitload of silicon and requires a lot of ram.,AMD,2025-12-06 01:27:03,9
AMD,nt0syx2,Strix Halo isn't getting younger and can't compete cost-wise for gaming/consumer use cases.,AMD,2025-12-08 23:24:50,2
AMD,ns00bs8,"It's tempting, even though I don't need one. I'm trying to hold out for the next generation.",AMD,2025-12-03 02:41:43,134
AMD,ns0c4u2,How about the GPUs and RAM? Oh wait.,AMD,2025-12-03 03:54:11,54
AMD,ns0r1ey,5800x3d is the best thing that I’ve purchased. We’re buddies and we ain’t stopping until the game developers decided to syphon our gas!(metaphorically).,AMD,2025-12-03 05:41:00,19
AMD,ns043iu,Great price!,AMD,2025-12-03 03:03:54,11
AMD,ns2gbdi,"I have a 7800x3d. What would be the point of buying this? It's a good price, but wouldn't it be overkill? All I do is game on my pc.  I'm more worried about gpu prices than anything.",AMD,2025-12-03 14:08:12,9
AMD,ns1gbgc,Anticipating the $499 9850X3D,AMD,2025-12-03 09:34:13,8
AMD,ns0xm5k,It was $379 at Central Computers in San Francisco this weekend,AMD,2025-12-03 06:35:08,7
AMD,ns36mzu,I really want to upgrade from my 5800x but ddr5 prices are insane right now... sigh,AMD,2025-12-03 16:21:32,2
AMD,ns37j79,"This deal is frustratingly good. This would be the best time to get one, but I know that Zen 6 is going to be such a substantial upgrade that this will feel like a huge waste of money. I don't expect Zen 7 to be nearly as much of one (though all those leaks sound crazy), and it will *probably* be AM6 anyway (I don't believe the ""still on AM5"" leaks at all), but we'll just have to see.",AMD,2025-12-03 16:25:47,2
AMD,ns3c9ox,I love that was able to convince Best Buy rep to price match Microcenters $399 price for my 9800X3D back during Primes October Prime Big Sale and able to complete my build before Halloween while my 32gig of Corsair RGB Vengeance was still at $150,AMD,2025-12-03 16:48:14,1
AMD,ns4stuo,def worth getting,AMD,2025-12-03 21:00:24,1
AMD,ns7zoft,good for existing owners that want an upgrade which will last generations,AMD,2025-12-04 09:48:17,1
AMD,ns8rp2i,I bought one from MC last week for $430 to replace my 5700x3D setup.  Can I get the $30 difference back?,AMD,2025-12-04 13:31:28,1
AMD,nskkxva,![gif](giphy|3ohuPua7QjtYGQVMze),AMD,2025-12-06 09:48:40,1
AMD,nrzsmai,"Cool, more crap to buy.",AMD,2025-12-03 01:56:54,-29
AMD,ns05foz,Riding this 5800X3D until there’s actually a game I’m not satisfied with performance-wise. I keep getting tempted but then remember there’s currently nothing my computer doesn’t do perfectly well for me.,AMD,2025-12-03 03:12:08,91
AMD,ns0rm1w,"I… gave in, late last week, since I have a need for a more portable system here at home. “Only” a 7800X3D bc the Newegg deal that bundled a AIO with it fit with the case I wanted.. the AIO with the 9800X3D’s bundle was too big. Also got a good deal on RAM, as it was bundled with a 4TB PCIe 5 SSD, and I was wanting more storage anyway. But originally I had been planning on holding off until Zen 6 (or possibly Intel Nova Lake). I guess I still will in the sense that I intend to swap in a top-end Zen 6 X3D CPU, when it’s out.",AMD,2025-12-03 05:45:33,5
AMD,ns1uqvr,"I sold my 13700k for a 9800X3D, super worth it. That being said, I am also an idiot.",AMD,2025-12-03 11:47:14,5
AMD,ns08y13,"next gen will be on the same socket so upgrading sooner than later is better assuming you end up getting one more gen out of the socket. but if you wait until next gen it's likely to be the last gen on the am5 socket so you won't really be able to upgrade, unless you did like a 11500f then later upgraded to a 11850x3d",AMD,2025-12-03 03:33:59,6
AMD,ns21qad,When is the next gen predicted to launch?,AMD,2025-12-03 12:39:15,2
AMD,ns7czck,Same boat I'm 20.minutes away and need a tablet for my daughter...I don't think ill hold out...it would plop right in my system,AMD,2025-12-04 06:12:56,2
AMD,ns32a7n,"Yuppp it’s literally around the corner, trust me time flies lol",AMD,2025-12-03 16:00:43,1
AMD,ns3scla,When is that?,AMD,2025-12-03 18:05:28,1
AMD,ns414gs,"I'm still running a 3600 with a b450 mb... And, at this moment, my temptation ends when I see the prices for RAMs kits...",AMD,2025-12-03 18:46:39,1
AMD,ns0cbw0,"""I'll sell you this CPU for only $10!""   ""Wow! Great deal!""   ""You just need this RAM for it""   ""How much?   ""Market price""",AMD,2025-12-03 03:55:27,36
AMD,ns27l5d,"If you get the CPU and Motherboard Bundle they do offer discounted RAM as an add-on but due to the current prices it is no longer a good deal.   A few weeks ago my friend got the 9800x3d / MSI X870E-P / Corsair Vengeance 32GB bundle for $699, same bundle is up to $819 today because the RAM went to $290.",AMD,2025-12-03 13:17:19,2
AMD,ns7veb1,Ordered mine just in time before it disappeared for good and I'm so glad I did. I do feel the itch to upgrade but certainly not until RAM prices normalize  surely they will normalize,AMD,2025-12-04 09:05:01,3
AMD,nskigsj,I’m seriously thinking about upgrading to an AM4 X3D from 5600 because it doesn’t look like I’ll be going to DDR5 and AM5 anytime soon judging by the prices.,AMD,2025-12-06 09:22:50,2
AMD,ns2zd9l,No it wouldn't be worth it if you have the 7800x3D. It's like 0-10% performance boost depending on the game if you have a 4090 or better.,AMD,2025-12-03 15:46:57,7
AMD,ns38032,"Yes, this would be overkill. Keep your 7800X3D, it's still going to be a good chip for a while. Wait and see what the ""11800X3D"" will be like.",AMD,2025-12-03 16:28:00,3
AMD,ns3tirj,"Also consider the fact that it will be another year (most likely) before the next gen, with no confirmation if 3D will be available at launch. And even if it has the same MSRP it would need to be what 40% better in gaming to exceed price/performance.   Getting a year of smooth gameplay at what will likely be equal p/p seems like a good deal to me.",AMD,2025-12-03 18:11:05,1
AMD,ns3cn3e,I love that was able to convince Best Buy rep to price match Microcenters $399 price for my 9800X3D back during Primes October Prime Big Sale and able to complete my build before Halloween while my 32gig of Corsair RGB Vengeance was still at $150,AMD,2025-12-03 16:49:59,1
AMD,nsk1iwc,Afaik yes https://community.microcenter.com/kb/articles/631-do-you-offer-price-protection,AMD,2025-12-06 06:34:37,2
AMD,ns095cr,"why are you subscribed to this sub if a 9800x3d is considered ""crap"". I can see if you're not into computers or gaming/productivity then most computer stuff would seem to be a waste of money. but in that scenario you wouldn't see this post since you wouldn't be subscribed",AMD,2025-12-03 03:35:15,18
AMD,ns07170,"Same for my 5700X3D, I probably will go the X3D for the last gen of AM5 when AM6 gets released like I did with this.",AMD,2025-12-03 03:22:03,43
AMD,ns0fcei,Bought mine intending to not upgrade until AM6 and ram prices are making sure of that.,AMD,2025-12-03 04:15:26,11
AMD,ns1qxxc,Same still rocking my 5800x3D 3080ti. I want to upgrade but tbh there is no need.    I have to listen to my brain telling me there is no need.,AMD,2025-12-03 11:15:14,6
AMD,ns1oo15,"As a fellow 5700X3D user, the most CPU intensive game I’d to have better performance in I’m playing is Monster Hunter Wilds and that’s more an issue of the game than the CPU.",AMD,2025-12-03 10:55:06,4
AMD,ns5nnkr,I sold my 5800x3d for $299 on eBay a few months ago could have probably gotten even more for it too. They sell like hotcakes because they’re the best chip for AM4 and discontinued,AMD,2025-12-03 23:39:17,2
AMD,ns0u9b1,Pretty much if you have a x3d cpu you are going to be solid for a long while.,AMD,2025-12-03 06:06:53,3
AMD,ns1yd87,"If you're on a 5800X3D at this point, wait until 2028-2029 for Zen 7 X3D and maybe sales on that + DDR5 being cheap hopefully by then.",AMD,2025-12-03 12:15:08,3
AMD,ns1p1r2,"I'm the same with one small note - unsatisfied with the performance because of the CPU, rather than shit optimisation. Lookin' at Lost Ark there.",AMD,2025-12-03 10:58:30,1
AMD,ns9ye0y,"Especially if you have a PCIe 4.0X16 motherboard for the GPU to in. A Ryzen 7 5800X3D with a Radeon RX 9070 XT or similar GPU, is such a great combo at 1440p or at 2160p.",AMD,2025-12-04 17:13:02,1
AMD,nsgph69,I just bought a 9800x3d after being only intel for 15 years or so.  This is making me feel good about my decision.,AMD,2025-12-05 18:15:05,1
AMD,ns1yaxc,"7800X3D? Honestly can wait until \*Zen 7 X3D\* since that's likely on AM5. Or Razor Lake, Or Titan/Hammer Lake.",AMD,2025-12-03 12:14:39,2
AMD,ns1z8ks,I don't think it's idiotic. The X3D will probably last a lot longer before you need to upgrade.,AMD,2025-12-03 12:21:34,2
AMD,ns0onu9,"Can we pronounce them “eleventy-five-hundred-eff” and “eleventy-eight-fifty-ex-three-dee”, respectively? Or…",AMD,2025-12-03 05:22:24,7
AMD,ns2bgok,"AMD and Intel release in roughly 2 year cycles, so I would guess fall of 2026. https://en.wikipedia.org/wiki/List_of_AMD_Ryzen_processors#Ryzen_9000_series",AMD,2025-12-03 13:40:21,4
AMD,ns0gjki,“One RAM please how much?”  “Treefiddy”   “Oh that’s cheap”  “No…$350”,AMD,2025-12-03 04:23:35,15
AMD,ns2m9l4,What market are you shopping at?,AMD,2025-12-03 14:41:13,2
AMD,nskor29,"My friend wants to do the same, unfortunately just can't find any x3d even second hand for sale here",AMD,2025-12-06 10:27:37,1
AMD,ns0nfh4,I’m not subbed.,AMD,2025-12-03 05:13:03,-20
AMD,ns08g3l,Would you say you remain on the edge of glory?,AMD,2025-12-03 03:30:50,19
AMD,ns1nbs6,why not buy the first X3D on AM6?,AMD,2025-12-03 10:42:55,12
AMD,ns1y1f7,Feels like that'll be the wisest move from now on; buy platforms when they've released they're last gen memory wise. Basically Zen 7 on AM5 and something on LGA 1954 (Hammer Lake?). DDR6 is going to be expensive and hopefully by then DDR5 will drop to low price for 8800+ MT/s Kits.,AMD,2025-12-03 12:12:41,2
AMD,ns19xxh,"Same, but I think I will wait for at least second gen AM6 so I dont beta test the new socket",AMD,2025-12-03 08:30:01,4
AMD,ns3cgm7,See you in 2028,AMD,2025-12-03 16:49:08,1
AMD,ns1zctl,"This was my setup before my roommate wanted to upgrade his cpu a while back, so I justified it then. It's fantastic and a noticeable upgrade, but my ram that I got for it now is $350 compared to $110 so it's definitely not worth rn.",AMD,2025-12-03 12:22:25,-1
AMD,ns2bbfo,"I could, but the rumors around Zen 6, make it *very* appealing, such as: 7+GHz clock speeds (over the 7800X3D’s 5), 12-core CCDs, an improved memory controller/IOdie, more IPC, 2 (4?) LP cores, a NPU, and.. possibly other things. More L1, L2 cache as well. If those are all true, a 50% or so performance boost, worth the upgrade I think.  Zen 7 is enticing too, but at least 2 years out from Zen 6. While the rumors have it on AM5, we also don’t know how the RAM crisis will change things. If, god forbid, RAM remains hideously expensive for years, then is there really a downside to going to DDR6, for the performance it brings? I could even see it being *cheaper* than DDR5 at launch bc of where the LLM demand is. But idk, that’s all just speculation, and hopefully the AI bubble will break soon and it’ll be a non-issue. Zen 7 on AM5 would be nice, yeah",AMD,2025-12-03 13:39:32,2
AMD,ns3irnm,Plot twist if the 13700k multithreading throughput magically/finally ends up mattering,AMD,2025-12-03 17:19:26,3
AMD,ns0qtb2,"Way more likely is it’ll be some abomination with “AI” prominently in the name. After all, Zen 6 CPUs will have an NPU. I do not like this thought, but I fully expect AMD to do it *unless* the AI bubble pops before then.",AMD,2025-12-03 05:39:13,3
AMD,ns2keyh,$350? Must only be getting a single 8gb stick....,AMD,2025-12-03 14:31:01,3
AMD,ns0ikk7,"350? That's still cheap by today's standard. I paid like 280 or something for 96GB for my 13th Gen Intel build. 16GB kits are aroubd that price now.   ""Nah dawg. $350,000"" lol",AMD,2025-12-03 04:37:47,2
AMD,nsl10uv,Yeah that seems to be another problem,AMD,2025-12-06 12:23:46,1
AMD,ns0qvtr,Then why comment if you’re not interested?,AMD,2025-12-03 05:39:46,6
AMD,ns1uhxs,"Because X3D of previous gen gets some discount when a new platform gets released, like 5800X3D was for a couple of months when AM5 got released (then price shot to the moon because last gen top chip of AM4 platform). So like the theoretical 11800X3D (if that gen is still AM5) or maybe a lower binned 11700X3D is gonna be cheaper when AM6 gets released.",AMD,2025-12-03 11:45:14,12
AMD,ns1y4li,Likely waiting until 2030+ for that and paying a \*huge\* premium. 2029/2030 for Zen 7 X3D> 2035-2037 for Zen 10/Zen 11 X3D (whichever is the final AM6 gen) seems much more wiser.,AMD,2025-12-03 12:13:19,3
AMD,nsa1o00,This is the way,AMD,2025-12-04 17:28:56,1
AMD,ns21eds,Yes AM5 7800x3D or 9800x3d would a good upgrade but i guess I'll wait till another gen. Currently my rig just sits at home most of the year because of work travel.   My current main setup is a LP7 7945HX 4080mobile. I have modded my laptop a bit which allowing me OC my CPU and ram which is running at 6000 CL36-37-37-34. Currently I am trying to tune a 6200 profile.   The FPS I get in both setups is pretty similar. I guess its because of the 5800x3D and slower ddr4.,AMD,2025-12-03 12:36:56,5
AMD,nsag3h7,">I could, but the rumors around Zen 6, make it *very* appealing, such as: 7+GHz clock speeds (over the 7800X3D’s 5), 12-core CCDs, an improved memory controller/IOdie, more IPC, 2 (4?) LP cores, a NPU, and.. possibly other things. More L1, L2 cache as well. If those are all true, a 50% or so performance boost, worth the upgrade I think.  This definitely sounds like something MLiD would come up with lol  Definitely a ""rumor""",AMD,2025-12-04 18:38:16,1
AMD,ns17nb0,Their mobile/laptop SoCs already have the AI moniker,AMD,2025-12-03 08:07:32,2
AMD,ns0vijc,Felt like it.,AMD,2025-12-03 06:17:19,-17
AMD,ns1xvj7,And DDR6 won't be cheap at all.,AMD,2025-12-03 12:11:26,9
AMD,ns1xra4,"Yes, but then you will have to rebuy literally everything, from motherboard to ram, instead of buying the first AM6 X3D and maybe last X3D when AM7 will be out or close to it",AMD,2025-12-03 12:10:33,3
AMD,ns2f44c,I’m planning the same except getting 11950x3d. If I’m going to be riding it for years might as well go with the best of the best chip.,AMD,2025-12-03 14:01:21,2
AMD,ns89l2m,Yup bought mine right at that moment (also because DDR5 was too expensive) and I‘m still very much bottlenecked by my GPU…,AMD,2025-12-04 11:20:49,1
AMD,ns23vhf,"exactly, first and last AM6 now that we re on last AM4.",AMD,2025-12-03 12:53:44,2
AMD,nsape3v,"MLiD usually predicts things further out, so he tends to be more inaccurate compared to many leakers, this isn't a surprise to me. Naturally, when you are talking about products a couple years out, lots of things aren't ""nailed down"" by the companies themselves, and therefore are subject to change. I'll also say something that applies to all leakers/rumormongers and not MLiD specifically: people like to slam them when they're wrong, but rarely give kudos when they're right. Either wrong or right, I do find the rumors to be entertaining though!  Zen 6 is jumping several nodes. Obviously we'll have to wait for released CPUs and independent testing to have an informed opinion as to how good they are, but with AMD jumping several nodes next-gen, the rumors at least strike me as plausible.. you may feel differently.",AMD,2025-12-04 19:23:22,3
AMD,ns2brcr,"Exactly. Though those products have had problematic names for years now. So I certainly wouldn’t be surprised if they did it with desktop too. AMD does make stupid decisions, sometimes.",AMD,2025-12-03 13:42:06,1
AMD,ns3ivmr,Gigachad,AMD,2025-12-03 17:19:58,0
AMD,ns23o08,"Except I don't because why should I upgrade within the platform? That's never necessary. I can just go to the next platform in like 8-10 years after the build, which is right around the timeframe from Ryzen 5000 and Ryzen 11000 too.",AMD,2025-12-03 12:52:22,3
AMD,nsav2gs,">Zen 6 is jumping several nodes  This is also speculation  Rumors are rumors, and they're at least half of AMD's source of disappointment, the other half being their own marketing team.  It's fine to see rumors and ""leaks"" as entertainment, but to refer to them as a reason to buy X or hold off till Y or whatever is nonsense.  The reason we shouldn't credit MLiD when he's right, is because he usually keeps spewing BS until we're down to the wire and starts predicting things right as things get confirmed. He's unreliable at best.",AMD,2025-12-04 19:51:57,1
AMD,ns7sh69,"thats exactly what im saying lol. Why but last AM5 and then again AM6 when you can buy directly first and last AM6, which you will do anyways, but you just make it more expensive by buying AM5 too ""again""",AMD,2025-12-04 08:35:25,0
AMD,nsay411,"> Zen 6 is jumping several nodes  > This is also speculation  No it's not, not really. AMD, unless they're morons, will be using whatever current node was on offer, and that's TSMC N2 (N2P/N2/N2X). That's several nodes improved.   > the other half being their own marketing team.  Yeah, I'm in agreement there. Their marketing team, that whole side of AMD is just terrible. And we just ""know"" they're going to name their desktop Zen 6 CPUs with some naming abomination that has ""AI"" all over it.  > It's fine to see rumors and ""leaks"" as entertainment, but to refer to them as a reason to buy X or hold off till Y or whatever is nonsense.  Context/details matter. Ofc one shouldn't hold off a buying decision for a couple years based on rumors that could easily turn out to be wildly off. However, a few months out, when there's substantive leaks from multiple sources, would be a whole different thing.  Obviously Zen 6 (and Intel Nova Lake for that matter) is at least a year out, and ofc has totally unproven performance. All that said, it's hardly any secret what the node jumps make possible, and so AMD *could* do what is rumored, which is obviously not the same as saying they *will* do it.  > The reason we shouldn't credit MLiD when he's right, is because he usually keeps spewing BS until we're down to the wire and starts predicting things right as things get confirmed. He's unreliable at best.  I'm not going to comment on MLiD or any other leaker or Youtuber specifically. People have their opinions, and I can respect those. Getting into that sort of discussion just invites drama, which I try to avoid.",AMD,2025-12-04 20:07:16,3
AMD,nsb8c5g,Zen 6 jumping nodes is revealed in public roadmaps though?,AMD,2025-12-04 20:59:13,1
AMD,ns9fycv,"Because we upgrade our CPU's after 7-8 years since the last one? By the time I feel the need to upgrade, if I bought in at the start of a generation, a new gen is already out.   I just saved hundreds by not buying into AM5 at the start of 7000 series and I feel like I don't need to upgrade until AM6 comes out since this AM4 gen still does everything I need",AMD,2025-12-04 15:43:39,3
AMD,nsba8j8,Is it?  I could only find articles talking about it being leaked by people within the industry,AMD,2025-12-04 21:08:42,2
AMD,nse00vg,"you are again proving my point, i dont get it.  Im saying the same thing, NOT BUYING AM5 wtf. Dont upgrade UNTIL AM6 COMES OUT.",AMD,2025-12-05 07:26:15,-2
AMD,nshas8p,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-05 20:01:31,1
AMD,nsi6rpc,No FSR4 for handheld PCs til RDNA5/UDNA1.,AMD,2025-12-05 22:53:44,73
AMD,nsjt4r9,"They are going to get their asses handed to them in the apu market by Intel….INTEL!  The company they borderline gave up, lost their insane market dominance to amd, didn’t even have a dedicated gpu 3 years ago, INTEL.  I really hope it happens and lights a fire under amd.",AMD,2025-12-06 05:20:48,29
AMD,nsj792l,"God AMD sucks, what the fuck is RDNA 3.5. Give us a RDNA 4.0 APU.",AMD,2025-12-06 02:42:53,42
AMD,nsjmtni,And what is the point of that if they refuse to add infinity cache to it when 890m barely scales with performance beyond even 2ghz due to memory bandwidth bottleneck?,AMD,2025-12-06 04:31:26,13
AMD,nsk9hv2,AMD as usual are not able to produce enough chips to fill Lenovo or Dell mainstream offers.   This chip 370/365 was available only in two Asus models and it's hard to buy it around the world.,AMD,2025-12-06 07:51:26,7
AMD,nsjs41i,You are going to see an Intel with Nvidia iGPU in handheld before you see FSR 4.0.,AMD,2025-12-06 05:12:39,50
AMD,nsjmmp0,Or any mobile AMD solution at all (laptops began to outsell desktops 20 years ago).,AMD,2025-12-06 04:29:57,13
AMD,nsmlff3,"The workload of FSR4 is pretty “constant” per frame for a given input and output resolution, and it doesn’t care about the original game graphics settings. So a smaller iGPU is going to use too much of the frame time and have too much overhead added.   Navi44 is a 16WGP card.  The Strix iGPU is 8, and Krackan is 4.   For example: At 1080p output, a (14WGP) RX 6700S uses ~3ms to process FSR4, compared to ~2.2ms for XeSS and ~0.7ms for FSR 3.1.5 according to some guys Reddit post.   9070XT (32WGP) was 0.6ms given native FP8.   Even RDNA4 on these iGPU, it would take 4.8ms alone just to run FSR4 upscaling.  Would take your 60fps lower res render down to 47fps. You’d have to get 84fps rendered to get a 60fps upscaled.   That ignores the fact that RDNA4 has physically larger compute units in silicon so it becomes an unfair comparison on an “equal CU” basis.",AMD,2025-12-06 17:58:14,7
AMD,nsj21gd,Isn't there still hope for an official int8 version ?,AMD,2025-12-06 02:09:11,4
AMD,nsr2xb8,"Not really, Intel made a good product with Lunar Lake, but not too many carriers are making it compared to those with AMD chips, and theyve obviously already secured contracts with PlayStation and Xbox.   They only really lost ground to the Switch which already happened.",AMD,2025-12-07 12:31:12,8
AMD,nsjf7q0,"Gorgon Point was always supposed to be a Strix Point refresh.  It's basically improved silicon running at higher clocks, that's pretty much it.",AMD,2025-12-06 03:36:03,18
AMD,nsmc737,">when 890m barely scales with performance beyond even 2ghz due to memory bandwidth bottleneck?  Source for this?   Also, I'm curious, if the source for this also compares how different iGPU archs also scale with additional frequency.",AMD,2025-12-06 17:09:05,3
AMD,nsqmgvu,"It is also getting an upgrade to 8533 mt/s which is sure only a 7% increase from 8000 but since most strix point devices only got 7500 ram, it should help quite a bit.",AMD,2025-12-07 09:52:27,1
AMD,nslu7hf,They are rather easily available and there are even miniPCs available. HX 370/375 overall aren't used in gaming laptops and end up in thin/lightweight pro models.,AMD,2025-12-06 15:32:38,5
AMD,nsqm7ws,"True if you don't count dGPU laptops which is a shame, hopefully that will improve with Gorgon point like hawk point did from phoenix but time will tell.",AMD,2025-12-07 09:50:01,1
AMD,nsjss4i,And ARM+Nvidia,AMD,2025-12-06 05:17:58,9
AMD,nsjolh3,Won't see FSR4 on handhelds and laptops til Zen 7 or even Zen 8.,AMD,2025-12-06 04:45:03,8
AMD,nsodseu,"INT8 FSR4 is only DP4a. It needs a WMMA version to be usable on RDNA3/3.5. The only WMMA INT8 version in development is for PS5 Pro's PSSR 2.0, which is a fork of FSR4's upscaler (and maybe FG component).   Technically, most of the code should be usable on RDNA3, excluding any Sony-specific changes. So, it wouldn't really take much extra work to bring it to PC hardware. Non-zero chance. I remain slightly pessimistic about it.   Because PS5 Pro works more with fixed frame rates and varying resolutions, the upscaler algorithms are markedly different vs variable frame rates and relatively static resolutions (like FSR4 Quality's 67% scale or 1440p -> 2160p). This is the largest issue and why PSSR 2.0 is a full fork of FSR4 and could actually have higher overhead as it tries to handle changing image resolutions. Could make it a superior implementation too, especially if it can maintain high upscaled image quality from varied source resolution frames.  WMMA INT8 is 512 ops per CU per clock on RDNA3.   DP4a is roughly 64 ops per CU per clock (4xINT8 ops packed into 32-wide SIMD x2, allowing for quadruple throughput vs a single INT8 op within SIMD32); unfortunately, INT32 cannot use dual ALUs like FP32 can, else output would be 128 ops/CU - this restriction ends in RDNA5 because INT ops are now important for neural rendering and denoising  So, WMMA INT8 (mixed with FP16 for decimal values) allows for an 8x increase in throughput, which will dramatically reduce time per frame.",AMD,2025-12-06 23:51:58,7
AMD,nsj9xx2,"Considering how much time has passed, highly unlikely at this point.",AMD,2025-12-06 03:00:29,13
AMD,nsjs5sx,Int8 version sucks.,AMD,2025-12-06 05:13:02,-1
AMD,nslkqf5,"wouldn't make a huge difference. the int8 version is visually noticeably worse and it'll still perform worse on RDNA 3 even with int8 I am fairly certain. It would be fair to call it FSR 3.5, but it's not FSR 4",AMD,2025-12-06 14:38:13,0
AMD,nsrn3l3,"I was being a bit hyperbolic.  But there are some different factors at play here.  When the 258v came out, it wasn’t good.  It took a software update fairly late after launch to make it the beast it is.  Which certainly kills momentum from partners to build devices.  This time around, it looks like the next chip will come swinging out of the gate.  Amd won’t have anything new, so there is little reason to refresh devices with them.  But Intels chip will be new, and the rumors are looking very strong.  The biggest difference is the 258v is tsmc.  The chip is super expensive and Intel can’t do much about it.  The next gen chip is built on Intel foundry, and will be far less expensive for them/they could more easily take a financial hit on it.  Amd has nothing until mid 2027.  Sure it’s going to be good and will likely power the steam deck 2 and both consoles, but that’s a year and a half+ away.  I don’t think amd is going to be in trouble or anything, but there is actual competition now.  We couldn’t even speak about a steam deck 2 being reasonable before.  Now it’s totally possible that deck 2 launches with a next gen intel chip.  Not that I think it will, but it’s possible.",AMD,2025-12-07 14:44:55,2
AMD,nsjmp3l,"The other rumour post seems to say even Medusa is rdna 3.5.  Absolutely pathetic if true.   Combined with the rumoured reduction of CU count from 16 to 8, we are looking at ***negative iGP improvement*** in the 15-45w class laptop from July 2024 to whenever Zen 7 mobile comes out maybe in 2028. Insanity.",AMD,2025-12-06 04:30:27,12
AMD,nsrldbg,lol my 890m overclocks from BIOS to 3.1Ghz easily. What’s new here?,AMD,2025-12-07 14:35:03,1
AMD,nsrlz2f,Yeah I keep hearing this but it’s simply false. My 890m goes to 2.9Ghz in games and AI workloads. It even does 3.1Ghz when over clocked from BIOS.,AMD,2025-12-07 14:38:31,1
AMD,nsr2ctn,"ARM+NVIDIA is already out, it's an AI solution. Dell is selling them as AI accelerators. Probably others as well. It runs Linux.    Unless you're referring to the Switch/Switch 2, but I don't see anything else on the market or down the pipe. NVIDIA doesn't seem interested in making a Windows/Linux PC handheld.",AMD,2025-12-07 12:26:24,10
AMD,nsjsoh8,"No ML-based upscaling for Mobility Radeon a full decade after Nvidia.  THAT is courage. Take that, Tim Apple.",AMD,2025-12-06 05:17:10,3
AMD,nskrua0,Medusa Halo (both versions) will definitely support FSR4.,AMD,2025-12-06 10:58:41,1
AMD,nskds9i,Probably more like a CES announcement or something Q1 2026 like that. That's when FSR3 was announced,AMD,2025-12-06 08:34:35,3
AMD,nskri5o,The presentation of the next version of FSR4 is December 10th - they might very well announce FSR4 for RDNA 3.5 then (but if they don’t then FSR4 will likely stay RDNA4+ exclusive),AMD,2025-12-06 10:55:21,2
AMD,nsjsjkb,It's not like any other option for rdna3 sucks any less,AMD,2025-12-06 05:16:05,6
AMD,nskrm51,"Compared to the FP8 version, kind of. Compared to FSR3.x not at all.",AMD,2025-12-06 10:56:27,6
AMD,nskdrrj,That sounds like a skill issue.,AMD,2025-12-06 08:34:27,-3
AMD,nskdzfb,"We had similar IGPU core count drop going from Ryzen 3000 mobile's 12CU Vega to Ryzen 4000 mobile's 8CU Vega, which despite lower CU count performed massively better. I'd say wait and see before we say for sure its gonna be worse.",AMD,2025-12-06 08:36:40,6
AMD,nsqlzh1,"That would be terrible, I heard that it was zen 6 + rDNA 5 for Medusa,   According to MLID, the same sauce that leaked strix halo suggested that medusa halo mini would be the iGPU upgrade to strix point with 24CU rDNA5 and lpddr6, so decent improvement if true in 2027.",AMD,2025-12-07 09:47:41,1
AMD,nsva30r,Instead of having to worry about hitting the silicon lottery you can get a guaranteed 3.1GHz which might actually overclock higher.,AMD,2025-12-08 02:12:16,2
AMD,nt53agh,And have been a thing for much longer than that too.  The tegras have been around for a while now,AMD,2025-12-09 17:06:15,1
AMD,nslksr0,"yes, AMD's skill issue",AMD,2025-12-06 14:38:35,0
AMD,nskehvc,"Vega 8 on ryzen 4000 compensated with drastically higher clocks and while it was not memory bound. It was also not “massively better”, far from it.   This thing is going from 2.9ghz to 3.1ghz all the while 890m already stopped scaling beyond low 2ghz never mind much higher.",AMD,2025-12-06 08:41:58,7
AMD,nskgtom,"It is roughly 20% faster than the older generation, pretty decent if you ask me.  Unless you have a crystal ball or works at AMD I recommend you to wait and see",AMD,2025-12-06 09:05:48,-4
AMD,nskivrj,Yes because an almost 50% clock increase from vega 11 to vega 8 with a node shrink from GloFlo 12nm to TSMC 7nm is comparable to 6% clock increase on the same node.   Some of you don’t even know discussion of rumours has to have _some_ underlying hardware facts to base off of just because y’all speculate without base you assume everyone does the same.,AMD,2025-12-06 09:27:09,9
AMD,nrzi3pu,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-03 00:54:59,1
AMD,ns1o264,I've been holding on for month not upgrading to a 9070xt; I bought one on Monday as I'm pretty sure that's the lowest it'll ever be (£550). Looks like I made the right decision.,AMD,2025-12-03 10:49:37,76
AMD,ns0q1ka,"I'm sure that will translate to a few hundred for us plebs in NZ, lol.",AMD,2025-12-03 05:33:08,86
AMD,ns2n4jm,"Glad my 6800 XT is still kicking ass in the games that I play, I can wait for UDNA.",AMD,2025-12-03 14:45:49,17
AMD,ns0x045,Pc gaming is gonna die at this rate,AMD,2025-12-03 06:29:54,64
AMD,ns6ndv9,Cheering for the eventual downfall of the AI bubble and all who supported it tbh,AMD,2025-12-04 03:09:46,5
AMD,ns0xpul,I wonder when Nvidia will do the same. It might take a bit longer to trickle down from AIB partners who now have to source their own VRAM.,AMD,2025-12-03 06:36:01,9
AMD,ns1uuu0,"Seeing other products like DDR5 RAM pretty much tripling or quadrupling in price, this seems relatively decent",AMD,2025-12-03 11:48:06,12
AMD,ns24ue1,9060XT 16GB was £299 in the sales last week. I grabbed one since that’s the lowest it’s been at so far. It’s already back up to £339 and it was £330 before the sales.,AMD,2025-12-03 13:00:02,3
AMD,ns0kva8,farewell AMD gpus getting more market share.,AMD,2025-12-03 04:54:07,11
AMD,ns8vxqe,Good luck when DDR6 comes out,AMD,2025-12-04 13:56:19,2
AMD,nsabypc,"Last few cycles got screwed crypto mining, current cycle has been screwed by ai, and the next couple are likely to be extremely screwed by ai....sucks.  Next pc cycle overall is going to suck bad as well. from gpus to ram to storage.... /sigh.",AMD,2025-12-04 18:18:35,2
AMD,ns1n4d7,Which means at minimum double that for AiBs.,AMD,2025-12-03 10:40:59,1
AMD,ns5a4d7,"Been thinking about going from my 6900xt to a 9070xt. Not sure if to get one now, or just hold out till my current card bites the dust.",AMD,2025-12-03 22:24:21,1
AMD,ns5whhe,I bit the bullet and ordered a standard 9070. I'm worried if my 6800xt bites the dust. It's getting a bit old at this point.,AMD,2025-12-04 00:29:47,1
AMD,ns5yvw6,20 bucks is 20 bucks,AMD,2025-12-04 00:43:32,1
AMD,nsbbtzs,Bought a RX 9060 XT 16 GB...then a RX 9070 XT.,AMD,2025-12-04 21:16:41,1
AMD,ns2g2ur,"I also just bought a sapphire pulse 9060 xt 16gb , the price was not great, nor terrible. (412 €). The card is tiny, cool and silent, I think I'm set for the next 10 years.",AMD,2025-12-03 14:06:52,23
AMD,ns5w1ow,I just hope my 6700xt won't die for next 4-6 years because I feel like yet another madness is coming.,AMD,2025-12-04 00:27:14,7
AMD,nsg5760,I'll probably only change my GPU when the AI bubble pops.,AMD,2025-12-05 16:36:32,2
AMD,ns51ugs,congratulations,AMD,2025-12-03 21:43:20,1
AMD,ns68yoi,Ditto. Just bought my 9070xt from microcenter this morning,AMD,2025-12-04 01:43:57,1
AMD,ns7pb57,Yeah panic buying is the right decision!!,AMD,2025-12-04 08:04:27,-2
AMD,ns4ajue,"I'm in the same boat. Waiting for UDNA/RDNA5 and Zen6 to build a new PC. How things are going, this might be mid 2027.",AMD,2025-12-03 19:31:34,8
AMD,ns1lptl,Soon it will be pc renting,AMD,2025-12-03 10:27:46,39
AMD,ns1tffq,Yeah it will. Now cant wait for people to start selling their PC components for 50-70% lower price so I can buy them myself,AMD,2025-12-03 11:36:32,8
AMD,nsbzo60,"The thing about PC gaming is that it continues to work even when people can't get new graphics cards.  The ones already out there continue to work, and plenty of new games work perfectly well on them.  It's already weathered two separate periods of difficulty in getting a graphics card (one fueled by crypto currency insanity, one fueled by a pandemic and topped off with more crypto insanity).    The neural machine learning bubble will pop at some point.  Even the RAM manufacturers know this, which is why they aren't investing in increased capacity.  Once that happens, sanity will start trickling back down into the things we're interested in buying.",AMD,2025-12-04 23:23:20,2
AMD,ns15tf9,That's a little bit overreacted. You can play the most popular games on a mid laptop. Most player doesn't need new PC.,AMD,2025-12-03 07:50:01,9
AMD,ns25fw6,just like it died during covid and bitcoin lol,AMD,2025-12-03 13:03:52,3
AMD,ns4c7wq,oh no my $610 9070 XT is gonna be $650 now the PC market will never recover from this,AMD,2025-12-03 19:39:46,-8
AMD,ns1ri35,"They won't, they stopped supplying VRAM to AiBs with the GPU so that they can say it's not their fault and not change the MSRP.",AMD,2025-12-03 11:20:09,7
AMD,ns1nitu,"Yeah, once that kicks in prices will jump and the funny part is nvidia probably didn’t lower their price to the aibs. That’s how they already raised their price.  What’s worse is nvidia got bulk rate cuts because all previous aib vram was bulk purchased by nvidia, now the individual aibs have to source their own at much smaller orders meaning bulk purchase discounts evaporate and nvidia gets to blame the aib.   Slick on their part.",AMD,2025-12-03 10:44:41,15
AMD,ns1uvpw,they already arent supplying their gpus alongside memory anymore which means the AIBs have to find their own memory,AMD,2025-12-03 11:48:19,4
AMD,ns2n2r5,"They're not raising the retail price, AIBs will pay 20-40 dollars more, customers will pay like 50-100 more in order for AIBs to maintain the same margins.",AMD,2025-12-03 14:45:32,14
AMD,ns3okeh,I can't wait for this AI bubble to burst.,AMD,2025-12-03 17:47:29,5
AMD,ns21u54,"They don't care. Neither does NVIDIA. 90% of NVIDIA's revenue is from AI. AMD was late to the game and sits around 45% and growing. Hyperscalers are willing to pay 100x per a teraflop. If AMD had their way, they'd be at 90% as well.  If AI doesn't implode, both would happily exit the consumer market entirely.",AMD,2025-12-03 12:39:59,16
AMD,ns0w7b4,:),AMD,2025-12-03 06:23:06,-3
AMD,ns69ubp,"dude i just bought the Asus Prime version from Amazon US during Black Friday, $300, i think it would be the lowest i could ever see.",AMD,2025-12-04 01:49:15,5
AMD,nsghq5h,"Just change when you really need to. My 3070 was starting to really struggle with 8GB of VRAM as I'm running UW 3440x1440, I had a good excuse!",AMD,2025-12-05 17:37:57,1
AMD,ns7ugii,"A few days later, the card is now £50 more expensive on OCUK, and I wanted to upgrade. My 3070 is starting to struggle in ultrawide.  The impulse won this time...",AMD,2025-12-04 08:55:26,2
AMD,nsabin4,It was slated to Q2 to late 2027 before this mess according to leakers. Now it might not even ship in 2027 :(  Really hope this mess ends at some point. Impossible to launch a new GPU without severe compromises in this current market.,AMD,2025-12-04 18:16:28,2
AMD,ns34kkn,That's the goal. They want everyone to stream games. It's rent-seeking.,AMD,2025-12-03 16:11:41,16
AMD,ns1rod0,Just wait until those mid PCs prices double.,AMD,2025-12-03 11:21:37,27
AMD,ns41a7g,Are you saying that there's still room for the prices to increase?,AMD,2025-12-03 18:47:24,3
AMD,ns3j9ho,Its been fukd since mining shite happened.,AMD,2025-12-03 17:21:49,16
AMD,ns4x7mf,People with no foresight be like:  but for real though this is not about AMD raising GPU prices without any context. RAM prices doubling or even tripling already in just a few months and now Micron is pulling out of the consumer side completely... you don't need a crystal ball to know prices are going to keep getting worse and not just this one time raise.,AMD,2025-12-03 21:21:09,14
AMD,ns1qhur,"I'm wondering whether Nvidia is still making the bulk vram purchases, they're just funneling it towards AI cards now rather than bundling it with GPUs for AIBs. If so the situation is even worse...",AMD,2025-12-03 11:11:20,3
AMD,ns45jr8,"AIB's will only maintain the same margin if customers are willing to pay more though. If they raise prices and no one buys, they'll have to come down.  I think prices will go up, but not by 50-100 bucks.",AMD,2025-12-03 19:07:23,3
AMD,ns24dao,I am affraid that AI bubble is the same as housing bubble. wish thing were different,AMD,2025-12-03 12:56:59,5
AMD,nsar4qf,"AI bubble popping doesn't mean what people think it means. It doesn't mean most go out of business and the consumer markets get flooded with products. It means most go out of business and the remaining ones buy up and consolidate all the datacenters and continue operating them and building more. AI isn't going anywhere. Hell, cloud computing is also going to accelerate and those too need lots of datacenters.  The days of affordable gaming died 4 years ago.",AMD,2025-12-04 19:32:02,1
AMD,ns6cayl,"Tbh the amazon price was like 30 € cheaper, but with a 6week+ delivery estimate, so I just ordered it from a local store.  I should have bought it 2 months ago when there was a meaningful sale, but it is what it is.",AMD,2025-12-04 02:03:51,2
AMD,ns4bx89,They've gotta find some use for all those GPU server racks 😅,AMD,2025-12-03 19:38:19,11
AMD,ns75yjl,Yet it's never been more popular. It's clearly not fucked.,AMD,2025-12-04 05:16:27,1
AMD,nsaq4jz,"From what I understood was that Micron is just pulling Ballistix and Crucial which is their direct to consumer lines. Instead they'll be going strictly B2B. There is nothing stopping them from selling their RAM and NAND to other consumer facing companies. For instance...they can still sell VRAM directly to MSI or Sapphire (although I'm pretty sure Micron doesn't really sell consumer GPU VRAM), sell NAND directly to Muskin, or RAM directly to Gskill.",AMD,2025-12-04 19:27:01,1
AMD,ns5wl5k,>If they raise prices and no one buys  I think its provent at this point that they are raising the prices are people are buying. Hell maybe they are buying more because they are constantly afraid that it will be more expensive.,AMD,2025-12-04 00:30:22,7
AMD,nrvomno,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-02 13:11:46,1
AMD,nrvq6q8,"I'm not even making bad money but at this point I just.... don't want to spend anymore on a PC like what the fuck... I should start some cheaper hobbies, like collecting and playing warhammer figues.... or beeing crack addicted...",AMD,2025-12-02 13:21:36,607
AMD,nrvs3ko,"Welp, I'm on a 5 year upgrade cycle. With all these shenanigans in the industry, might just have to lengthen it to 10 years. Good luck everybody.",AMD,2025-12-02 13:33:22,178
AMD,nrvsz4y,My plans of upgrading my i7 8700k to a 9800x3d have been blown out the window since the RAM prices exploded. Now even more. Here's hoping my current CPU stays alive for 2-3 more years somehow although I am starting to doubt its viability in games within the next 2-3 years despite it handling MonsterHunter Wilds better than I suspected.,AMD,2025-12-02 13:38:44,68
AMD,nrvvys3,I hope my 5800x3d holds up for a few years more. Prices are already ridiculous. I saw 16gb of RAM at microcenter going for over $250!,AMD,2025-12-02 13:56:28,36
AMD,nrvw4yf,Every company using Tomahawk cruise missiles to torpedo PC gamer's dreams.,AMD,2025-12-02 13:57:28,26
AMD,nrzgziu,"They're all forcing us to use cloud services intentionally, where they heavily invested.",AMD,2025-12-03 00:48:41,12
AMD,nrvvpzt,I just upgraded to 7800X3D from 3600 last August. I dodged a freaking bullet 😭,AMD,2025-12-02 13:55:03,33
AMD,nrvw51v,Please rise 5800X3D. Just make it available again.,AMD,2025-12-02 13:57:29,18
AMD,nry1iaz,All these price jumps it feels like I’m back in my third world country Venezuela with 1million inflation.   It was cheaper to buy in credit now that saving.,AMD,2025-12-02 20:20:27,9
AMD,nrw9vdc,"I’ve already decided that if things stay the way they are now, I’m just going to go the PlayStation/Nintendo route. I just can’t justify overhauling my 3800X & RTX 3080 build at these prices. The AI bubble needs to hurry up and pop.",AMD,2025-12-02 15:13:49,9
AMD,nrwssk2,This is probably one of the reasons why retro games have been gaining popularity recently...,AMD,2025-12-02 16:45:59,4
AMD,nrxzg6w,Everyone who constantly says they are going to wait to upgrade are the ones getting screwed.   The takeaway here is that life is short and if you want to upgrade your computer you should do it. Don’t wait for 5 years with bated breath that it’s going to be suddenly “worth it”.,AMD,2025-12-02 20:10:19,11
AMD,nrw6xed,welp spent money on a b650e during black friday was hoping cpus wont have a price increase will still go for a single stick of 16gb and a 7800x3d fingers crossed bois,AMD,2025-12-02 14:58:04,3
AMD,ns2dn2j,… luckily nothing happened,AMD,2025-12-03 13:52:52,3
AMD,nrvsm93,My 7500F looks more and more like a steal     [https://imgur.com/a/VTdCfvK](https://imgur.com/a/VTdCfvK),AMD,2025-12-02 13:36:32,7
AMD,nrw1kf8,Just bring back am4 x3d chips.,AMD,2025-12-02 14:28:33,5
AMD,nrwdnnc,"*Patting my 5800X.*  *- You will stay with me until you die, buddy.*  I wanted to get 9800x3d once 11800x3d is released.",AMD,2025-12-02 15:32:56,4
AMD,nrwdpy0,raising the MSRP I guess  So those CPUs with already noticeable discounts vs MSRP should still have those discounts,AMD,2025-12-02 15:33:15,2
AMD,nrwfjb9,"Got my 7800X3D last month for $320 when I upgraded to AM5. Also got my RAM for $167 when it started to go up. Yeah, I overpaid, but right now that same kit is over $400, so I dodged a couple of bullets. Even upgraded my GPU, too.",AMD,2025-12-02 15:42:12,2
AMD,nrwfrda,Got 9800x3d and 32gb ddr5 for 700 bucks. Considering the circumstances i suppose i made the right choice,AMD,2025-12-02 15:43:18,2
AMD,nrxgy3n,I'll stick with my 5800X3D for another few years then.,AMD,2025-12-02 18:40:56,2
AMD,nrw6vi0,Might as well ditch my upgrade path and get a prebuild like a mac or a laptop.,AMD,2025-12-02 14:57:48,1
AMD,nrwoxqs,"And here I am, had to sell my PC to pay some emergency bills a few months ago, just got a new x870 mobo and still saving to get a processor and ram... Maybe in February I get at least one of those lmao",AMD,2025-12-02 16:27:36,1
AMD,nrwqxuy,Why when most people can't even build a pc with ram being crazy. Mental decision if true.,AMD,2025-12-02 16:37:13,1
AMD,nrwz970,"I just bought the 7500x3d making the jump to AM5 replacing a 3600 system. If they keep doing this shit, next CPU will be Intel",AMD,2025-12-02 17:17:04,1
AMD,nrwzu5n,my 5700x and 32bg ram will last for a long time,AMD,2025-12-02 17:19:52,1
AMD,nrwzza6,No they wont. Not with RAM prices killing sales.,AMD,2025-12-02 17:20:33,1
AMD,nrx2ah7,It's honestly becoming increasingly difficult to be convinced to buy a PC if every year there are several price increases and/or shortages.,AMD,2025-12-02 17:31:40,1
AMD,nrxedhg,They're all doing that. Black Friday prices aren't supposed to be the new normal.,AMD,2025-12-02 18:28:44,1
AMD,nrxk40d,Just ordered 9800X3D today. Nice.,AMD,2025-12-02 18:55:48,1
AMD,nrxld4z,Do you guys thinking upgrading from an 11400f to a 7600x3d is a good idea? I do have a gift card for 200 that I won and can use for some ram but I am debating whether it'll be a good performance jump or just stick to the 11400f till all this dies down,AMD,2025-12-02 19:01:44,1
AMD,nrxqyp7,Literally just upgraded from an r7 1700 to a 5800x and a rx 5700 to a 9060xt lol I just knew things were about to get bad.,AMD,2025-12-02 19:28:56,1
AMD,nry3eui,My wallet is closed for the next few years..,AMD,2025-12-02 20:29:44,1
AMD,nrylnay,The famous CPU vram chips.,AMD,2025-12-02 21:57:26,1
AMD,nryo0yo,I built my new machine 6 weeks ago. Feeling pretty lucky. My 96gb 6000mts ram feels pretty luxurious now.,AMD,2025-12-02 22:09:07,1
AMD,nrz5uol,"Recent news totally killed my interest in the PC market. I’m in Taiwan, the place that literally manufactures all the high-end chips, yet somehow our prices shoot up even more than other countries whenever there’s a price hike. Make it make sense.",AMD,2025-12-02 23:45:50,1
AMD,nrzqn9y,"I'm still going with my 5800 X 3D just got a 9070 XT, got it for $629 bucks.  Will sell my 6800 XT still have the box and everything for about $300 or so will cost me about $350 for about 50% extra power and can  inject fsr4 now into most games with opti-scaler.  Undervolted the 5800 X 3D temperatures run a little bit warm now got it down to the mid 60s at load crazy with a little bit of undervolting can do.   In a few years will upgrade then we'll sell the beautiful 5800 X 3D and RAM 32 GB to upgrade to a new CPU and motherboard.   As long as you have this AI scam running, we're doomed hard drives ram gpus, really this past week was the time to buy maybe there's still some time left if some places have some stuff.   Sorry for the long rant.   I use voice dictation, but just want to say people if you're going to upgrade your video card or anything besides the memory best to do it now.   I cannot stress enough for people who do have the 5800X3D, undervolt your temperatures will drop minimum 10°, but probably even more depending on your cooling.   I don't remember the numbers I used but I use an MSI motherboard I updated the bio so it's easier to do.   Not messing with the clocks because you can't with this cpu you're just doing some offsets!   Good luck to all!",AMD,2025-12-03 01:45:05,1
AMD,nrzsvfs,"I just glad I upgraded when all this released back in November 2024 and 5080 early 2025. Everything is bass ackward as usual. Also nothing wrong going into the console ecosystem, just to separate yourself from the PC madness.",AMD,2025-12-03 01:58:22,1
AMD,ns009n4,"Lucky for me I dodged a bullet, in the form of a tank! If I bought my PC 2 months later than I did, I would be in the worst possible price. Thank the Almighty all my used parts are working in perfect condition still",AMD,2025-12-03 02:41:22,1
AMD,ns00maa,tomorrow morning,AMD,2025-12-03 02:43:23,1
AMD,ns0iz8x,"As if BS MOBO prices isn't bad enough, AI boom made RAM super expensive. Why not increase CPU to complete the trifecta right?",AMD,2025-12-03 04:40:42,1
AMD,ns0lcwc,My i7 8700 is still holding strong,AMD,2025-12-03 04:57:43,1
AMD,ns1f66p,Recession is also incoming. You guys please stop spending money on things you dont need. It will be bad.,AMD,2025-12-03 09:22:38,1
AMD,ns1in5j,I guess Im sticking to my upgrade cycle.   BF6 highlighted that my PC needed upgrading since I had a hard time staying on 120-140fps in 1440p which for me is the limit. Just being on 160-180 is much more pleasant so I spend more than I wanted this time.  Hopefully it will see another 5 years or so before upgrading. Skipping a generation of GPUs on Nvidia is a no brainer but hopefully the 7000 or 8000 series delivers in spades.,AMD,2025-12-03 09:57:19,1
AMD,ns1ydc3,I'm considering becoming a console gamer :(,AMD,2025-12-03 12:15:09,1
AMD,ns2ec85,Yup 20% price hike on most electronic hardware prices next year.,AMD,2025-12-03 13:56:52,1
AMD,ns34q1d,We ain't buying anything that's overpriced. Just hire more to clean dust,AMD,2025-12-03 16:12:25,1
AMD,ns3jxm9,People keep paying. Companies have no reason to stop raising prices.,AMD,2025-12-03 17:25:03,1
AMD,ns4az93,"I mean... If you buy OEM tray version of any AM5 CPU on Ali they are already dirt cheap compared to retail... Even if they rises the CPU price they're still cheaper than Intel...    Just two days ago for black Monday you could get 7500F for around 80-90 $/€, 7700 for 130 $/€, 7800x3D for 220 $/€ and 9800 x3d for 330 $/€.    What is decimating PC builders are RAM prices and abysmal GPU price/performance ratio...",AMD,2025-12-03 19:33:39,1
AMD,ns4tsri,I'm not playing triple a titles because alof of them suck so I'm fine with 10900k. I can wait a few years.,AMD,2025-12-03 21:04:58,1
AMD,ns7yxrs,So my 9950x3d will only appreciate 🔥🔥🔥,AMD,2025-12-04 09:41:00,1
AMD,ns7zsoi,"5800x3d owners unite, it's not that bad when games aren't limited as much by your cpu vs gpu",AMD,2025-12-04 09:49:27,1
AMD,ns9e79j,GPUs.  Hard drives.  Memory.  Why not CPUs now?  Next it will be motherboards and power supplies.  I'm honestly waiting for all consumer production to stop so they can all suck on the AI tit.,AMD,2025-12-04 15:35:03,1
AMD,nsa1c30,"Seems like this isn't happening, at least not now [https://videocardz.com/newz/amd-reportedly-increases-gpu-pricing-10-per-8gb-ryzen-cpu-price-hike-unconfirmed](https://videocardz.com/newz/amd-reportedly-increases-gpu-pricing-10-per-8gb-ryzen-cpu-price-hike-unconfirmed)",AMD,2025-12-04 17:27:20,1
AMD,nsd2yo5,"My son asked me if we could build a pc for his bday, I said sure without realizing how stressful its been choosing parts! He said we could build one for 500 but im already at nearly 1k! Anyways the money is whatever im just not sure if I can even build it! I bought I've never been into computers. Anyways I bought him a ryzen 7 5800xt paired with a msi b550 tomahawk max and a gigabyte radeon rx 9060xt it will be going into a corsair 3500x argb case I ordered an 850w power source and just need a cooler or fan of which I have no idea what is compatible or how to install any advice is appreciated! Im willing to spend 150 on a aio or fan!",AMD,2025-12-05 03:15:08,1
AMD,nsni48n,But how does this affect me exactly?,AMD,2025-12-06 20:51:44,1
AMD,nso3j5l,Is the price increase in the room with us right now 6th Dec 2025 10:51pm GMT? I guess not unless it's America only.,AMD,2025-12-06 22:51:28,1
AMD,nrvxylk,"I bought R7 5800x, 2*16gb cl16 3600mhz, RX 6900 XT, Asus Prime x570-p mthb and 2tb pcie4 SSD in 2020, and I don't feel like I need an update any time soon. I recently switched my GPU to overclocked bios and transitioned to Bazzite OS and the performance got even better. Thanks to Goverlay I'm now able to use FSR4 in any game I need so there is absolutely no reason for me to update, especially considering the prices. I don't think I will need update in another 3 years.",AMD,2025-12-02 14:07:59,1
AMD,nrw5b34,I am glad I upgraded last year. I thought I was paying the early adopter tax but I sure was wrong. Sucks for those who wanted to build a PC this year though.,AMD,2025-12-02 14:49:20,1
AMD,nrw1hvb,"Not to get super political (I know mods, but it's relevant - hear me out) - the reason that companies engage in this ""chasing margin"" behavior is because of right-wing low-tax policy.    There's no disincentive (in fact, there's a huge tax incentive) to raise prices.  If you want to make a billion dollars, there are two ways to do it - sell a billion things with a $1 profit margin, or 1 this with a $1 billion profit margin.  The structure of the taxes determines which is 'easier' or more highly incentivized.    Our tax system right now awards some of the lowest tax rates to the higher income earners (corporations and very wealthy individuals) - thus it is favorable to raise prices and sell fewer copies of each item.    This is why inflation has been so bad and uncontrollable - because there is a strong incentive to raise prices.",AMD,2025-12-02 14:28:09,-3
AMD,nrvpbka,[https://overclock3d.net/news/cpu\_mainboard/amd-price-increases-confirmed-for-midnight-tonight/](https://overclock3d.net/news/cpu_mainboard/amd-price-increases-confirmed-for-midnight-tonight/)  confirmed,AMD,2025-12-02 13:16:09,-8
AMD,nrw0wgz,What a headline.  ![gif](giphy|MBV0c80qHJVGbPzcp3),AMD,2025-12-02 14:24:47,0
AMD,nryupqx,GPU's and now the rest. Turning into Intel 2.0.,AMD,2025-12-02 22:43:42,0
AMD,nryvbv0,Killing their own market at the worst time,AMD,2025-12-02 22:46:58,0
AMD,nrwhxzr,Just buy the PS5 pro fellas. I'm playing battlefield 6 at 100fps.   It's not the end of the world if you can't game at 144fps.,AMD,2025-12-02 15:53:57,-2
AMD,nrvukt3,"Thank goodness I got my 7600X for $150 earlier this year, I sold my previous 5700X3D for $250 before that. it's looking like a same feeling of dodging the bullet as I got my Trident Z5 Neo 32GB DDR5 kit for $80 earlier this year as well, totalling the cost of my switch to AM5 platform at just around $320 / $90 for the B650 motherboard, and then I ended up selling my previous gen AM4 platform for total of $340, so, my switch to modern AM5 / DDR5 practically didn't cost me anything.",AMD,2025-12-02 13:48:22,-2
AMD,nrvtiva,"Literally JUST upgraded my R5 3600 to an R7 58XT, now I feel like I’m being strong-armed into blowing my budget for a quick am5 grab to save me money",AMD,2025-12-02 13:42:01,-4
AMD,nrvrbqg,"True, i just wont buy hardware till the prices are back on a normal level",AMD,2025-12-02 13:28:37,114
AMD,nrwc9lq,Or buying property lol,AMD,2025-12-02 15:26:01,27
AMD,nrwek9s,"Well at least we're still not even approaching 80s and 90s pricing. 1000 dollar Pentiums were the norm, back in 1993.  It can get way way way worse. Hopefully it doesn't",AMD,2025-12-02 15:37:25,13
AMD,nrwq7s1,It's a great time to keep our current machines and start working through that steam backlog.  How many of us are actually trying to play cyberpunk at 4k on a daily basis?,AMD,2025-12-02 16:33:44,9
AMD,nrx82kv,My i7700k lasted 8 years. I'm on 7950X3D. I expect it to last the same at least.,AMD,2025-12-02 17:59:08,7
AMD,nrw1ewn,Have you checked out some model train sets?,AMD,2025-12-02 14:27:41,19
AMD,nrwe9kh,Photography is fun! And at this point maybe cheaper,AMD,2025-12-02 15:35:57,10
AMD,nrxjbo6,This is not AMD raising prices. This is prices going back to normal after Black Friday is over. They even say so in the article that I guess no one read.,AMD,2025-12-02 18:52:05,10
AMD,nrxz6mv,"Prices will continue to rise, if you want something you should just buy it imo",AMD,2025-12-02 20:09:01,3
AMD,nrwidjk,Try r/wallstreetbets,AMD,2025-12-02 15:56:01,2
AMD,nrwm3b8,"Golf is a fairly expensive hobby.  The difference is it's a slow burning of a hole in your pocket, it's not all at once.  My best friend's yearly golf expenses have made me feel better about many purchases.  I've been stocking up on RAM and Storage the last month.  Just picked up a 5090FE @ MSRP as well.  Outside of a dual-CCD CPU with dual 3D V-Cache, I think I'm done for a few years.",AMD,2025-12-02 16:13:59,3
AMD,nrz57ug,Could start Magic the gathering. It’s more expensive than cocaine,AMD,2025-12-02 23:42:07,3
AMD,nrycvzw,"""you save so much money PC gaming with game sales and no subscription costs!""  ok but who wants to spend 1k to *get started*, and thats with an Intel graphics card, with all the driver compromises that comes with.  the starting cost for PC gaming puts it squarely into rich person's hobby tbqh.  I'm also starting to think that actually that whole PCMR line is entirely bullshit with how little my couple console friends spend on games they keep buying+flipping on FB/ebay/offerup/etc  Or rather maybe the play is to only ever play GaaS games on PC so you avoid sub fees. But then you're still spending 1k on a PC to play those sooooo",AMD,2025-12-02 21:15:44,3
AMD,nrvwn6a,And that's where the GabeCube comes in,AMD,2025-12-02 14:00:22,1
AMD,nrx0wdd,I started collecting flat caps lol,AMD,2025-12-02 17:25:00,1
AMD,nrx3u3c,"If you are already sweating the pricing of PC parts, you already cannot/should not afford Warhammer figures, especially if you actually get into the game itself. Unless you buy a cheap resin 3D printer and navigate dealing with off gassing/toxic chemicals.  Probably easier to stick with crack.",AMD,2025-12-02 17:39:11,1
AMD,nrxmzz7,I'll be making my 5950x system last for the foreseeable future. Might grab surplus stock when this AI bubble bursts.,AMD,2025-12-02 19:09:37,1
AMD,nrxxxxv,I was dnd. Yea its just as expensive.,AMD,2025-12-02 20:02:56,1
AMD,nryiat8,So now i see the appeal of gabecube for pc players huh,AMD,2025-12-02 21:41:28,1
AMD,nrylzd4,lol you will spend alot more on a crack addiction which will include selling the TV and the computer to buy more.,AMD,2025-12-02 21:59:03,1
AMD,ns0588w,Get a retroid pocket G2 and stick with PS2 era. You'll save thousands and the games are better,AMD,2025-12-03 03:10:52,1
AMD,ns0c1zl,What about limited run group buy Mechanical Keyboards?,AMD,2025-12-03 03:53:41,1
AMD,ns10k4y,Crack might be a cheaper hobby. lol,AMD,2025-12-03 07:00:47,1
AMD,ns19ho2,Hahaha def. cheaper hobbies :D,AMD,2025-12-03 08:25:32,1
AMD,ns1ix2e,This right here. The more prices raise the more I stop spending. I went from I wanna upgrade to maybe i should wait to my 5900x and 3080 is my forever pc,AMD,2025-12-03 10:00:02,1
AMD,nsg2wah,why are you spending more? it should always be if you need to upgrade or not...don't let fomo clear out your wallet,AMD,2025-12-05 16:25:20,1
AMD,nry24ej,"Just checked the RAM prices, while it definitely sucks don't see how someone's plan to upgrade once in 5+y changes because ram is 50-100e more. GPU shortage was way worse, like multiple times worse",AMD,2025-12-02 20:23:25,1
AMD,nrwfcpz,Imagine how much crack you can buy for 3k usd,AMD,2025-12-02 15:41:20,1
AMD,nrz9hxb,"Same, I've stopped buying so many things just because. I make more money than ever now and can afford these hobbies, but I feel like participating in this nonsense just means enabling worse behvior later, so I have bailed on a lot of things. Went from wanting to do frequent upgrades to my PC to making it last 5-6 years for no reason other than displeasure over the market.",AMD,2025-12-03 00:07:07,1
AMD,nrwex1w,"The key is not to buy components at the height of their prices. I waited 10 years to upgrade my CPU, and I can wait another 10 if I need to with the 7800x3D. Instead I got myself a 27 in 240 Hz QD-OLED on sale for $400.",AMD,2025-12-02 15:39:10,-1
AMD,nrvsnlz,I waited 12 years before my upgrade in 2024.,AMD,2025-12-02 13:36:46,51
AMD,nrwg6dg,"I was on a 5 year upgrade cycle too, but due to Covid-flation, crypto, war in Ukraine, and now AI, all I could do is swap out my GPU and call it a day.  At this rate, I'm also on a 10 year upgrade cycle too.",AMD,2025-12-02 15:45:21,6
AMD,ns0flls,I went from an i7-4790k to a 9800X3D. Waiting is no issue.,AMD,2025-12-03 04:17:10,2
AMD,nrxd7uz,My phenom II lasted until the 3700x. Played MGS V great. I'm going to wait for am6 anyways. Am2 to am4 to am6.,AMD,2025-12-02 18:23:16,2
AMD,nryxehr,"Was getting ready to finally upgrade my 1950X, yeah, might have to tough it out a bit longer.",AMD,2025-12-02 22:58:06,1
AMD,nrz5ylo,"Go invest on big insane cooling systems and starting to develop scripts to make perfect for overcloking for each component adapted with control measures, like metrology but only for cheap or old conf'",AMD,2025-12-02 23:46:28,1
AMD,nrzr1go,I've already decided that I'm skipping DDR5 entirely.,AMD,2025-12-03 01:47:25,1
AMD,nrvwita,"I already purchased everything on discounts over the last month or so except for the CPU and motherboard. I was actually going to wait until CES early January because of the 9850X3D announcement to decide. I didn't expect with everything else going up that AMD were going to fuck us too, because it makes no sense to me at all.  In fact I thought CPU prices might go down because DEMAND will go down due to the price increase of everything else. But clearly they now see it as ""oh if people can afford these RAM prices, then they can afford the increased CPU prices too""... Basically they know whoever is left in the PC market purchasing at this point, that they can somehow afford all this insanity.  FFS! Now I don't know whether to buy tonight after skipping the Black Friday/Cyber Monday specials, or just go with my original plan and wait until January to see how everything is going and the new rumoured CPUs.",AMD,2025-12-02 13:59:40,19
AMD,nryaz0f,"Not trying to rub it in, but.... I went from a delidded 8700K to a 7800X3D last year. Massive improvement in framerate and all games feel smoother with less hitching or frame drops.",AMD,2025-12-02 21:06:29,4
AMD,nrxkem6,I pulled the trigger today. I fear that RAM prices will keep rising.,AMD,2025-12-02 18:57:10,1
AMD,nrwx4xb,I made the upgrade about 2 weeks before prices exploded. Got 2x16GB DDR5 6000 for £86. I wasn’t sure I was making a sensible purchase but thought fuck it. It’s not often I time the market.,AMD,2025-12-02 17:06:51,1
AMD,nrwdo0o,you had a whole year to do this...,AMD,2025-12-02 15:32:59,-4
AMD,nrwccls,"Yes, my 5800x3d+9070xt+4kOled will carry me through this shit too",AMD,2025-12-02 15:26:26,6
AMD,nrvymwl,"My 5800X3D / 4070 Super combo plays everything maxed out, Performance DLSS at 100 FPS or more, even on ultrawide 3440x1440.  They were probably two of the best purchases I've ever made for PC hardware.",AMD,2025-12-02 14:11:52,16
AMD,nrwvamd,"Well seem like my 5700X3D 4080 combo will serve me at least 3 more years,",AMD,2025-12-02 16:57:54,3
AMD,nryprj8,"> I hope my 5800x3d holds up for a few years more.  Damn, and my plan is to upgrade to 5800x3d in the future.",AMD,2025-12-02 22:17:55,2
AMD,nrxgsvf,I bet it'll be very similar to next gen console CPUs so you're likely to be fine.,AMD,2025-12-02 18:40:15,1
AMD,nrvzgs6,$250 for 32gb you mean,AMD,2025-12-02 14:16:37,-1
AMD,nrzprr6,"that's the endgame, make personal computing too expensive and force us all to subscribe to cloud computing where they control and scrape our data",AMD,2025-12-03 01:39:53,10
AMD,nse0evp,Noone is going to play that garbage.,AMD,2025-12-05 07:29:57,2
AMD,ns3vj9p,"I have 32 gb of ram which is enough for gaming, but now I wish I had bought 64 gb to future proof better because I don't know how long this rigged AI bubble will last.",AMD,2025-12-03 18:20:36,3
AMD,nrvzw42,Doubt it ever will.  I just bought a 5700x3d on the used market (eBay).,AMD,2025-12-02 14:19:02,11
AMD,ns6k3gq,I keep plenty busy with retro games.,AMD,2025-12-04 02:49:45,3
AMD,nrzthn2,"It's not that, we're not waiting for things to improve. It's that PC gaming is no longer worth the expense and we're giving up on it.    I'm still on a R5 3600 and now it's a choice between paying $3000 for a new system or just not playing modern AAA games anymore. I choose the latter, as the gaming time I'd get for that much money is not really worth it. I'll spend it on other stuff instead.   If my PC dies maybe I'll get a steam machine or something. It sounds like it's gonna be $800 ish dollars which sounded too expensive for what it is but at this rate it'll be great value next year.",AMD,2025-12-03 02:01:56,11
AMD,nse1iqu,"My question to AMD/Intel is ""Who is going to buy these CPU""s you are increasing the price of when DDR5 ram is increasing daily?"".    I think the CPUs are going to drop by quarter 3 2026. If AMD was smart they'd start producing more 5700x3d. If Intel was smart they'd keep making their next CPUs ddr4 compatible.",AMD,2025-12-05 07:40:51,1
AMD,nrvvsrl,I got a 7700 for 154€,AMD,2025-12-02 13:55:29,2
AMD,nrxz8gj,I’m already on the AM5 platform with a 7900x. I’m hoping it lasts long enough for me to get a 11950x3d when they start the transition to the next socket so I can cash in on a discounted CPU. I built my entire setup last year for less than the ram kit would cost today.,AMD,2025-12-02 20:09:16,1
AMD,nsai8of,"It has been 24 hours, no real confirmation yet",AMD,2025-12-04 18:48:25,1
AMD,nrxz01a,probably to set the stage for a next gen price increase,AMD,2025-12-02 20:08:09,1
AMD,ns4bumh,"How's the 7500x3d going? I'm planning on getting that myself to replace my own 3600 system, as it's just been released in Australia, but there's not a whole lot about it online.",AMD,2025-12-03 19:37:58,1
AMD,nse5l5h,Exactly.,AMD,2025-12-05 08:20:35,1
AMD,nryy6mx,It's going to take at least 2 years for things to calm down.  Can you hold out for that long on a dead platform?,AMD,2025-12-02 23:02:21,1
AMD,ns7xr20,you cant put an amd cpu on a intel motherboard,AMD,2025-12-04 09:28:52,0
AMD,nse5w49,I just put a 360 aio on it. That CPU needed it. It ran hot with a big fat air cooler on it. It runs cool with the aio.,AMD,2025-12-05 08:23:40,1
AMD,nse62h7,"Until noone buys them. Then they""ll be bundling them up. Newegg already doing it with ddr5 ram and b650 mobos",AMD,2025-12-05 08:25:27,1
AMD,nse6dlb,"I'm not so sure about that. We live in an asset inflated economy. The ""crash"" is going to be overvalued assets. It is going to harm rich people. Commercial real estate, probably all real estate, investors, bankers, etc. They""ll scream and yell and cry, but all things must come down. It will kill 401ks though.",AMD,2025-12-05 08:28:33,0
AMD,nse6m7m,Power supplies are already pricey. They used to be like $50.,AMD,2025-12-05 08:31:00,1
AMD,nse74xs,"Don't buy expensive fans. There are so many kits of solid fans for like $30-40. By kits I mean 5-6 fans. Just gotta watch some review videos to avoid the bad ones. I look for higher cfm fans for my case. CFM is basically how much air they push. A lit if cheap pretty fans have cool rgb, but low cfm. The trick is finding a cool looking fan with higher cfm.    The aio will come with its own fans that deal with static pressure, etc. Basically, leave those fans on the aio. I just bought a cooler master core 360 aio for $60 on Amazon. It works well. Just don't get upsold on all the different aios and fans out there. They aren't worth it.   For that cpu, I""d go air cooler though. They are simpler, cheaper, less to go wrong. Just make sure the cpu cooler height is lower than the case CPU cooler height. It will be listed on the Corsair case specs. The rest is easy. It just goes on the CPU. Intake fan on front blowing air into the metal hunk, exhaust fan pulling air off of the cooler towards the rear of the case blowing air away from the cooler.   And for fan placement. Intake on the front, hopefully the fan has an arrow on it. The fan arrow shows you which way the intake is flowing, so arrow pointing in. Exhaust fans on the top of the case blowing out.",AMD,2025-12-05 08:36:17,1
AMD,nse7m2a,"I like your choices too. A 9960 xt is awesome, a 5800 xt is still a strong cpu.",AMD,2025-12-05 08:41:06,1
AMD,nrwcgpb,"As long as you don't suffer from an expected failure.  You're already at the end of AM4, if you're faced with replacing the CPU and/or the motherboard the money would have been better spent moving to AM5 instead of just part replacement, but that'll involve the DDR5 move which is absurd right now.",AMD,2025-12-02 15:27:01,2
AMD,nry004s,"I didn’t realize the 5xxx CPUs were that old. That’s the same point I purchased my 3xxx, apparently I was chasing the last years model then too lol.",AMD,2025-12-02 20:13:03,1
AMD,nry03wp,I’m just glad I already have my ram and hoping against all hope it never craps out on me.,AMD,2025-12-02 20:13:33,1
AMD,nrwai6p,Do you have any examples of this? Raising taxes to lower prices doesn't make intuitive sense.,AMD,2025-12-02 15:17:08,3
AMD,nse2dvu,"I would disagree. It is not really political. It is inflation based. Inflation in reality (not in media or political commentary) is preceded by wage disparity. The rich get richer, the poor get poorer. Companies are then able to ""markup"" a product and ""limit"" supply, increasing demand. Those with greater wealth will just outbid those with less money, much like an eBay auction. The producers see this and maintain the higher margin, and then raise the bar again to test a new level.    So this problem has been brewing for 50 years and is unlikely to change anytime soon. You need more wealth (greater pay) to the working class and modest middle class and less money to the upper class/mgmt class. Then you would see a reversion to quantity based pricing because more people have more money to purchase goods compared to a concentration of greater wealth into a smaller group of people (where we are today).    We all hate it, but a business has the goal of maximizing revenue. In a world where wealth concentrates into fewer hands, the business has to target that group of people and get them to pay higher margins. It is the world we have allowed our government officials to create for us all the while worshipping the ground they walk on and campaigning for them instead of questioning them.",AMD,2025-12-05 07:49:12,2
AMD,nrw40tw,"High net worth individuals (Musk, Bezos, etc.) do not have income, or have very little of it. Their money is made through massively appreciated assets (stock).    Companies do not have income. They have revenue. They can also choose to reinvest anything over their operating budget, lowering taxable dollars. They, at least in my location, have inventory tax. Keeping anything around and risking not selling it is a bad idea.",AMD,2025-12-02 14:42:18,5
AMD,nrvrtjb,That's the same source.,AMD,2025-12-02 13:31:39,15
AMD,nrwmvbx,">  Just buy the PS5 pro fellas.   Console gaming just doesn't hit the same as PC though. It's not even about graphics or framerates, consoles are just not PCs.",AMD,2025-12-02 16:17:40,9
AMD,nrvw68s,I was also lucky to get my RAM kit earlier this year for a normal price  https://imgur.com/a/UcVGSUK,AMD,2025-12-02 13:57:40,-1
AMD,nrw8qr4,Why would you do something silly like that? The 5800XT is a fine chip.,AMD,2025-12-02 15:07:54,5
AMD,nse5hlr,"For sure, but doesn't Intel make ddr4 boards for relatively modern CPUs? And AM4 is still capable. If they are going to make ddr5 $20/GB, then ddr4 might be the way.",AMD,2025-12-05 08:19:35,1
AMD,nrw88el,See you in 2027,AMD,2025-12-02 15:05:10,39
AMD,nrwxjd1,I said that about GPUs... 6-7 years ago....  They are still overpriced.,AMD,2025-12-02 17:08:48,22
AMD,nrvuyr7,\*if because GPUs still are hella expensive.,AMD,2025-12-02 13:50:40,16
AMD,nrwpevk,Good luck.,AMD,2025-12-02 16:29:53,3
AMD,nrxh164,"What is a ""normal level""?    Seems like a very subjective measurement. ""Normal"" is whatever the price is that the manufacturer sets for the item. That might be higher than you want to pay, but that doesn't mean the price isn't ""normal"".    I mean hell, I used to buy top end GPUs back when they were like $600-700. That gets you a mid range card at best these days...so are those prices not ""normal""?    I think if you're waiting for hardware to be what it was even a year ago, you're probably never buying hardware again....",AMD,2025-12-02 18:41:21,5
AMD,ns155v0,How quickly can you run?,AMD,2025-12-03 07:43:48,1
AMD,ns1p1by,Exactly ...,AMD,2025-12-03 10:58:23,1
AMD,ns6ux76,They are normal except ram so buy before it gets crazy,AMD,2025-12-04 03:57:45,1
AMD,ns9fnjn,Prices are never coming back down. GPU MSRP prices made a huge leap of several hundred dollars during the crypto craze. They're still at those price levels several years later.,AMD,2025-12-04 15:42:11,1
AMD,nrwcyio,LoL jeah if we continue that trend I rather can buy an appartment then an high end gaming PC,AMD,2025-12-02 15:29:28,20
AMD,ns36uw8,Yeah I remember scoring a P4 3.0 Ghz cpu for like 350 bucks that went to 4Ghz on a home brew h2o kit back then and being super happy.,AMD,2025-12-03 16:22:35,2
AMD,nrwwmj9,""" How many of us are actually trying to play cyberpunk at 4k on a daily basis?""  ![gif](giphy|1201hONkUdpK36)",AMD,2025-12-02 17:04:22,13
AMD,nryu8hv,My 7700K is still going strong as a fileserver! Hoping the 7800X3D will do as well as it did.,AMD,2025-12-02 22:41:09,5
AMD,nrw1o2w,"Actually thought about it, but I have even less space for a model train then for warhammer figurines :D",AMD,2025-12-02 14:29:08,11
AMD,nrww8g3,"Friend of mine was into photography a bit, but I'm just not good at ""visually creative"" tasks if you get what I mean?   Like I just don't have an ""Eye"" for what looks good to other people, OR me for that matter.   Doesn't matter if it's 2d/3d art or Photography OFC all of that comes with time and experience, but if i'd come home after taking a bunch of photos and I don't like any of them I'd just get frustrated",AMD,2025-12-02 17:02:28,4
AMD,ns2m69c,Nobody does news on sale.lruces going away lol. These are direct sku price increases.,AMD,2025-12-03 14:40:43,1
AMD,ns1aapi,No it says that the sale prices ending is being used as a cover for the price increase. It's a manipulation strategy.,AMD,2025-12-03 08:33:32,-1
AMD,nrwj31y,TO THE MOON! BUY GAME STONKS,AMD,2025-12-02 15:59:26,5
AMD,nrxwott,That 5090 was tempting but hard to justify for sure,AMD,2025-12-02 19:56:51,1
AMD,ns0l9mi,"Nah, Hasbro has made it unmanageable. Too many releases. You don't even want to keep up.",AMD,2025-12-03 04:57:03,6
AMD,nryr8rh,"I think consoles are the better value pick now if you mostly play singleplayer titles. Entry price is way lower and game prices aren't that much worse. But if you pay $200 a year to play online, a PC might end up being better value after a few years.  The appeal of PC for me is the potential for higher settings, modding capability, emulation and that it can do a lot more than just games. But you do pay a premium for that.",AMD,2025-12-02 22:25:30,3
AMD,ns9rs5v,"$1000 for an entire PC genuinely isn't a lot of money anymore with the Series X already being $650 now. That $650 is gonna go up too, this memory nonsense doesn't just affect PCs.  > I'm also starting to think that actually that whole PCMR line is entirely bullshit with how little my couple console friends spend on games they keep buying+flipping on FB/ebay/offerup/etc  This clearly won't last much longer either.",AMD,2025-12-04 16:40:55,1
AMD,nrystza,"It did _used to be_ pretty cheap to get a console-equivalent PC. Then all the crypto-mining rubbish kicked off and screwed up the prices, and now we have all the mostly-useless AI rubbish to screw up the prices even more.",AMD,2025-12-02 22:33:49,1
AMD,nrvwvn1,I play Destiny 2 and CoD the gabecube is not an option.   Besides the fact that we still need to wait on the price because I kinda still doubt it'll be a very good bang for the buck but we'll see.,AMD,2025-12-02 14:01:43,12
AMD,nrwobzy,Comes in with what? 2024 entry level hardware for 2026 prices?,AMD,2025-12-02 16:24:42,3
AMD,nryu3c3,"Rather depends which GW game you get into. Can put together a Necromunda gang or Kill Team for £60, or even £30 if you're happy to convert a few models. If you're talking 40K, WHFB or AOS then that'll be more expensive, sure, but still probably less for an army and required hobby supplies than a whole PC.  Not had any issues with the toxic chemicals in my resin printer, and I just keep it in a well-ventilated room. Takes a while to get settings dialled in but mostly all you waste is resin, which is dirt cheap.",AMD,2025-12-02 22:40:24,1
AMD,nryp2po,well atleast I am passioned about my hobby :D,AMD,2025-12-02 22:14:24,3
AMD,nrwg2tl,I hope enough to blast myself away until I can afford a PC again,AMD,2025-12-02 15:44:52,3
AMD,nrx31r5,"Waiting until some time after launch also has the advantage of letting bios bugs get discovered and fixed, as well as allowing time to find out which brands of hardware have issues.",AMD,2025-12-02 17:35:22,5
AMD,nrwqytp,What CPU did you come from? A 6700k?,AMD,2025-12-02 16:37:21,2
AMD,nrvtalt,14 years before end of 2022 here. Things were getting pretty dire!,AMD,2025-12-02 13:40:39,26
AMD,nrwefuk,"I hit the 10 year mark and jumped onto 7800x3D. My spidey sense was tingling, told me all this bullshit wouldn't just blow over.",AMD,2025-12-02 15:36:49,9
AMD,nrz7hc8,What an upgrade looking at your flair! My 7900xtx isnt getting replaced anytime soon either.,AMD,2025-12-02 23:55:17,3
AMD,nsbp99m,"I’m having a great run with AM4, early adopter in 2017 and still running mostly the same setup since then. With the exception of the 3090, that was in 2022 and the 5900x from a 1700x.",AMD,2025-12-04 22:24:48,2
AMD,ns0lcbe,"Funny you mentioned this. I paired the 7950X with NH-D15 with under-volt, under-clocked, TDP limited it to make sure it runs nice and cool for the longest possible life. Also modded 6800XT with a pair of Noctua A12 fans and a healthy under-volt. I developed my own script to control all fans based on some heurstics. When the default under-volt, power limited performance is not enough then I will start to overclock it to try and squeeze some last drops of performance. For now I'm happy with the stock performance.",AMD,2025-12-03 04:57:36,3
AMD,nrvzbjd,"I can only suspect that the silicon wafer price is going up, too. Otherwise I honestly can't think of another reason. Well. Besides political reasons surrounding Taiwan perhaps. But then again I'm merely making assumptions here.  But I definitely agree with your point. With the DDR5 RAM prices shooting to the moonand AM5 requiring DDR5 the demand of AM5 CPUs and thus the price of said CPUs should be expected to go down not to mention with that new flagship CPU on the way the top benchmark people would seek to buy the 9850x3d instead whilst also flooding the 2nd hand market with used 9800x3d unless said people are hardware collectors, too.",AMD,2025-12-02 14:15:47,15
AMD,nrx19t3,"Dude, the 9800X3D currently has the lowest price it ever had. Just build your PC and have fun.    A bus might hit you tomorrow, so enjoy life. The parts you have lying around won't get any better.",AMD,2025-12-02 17:26:46,8
AMD,nrwyzu4,Just buy the 9800x3d bro its an amazing CPU that would last you for years to come. Dont worry yourself about an extra 5 to 10fps that you need a 5090 to notice. The 9850x3d is hardly going to be better.,AMD,2025-12-02 17:15:48,5
AMD,nrzbc27,"I went from a delidded 8700k to a 5800X3D and that was an enormous performance leap. My 8700K was struggling then, mad respect for anyone still holding out.",AMD,2025-12-03 00:17:37,3
AMD,nrxgg75,Because we all knew things will be in this state right?,AMD,2025-12-02 18:38:35,6
AMD,nrxxtfb,How well is the 9070xt handling 4K? My monitor is 4@240hz and the 3090 is definitely underpowered a bit for it.,AMD,2025-12-02 20:02:19,3
AMD,nryi0jd,> maxed out^*  >!^^* ^ᵈˡˢˢ ^ᵖᵉʳᶠᵒʳᵐᵃⁿᶜᵉ!<,AMD,2025-12-02 21:40:08,13
AMD,nrxgyw9,It'll likely be more than enough foroew than 3 years imo.,AMD,2025-12-02 18:41:03,3
AMD,nryk3uj,There are dozens of us!,AMD,2025-12-02 21:50:06,1
AMD,ns0rk2c,I got the 5700x3d year when it was dirt cheap. Feels like one of my best pc part purchases ever.,AMD,2025-12-03 05:45:06,1
AMD,nrxyukh,"Got the exact same combo, it's quite rare",AMD,2025-12-02 20:07:23,1
AMD,nrw03w0,"Yeah, I know, but dreaming is free",AMD,2025-12-02 14:20:16,6
AMD,nrzil4r,"As long as AMD is still making Milan-X Epyc processors, there's hope for new supply of Ryzen 5000x3D CPUs!",AMD,2025-12-03 00:57:46,2
AMD,nse0si8,"I just bought an RX 9060 xt 16 GB for $350. That's really not that bad.    A ryzen 5800 xt is like $180. Ddr4 ram is still reasonable in some places, like $125 for 32 GB. Am4 mobos are cheap.    Aiming for $1000 titan graphic cards isn't necessary to game. 4k monitors aren't necessary to game. Ryzenx3D processors on ddr5 ram is not necessary to game. We all suffer from wanting the best but its not necessary. Just gotta live in the midrange.    Unless you do pcvr...then you are going to suffer financially",AMD,2025-12-05 07:33:41,3
AMD,ns4wg4r,youre dreaming to think steam machine will be under $1000 bucks pal,AMD,2025-12-03 21:17:33,1
AMD,nrvwsm4,"7700 was my first consideration as well but it was 230€ at the time in my region and the 7500f 130€. So it was a no brainer. It does the job perfectly fine, even in heavier games like Cyberpunk 2077 or Squad :)",AMD,2025-12-02 14:01:14,3
AMD,nryj516,Makes it easier for them,AMD,2025-12-02 21:45:28,1
AMD,ns4ecod,"Once all the parts arrive, I will give you an update. Have the CPU, the motherboard is on the road from Spain to Germany right now...",AMD,2025-12-03 19:50:06,1
AMD,nse96c4,"401k is retirement money and the middle class absolutely relies on that. Also, the prices of everything has already gone up. Eating out, buying normal goods like clothing, groceries and anything for your car is going up. It is so messed up that our government cares more about other countries than our own. Inflation is already hitting us here.",AMD,2025-12-05 08:56:22,1
AMD,nsgpxzy,Thank u so much I appreciate it very much we'll be looking to pick one up this weekend hopefully it all works out,AMD,2025-12-05 18:17:18,1
AMD,nsgpbwm,Thnx hope he likes it,AMD,2025-12-05 18:14:22,1
AMD,ns1mjqb,"They have a little more to go… 5800xt,5900x and 5950x?",AMD,2025-12-03 10:35:40,1
AMD,nrz0ycp,"> High net worth individuals (Musk, Bezos, etc.) do not have income, or have very little of it. Their money is made through massively appreciated assets (stock).   This is income.  It's called ""investment income.""  https://www.irs.gov/individuals/net-investment-income-tax  >Companies do not have income.  lol, yes they do.  There's a tax specifically for Corporate income.  It's called the ""Corporate Income Tax."" https://www.irs.gov/forms-pubs/about-form-1120",AMD,2025-12-02 23:17:42,3
AMD,nse3y1y,"I think people do not understand ""capital gains"". Those type of individuals paid in stock and assets pay lower taxes because there is a difference between short term capital gains (sold in under 1 year) and long term capital gains (sold after 1 year). Short term capital gains have very high tax rates, long term capital gains have low tax rates (15%, the basis of ""he pays less tax than you""). Long term capital gains tax rates exist as a reward for holding an asset because it creates market stability. It lowers the risk of great depression type volatility, or bitcoin type volatility. We also value 401k values in the Western World, so stability is highly desired.",AMD,2025-12-05 08:04:18,1
AMD,nrwzwtm,"they absolutely have income, and its fucking enormous compared to a worker's income.   The point you are failing to make is, despite their extraordinary income, it is dwarfed by the size of their non-liquid assets.   Dont be so naive",AMD,2025-12-02 17:20:13,1
AMD,nrvyg31,Some news site will now just use this Reddit post as a source and write an article lol,AMD,2025-12-02 14:10:47,9
AMD,nrvu57u,"confirmed enough for *me* tho.   I trooly won the Pee Cee Game by getting mine in September.   The holiest month of the year.   Thank u, Holy Pee Cee Pricing Gods.",AMD,2025-12-02 13:45:46,-3
AMD,nrxd893,I have both and console gaming feels better on a big screen.  The only genre I enjoy more on PC is FPS games.,AMD,2025-12-02 18:23:19,1
AMD,nrvx7h2,"Yep, I never thought that same DDR5 kit that I just bought on sale earlier this year, will shoot up above $300, it really does feel like I lucked out lol.",AMD,2025-12-02 14:03:39,0
AMD,nrx3fm0,"You’re not wrong, I’m just being dramatic",AMD,2025-12-02 17:37:13,2
AMD,nrwpibp,"I don't think they'll ever get ""normal"" again, ever.",AMD,2025-12-02 16:30:20,80
AMD,ns1p4g3,The AI bubble will burst in 2026. It will be a price massacre.,AMD,2025-12-03 10:59:09,6
AMD,ns1u8s5,2028,AMD,2025-12-03 11:43:12,1
AMD,ns2arhk,"Haha, we have at least 3 more years of this bullshit. Worse is coming.",AMD,2025-12-03 13:36:15,1
AMD,ns6z67w,More like 2077,AMD,2025-12-04 04:26:53,1
AMD,nryvvou,"I moved to the used market for the time being, lol.",AMD,2025-12-02 22:49:53,10
AMD,ns0xwbs,I don't really feel like GPU's are overpriced (at least right now). They last a lot longer then before thanks to upscaling.,AMD,2025-12-03 06:37:35,1
AMD,ns19zf4,because we keep buying them.,AMD,2025-12-03 08:30:25,1
AMD,nrxn19a,67 HAHAHAHAHHAHAHAHAHHAHHAHAHAHAHAHHAHAHAHAHHAHAHHAHAHHAHAHAHAHAHAHAHHAHAHAHAHAHAHHJAHAHAHAHAHAHAHAH,AMD,2025-12-02 19:09:47,-19
AMD,ns12435,"If you account for inflation, increased relative wafer and rnd costs, the prices aren't terrible. Of course they're worse but not much so. The egregious part is how cut down gpu's of a supposed ""tier"" are with the current 60 tier gpu's being more like a 50 tier in terms of the chip.",AMD,2025-12-03 07:14:54,4
AMD,nrzshv4,"Soon it's just going to be cloud, have a cheap laptop have ethernet hooked up and just use everything from the cloud you don't need expensive hardware.   Remember the saying that came as a joke you will own nothing to be happy about it well it's pretty much true!   I feel really bad for the younger generation they don't even have a chance, buying a house email renting is ridiculous.   No wonder why they are so depressed, it just feels like no matter how much they make it's going to medical renting and then society itself is collapsing.   Then again I do believe this is all done by design.",AMD,2025-12-03 01:56:12,6
AMD,nrzmka9,Bro I went from 7700k to 7900 non-x and the upgrade was huge. You're doubling your core count as well as getting some of the best gaming performance. The 7800x3d is likely to age much better than the 7700k 👍,AMD,2025-12-03 01:20:56,3
AMD,nryqx53,"They're not figurines, they're miniatures!",AMD,2025-12-02 22:23:49,3
AMD,nry0fkx,I'm not the best with it but I stick to wildlife/ nature/ fungi where I just kinda take pictures that look cool  for the most part and some work out,AMD,2025-12-02 20:15:09,2
AMD,ns2udbn,"But wouldn't that require them actually raising prices? I've been watching prices for the last couple weeks, since I was going to get a 9070 XT, and they're at the same level today as they were pre-Black Friday.  I got lucky and snatched a 9070XT for $50 cheaper than any 9070 (non XT) is sold for, so I'm definitely not mad :)",AMD,2025-12-03 15:22:48,2
AMD,ns1seg9,"Not sure if you've seen RAM prices, but if those don't settle down expect all consoles to sky rocket in price when their stockpile runs out.       Crazy times lol.",AMD,2025-12-03 11:27:48,2
AMD,nrziban,"> potential for higher settings  Reeeeaally hard to sell that one anymore since the cost spirals out of control to do RT well, which is the new sexy tech.  >  modding capability,  if the games you like are moddable  > emulation  your phone can do that pretty well too now! Even mid phones. Guess you need to hook up your old PS3 to play those tho. Give it a few more years for somebody to rip off RPCS3 and put it on android.  > it can do a lot more than just games.   And so can your phone. Which the majority seem to be happier using for some god forsaken reason.  This whole argument grew weaker over time as phones got better, games moved to multi-plat, and people just bought PCs less.  but hey, if you're really into RT and PS3 emu, you're right. PCs can't be beat.",AMD,2025-12-03 00:56:11,1
AMD,nrzxlle,"> But if you pay $200 a year to play online  I feel like this is way less of a ""requirement"" than it used to be. F2P games don't require a Live/PSN sub, and that encompasses some of the most popular games like Apex, Fortnite and COD Warzone",AMD,2025-12-03 02:25:51,1
AMD,nrw387j,100% sure they fuck it up. I don't see it being priced any less than $700 on release with a 2-3 month drop to $600.,AMD,2025-12-02 14:37:54,9
AMD,nrvxaz2,Hopefully it does bring the Devs to start compatibilizing Anti-Cheat with Linux,AMD,2025-12-02 14:04:13,6
AMD,nrwunif,"Time to find some new games, then.",AMD,2025-12-02 16:54:50,1
AMD,nrzke29,Definitely. Theres an inflection point where prices hit a near-all time low after a few years of release and the product is stable in software. Then prices slowly climb back up once production stops. At which point it then becomes cheaper to buy something new again.,AMD,2025-12-03 01:08:13,3
AMD,nrzk1xu,Yep i7 6700k on a Z170-AR motherboard. Overclocked held up fine for a lot longer than I expected.,AMD,2025-12-03 01:06:18,1
AMD,ns1ouy0,"Same. I thought upgrading it, but nah. Its still good minus RT that i wont use anyway, pure raster is king :)  I'm aiming towards 2027 as it seems nothing comes in 2026 even if Nvidia release next gen and skip super launch the availability wont be good before 2027. AMD is a big mystery if they even launch anything in 2026.",AMD,2025-12-03 10:56:47,2
AMD,nscu7ty,"Those are some pretty good upgrades and a great use of AM4. I was dumb enough to go with Intel's B360 instead of AM4, and am still regretting it.  I didn't think that upgrading the CPU only was going to be a thing, but then AM4 kept getting supported and x3D came out. Oh well, there's always next time.",AMD,2025-12-05 02:23:50,1
AMD,ns4c00w,"Good job, it is path we'll all need to take!",AMD,2025-12-03 19:38:42,1
AMD,nryo751,"Dude it's not just the processor cost that's preventing a lot of us from upgrading, right now DDR5 ram is at some of the highest points it's every been at, a 32GB pack that cost $150 or so a month ago is currently going for more than said 9800x3d at $410",AMD,2025-12-02 22:09:58,5
AMD,nrykysk,"Can't compete with 4090/5080/5090 obviously, typical PlayStation titles like God of War, Horizon, Spiderman can do 4k@100fps maxed out easily with FSR4 quality which looks amazing, the multiplayer games like BF6 or Arc Raiders can do 100-130 fps turning down a bit some settings, I think its good but I would wait for next gen tbh when AMD allegedly will release a new flagship",AMD,2025-12-02 21:54:10,3
AMD,ns3v6w2,I second that. My 9070xt gets about 100 fps in modern games at 4k. My monitor can go higher but it isn't as though I'll buy a new and better gpu anytime soon when 5090s still sell out in minutes at msrp.,AMD,2025-12-03 18:19:00,1
AMD,nsehz9i,"RIP basically if not for VR I could  use my 3600 for another 10 years. My playtime is split between tiny flatscreen indies and VR games almost exclusively.    My R5 3600 + Rx 6800xt build is five years old now and there ain't any system worth upgrading too that's less than $3000. Four generations of CPU and three GPU generations later, the modern mid range stuff is basically a side-grade to my existing setup. A side grade that costs more than I paid five years ago.",AMD,2025-12-05 10:21:45,1
AMD,ns5c8g5,RemindMe! 60 days,AMD,2025-12-03 22:35:38,1
AMD,ns4vbvs,"Thank you, and good luck with your build! Hoping to do a similar one in the next week or two.",AMD,2025-12-03 21:12:14,2
AMD,nsgt8dk,"Oh he""ll love it. Its an awesome gift and nice of you to build for him (because it ain't cheap lol). But YOLO.",AMD,2025-12-05 18:33:08,1
AMD,nrx3ckz,"No, they don't. ""Income"" with respect to taxation has a very specific definition.    Dividends on massive stock holdings? Not income. Appreciation on stocks/land/houses/yachts/unicorns? Not income.    Regardless of the arguments to be made on taxing the wealthy, as a country we are set up to tax a very specific way and they have found a way around it. You cannot scream ""tax their income"" and expect anything to happen.",AMD,2025-12-02 17:36:49,2
AMD,nse4r4n,"They also have cost. They avoid paying taxes by increasing their costs. They buy other companies, build out logistics chains or warehouses, hire more employees, etc. Its either grow or pay uncle Sam. They choose to grow.    But you are correct in it being unhealthy. It has led to the development of giant, monopolistic companies and has decreased competition by killing small businesses, leading to higher prices for consumers. But that wasn't by design. The intention was to create more jobs for more workers in better paying companies. Unfortunately there was no plan to stop those companies from growing out of control into giant behemoths. It is a complicated topic.",AMD,2025-12-05 08:12:14,1
AMD,nrxdou7,"Personally, I use my PC connected to my TV. I use a Dualsense Edge for games that have controller support and a Steam Controller for games that don't. Couple that with a wireless mouse and keyboard and it has been a blast.",AMD,2025-12-02 18:25:30,1
AMD,nrvxijl,I sold a 32GB DDR4 3600MHz Kit earlier this year for 80€... now I feel dumb,AMD,2025-12-02 14:05:25,1
AMD,ns0lskg,"Haha, real",AMD,2025-12-03 05:00:54,1
AMD,nry3ixk,"It's not just hardware, food prices more than doubled in last 5y in many places. You can still build a great machine for 1500e",AMD,2025-12-02 20:30:18,23
AMD,nrxy87h,Sad truth,AMD,2025-12-02 20:04:20,2
AMD,ns8iq7h,"That's generally how inflation is, the only way out is to increase your income over time.",AMD,2025-12-04 12:32:46,2
AMD,nsb0rfk,Lol ok. You do realize that datacenters arent going anywhere right? When companies go out of business they just get consolidated and bought up by the ones who didn't go under or exit the market. AI isn't going anywhere and neither are the datacenters.,AMD,2025-12-04 20:20:45,1
AMD,nsberhw,"Then because the entire economy is propped up by AI bull, there will be a recession and your wage will also be massacred. Lose-lose.",AMD,2025-12-04 21:31:19,1
AMD,ns8eveq,Why 3?,AMD,2025-12-04 12:03:50,1
AMD,ns12me2,Dont know what they are talking about.  A used 7800xt and 7900gre are perfectly fine card sold at reasonable price.  A 3600 is still a fine cpu. Just dont get fixed on the newest and shiniest stuff.,AMD,2025-12-03 07:19:40,4
AMD,nrw7u56,"I think that's still unrealistic, they said they'd compete with PCs not with consoles and consoles are already around 600-700€ (and I think USD should be around the same)  So I don't expect the gabecube to be under 800",AMD,2025-12-02 15:03:01,9
AMD,nrw7d4y,"There are fundamental differences in how Windows and Linux are structured, and as such, it's a non-trival problem to make an equivalent kernel level anti-cheat for Linux.  Not saying it's not a solvable problem, it is. But it's not going to be a quick and dirty port job, and it's something that's going to take not only game developer buy-in, but also probably Linux kernel dev buy-in to make work.",AMD,2025-12-02 15:00:27,6
AMD,nrwb45g,"ARC Raiders is Steam Deck verified.  And it, according to the Steam page, uses kernel-level anticheat.",AMD,2025-12-02 15:20:14,4
AMD,nry9njf,"The best thing we can hope for is that games will now have a ""Steam Machine"" setting as one of the presets",AMD,2025-12-02 21:00:05,1
AMD,nryrvf8,"Dude, I wasn't talking to you. The guy I replied to literally said he already bought all parts.    **Except the CPU and Motherboard**. He got his RAM! He got the GPU! Waiting for the rest is stupid.",AMD,2025-12-02 22:28:45,6
AMD,nrzc1ql,The only thing making me even think about the 9070xt is there are still some tasty deals to be had on some absolutely gorgeous cards.,AMD,2025-12-03 00:21:34,1
AMD,nsemvx2,I'm not so sure. I have a 5800x3d and a 7900 xt and it runs my Quest 3 no problem. My old GPU was actually the 6800 and it was really good. I bet a 5800 xt and a 9070 xt would kill it in vr. You can easily get $300 for the 6800.    But I feel your pain. The VR is demanding on the wallet,AMD,2025-12-05 11:08:05,1
AMD,ns5cezq,I will be messaging you in 1 month on [**2026-02-01 22:35:38 UTC**](http://www.wolframalpha.com/input/?i=2026-02-01%2022:35:38%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/1pc48oj/amd_rumored_to_raise_ryzen_9000_and_older_cpu/ns5c8g5/?context=3)  [**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F1pc48oj%2Famd_rumored_to_raise_ryzen_9000_and_older_cpu%2Fns5c8g5%2F%5D%0A%0ARemindMe%21%202026-02-01%2022%3A35%3A38%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201pc48oj)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2025-12-03 22:36:35,1
AMD,nshdjvw,LOL YES SIR!!,AMD,2025-12-05 20:16:09,1
AMD,nrvymnj,"I sold my 32GB 3200 kit for half that price back when I sold my DDR4 platform too, but I don't feel the FOMO, because I wouldn't have known the price of DDR4 will increase just the same as I do on DDR5 as well.",AMD,2025-12-02 14:11:49,1
AMD,ns1p8j6,I bought 2x16gb b die kit 3600 cl 16 for only 40€ this year😂,AMD,2025-12-03 11:00:09,1
AMD,ns1srdm,Electricity bill in my area is going up as well.,AMD,2025-12-03 11:30:53,7
AMD,nrwvggq,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-02 16:58:39,-1
AMD,nsbuytq,It will push down prices down like used graphic cards a few years ago.,AMD,2025-12-04 22:56:14,2
AMD,nsbv2hb,I have a safe job.,AMD,2025-12-04 22:56:49,2
AMD,ns2qqdy,Guess which CPU I had for 5 freaking years and loved every minute of it?  I bought my trusty R5 3600 for cheap in early 2020 and it's been such a trooper. A 8700 non-k for the masses.   It won't win any benchmark but it plays plenty of games just fine.,AMD,2025-12-03 15:04:35,3
AMD,nrwms82,$800 would be laughable. My local Walmart currently has a stack of i7/4050 laptops for $500 each and even that's a hard sell for me.,AMD,2025-12-02 16:17:16,7
AMD,nrym5wj,I can buy a new ps5 disc edition for €440....   https://nl.nbb.com/nl/p/sony-playstation-5-standaardconsole-slim-met-schijf-1tb-ssd-geheugen-slank-ontwerp-inclusief-1x-dualsense-draadloze-controller/a1051567,AMD,2025-12-02 21:59:55,2
AMD,nrwdlbf,"Destiny 2 uses BattlEye for its anti-cheat... which already exists for Linux, Bungie just haven't incorporated it for whatever reason.",AMD,2025-12-02 15:32:37,5
AMD,ns9sahk,"No, the problem is that devs are using kernel-level anticheat at all. It's a scam that doesn't actually work. Anticheat is as much of a problem on Windows as it is on Linux, to the point that *allegedly* Windows is cracking down on kernel-level garbage.",AMD,2025-12-04 16:43:23,0
AMD,nryypfy,"Holy shit I'm tired, that's my bad I completely misread the whole thread to see what you were replying to. After reading it, I agree with you, that user should just buy the mobo/cpu now since they already have everything else including the RAM which is the most inflated price and will be the hardest thing to find for a decent price the next few months and more.",AMD,2025-12-02 23:05:12,5
AMD,nrzhqmx,"Ya, at this point, just buy the CPU and motherboard and enjoy the build.",AMD,2025-12-03 00:52:57,1
AMD,ns7hzyx,"Hey, billionaires gotta buy a third yacht too.",AMD,2025-12-04 06:56:28,1
AMD,nsd0psm,"That isn't even remotely comparable. How will it push prices down when datacenters arent going anywhere? You think this server hardware isn't going to be purchased and be continued to be used by the purchaser? You think ChatGPT and Gemini are going to disappear? Tell me, how many consumers have motherboards capable of handling ECC RDIMM RAM?",AMD,2025-12-05 03:01:34,1
AMD,nrwfbyc,"Well, for one thing, it's Bungie, so... yah...  But my understanding is that while BattlEye is available on Linux via Proton, it's not an automatic option. Basically it's down to the developers deciding to support it. If devs don't want to have to manage something like an extra PC platform for support purposes, then they're just not going to I'm afraid.  Easy Anti Cheat also has a Linux version, though it's not as secure as the Windows equivalent.",AMD,2025-12-02 15:41:13,2
AMD,nsgp5vu,"AI data centers and traditional data centers are not the same, and they don't have the same requirements.  Memory hardware prices will fall as demand declines. Especially if a large number of AI companies go bankrupt. That's Economy 101.",AMD,2025-12-05 18:13:34,1
AMD,nrwwdut,"""Well, for one thing, it's Bungie, so... yah...""   As a Destiny fan... HEY! but also, lol jeah you are right they kinda suck these days.",AMD,2025-12-02 17:03:11,1
AMD,nshzabm,!remindme 2 years,AMD,2025-12-05 22:11:26,1
AMD,nsig940,I accept your apology.,AMD,2025-12-05 23:51:29,2
AMD,nt7kdib,So similar concept to the old minisforum v3. It is quite sad that they dont consider releasing a cutdown model using the older cheaper 7840/8840 chip.,AMD,2025-12-10 00:48:18,6
AMD,nt7ntay,Another $2000 tablet 🙄 still only one laptop.,AMD,2025-12-10 01:08:32,5
AMD,nt87i00,Curious about the fan noise on this bitch lol,AMD,2025-12-10 03:05:43,3
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,54
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,8
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,4
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,2
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,5
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,2
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,1
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,4
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,50
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,10
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,12
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,3
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,7
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,0
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,0
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-14
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,7
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,7
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,5
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,2
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,1
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,28
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,18
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,5
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,3
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,3
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-9
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,7
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,12
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,4
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,2
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,79
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,30
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,6
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,32
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,35
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,8
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,50
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,10
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,1
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,10
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,5
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,3
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,2
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,13
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,20
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,3
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,0
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,2
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-3
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,3
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,3
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,6
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,18
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,12
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,7
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,11
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,3
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,6
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,4
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,3
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,14
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,2
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,20
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,3
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,7
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,10
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,1
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,0
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,0
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-12
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,40
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,12
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,3
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-5
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,18
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-7
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,4
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,5
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,10
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,8
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,2
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,1
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-3
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,11
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,4
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-2
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,10
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,7
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,3
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,nq4im71,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Intel,2025-11-22 01:57:26,1
AMD,nq9ltvl,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Intel,2025-11-22 22:42:16,1
AMD,nqoo97z,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Intel,2025-11-25 10:53:08,1
AMD,nr6yiuz,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Intel,2025-11-28 09:31:06,1
AMD,nsz9e66,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Intel,2025-12-08 18:41:34,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nqg4c7m,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Intel,2025-11-24 00:10:02,1
AMD,nqg5tvg,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Intel,2025-11-24 00:18:36,1
AMD,nqry4gw,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Intel,2025-11-25 21:43:16,1
AMD,nrmm6fq,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Intel,2025-12-01 00:19:07,1
AMD,ns3syx7,Nope got cash back,Intel,2025-12-03 18:08:27,1
AMD,nt12wk5,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Intel,2025-12-09 00:22:32,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nqkt81n,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Intel,2025-11-24 19:16:09,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,54
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,48
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,4
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,9
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,16
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,-1
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,-1
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-3
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-5
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-8
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,22
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,7
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,10
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,12
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,2
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,3
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,7
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,6
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,15
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,11
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,10
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,4
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,7
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,14
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,-1
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-2
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,5
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,9
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,1
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,46
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,17
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,35
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,11
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,5
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,1
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,35
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-2
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,4
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,5
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,33
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,65
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,28
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-1
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-6
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,4
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,8
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,6
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,13
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,npq13eo,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Intel,2025-11-19 19:51:12,1
AMD,npq0u97,Probably only on the skus with less cores.,Intel,2025-11-19 19:49:56,1
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,4
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,8
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,7
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,2
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,6
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,16
AMD,npq1inb,"No, because it would be the same die. Just won't fit.",Intel,2025-11-19 19:53:18,1
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,5
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,4
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,65
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,79
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,6
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,9
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,8
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,8
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,nd42fep,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Intel,2025-09-08 16:53:22,1
AMD,ndg3xrf,I will follow all!,Intel,2025-09-10 13:49:20,1
AMD,n9j60kj,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-6
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,11
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-5
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,4
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,5
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,5
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,5
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,6
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,4
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,21
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because it’s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,23
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Intel,2025-08-19 13:20:05,4
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,4
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-5
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,6
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,4
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,7
AMD,n9ks0ao,It’s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,4
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Intel,2025-08-19 13:34:10,3
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-4
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,-1
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,5
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Intel,2025-08-19 20:05:16,3
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,6
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,4
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,5
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,62
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,31
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,5
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-23
AMD,n6bwcya,It’s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,19
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,11
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,3
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,27
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,22
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,11
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,18
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,7
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-8
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-3
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
AMD,n6klfjc,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Intel,2025-08-02 19:05:15,23
AMD,n6q6qct,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Intel,2025-08-03 17:28:08,2
AMD,n7ht68l,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Intel,2025-08-07 21:45:26,1
AMD,n72229n,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Intel,2025-08-05 14:15:36,1
AMD,n7ewvah,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Intel,2025-08-07 13:19:41,2
AMD,nanq5zn,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Intel,2025-08-25 21:47:41,3
AMD,n7m0sli,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Intel,2025-08-08 14:57:07,1
AMD,n6m90qi,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Intel,2025-08-03 00:48:59,7
AMD,n6nytko,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Intel,2025-08-03 08:55:06,3
AMD,n6l6ttw,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Intel,2025-08-02 21:03:36,4
AMD,n7m8w5n,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Intel,2025-08-08 15:35:34,1
AMD,n8s3s3l,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Intel,2025-08-15 04:54:15,1
AMD,n6orqc9,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Intel,2025-08-03 12:59:19,5
AMD,n6qeres,Thank you :),Intel,2025-08-03 18:07:58,1
AMD,n4v3wzh,Wouldn't the 300 series actually be Arrow Lake Refresh?,Intel,2025-07-24 08:11:30,42
AMD,n4v46fh,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Intel,2025-07-24 08:13:59,44
AMD,n4y92ft,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Intel,2025-07-24 18:56:57,13
AMD,n4w6o9h,This is pat's work,Intel,2025-07-24 13:07:04,14
AMD,n4v6nq7,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Intel,2025-07-24 08:37:59,6
AMD,n4vlaga,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Intel,2025-07-24 10:49:31,8
AMD,n4w07wg,Is Intel finally making a comeback with their cpus? I hope so,Intel,2025-07-24 12:30:48,12
AMD,n4vi19u,Large L3 cache without reducing latency will be fun to watch.,Intel,2025-07-24 10:22:42,19
AMD,n4v3ue7,the specs sure do shift a lot..,Intel,2025-07-24 08:10:48,4
AMD,n4v8k6w,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Intel,2025-07-24 08:55:55,5
AMD,n4z4yed,it's just a bunch of cores glued together - intel circa 2016 probably,Intel,2025-07-24 21:27:20,2
AMD,n5798px,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Intel,2025-07-26 02:39:20,2
AMD,n4w3arv,Bout time,Intel,2025-07-24 12:48:32,1
AMD,n4w52bq,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Intel,2025-07-24 12:58:18,1
AMD,n510yns,Will it be available in fall of this year or 26Q1?,Intel,2025-07-25 04:05:49,1
AMD,n525u2m,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Intel,2025-07-25 10:03:56,1
AMD,n54i8vo,Noob question:  Is this their 16th gen chips?,Intel,2025-07-25 17:44:39,1
AMD,n5c2bcs,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Intel,2025-07-26 21:56:10,1
AMD,n5r7teb,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Intel,2025-07-29 07:47:01,1
AMD,n5rui8a,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Intel,2025-07-29 11:14:18,1
AMD,nrqgmjh,"What is it with Intel and 8 ""pcores""  how about just do 24 cores 48 threads",Intel,2025-12-01 16:47:12,1
AMD,n4yqz30,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Intel,2025-07-24 20:20:50,1
AMD,n55s05h,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Intel,2025-07-25 21:27:44,1
AMD,n4xrnev,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Intel,2025-07-24 17:35:44,-1
AMD,n4vu505,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Intel,2025-07-24 11:52:38,0
AMD,n4wlmkp,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Intel,2025-07-24 14:23:50,0
AMD,n4vi1cf,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Intel,2025-07-24 10:22:43,-12
AMD,n50egti,"""E-Cores"",  Ewww, Gross.",Intel,2025-07-25 01:40:31,-3
AMD,n4y77tz,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Intel,2025-07-24 18:47:57,0
AMD,n4vx5x8,NVL will definitely be the 400 series. PTL is 300.,Intel,2025-07-24 12:12:08,14
AMD,n4vqabe,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Intel,2025-07-24 11:26:19,25
AMD,n4x2p0n,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Intel,2025-07-24 15:43:01,7
AMD,n4vnjk4,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Intel,2025-07-24 11:06:28,42
AMD,n4vbjjr,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Intel,2025-07-24 09:24:22,5
AMD,n4vqnzd,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Intel,2025-07-24 11:29:01,13
AMD,n4xnlen,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Intel,2025-07-24 17:17:52,2
AMD,n4y6q9e,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Intel,2025-07-24 18:45:35,2
AMD,n9a1zq5,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Intel,2025-08-18 01:51:09,1
AMD,n54f312,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Intel,2025-07-25 17:30:13,1
AMD,n50iof9,More like *despite* him.,Intel,2025-07-25 02:05:32,-4
AMD,n4wx0ud,All the SKUs rumored so far are BLLC,Intel,2025-07-24 15:17:14,1
AMD,n4w5o0o,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Intel,2025-07-24 13:01:33,10
AMD,n4vwjon,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Intel,2025-07-24 12:08:14,-1
AMD,n51lyb6,Why not 8 P cores with HT + even more cache and no e cores at all,Intel,2025-07-25 06:56:21,-1
AMD,n4wel6l,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Intel,2025-07-24 13:49:07,15
AMD,n4w5tga,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Intel,2025-07-24 13:02:22,4
AMD,n4wwsgj,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Intel,2025-07-24 15:16:10,2
AMD,n8gsgik,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Intel,2025-08-13 13:37:20,2
AMD,n4xic5y,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Intel,2025-07-24 16:54:39,0
AMD,n51u6hg,Nova lake? More like 26Q4.,Intel,2025-07-25 08:13:29,1
AMD,n5272v5,Only Pantherlake for mobile,Intel,2025-07-25 10:15:01,1
AMD,n52fjpx,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Intel,2025-07-25 11:22:11,2
AMD,n4yb3g9,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Intel,2025-07-24 19:06:33,0
AMD,n4vwh5f,It is really happening. The only question is when? Or can they release it on the next year without delay?,Intel,2025-07-24 12:07:46,1
AMD,n4wxgvv,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Intel,2025-07-24 15:19:15,3
AMD,n4wm6y8,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Intel,2025-07-24 14:26:35,1
AMD,n514twx,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Intel,2025-07-25 04:33:51,1
AMD,n5153vl,"E cores are the future, P cores days are numbered.",Intel,2025-07-25 04:35:54,7
AMD,n50iukn,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Intel,2025-07-25 02:06:36,2
AMD,n514kx3,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Intel,2025-07-25 04:32:00,1
AMD,n65knix,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Intel,2025-07-31 12:35:52,1
AMD,n4vw5ub,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Intel,2025-07-24 12:05:46,10
AMD,n4vrhxo,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Intel,2025-07-24 11:34:53,19
AMD,n588bec,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Intel,2025-07-26 07:21:16,2
AMD,n4xxr8n,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Intel,2025-07-24 18:03:15,3
AMD,n50iwkc,The former ceo,Intel,2025-07-25 02:06:56,3
AMD,n4xy0ww,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Intel,2025-07-24 18:04:31,3
AMD,n4x3wyl,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Intel,2025-07-24 15:48:35,9
AMD,n4wb0qc,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Intel,2025-07-24 13:30:18,13
AMD,n4xgvaw,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Intel,2025-07-24 16:48:02,6
AMD,n4vzlh2,No different to 12-14th gen then.,Intel,2025-07-24 12:27:02,-1
AMD,n4wathc,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Intel,2025-07-24 13:29:14,-1
AMD,n51m5t6,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Intel,2025-07-25 06:58:12,3
AMD,n5267k5,E core is 30% faster than hyper threading,Intel,2025-07-25 10:07:14,2
AMD,n9ruz42,HT is worse than E cores,Intel,2025-08-20 20:15:01,2
AMD,n4wi0su,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Intel,2025-07-24 14:06:24,5
AMD,n5m42rc,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Intel,2025-07-28 14:26:44,1
AMD,n4xxluj,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Intel,2025-07-24 18:02:33,4
AMD,n50irzt,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Intel,2025-07-25 02:06:09,3
AMD,n526u6b,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Intel,2025-07-25 10:12:51,1
AMD,n514fo0,ARM chips regularly do this sometimes on a yearly basis.,Intel,2025-07-25 04:30:55,1
AMD,n5maqpy,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Intel,2025-07-28 14:59:12,3
AMD,n6ijxrr,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Intel,2025-08-02 12:13:51,1
AMD,n4yg12q,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Intel,2025-07-24 19:29:54,-2
AMD,n4wewa9,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Intel,2025-07-24 13:50:43,2
AMD,n4x5x1h,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Intel,2025-07-24 15:57:43,0
AMD,n5m9vma,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Intel,2025-07-28 14:55:00,1
AMD,n5awbw7,"Lmao, some of my games still runs on e cores",Intel,2025-07-26 18:06:33,1
AMD,n50x83s,People are still disabling e cores for more performance.,Intel,2025-07-25 03:39:29,1
AMD,n525z7c,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Intel,2025-07-25 10:05:10,1
AMD,n5m4vc2,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Intel,2025-07-28 14:30:40,2
AMD,n511bh4,fym no different they're 50% faster,Intel,2025-07-25 04:08:20,3
AMD,n5m5ymk,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Intel,2025-07-28 14:36:05,3
AMD,n528glp,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Intel,2025-07-25 10:27:04,1
AMD,n4wm3t2,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Intel,2025-07-24 14:26:09,5
AMD,n4wln0q,"As a advice, Nova Lake will further increase memory latency.",Intel,2025-07-24 14:23:54,-2
AMD,n50qnfg,In which gen iteration?,Intel,2025-07-25 02:55:31,0
AMD,n526q2r,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Intel,2025-07-25 10:11:49,1
AMD,n5m8bv4,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Intel,2025-07-28 14:47:35,1
AMD,n4xhpq9,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Intel,2025-07-24 16:51:53,1
AMD,n4xw2bt,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Intel,2025-07-24 17:55:25,8
AMD,n4zgeej,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Intel,2025-07-24 22:26:02,5
AMD,n52evsy,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Intel,2025-07-25 11:17:22,4
AMD,n588e12,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Intel,2025-07-26 07:21:58,2
AMD,n5266cc,They don't pay attention,Intel,2025-07-25 10:06:55,2
AMD,n5m6uax,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Intel,2025-07-28 14:40:21,1
AMD,n5dzs6f,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Intel,2025-07-27 05:47:15,1
AMD,n51sxrb,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Intel,2025-07-25 08:01:41,3
AMD,n50ijg8,How do you think they're doing 2 compute tiles if the memory controller is on one?,Intel,2025-07-25 02:04:42,2
AMD,n4wml1q,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Intel,2025-07-24 14:28:27,3
AMD,n5m2xjb,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Intel,2025-07-28 14:21:04,2
AMD,n528fgg,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Intel,2025-07-25 10:26:47,3
AMD,n5m9sth,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Intel,2025-07-28 14:54:37,1
AMD,n4xx7gl,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Intel,2025-07-24 18:00:40,2
AMD,n51508q,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Intel,2025-07-25 04:35:11,1
AMD,n526cpd,That didn't stop Intel with N3B,Intel,2025-07-25 10:08:30,0
AMD,n55ndnc,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Intel,2025-07-25 21:04:09,-1
AMD,n5qxo88,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Intel,2025-07-29 06:12:03,1
AMD,n80drlt,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Intel,2025-08-10 21:49:03,1
AMD,n4yjbs7,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Intel,2025-07-24 19:45:28,0
AMD,n55qjrv,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Intel,2025-07-25 21:20:18,5
AMD,n4z1sm2,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Intel,2025-07-24 21:11:47,2
AMD,n4z2fz2,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Intel,2025-07-24 21:14:56,1
AMD,n526jxa,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Intel,2025-07-25 10:10:18,1
AMD,n4znijc,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Intel,2025-07-24 23:05:11,0
AMD,n53d0lp,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Intel,2025-07-25 14:33:18,0
AMD,n3haicz,Will it also introduce Lunar Lake successor?,Intel,2025-07-16 17:36:05,17
AMD,n3htfns,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Intel,2025-07-16 19:02:16,10
AMD,n3hw0vs,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Intel,2025-07-16 19:14:21,5
AMD,n3h5hhc,Is anyone left?,Intel,2025-07-16 17:13:41,22
AMD,n3h8963,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Intel,2025-07-16 17:26:01,8
AMD,n3hcp5u,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Intel,2025-07-16 17:45:45,6
AMD,n3jhgvd,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Intel,2025-07-16 23:53:49,1
AMD,n3k7s38,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Intel,2025-07-17 02:31:14,1
AMD,n3mdvwq,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Intel,2025-07-17 12:44:29,1
AMD,n4c03u2,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Intel,2025-07-21 12:54:03,1
AMD,n3huqus,That's Panther Lake in a few months,Intel,2025-07-16 19:08:25,13
AMD,n3ik298,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Intel,2025-07-16 21:05:21,11
AMD,n3lcumc,"No, the memory config wouldn't work.",Intel,2025-07-17 07:51:04,2
AMD,n3i46cl,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Intel,2025-07-16 19:53:15,7
AMD,n3h7xps,No not really.  It's pretty f'n bleak atm.,Intel,2025-07-16 17:24:37,18
AMD,n3h9kp1,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Intel,2025-07-16 17:31:54,6
AMD,n3l4fgi,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Intel,2025-07-17 06:35:04,-1
AMD,n3irdqf,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Intel,2025-07-16 21:39:27,6
AMD,n3hywlr,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Intel,2025-07-16 19:27:56,5
AMD,n3k42qb,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Intel,2025-07-17 02:08:16,7
AMD,n3k6ewf,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Intel,2025-07-17 02:22:43,3
AMD,n3jnkuo,PTL's not really a LNL successor.,Intel,2025-07-17 00:29:37,2
AMD,n48vx1p,"Yeah, it was just to prove a point that ARM is overrated.",Intel,2025-07-20 22:58:27,4
AMD,n3k5a4s,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Intel,2025-07-17 02:15:39,2
AMD,n4sfvrm,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Intel,2025-07-23 21:35:03,2
AMD,n3h8tf7,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Intel,2025-07-16 17:28:32,13
AMD,n3kgv3j,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Intel,2025-07-17 03:30:53,6
AMD,n3jgnpe,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Intel,2025-07-16 23:49:16,4
AMD,n3hafqh,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Intel,2025-07-16 17:35:45,3
AMD,ngib62p,Source?,Intel,2025-09-27 17:03:09,2
AMD,n3jfseq,"The BOM is lower, so the question is where the markup is coming from.",Intel,2025-07-16 23:44:28,2
AMD,n3k5vvd,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Intel,2025-07-17 02:19:23,2
AMD,n3k8ay7,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Intel,2025-07-17 02:34:31,2
AMD,n3khaz8,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Intel,2025-07-17 03:33:57,1
AMD,n3k63l5,>still cheaper,Intel,2025-07-17 02:20:44,1
AMD,n3k776n,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Intel,2025-07-17 02:27:36,10
AMD,n3ugqr7,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Intel,2025-07-18 16:50:56,1
AMD,n3l78dq,It already has 32MB infinity cache.,Intel,2025-07-17 06:59:18,2
AMD,n3k81ho,The 7840HS is cheaper because it is older.,Intel,2025-07-17 02:32:52,2
AMD,n3lcowt,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Intel,2025-07-17 07:49:34,2
AMD,n3ut4ta,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Intel,2025-07-18 17:47:58,1
AMD,n3kh6ij,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Intel,2025-07-17 03:33:05,-1
AMD,n3levrg,I will see the performance envelope of PTL U and decide should dump my LNL or not,Intel,2025-07-17 08:10:07,4
AMD,n3uubsu,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Intel,2025-07-18 17:53:36,1
AMD,n3ki3cu,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Intel,2025-07-17 03:39:32,5
AMD,n3l7fsl,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Intel,2025-07-17 07:01:04,1
AMD,n3laz95,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Intel,2025-07-17 07:33:37,1
AMD,n3l7mu1,I think I didn't say anything that deviates from what you just said.,Intel,2025-07-17 07:02:48,1
AMD,n3lach0,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Intel,2025-07-17 07:27:39,1
AMD,n1d5sl1,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Intel,2025-07-04 20:26:29,20
AMD,n1d44wc,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Intel,2025-07-04 20:17:30,15
AMD,n1hmxgn,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Intel,2025-07-05 16:10:42,3
AMD,n1je3rl,"who needs quality control, what can go wrong?",Intel,2025-07-05 21:53:07,3
AMD,n4zj8x9,Gigabyte is trash,Intel,2025-07-24 22:41:41,1
AMD,n1j4fq9,The Elon Musk method,Intel,2025-07-05 20:58:12,3
AMD,n2m6czi,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Intel,2025-07-11 20:40:54,1
AMD,n25f2x8,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Intel,2025-07-09 10:28:21,1
