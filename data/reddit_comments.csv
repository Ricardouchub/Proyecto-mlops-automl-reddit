brand,comment_id,text,subreddit,created_utc,score
Intel,o3ugjxp,"> At first I was told between the balanced mode on Microsoft Windows and balanced mode on Linux there can be up to a 15 Watt difference in the PL1 state. Then I was told the power limits were the same under Windows and Linux for this laptop, but that MSI programmed their balanced defaults lower than Intel recommends. Intel expects their OEMs to have higher PL1 minimum values for balanced mode but MSI set it lower at 15 Watts min and 30 Watts max for at least this laptop model.   Wait what, why did MSI set the power so low on the Prestige 14....   I really wanted the Prestige 16 Flip, hope that one has a higher power limit",hardware,2026-02-06 04:18:57,12
Intel,o3ucdeu,"This is a continuation of [the CPU-focused article from a couple of days ago](https://www.reddit.com/r/hardware/comments/1qv0bxv/intel_panther_lake_shows_strong_linux_cpu/), reviewing the integrated graphics now.  Bit disappointing compared to Windows (down ~31% in Cyberpunk), but it is Intel after all - there had to be _some_ flaw.  Hopefully drivers will improve over time.",hardware,2026-02-06 03:51:26,23
Intel,o3uj0fo,"The numbers I found interesting are the vkpeak tests, just like the [B580 ](https://www.phoronix.com/review/intel-arc-b580-gpu-compute/3)before it (though, OpenCL), Xe arch has a strong showing for precision compute.",hardware,2026-02-06 04:35:58,9
Intel,o3vp5tb,Looks like AMD is still king on Linux thanks to Valve doing a lot of the heavy lifting in mesa for them.,hardware,2026-02-06 10:41:48,10
Intel,o3ucpvf,"Seems like Xe drivers are still horrible, but Panther Lake brute forces its way to a win.",hardware,2026-02-06 03:53:39,17
Intel,o3uvucl,Has there been any articles commenting on battery life on Linux with Panther Lake? I know this article and the previous one was focused on perforamnce in the CPU and now iGPU but any word if battery life can be as ood as windows?,hardware,2026-02-06 06:12:20,3
Intel,o3ufgw7,"It is so odd to see the infatuation of the writer with Strix Halo wanting to compare it. It isn't comparable, it is a much larger chip, it is like comparing a Snapdragon Elite to a M4 Max. The writer keeps lamenting paragraph after paragraph about it.  They should compare comparable things. This fiat 500 isn't as fast as this Tesla type of thing is weird.",hardware,2026-02-06 04:11:38,16
Intel,o3ui8jz,"I understand there's a lot of fanfare around Arc and Panther Lake in this sub. Many are firm believers that Intel will finally break the duopoly of AMD and Nvidia and while that may be the case, everyone seems to gloss over the driver side of things, not to mention the massive CPU overhead.   Just yesterday, I was recommended the latest video from YouTuber ""zWORMz"" benchmarking the 'ancient' RDR2 on a B580.  And even on Windows, the guy was frustrated with various driver issues and how the GPU wouldn't even run in his mid-range machine with an i5-12600K. Some issues even popped up right when he was recording.  With that in mind, I can only imagine the experience on Linux!",hardware,2026-02-06 04:30:33,3
Intel,o3vi0et,Should be compared to Strix Halo based on price.   The power consumption does not look all that great:  > The Core Ultra X7 358H came out on a geo mean basis to 1.23x the performance of the Radeon 890M graphics with Strix Halo but at 1.25x the power consumption on average. The peak power consumption was also around 1.25x that of the Ryzen AI 9 HX 370. So rather even on a performance-per-Watt basis overall.  Intel has a lot of work to do. 2 nodes advantage and still just matching Strix Point in power efficiency.,hardware,2026-02-06 09:35:17,3
Intel,o3vnv5t,Semi serious answer probably they're trying to not making raptor lake repeat. On more related to the laptop msi probably trying to win some battery life benchmark or trying to not breaking their laptop chassis and components quickly because of heat.,hardware,2026-02-06 10:30:06,9
Intel,o3vr1i3,It’s also not the highest performance profile.,hardware,2026-02-06 10:58:19,2
Intel,o3uen4k,The fix that Will improve Nvidia performance in Linux should improve Intel performance as well,hardware,2026-02-06 04:06:09,15
Intel,o3xdz6s,"Redhat/Fedora worked a ton on the AMD Drivers as well, a lot of stuff in the RADV Driver says  >> based in part on anv driver which is Copyright © 2015 Intel Corporation  I think some of the people working at Valve use to work for redhat.",hardware,2026-02-06 16:36:57,7
Intel,o3uiby5,"at least the drivers were 100% stable  >Once sorting through the low power limits on the MSI Prestige 14 laptop, the Arc B390 graphics with the Core Ultra X7 358H were rather impressive on Linux. Besides the stellar integrated graphics performance, there were not any stability issues or any other functional problems to report with the Panther Lake open-source Linux graphics driver testing thus far. Thanks to Intel for providing the Panther Lake laptop review sample and more Panther Lake Linux benchmarks are forthcoming on Phoronix.",hardware,2026-02-06 04:31:12,29
Intel,o3ui2st,"I think they're trying to get ahead of all the comments complaining that Strix Halo wasn't included/accusations of bias in favor of Intel by not including AMD's top of the line (even though, as you say, it's in a very different weight class).",hardware,2026-02-06 04:29:26,12
Intel,o3uk2zw,"There's nothing wrong comparing it to Strix Halo. Regardless of the data measured from Strix Halo the conclusion will inevitably be made on price/perf, perf/area etc. It's simply another interesting data point that is shared and pull comparisons from, which I am all for. Phoronix is one of the few outlets that have such a wealth of data for every HW release, especially when it comes to linux and compute",hardware,2026-02-06 04:43:26,19
Intel,o3vop8u,"If Intel tries to sell the 12 xe version in $1500 plus laptops, it has to compete with strix halo not just with strix point.",hardware,2026-02-06 10:37:41,-6
Intel,o3uko6t,[https://www.phoronix.com/review/intel-b580-opengl-vulkan-eoy2025](https://www.phoronix.com/review/intel-b580-opengl-vulkan-eoy2025)  It's getting there,hardware,2026-02-06 04:47:33,9
Intel,o3v0tsd,"Yeah, the Intel Vulkan drivers on Linux especially is still pretty bad, among the benchmarks linked in the post is FurMark which has both an OpenGL and Vulkan version, and Intel takes a noticeable hit in performance when using Vulkan while AMD sees basically no difference between OpenGL and Vulkan. The Vulkan driver being the weak part is a big problem since Vulkan is what's used for running games from DirectX 8 up to 12 by default, and for DX12 Vulkan is the only choice.  In my case I have a Framework 13 with the Intel Core Ultra 5 125H (Meteor Lake) and the Arc (Alchemist) 7-core iGPU, and I can't get a stable 60fps on Forza Horizon 5 no matter what I do, even disabling every graphical effect possible and dropping the resolution to 1024x768 doesn't work (if anything it's worse since the minimums stay the same so the difference between average and minimum just gets larger and the stutters feel way worse), while the LCD Steam Deck can manage 60fps flat perfectly fine at 1280x800 minimum settings at least, and it sips less power then too (under 15W vs 25W for the Framework), even though the CPU is just Zen2 which shouldn't be faster than the Meteor Lake P-cores, and since it's the LCD version the RAM is just LPDDR5-5500 so memory bandwidth shouldn't be too different from the DDR5-5600 in the Framework 13.",hardware,2026-02-06 06:54:34,8
Intel,o3vlhnc,"This is a GPU test on linux, which is not Intel arc driver's strongest suit. They did great in CPU test on linux actually, a good improvement.",hardware,2026-02-06 10:08:11,9
Intel,o3voe66,"There is definitely something wrong with the drivers on the Linux side. In Windows Panther Lake was measured to beat both Arrow Lake-H and Strix Point in perf/watt comparison easily, and was even beating Radeon 8060S in the Ryzen AI Max+ 395 until 30W.",hardware,2026-02-06 10:34:52,10
Intel,o3wz2cx,"This is due to Linux drivers being horrible, not 12Xe being deficient.  In Windows, the gains are pretty obvious.  Also, this is a GPU test",hardware,2026-02-06 15:26:29,5
Intel,o40mp0b,It's 1 node advantage N4->N3.,hardware,2026-02-07 02:51:00,2
Intel,o3vrdxo,What makes you think so?,hardware,2026-02-06 11:01:18,5
Intel,o3x4ghu,I'm out of the loop. What fix are you referring to?,hardware,2026-02-06 15:52:25,3
Intel,o3vr8fn,What’s the cheapest LAPTOP with strix halo?   Mind you strix point launched back in July 2024 in also $1500 up laptops for 890m models.,hardware,2026-02-06 10:59:59,14
Intel,o3vob4g,"> Yeah, the Intel Vulkan drivers on Linux especially is still pretty bad  I hope Intel layoffs of Linux team didn't impact their driver development long term. I'm not confident if AMD is the only viable choice in Linux ecosystem in the future because of potential intel linux performance regression compared to pre xe graphics.",hardware,2026-02-06 10:34:06,2
Intel,o3vgo2h,"I feel you. Assassin's Creed Syndicate runs at maximum \~30 FPS on Linux (normally lower) and almost double under Windows on my Intel Lunar Lake. Even on CachyOS and Nobara, and using some additional flags, the performance is not as good as on Windows.  My workaround was to create a Windows 2 Go installation on an external disk and use it to play games, which is a pity because I really wanted to get rid of Microsoft.  As for the Steam Deck, do you feel that you can play all your titles reasonably well? I was holding off on buying it because hardware-wise it seems to be weaker. But it seems to still manage to be more optimised?   Thanks",hardware,2026-02-06 09:22:12,1
Intel,o42kewf,"point taken, but the CPU is on 18A? So it's like 1.5 node advantage.",hardware,2026-02-07 12:40:31,1
Intel,o3xr6bq,"For DX12 games at least, Intel suffers from the same issues NVIDIA does due to a similar reason",hardware,2026-02-06 17:39:36,10
Intel,o3warsa,"I read that the issue on Nvidia is also present in Intel, and the fix will be done for all cards, so thats why i believe that it should help Intel GPUs there   But we have to wait and see",hardware,2026-02-06 13:19:29,7
Intel,o3xbmt3,"Vulkan API has a fix for the DX12 translation layer, its in the Nvidia Drivers now just it has to be added to DXVK, Wine, Proton etc.",hardware,2026-02-06 16:26:10,8
Intel,o3vjhl0,"Unfortunately I don't have the Deck anymore since I realized I'm not really in the target demographic, I was mostly interested because it looked like it could be a little PC that runs Linux natively, but the thickness of the controller part means it takes up more space than a laptop when put in a backpack, and I don't game on the go much anyway, hence the Framework lol.  As for ""can it play all my titles reasonably well"", I don't remember any specific titles that didn't run at all, so I guess the answer is yes? I do remember having tested Oblivion Remastered and Dragon Age Veilguard, both are Steam Deck Verified, and they do indeed run, but it's very much a 30fps experience reliant on FSR to achieve even that, so I'd guess it probably won't handle newer or heavier titles any better. If you have any specific titles you care about I'd suggest checking ProtonDB and filter the reports to just Steam Deck ones to get a picture of how the game might run, and sometimes games are marked unsupported by Valve but does actually run with caveats like input quirks so you'd be able to play if you're fine with those quirks.",hardware,2026-02-06 09:49:24,3
Intel,o49ogdh,"18A is basically N3B, so no",hardware,2026-02-08 15:51:54,1
Intel,o3vmjv5,"Thanks for sharing your experience. I'm not a big gamer myself, that's why I ended up buying the Lunar Lake laptop (battery life is more important than gaming), but yeah, I should have read a bit more on the Intel situation regarding gaming on Linux.   Anyway, thanks again. I'll take a read on the protondb page",hardware,2026-02-06 10:17:58,2
Intel,o49osgb,N3B doesn't have backside power delivery,hardware,2026-02-08 15:53:34,1
Intel,o43a5qo,Can't wait for the 10% improvement and 50% extra cost gen over gen.,hardware,2026-02-07 15:14:25,31
Intel,o44108k,">The fact that Intel will use nVidia technology in the iGPU sector in the long term  You're takin the nv intel deal outta context. Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus. It's the premium dgx or halo type prosumer skus that are going with nv ip. There might be some gaming capable devices but just like strix halo they ain't primarily aimed at gamers. The price just ain't worth it compared to a standard mobile cpu + nvidia dgpu combo  Idk why this post is even a thing. All manufacturers are delaying dgpu products for gamers because they have shit margins compared to dc gpus. Nvidia's dc margins in the 70+%, why would they care about gamers? Gaming gpus have worse margins than diy cpus which have worse margins than dc gpus. It's the same for amd  Do ya know **how much money gamers are making for nvidia each quarter? $4.5-5b. How much do ai gpus make for nvidia? Over $50b.** How much for amd? Estimated at $4b+ for 2026. **Amd's making the equivalent of capturing 100% of the gaming gpu market from ai gpus and with even higher margins.** Why would they want to sell to gamers?  Fact that people just gotta face even if they hate it: The gamer money is nothing compared to the ai money. People are mad, but no amount of yelling at the sky and hoping for the ""bubble to pop"" will change that.",hardware,2026-02-07 17:26:53,16
Intel,o447pem,">The fact that Intel will use nVidia technology in the iGPU sector in the long term and that the development of HPC/AI accelerators no longer has much to do with consumer graphics cards from a technical standpoint also plays a role here.  If anything Nvidia just gave Intel another free year to catch up to the 50 series   By the time 2027 ends, Core Ultra 400 series iGPUs might actually be as powerful as a 5050 (but only use 1/3 of the power) lmao",hardware,2026-02-07 18:00:04,7
Intel,o4505hr,"Nvidia officially announced a vera rubin 128gb using gddr7 that's coming late 2026, the cutdown version of that should be the 6090, like the 5090 is to the rtx pro 6000 96gb. So I think there will be at least an announcement of the rtx 60 series in early 2027, even if its a paper launch and only for the 6090. I'd trust that more than rumors",hardware,2026-02-07 20:24:56,1
Intel,o43qon0,"If Intel seizes the opportunity and makes the follow on to Panther Lake and Nova Lake with twice the number of Xe tiles, then this may meet the needs of 80% of the gamers, and Nvidia and AMD may find themselves with no commercial market for discrete GPU cards.  The high end Pather Lake has 12 Xe cores, and it brings 1080p gaming to all day laptops. I wonder what 24 Xe would do, and of course the same approach applies to doubling whatever Nova Lake comes out with.  Intel could repeat the early days of the PC market when embedded VGA graphics eliminated the need for cards like the Diamond Speedstar and all the other discrete cards like it.",hardware,2026-02-07 16:36:03,1
Intel,o45ontc,"I bought a 5090 a few months ago as I saw the RAM situation starting to go crazy in the background in the industry, plus I game at 4K  Honestly this will be very tough years for PC gaming across DYI and gaming laptops",hardware,2026-02-07 22:38:00,1
Intel,o44er96,Outlook is good for those who bought a 5090 T MSRP,hardware,2026-02-07 18:34:40,-3
Intel,o43rnqt,"The sad thing is that I'll likely get it anyways, not for the performance, but because of the spicy power connectors I want to keep my card under warranty...",hardware,2026-02-07 16:40:51,3
Intel,o43xyqx,"Don’t worry, DLSS 5.5 / FSR 4.0 will be software locked to only work with the new ones",hardware,2026-02-07 17:11:50,-12
Intel,o45ny6i,"> Why would they want to sell to gamers?  Well.   > $4.5-5b.   4-5 billion dollars isn't nothing. Yes, it won't be the main focus anymore, but they would be fools to just ignore the billions",hardware,2026-02-07 22:34:01,3
Intel,o46frs3,"> Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus.  Indeed. But for this, their Xe3p will be probably good enough for years. So, why invest in Xe4 or more Xes - from Intel's perspective?",hardware,2026-02-08 01:22:32,1
Intel,o43bpnk,"Thank you for your submission! Unfortunately, your submission has been removed for the following reason:  * It is a submission that is largely speculative and/or lacks sufficient information to be discussed.  Rumours or other claims/information not directly from official sources **must have evidence to support them.** Any rumor or claim that is just a statement from an unknown source containing no supporting evidence will be removed.",hardware,2026-02-07 15:22:20,1
Intel,o46fik4,"Consumer/Gaming-Rubin is _not_ a cutdown version of HPC/AI-Rubin. Different architectures, different chips, completely different projects. nVidia just give the same codename, nothing more (same as with Ampere und Blackwell before).",hardware,2026-02-08 01:20:54,4
Intel,o43vj7c,We've already seen how this plays out with Strix Halo. Large iGPUs drive up costs to the point they're not price competitive with discrete cards.,hardware,2026-02-07 16:59:50,13
Intel,o48ep11,Don't big iGPUs like that just get bottlenecked hard by memory bandwidth? Which is why they're not really a thing outside of platforms that sacrifice general memory throughput for iGPU memory throughput?,hardware,2026-02-08 10:44:09,2
Intel,o43t2bg,Intel will release a CPU with a RTX GPU chiplet soon.  It was part of the Intel-Nvidia deal,hardware,2026-02-07 16:47:45,2
Intel,o445r5s,"DLSS works on all RTX cards from 20 series to 50 series, currently. Framegen is limited to some cards, and Multi Framegen is 50 series only. DLSS 4.5 on 20 series cards does seem to incur a bigger performance penalty, but end users still have the option of whether to use or not.  FSR on the other hand is more complicated, and AMD isn't helping. AMD has shown they can bring FSR4 to older cards with the INT8 implementation with very little loss of fidelity, but refuse to so far.",hardware,2026-02-07 17:50:29,12
Intel,o45oto5,everything has an opportunity cost  they don't have unlimited capacity,hardware,2026-02-07 22:38:55,11
Intel,o46ladu,"There are 2 types, the one that uses HBM (which is datacenter only) and the gddr versions, which are both workstation and become consumer cards. Same in current gens the workstation and consumer ones use the same dies when they both have gddr7. The 5090 is cut down rtx pro 6000, and its successor vera rubin cpx is confirmed as gddr7 and coming in late 2026: https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference",hardware,2026-02-08 01:57:26,5
Intel,o447asl,i think Intel going from 12 to 24 cores would be less of a size increase than AMD going from 16 core GPUs on Strix Point all the way to 40 on Strix Halo,hardware,2026-02-07 17:58:05,2
Intel,o449aml,"Rumoured to be Hammer Lake, 2027 or 2028. Probably 2028.",hardware,2026-02-07 18:07:53,1
Intel,o44ray0,"Nvidia refused to backport dlss for a while too. It's probably not a priority for them and I expect amd to work on framegen first for current gen gpus, since the 9070 actually CAN compete with the 5070ti even before you get into the crazy prices I see for the 5070 ti most of the time.",hardware,2026-02-07 19:37:58,-5
Intel,o47gl4v,But there is also value in diversification. Nvidia diversified away from gaming even before LLMs blew up which is what allowed them to be perfectly positioned with CUDA.,hardware,2026-02-08 05:30:49,5
Intel,o46ur4l,"I wouldn't call these “two types,” because that implies a closeness that simply doesn't exist technologically. HPC/AI and consumer/gaming are completely separate fields of development with completely separate chips. I mean different chips at the chip level, not just versions of chips. Take Blackwell, for example:  - HPC/AI Blackwell is based on GB100, which is the dual-chip name for GB102 (~800mm²) - HPC/AI Blackwell Ultra is based on GB110, which is the dual-chip name for GB112 (~800mm²) - Consumer/Gaming Blackwell is based on GB202 (750mm²), GB203 (378mm²), GB205 (263mm²), GB206 (181mm²), GB207 (113mm²)  Regarding Rubin CPX: This is also an HPC/AI chip that will not be used in the consumer/gaming segment, even though it looks technically similar. It is not a successor to RTX Pro 6000, but rather a separate development entirely for inference tasks.",hardware,2026-02-08 02:57:07,3
Intel,o44mvc2,"Maybe, hard to say without knowing more about Xe3P density. Either way it would still end up being a large chip that would in turn require to upgrade memory bandwidth to keep it fed.",hardware,2026-02-07 19:15:04,2
Intel,o46oom2,> Nvidia refused to backport dlss for a while too.  Where do people come up with these fantasies?,hardware,2026-02-08 02:18:45,6
Intel,o473kff,"blackwell has been a bit of an oddball. historically we got datacentre cards like the L40, T4, A40, P40, etc. that were fanless versions of the workstation cards. there's no B40 or B4 which is odd.",hardware,2026-02-08 03:54:55,1
Intel,o48rjeo,[https://steamcommunity.com/discussions/forum/11/3361397532252230605/?l=hungarian](https://steamcommunity.com/discussions/forum/11/3361397532252230605/?l=hungarian)  Now. APOLOGIZE.,hardware,2026-02-08 12:36:47,-3
Intel,o495bny,"This is purely frame generation. 20-series and above has always had day 1 access to the newest upscaling models, which is way more than can be said of AMD.",hardware,2026-02-08 14:08:05,3
Intel,o495jve,No. Locked features are locked features and you are shifting the goalposts on what is good or not.,hardware,2026-02-08 14:09:27,-2
Intel,o32auxy,"Nice, I like this approach, stops the end user who isn't an expert when reading specs from getting screwed over by slow ram killing their igpu performance. If you see the Arc B branding on the laptop you can be rest assured you're getting the right setup",hardware,2026-02-01 23:46:25,49
Intel,o31467g,"It is really impressive what Intel were able to achieve with Arc B390, using a traditional 128-bit memory bus.  Nova Lake (2027) is rumoured to keep the same memory setup same (perhaps bump up to 10700 MT/s), while [bringing a modest 25% improvement](https://www.reddit.com/r/hardware/comments/1qmk1e9/intel_nova_lake_xe3p_igpus_could_be_25_more/).  Razer Lake (2028) could be the next big leap forward, if it adopts LPDDR6.",hardware,2026-02-01 20:10:16,99
Intel,o352spx,"so 7467 MT/s or you lose the Arc badge, kinda savage lol",hardware,2026-02-02 11:51:51,7
Intel,o31rb60,I'm guessing lpCAMM2 would work as it can reach that 7467 MT/s no?,hardware,2026-02-01 22:03:16,14
Intel,o314ms4,I assume they also need 2 DIMMs to get the Arc branding?,hardware,2026-02-01 20:12:30,9
Intel,o33jrpo,Can someone with that special kind of mental situation explain the 140T vs b390 vs Iris Xe to me,hardware,2026-02-02 04:03:22,2
Intel,o316x95,They don't sell them with on-package memory?,hardware,2026-02-01 20:23:43,2
Intel,o35u0pc,"Man it's tragic that the CEO that saved the company, pat gelsinger created this product and got fired before he could see his projects come to fruition because the board was only concerned with short term profits and building chips is a long term endeavor. Now they are going to ruin the company again",hardware,2026-02-02 14:40:28,1
Intel,o31f948,Nobody seems to be talking about how Panther Lake is going to be more expensive than Gorgon Point,hardware,2026-02-01 21:04:57,-9
Intel,o329l0l,"So yeah, as I assumed, these will be closer to Strix Halo pricing than Strix Point.",hardware,2026-02-01 23:39:18,-9
Intel,o312xyz,"Hello Forsaken_Arm5698! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-01 20:04:19,0
Intel,o32rd2i,"This isn’t entirely new, I think with the first generation of Arc iGPUs you had to have some minimum bandwidth to get to call it “Arc”, otherwise it was the same generic “Intel graphics” listed here. _Might_ even date back before Arc with “Iris” graphics branding?",hardware,2026-02-02 01:18:11,19
Intel,o3159fc,"> Razer Lake (2028) could be the next big leap forward, if it adopts LPDDR6.  Razor Lake won't. They'll just reuse the NVL construction. Titan Lake (2029?) would be the next big step assuming they don't go for a 3rd reuse, though there are rumors about it sticking with Xe3p. Xe4 should be a big leap though, assuming it's timely enough.",hardware,2026-02-01 20:15:34,40
Intel,o3201du,It should work fine based on their documentation.,hardware,2026-02-01 22:47:20,9
Intel,o319lrn,"Yeah, it's called Strix Halo and you do not see it anywhere because of cost.",hardware,2026-02-01 20:36:55,95
Intel,o315f3c,It raises costs a lot for a mainstream platform. Intel seems to have struck a good balance for the target market.,hardware,2026-02-01 20:16:19,44
Intel,o31hvo3,We used to have triple channel with Intel Nehalem.,hardware,2026-02-01 21:17:39,24
Intel,o316lgl,"256 bit memory bus you mean, which is technically 16 channels of LPDDR.",hardware,2026-02-01 20:22:06,6
Intel,o31b4xs,"Unfortunately only way such chips will get quad channels is if RAM will get into SoC. Like in Apple M series, ~~AMD's Strix Halo~~ or Intel Lunar Lake. Or wide bus soldered RAM with APUs like AMD's Strix Halo.   But that comes with no upgradeability/fixability and higher cost in general.  edit: correction as u/bazhvn pointed out",hardware,2026-02-01 20:44:28,3
Intel,o33z41s,wouldnt that make echo issues worse? the current board manufacturers are already doing everything possible to not invest a single cent into fixing this.,hardware,2026-02-02 05:51:39,1
Intel,o33reiw,8 ram pieces per laptop? thats a lotta ram,hardware,2026-02-02 04:54:05,1
Intel,o32veo9,The problem is they're doing better in the sector that isn't making any money these days. All the money is in AI currently and they're not getting any of it.,hardware,2026-02-02 01:41:43,5
Intel,o35uvib,"The guy who turned Intel around got fired lol. They were mad that Intel wasn't doing well but they didn't give enough time for the dudes projects to come to fruition. I.g. he was CEO for a few years but new chips take 5 years to build. Intel is going to have a short golden period while pat gelsigners projects come to fruition and then after like 3 years they are gonna be back to shit. The new CEO seems like the same as the ones before pat, just cares about next quarters profit and nothing else.",hardware,2026-02-02 14:44:54,4
Intel,o32syqr,Not the kind of being on fire you want though. Let's be honest.,hardware,2026-02-02 01:27:34,-6
Intel,o31o68v,"Given Intel only advertises LPDDR5x with the 12Xe tile regardless of the memory controller, I think it's safe to say they will not allow OEMs to use DIMMs with it.",hardware,2026-02-01 21:48:00,28
Intel,o32a083,"doesn't matter, can't get SODIMM LPPDR5X. and CAMM2 modules are dual channel",hardware,2026-02-01 23:41:40,12
Intel,o33r45n,"Iris Xe was Intel's more premium iGPUs for a while. Then Lunar Lake launched with a ~~140T~~ 140V iGPU that was their first actually competitive iGPU.  Now Panther Lake has a B390 iGPU option. Intel has finally, with the launch of Panther Lake, decided to give their premium iGPUs the same naming scheme as their dGPU line, because ~~""140T""~~ 140V made no sense, but ""B390"" does (for the most part)  Edit: Case and Point: Got 140V and 140T mixed up",hardware,2026-02-02 04:52:06,9
Intel,o33qlif,"You have a CPU that costs $150 for you to make that you sell to OEMs for $300. This gets you a 50% margin.  You have a CPU that costs $150 to make, then you add $50 of RAM on the package, so total is $200. OEMs aren't gonna like you taking a profit on RAM as a middleman when taking profit on RAM is one of their key revenue sources on laptop upsells. So you sell the RAM for cost, meaning you charge $350 for the SoC.  Yes, you've made the same nominal profit of $150, but your margin % dropped from 50% to ~43% *and* this simple math example doesn't take into account that there is a cost involved with having to source the memory yourself (i.e pay staff to manage contracts, make and track orders, receive shipments, package the RAM, etc.)  That's why it's very difficult for on-package memory to gain traction in the Windows PC world: Most CPU makers don't want to take the margin hit, and most OEMs would prefer to just handle memory quantity and product tiering themselves.",hardware,2026-02-02 04:48:31,13
Intel,o317syc,"Nope, that's why lunar lake is a one time thing, it's too expensive and has little flexibility",hardware,2026-02-01 20:28:03,20
Intel,o36g7i5,"I don’t think that concern for short term profits was unfounded in this case, Intel was having crisis levels of cash flow problems and radical measures needed to be taken to shore up investors. Intel was on the verge of being a failed business and frankly they are not out of the woods yet",hardware,2026-02-02 16:27:39,6
Intel,o37mzf9,"> the CEO that saved the company  How did he save the company? His legacy is the fabs, which are still a dumpster fire.",hardware,2026-02-02 19:43:09,3
Intel,o31i1hq,"Because unless someone can provide the figures, then what's there to discuss? Whats the cost of a PTL-H SoC? How much more does it cost than Gorgon Point? And how does the cost of RAM/SSDs impact that overall total cost between the two when factoring in the total laptop BOM?",hardware,2026-02-01 21:18:26,24
Intel,o31g71c,"PTL being a better product prob helps justify a higher price tag.   Though how much higher is kinda hard to tell, with the whole component shortages due to the AI boom prob also playing a part.",hardware,2026-02-01 21:09:34,16
Intel,o32b1fs,Does not seem to be the case based on newly released laptop prices. Remember when full die strix point laptops started at $1600 back in July 2024?,hardware,2026-02-01 23:47:25,14
Intel,o337tk4,"If your laptop only has 1 stick of RAM, it'll show up as Intel UHD graphics and a few of the execution units are disabled. Slapping in another stick changes it to Iris Xe and unlocks all the EU.",hardware,2026-02-02 02:51:42,19
Intel,o31k751,> Razer Lake  Bloated batteries after 1 year will be a requirement for laptop manufacturers to use this./s,hardware,2026-02-01 21:28:47,16
Intel,o31fzuf,Xe4? When does nvidia come into the picture?,hardware,2026-02-01 21:08:36,6
Intel,o3loez0,It can be a leap in performance with LPDDR6 and more Xe3p cores. NVL-P stays on 12 Xe cores and same bandwidth.,hardware,2026-02-04 21:13:42,1
Intel,o32ir90,The power of a 4060 for the cost of a 4080!,hardware,2026-02-02 00:29:55,50
Intel,o33094p,"A better compromise for gaming handhelds would probably be 192 bit bus, running ~16 XE3 cores. Wider and slower can work very nicely for power efficiency but cost is an important consideration too. Ofc the real killer is economies of scale - nobody's going to do a bulk order of 5 million of these things to make it worth their while.",hardware,2026-02-02 02:08:56,9
Intel,o34in6h,"Intrinsically, 4ch is not *that* much more expensive. And even considering the CPU config STX-H has more than just product cost being reflected in its pricing.",hardware,2026-02-02 08:46:30,1
Intel,o32nfew,> Intel seems to have struck a good balance for the target market.  Technology and market-awareness... both are important,hardware,2026-02-02 00:55:36,1
Intel,o31mfsq,Shout out to my sweet i7-920 and 6GB of Dominator DDR3,hardware,2026-02-01 21:39:36,14
Intel,o34t7gy,quad channel on X79/X99,hardware,2026-02-02 10:28:25,3
Intel,o35piou,On consumer boards?,hardware,2026-02-02 14:16:33,3
Intel,o33zd42,5 different sticks running single channel was perfectly fine ever more further back :),hardware,2026-02-02 05:53:40,4
Intel,o31i4ua,"256=32*8, and also ddr5 channels being 32-bit is superficial",hardware,2026-02-01 21:18:53,12
Intel,o31mxrs,Eh Strix Halo doesn’t employ DRAM on package.,hardware,2026-02-01 21:42:02,17
Intel,o33zm3i,"Yes, the sector does only 12 billion in revenue, completely nothing.",hardware,2026-02-02 05:55:41,1
Intel,o3783no,">The guy who turned Intel around got fired lol.  Intel isn't even turned around yet.   >They were mad that Intel wasn't doing well but they didn't give enough time for the dudes projects to come to fruition  The problem was that, if external customers would have ended up using their fabs, they would have known before hand, since the contracts have to be inked and it takes a year or two at least to even just port IP over.   >Intel is going to have a short golden period while pat gelsigners projects come to fruition and then after like 3 years they are gonna be back to shit.  Why do you expect Intel to have a short golden period soon?",hardware,2026-02-02 18:34:50,4
Intel,o31org8,what I meant was that there would be 2 memory modules. A laptop with only one gets half the memory bandwidth,hardware,2026-02-01 21:50:51,1
Intel,o33zwob,"Correction, Lunar Lake had the 140V iGPU, which was based on Xe2, or Battlemage.     Arrow Lake was the one that got 140T, which was basically a A380 (Meteor Lake got an A380 but without XMX).",hardware,2026-02-02 05:58:06,10
Intel,o34izoh,"There is one possible out, however. If someone can figure out how to design the package such that OEMs can own the memory attach.",hardware,2026-02-02 08:49:46,7
Intel,o31odce,"There is some humor in how their necessary front-loading of LNL packages mitigated needing to source more LPDDR5x after it spiked in cost. They sort of lucked into that one, with LNL able to simply clear out existing stock at its own natural pace during the transition to PTL.  \[Edited a weird sentence\]",hardware,2026-02-01 21:48:57,12
Intel,o3198jm,It's not expensive. The challenge is with the margins of Intel reselling memory to OEMs.,hardware,2026-02-01 20:35:07,12
Intel,o319pjz,Even QC is only doing on package memory for 1 SKU (now 2 but same),hardware,2026-02-01 20:37:26,5
Intel,o31hxkg,"> Though how much higher is kinda hard to tell, with the whole component shortages due to the AI boom prob also playing a part.  Ye, people comparing launch prices of today vs previously launched products. Are up for a rough awakening of what those previously launched products will cost in 6+ months from now when the DRAM apocalypse catches up to them and current stock is depleted.",hardware,2026-02-01 21:17:54,19
Intel,o32in8w,You can buy mini pc's with Strix Point for $600 now though. I doubt they're ever going to sell devices with this iGPU for a price that low with those requirements. Not with the way RAM prices are now.  It's going to be closer to Strix Halo pricing than Strix Point.,hardware,2026-02-02 00:29:19,-10
Intel,o32pvfn,"And overhauling the BIOS every 6 months, with a new BIOS X+1.",hardware,2026-02-02 01:09:34,10
Intel,o31xpgj,"If they ever use Nvidia, it'll be for Strix Halo tier chips. Intel isn't getting rid of their GPU department entirely.",hardware,2026-02-01 22:35:14,27
Intel,o338hbo,No idea. I *am* working on talk from before that announcement.,hardware,2026-02-02 02:55:30,6
Intel,o31k8uo,Hopefully never at this point.,hardware,2026-02-01 21:29:01,12
Intel,o35quus,I think those will be very AI-centered products.,hardware,2026-02-02 14:23:43,1
Intel,o3lq2nb,"When I say ""reuse NVL construction"", they will almost certainly reuse the SoC die. Meaning, no LPDDR6, and at best a bin or two higher LPDDR5 speeds. For that matter, they will almost certainly reuse the NVL graphics tiles as well.",hardware,2026-02-04 21:21:42,1
Intel,o33dguo,"idk man, it has a 16c32t zen5 CPU attached to it... so...",hardware,2026-02-02 03:24:50,16
Intel,o32ull5,More like mid range performance (5060 at BEST) for high end pricing.,hardware,2026-02-02 01:37:03,12
Intel,o319037,"But think of what range the Panther Lake platform covers. The B390 is the top end. You have essentially the same platform support down to the 4Xe entry GPU. So yeah, maybe dual channel is not quite ideal at the far end of the distribution, but you need to compromise somewhere.    If they went for 256b, they'd really need a significantly bigger GPU to justify it. Something like Strix Halo or the cancelled NVL-AX.",hardware,2026-02-01 20:33:57,20
Intel,o317v6i,"Yes, though it has tons of cache to service it (16 MB GPU L2, 8 MB SLC).  Whatever the means, the end result is what matters. In that regard, it's as good as a 4050. ​",hardware,2026-02-01 20:28:22,19
Intel,o373uqe,"oh boy, this brings back memories, my second ever PC build was this exact setup!",hardware,2026-02-02 18:15:42,2
Intel,o3agic1,My 7900x Intel at 5ghz all core still running today.,hardware,2026-02-03 04:52:26,2
Intel,o3886ls,"Yup, I used a Intel i7 920 with my Asus P6T motherboard and had 3x2GB memory.",hardware,2026-02-02 21:23:14,3
Intel,o3bnzxi,"High-End Desktop (HEDT), but yes. Would later move to quad channel.",hardware,2026-02-03 11:18:18,2
Intel,o34wun0,"Strix Halo has 16x16b LPDDR5X, not DDR5. And it's not superficial, they function exactly like separate channels always have.  It's just a convention to normalise to 64b channels when discussing components to make it clearer what the total bus width is, independent of differing channel widths.",hardware,2026-02-02 11:01:44,6
Intel,o31o7p0,Yeap. Thanks. I've made correction.,hardware,2026-02-01 21:48:12,3
Intel,o3ap2wy,"yea my bad, I was referring to Strix Halo which [literally uses 8 soldered ram pieces](https://cdn.discordapp.com/attachments/722681490097569804/1458271170729082982/Screenshot_20260107_082523_YouTube.jpg?ex=6982a0dc&is=69814f5c&hm=3797caca5cee2b406f2ab3e6567eb04103242084d2efac8f33436c4df44099d3)",hardware,2026-02-03 05:56:56,1
Intel,o341re1,He means by comparison duuh,hardware,2026-02-02 06:13:23,17
Intel,o31unz7,it'd be fabulously stupid to have half-populated soldered memory.  lpddr = soldered.  dimms are slow and power hungry.,hardware,2026-02-01 22:19:49,20
Intel,o34crnr,"> (Meteor Lake got an A380 but without XMX)  This was such a bizarre omission, I don't understand it.",hardware,2026-02-02 07:50:37,8
Intel,o39h81m,"Interesting. So chipmaker (such as Intel or Qualcomm), solders the SoC on package, then sends it off to the device OEMs, for them to solder their choice of RAM on the package.?",hardware,2026-02-03 01:19:19,2
Intel,o31gh6n,In what world is DRAM not expensive rn brother,hardware,2026-02-01 21:10:57,-4
Intel,o32lowc,"Why would the ram affect this but not strix point?   It doesn’t say it can’t support SODIMM to ship barebone mini pcs without ram. It just says it won’t be badged “B390”.   Again why are you using 1.5 year old product pricing vs launch pricing in the first place? Like I said, do July 2024 pricing. If you can’t, then i can’t help you with current pricing.    https://www.lenovo.com/gb/en/configurator/cto/index.html?bundleId=83RWCTO1WWGB1   https://www.lenovo.com/gb/en/configurator/cto/index.html?bundleId=83Q6CTO1WWGB2   Here’s identical Legion 5 laptops with Panther vs Gorgon. Very similar pricing. Anything else?",hardware,2026-02-02 00:45:52,19
Intel,o31scit,Intel would have already been very familiar with how the B390 was going to perform when they signed that deal.,hardware,2026-02-01 22:08:22,17
Intel,o33fkah,"I would've been more interested in a Strix Halo machine if it had FSR4.  I'll be waiting for the next version of it, cause that would make for an amazing gaming tablet on the go, and a great workstation connected to a dock on the desk.",hardware,2026-02-02 03:37:29,22
Intel,o34bmf4,In a power envelope where it can't be used correctly.  It loses in a bunch of benchmarks to M4 Pro that only has 10P cores    That's why it is mini pc only,hardware,2026-02-02 07:39:54,4
Intel,o31bgh1,Well the 4Xe3 nearly at level of 8Xe2 in LNL if you look at game benchmarks that alone is a achievement https://www.ultrabookreview.com/74624-intel-panther-lake-laptops/,hardware,2026-02-01 20:46:04,8
Intel,o31b0m7,Since when is nvlax cancelled?,hardware,2026-02-01 20:43:52,5
Intel,o39mluv,It’s an improper consumer convention to normalize 64b as a channel*,hardware,2026-02-03 01:50:09,0
Intel,o31qc1d,"On the topic, SOCAMM would be a rather nice solution repairability wise whilst not trading much real estate since 256bit requires only 2 modules. Not quite thin and light orientated but mobility is not out of question.",hardware,2026-02-01 21:58:29,8
Intel,o3au2pl,By that comparison noone should ever bake bread because all the money is in cakes.,hardware,2026-02-03 06:39:01,2
Intel,o32qtnw,"Fabulously stupid, but not without precedent. I’m pretty sure “Nuclear Laptops” has covered at least one laptop that supported dual channel but the manufacturer only soldered a single channel without even an empty DIMM slot for the other channel. Granted, the premise of those videos are they’re cheap, ~$300 laptops that are all terrible in one way or another, probably not the _most_ likely to happen to top end Panther Lake.",hardware,2026-02-02 01:15:04,8
Intel,o34is6s,"> lpddr = soldered  Not with LPCAMM, but they haven't yet proposed a single channel module. Definitely not impossible though, and may even be likely.",hardware,2026-02-02 08:47:49,3
Intel,o3args2,pre-2024 Asus Zephyrus laptops actually do use 1 soldered memory + 1 SODIMM slot,hardware,2026-02-03 06:16:35,2
Intel,o39reli,"Yes, that would be the ideal. The current process has both the memory and SoC attached at once. If you could separate out those two steps, then you could have the customer source the memory, and problem solved.  And this isn't some pie-in-the-sky fantasy either. Would require some packaging innovation, sure, but I don't think anyone seriously questions the feasibility at a conceptual level. The problem is that the overlap between both ""can"" and ""want to"" has been pretty low.",hardware,2026-02-03 02:17:08,3
Intel,o31j798,It was removed before the DRAM crisis because managing so many SKUs is a nightmare for Intel.,hardware,2026-02-01 21:24:01,11
Intel,o33d7ur,Having it be on package isn't more expensive then on the motherboard.,hardware,2026-02-02 03:23:20,1
Intel,o33bj4i,"The Intel version of that laptop only has either a 356H or 386H, those SoCs only have 4 Xe cores so they wouldn't have the B390 or B370 branding regardless (that's only on the models ending in --8H)  Soldered RAM is mandatory for the --8H SKUs, the Intel product pages for them only mention LPDDR5X  [Intel® Core™ Ultra X9 Processor 388H](https://www.intel.com/content/www/us/en/products/sku/245526/intel-core-ultra-x9-processor-388h-18m-cache-up-to-5-10-ghz/specifications.html)  Other SKUs mention both LPDDR5X and DDR5  [Intel® Core™ Ultra 9 Processor 386H](https://www.intel.com/content/www/us/en/products/sku/245529/intel-core-ultra-9-processor-386h-18m-cache-up-to-4-90-ghz/specifications.html)",hardware,2026-02-02 03:13:18,-2
Intel,o32p8rq,">Why would the ram affect this but not strix point?   Because Strix Point doesn't have a 7467 MT/s RAM requirement like this one does to be called a B390 iGPU product.  That's the whole point of the article, for it to actually be a B390 it needs to have 7467 MT7s RAM or better. Which will make it very expensive.  >Again why are you using 1.5 year old product pricing vs launch pricing in the first place?   You skipped my point, again. This chip (in the configuration which makes it ""B390"") will never be as cheap as Strix Point. The B390 going to be a premium priced product closer to Strix Halo than to Strix Point. Yet Intel compares its performance to Strix Point and not Strix Halo.  >Here’s identical Legion 5 laptops with Panther vs Gorgon. Very similar pricing. Anything else?  None of your examples are B390 certified.",hardware,2026-02-02 01:05:53,-9
Intel,o33omqo,"Intel was also not really in a position to turn down a $5B investment. And the deal adds native NVLink support for Xeon, which benefits Intel more than any potential Nvidia deal for iGPUs may hurt them",hardware,2026-02-02 04:35:06,14
Intel,o33ph8o,"I had an HP G1A with 128GB RAM on release in checkout when I found out it was RDNA 3.5, I rubbed my eyes in disbelief but the spec list didn't change. It was the saddest tab closure of my days  That literally more than halved the value proposition of an already obscene price in an instant.",hardware,2026-02-02 04:40:52,7
Intel,o355lxb,"X86 loses in single threaded performance at every power range for consumers. As for the 10P cores. E cores are known to boost multi threaded performance by a lot. The M4 pro has 10 of them. Yes, it'll beat it in some benchmarks.",hardware,2026-02-02 12:13:20,9
Intel,o31chxu,What? Your link is showing the full LNL config being roughly 1/3rd better.,hardware,2026-02-01 20:51:13,6
Intel,o31cw26,"There've been rumors about its cancellation for a while, and with Intel marketing just saying they have no plans for such a product, think it's safe to say it's dead.",hardware,2026-02-01 20:53:10,10
Intel,o31tzax,"Yea, i hope that CAMM2/LPCAMM2 will succeed both on laptops and on PCs.   While CPU dont need typically wide bus with high bandwith, higher MT/s is helpful and CAMM helps with that and thats pain point on PCs.   While simultaneously can have wide bus for iGPU as You say.  Unfortunately with current RAM shortage getting any kind of ram is pain :/.",hardware,2026-02-01 22:16:26,10
Intel,o3aszw4,"so do lenovo thinkbooks.  it's the worst of all worlds- no lpddr, and you have to match the memory speeds and timings to the soldered memory.",hardware,2026-02-03 06:29:35,3
Intel,o336pbe,"No, it's primarily the margin issue. Intel isn't shy about SKUs.",hardware,2026-02-02 02:45:24,9
Intel,o32ughi,"> Because Strix Point doesn't have a 7467 MT/s RAM requirement like this one does to be called a B390 iGPU product.  It does not need to be called B390 to obliterate the 890m. 890m isn’t as fast when using DDR5-5600 as with 7500 (promo material used during the original strix point presentation), either.   > You skipped my point, again. This chip (in the configuration which makes it ""B390) will never be as cheap as Strix Point. It's going to be a premium priced product closer to Strix Halo than to Strix Point.  > You skipped my point, again. This chip (in the configuration which makes it ""B390) will never be as cheap as Strix Point. It's going to be a premium priced product closer to Strix Halo than to Strix Point.  Strix halo is 256 bit memory and a significantly bigger die. Why would just the mere fact that Panther B390 requires soldered LPDDR ram mean it will be comparable to Halo? Does soldered memory automatically make strix point expensive like Halo?   > None of your examples are B390 certified.  It is sufficient to demonstrate that with equalised ram it’s not more expensive than Gorgon.",hardware,2026-02-02 01:36:13,15
Intel,o3407ej,"Also an NVIDIA deal for iGPUs really doesn't hurt them, because it allows them to  develop a Strix Halo level APU competitor for laptops without having to take a risk on trying to compete with NVIDIA's mindshare or technology.     They are still perfectly free to pursue developing lower end iGPUs, which they most likely will be.",hardware,2026-02-02 06:00:34,14
Intel,o31csls,No? Have you checked the correct table it's not 1/3 better in game s,hardware,2026-02-01 20:52:41,6
Intel,o33le5u,With the DRAM shortages Intel could sell more chips with the supply it would eat   Doesn't make sense in current environment sadly,hardware,2026-02-02 04:13:46,1
Intel,o333red,"But we're talking about the B390. And that's what Intel is using in its presentations. I'm making the point that according to this article, the B390 will most likely be priced closer to Strix Halo than Strix Point.  7467MT/s RAM (and above) is expensive. RAM is much more expensive than it used to be, cutting edge RAM speeds even more so.  >It is sufficient to demonstrate that with equalised ram it’s not more expensive than Gorgon.  We're talking about the B390. The one Intel is using in its presentations. None of your examples are B390. The article is about the B-series. We shouldn't use the B390's performance numbers to compare to Strix Point, but only use the prices of the non B-series. They are irrelevant for this discussion.",hardware,2026-02-02 02:28:45,-5
Intel,o31dlw1,"Ah, you're right. Was scrolling on mobile and stopped at the synthetics. Well, good showing then, and also good to see more a focus on actual game performance.",hardware,2026-02-01 20:56:43,7
Intel,o33umwd,"What do you mean? If anything, the current environment would favor such a system.",hardware,2026-02-02 05:17:10,2
Intel,o365bez,MSI Prestige 14 Flip with B390 is up for $1300. The cheapest upcoming Strix Halo laptop is ASUS TUF A14 $1800. Other laptops with Strix Halo are over $2000. Intel Panther Lake is closer to Strix Point in price,hardware,2026-02-02 15:36:52,6
Intel,o31dtxf,No problem seeing 4Xe3 performance so well feels like 12Xe3 is starving for more memory bandwidth,hardware,2026-02-01 20:57:50,3
Intel,o35gckm,OEM's can't buy enough DRAM   Intel can sell panther lake in 16GB laptops    While all the halo stuff is 32GB and mini pcs are 128GB,hardware,2026-02-02 13:25:13,1
Intel,o3410v8,"Yeah, I feared PTL-U is going to be a significant regression from LNL, where the GPU is concerned. Well, I see that's not the case.​",hardware,2026-02-02 06:07:17,3
Intel,o36eimp,"Problem is that dGPU systems would have to source VRAM in addition to DRAM, whereas a large APU only has to source slightly more DRAM.",hardware,2026-02-02 16:19:47,2
Intel,o1yuwnf,"TLDR:    ""The performance advantage over the previous Arc Graphics 140T/140V iGPUs is around 70%. The advantage over the smaller Radeon 800 series iGPUs of AMD Zen 5 is also considerably high (between 50-80% depending on the benchmark).   Although the Strix Halo GPUs are even faster (but not more efficient), they operate at higher power limits. There are only a handful of corresponding devices on the market, which then are also quite expensive.""",hardware,2026-01-27 05:48:08,53
Intel,o22xnz4,"This will be nasty on handhelds, because a 4050 at 720p runs just as fast as a 3070 at 1440p.",hardware,2026-01-27 20:13:29,13
Intel,o20hx1c,What does this mean for handhelds.,hardware,2026-01-27 13:39:04,10
Intel,o1zktl8,"As much as its nice to see it competing with low powered 4050. The 4050 and even  full powered 5050 and 5060 laptops are significantly cheaper than the B390 laptops.  You can get 5070 to 5070 ti laptops for the price of of laptops using the b390  On Best Buy  Dells XPS 14 is 2249 with the X9 388H with 32 GB ram  https://www.bestbuy.com/product/dell-xps-14-copilot-pc-14-2-8k-oled-touchscreen-laptop-intel-core-ultra-x9-388h-2026-32gb-memory-1tb-storage-graphite/J3K4L6QWVR   The G14 is 2399 with a Ryzen AI 9 hx with 32 GB ram AND A RTX 5070 Ti  https://www.bestbuy.com/product/asus-rog-zephyrus-g14-14-3k-oled-120hz-gaming-laptop-copilot-pc-amd-ryzen-ai-9-hx-32gb-ram-nvidia-rtx-5070-ti-1tb-platinum-white/JJGGLHJXQ9/sku/6613954  And that's a premium G14. The Acer 16S with Ultra 9 288H, 32 GB ram and 5070 ti is currently 1699  https://www.bestbuy.com/product/acer-predator-helios-neo-16s-ai-gaming-laptop-16-oled-240hz-intel-core-ultra-9-nvidia-geforce-rtx-5070ti-32gb-1tb-obsidian-black/JJ8V8H38XT  Intel needs to get pricing to RTX 5050 levels or below to be actually competitive.",hardware,2026-01-27 09:32:42,33
Intel,o1z66zy,">and the two render slices each consist of six Xe cores.  I assumed it was 3 slices, of 4 cores each. Is there a benefit to doing it this way, or does it not matter?",hardware,2026-01-27 07:18:46,6
Intel,o2cfs7p,"Intel is back, it will be at 1 trillion market cap in next 18-24 months",hardware,2026-01-29 03:17:22,1
Intel,o1yuic6,"Hello Antonis_32! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-27 05:45:08,0
Intel,o1zdksn,"Stop with these fake benchmarks.  You will see when the benchmarks are not cherry picked, it will be no where near a 4050 lol. They had to gimp the poor 4050 all the way down to 30w, which I don’t even know how it even operates as such low wattages for the b390 to have a chance.  The full wattage 4050 (100w+) is about as fast as a 3060 desktop, this tiny iGPU in real world gaming tests will be at most best case scenario as fast as a full powered 3050, which is still  a massive 50-60% slower than a 4050 laptop.",hardware,2026-01-27 08:24:59,-17
Intel,o200pvk,"Digital foundry did a video where they ran a strix halo at the same tdp as panther lake and it was notably faster, albeit with a much larger die size.",hardware,2026-01-27 11:49:39,16
Intel,o25qn5j,It means the next MSI Claw will be sweet,hardware,2026-01-28 04:46:58,9
Intel,o248cwm,For now nothing but they're in development of a handheld version and when that comes out handhelds are gonna be a lot better. Right now the only strix halo handheld is the gpd 5 and it's expensive the ultra 9 CPUs are going to be more affordable so we might see more options,hardware,2026-01-27 23:52:34,5
Intel,o203epu,"[MSai Prestige 14 Flip AI](https://www.bhphotovideo.com/c/product/1939343-REG/msi_prestige_14_flip_ai_d3mtg_001us_prestige_14_flip_ai.html) $1299, 358H, 32GB/1TB.",hardware,2026-01-27 12:09:32,13
Intel,o1zqy52,"You must have no idea how having to deal with only the iGPU in a laptop simplifies things in terms of drivers on Linux, and especially compared to something like a 4050 which only has 6GB of VRAM.   Not to mention having no need to lug around a 300+ W power brick for a gaming laptop.",hardware,2026-01-27 10:28:33,15
Intel,o1zu2ha,"> Intel needs to get pricing to RTX 5050 levels or below to be actually competitive.  Not really. They offer a product that fits in small form factor and gives very good battery life while also providing with graphics that can play pretty much any game on steam if you know how to navigate the options menu. If you are a student and need a device that is light/portable, has good battery life but also plays games it is a perfect fit.",hardware,2026-01-27 10:55:41,14
Intel,o208cuc,the MSI Prestige 14 is like 1300 iirc,hardware,2026-01-27 12:43:26,5
Intel,o24s54f,It’s waaaaaay too early to compare prices. Give it 6 months time and manufacturing ramping up to see what the b390 laptops are actually priced at. It’s the same for any new shiny device,hardware,2026-01-28 01:34:28,4
Intel,o1zlqot,"Do they? It's not marketed as a gaming laptop in the first place, it was never meant to compete with a 5070 Ti laptop.  We will also have to see prices in a few weeks once more models drop and see how X7 models pricing looks.",hardware,2026-01-27 09:41:21,15
Intel,o1zm4m2,"Intel could maybe possibily drop prices…but they don’t have to. These new devices are targeted for people who dont necessarily care about perf/$. Students, office, devs, travelers, creators   “Does it last all day, is it decent and not sound like a jet?”  And they’ll pay extra for that. Its basically M series   But at least with apple the laptops are made to last...but Intel comptiability is advantagous & people are used to windows.",hardware,2026-01-27 09:44:58,18
Intel,o20nb5q,That's a good price for a Helios you linked... don't expect the 2026 refresh of it to be priced the same when it replaces that model in a few months.   XPS 14 *is* very expensive. But that's always been the case for the XPS. They've always been expensive given their performance level. But you do get some of the best build quality available on a Windows machine in exchange.,hardware,2026-01-27 14:07:08,3
Intel,o206djg,"This is just how the Windows laptop market is. MSRP on new laptops are high, while last gen are on sale, it's always like this I'm guessing you forget that AMD once marketed Mendocino as being in up to $700 laptops.. B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Like you can find Lunar Lake for $500-$600 new these days if you wait for sales and aren't picky about other aspects.  Anyways, yes you can get a stronger GPU laptop for not much more, but those dGPU laptops will eat through battery like no other, so I wouldn't even consider them a direct competition.",hardware,2026-01-27 12:30:12,1
Intel,o20x7et,People aren't buying these for price alone.,hardware,2026-01-27 14:56:20,1
Intel,o1zws13,"I saw this yesterday as well... The edge is dead on arrival at these prices. At those prices, I'd rather get HP zbook with strix halo.",hardware,2026-01-27 11:18:24,-1
Intel,o1zci8w,"It means less of the fixed function units in the render slice, which IIRC is rasterisers and ROPs. Compared to 3x4 it'll be less performant, but also smaller and less power hungry. More generally the ratio of compute to fixed function depends on the complexity of the shading - coarsely, low settings at high resolution would prefer more fixed function, high settings at lower resolution prefer more compute",hardware,2026-01-27 08:15:06,12
Intel,o1zie3d,"It pretty much matches 60w version of 4050 which is good result for an iGPU, it gets beaten handily by 90w but considering the form factor of devices B390 can be used in, the most relevant comparison is to the 60 and 30w version.",hardware,2026-01-27 09:09:56,15
Intel,o204xz4,"It depends on the TDP chosen, as we see in other reviews as at around 35W it's even and under that Strix Halo chokes itself. No point putting Strix Halo in most laptops or handhelds at its cost and power curve. Desktop replacements/workstation, absolutely but if you want to play games on battery you might as well just use PTL.",hardware,2026-01-27 12:20:24,33
Intel,o23xs88,What's that supposed to prove? Obviously a chip with way more transistors is going to perform better.,hardware,2026-01-27 22:58:45,5
Intel,o209i6f,"That's significantly better pricing but by a 65w charger suggests, its a lower-powered X7 chip. Not the full power used in the XPS 14, which comes with 100w charger  Probably 25w tdp. Instead of 45 tdp.",hardware,2026-01-27 12:50:50,4
Intel,o1zxvm6,"To be completely fair: 4050 laptops don't use 300W power bricks. It's a 50W GPU typically, I think the very highest power limit I have seen for one of those is 65W.  It's fair criticism though, you can absolutely squeeze this type of GPU into most ultrabooks if desired though and a 50 tier chip used to be a common inclusion in ultrabook machines with reasonable power requirements.",hardware,2026-01-27 11:27:23,5
Intel,o1zx762,Why do people think the G14 is some chonky laptop? It's similar to a MacBook Pro is terms of weight and size.  And it has all day battery life as well.,hardware,2026-01-27 11:21:52,-3
Intel,o20nnmm,"Different classes of machines each putting their budgets into different things. XPS 14 is basically the most expensive Windows Thin and Light, but it's also *the* standard (on Windows). That budget is going into materials and build quality. You'll always be able to find more performance for your dollar if you're willing to get a thicker, heavier laptop with a plastic chassis",hardware,2026-01-27 14:08:56,5
Intel,o1zm0hr,Its need to drop to like $1000 to be competitive with rtx 5050,hardware,2026-01-27 09:43:55,1
Intel,o203e4o,">These new devices are targeted for people who dont necessarily care about perf/$. Students,  Ah students, famously wealthy and like to splash their cash 😀 /s",hardware,2026-01-27 12:09:25,10
Intel,o1zn5ov,The G14 pretty much lasts all day without sounding like a jet,hardware,2026-01-27 09:54:24,6
Intel,o218o6h,"> B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Not sure how many budget models with B390 will be there, and price might as well go up (for basically every laptop) due to RAM and SSD becoming significantly more expensive over the year.",hardware,2026-01-27 15:49:18,3
Intel,o20odih,There’s a 1300 MSI model. The other models pricing is likely because they’re extra premium laptops that OEMs typically ship new chips in first to jack up prices,hardware,2026-01-27 14:12:36,3
Intel,o1zm816,"It does not lol LMAO.   It will get no where close to a 60w 4050, with its puny 12 low powered xe3 cores, the 8060s with its massive gpu die and high tdp is still 13% slower than a 4060 laptop in 20+ game average tested by Jarrod’s tech.    Watch when the real gaming benchmarks come, it’s good for an iGPU but no where close to a 4050.",hardware,2026-01-27 09:45:50,-12
Intel,o20nv3l,"XPS 14 is likely using the 100W charger for faster charging times, not to accommodate higher TDPs.",hardware,2026-01-27 14:09:59,13
Intel,o21e19g,"Ptl doesn't need more than 45w at most, perf gains after 30w are low. The difference is just core count, GPU is the same. I'd argue more cores for a regular person doesn't matter much.",hardware,2026-01-27 16:12:37,4
Intel,o20dh0d,"Probably, but that's what PTL is specified at. 25W TDP and 65-80W power cap. The B390 that everyone wants out of these should perform basically the same here. You don't need a 388H to have that GPU. One thing I'll admit I didn't notice before, and that may hurt it slightly, is that the RAM is running at 8533 instead of the full 9600mt/s.",hardware,2026-01-27 13:14:40,6
Intel,o20nor6,No that seems to be just about enough power considering that the full power draw of a CES display unit used to test games by reviewers drew 60 watts.,hardware,2026-01-27 14:09:05,2
Intel,o236a2c,"65W seems right, they noted 60W total system draw when testing it in games at CES on one machine.   The X9 only seems to have +100MHz clock speed over the X7, everything else seems the same.",hardware,2026-01-27 20:52:35,2
Intel,o26ji0i,Funny thing is a lot of laptops now come with power bricks much stronger than the laptop actually needs and this is because its used for fast charging.,hardware,2026-01-28 08:34:24,2
Intel,o2060jj,"What will you do when you run out of 6GB VRAM though? Not to mention that when gaming, a laptop with it will typically have to dissipate twice as much power as Panther Lake.  Either way, the 300 W power brick comes with the 5070 Ti Laptops that the OP is comparing against.",hardware,2026-01-27 12:27:47,0
Intel,o2062tj,"Well that thing is also expensive, only a bit less than the Dell XPS. We are talking about devices like the MSI Prestige that is like $1300.",hardware,2026-01-27 12:28:12,4
Intel,o20o6y5,"It definitely doesn’t have all day battery life, and I doubt the PTL+dGPU model won’t have drawbacks the normal PTL models don’t in battery life",hardware,2026-01-27 14:11:40,1
Intel,o1zn99q,Walk into an office or university and see how many people pay $1400+ for gtx 1050 perf.   Not everyone wants the performance if they can get other things.,hardware,2026-01-27 09:55:19,12
Intel,o26jaar,So you want it to be sold for 400 dollars?,hardware,2026-01-28 08:32:25,4
Intel,o200wfi,"You're comparing an iGPU to a dGPU. These are in completely different categories and they do not compete directly with each other. These devices are literally in different classes and are aimed at different customers.  Panther Lake can be paired with a dGPU too, but for anyone who does not want that, this iGPU is simply phenomenal.",hardware,2026-01-27 11:51:00,10
Intel,o217sc9,How is battery life on those ~$1k 5050 laptops and how much do they weight?,hardware,2026-01-27 15:45:21,7
Intel,o26jkrw,"Students are extremely likely to make poor financial decisions, yes. Theres a reason so many predatory loan agencies have offices on campuses.",hardware,2026-01-28 08:35:08,5
Intel,o20ec0f,"Depends on the college. I find my university to be basically just that, though it’s typically less students being strapped and more parents being strapped with cash",hardware,2026-01-27 13:19:33,4
Intel,o1znsjn,Dude go to uni or office and see the laptops people using. G14 is not it. Too big and fat,hardware,2026-01-27 10:00:11,-4
Intel,o1ztvi2,What are you on about? there are gaming benchmarks in that review... difference between B390 and 4050 60w is 9%,hardware,2026-01-27 10:54:00,12
Intel,o20p27x,The real gaming benchmarks are saying exactly this: that it’s only slightly behind a 60 watt 4050,hardware,2026-01-27 14:16:05,7
Intel,o26k0ub,The 8060s is much slower than this iGPU.,hardware,2026-01-28 08:39:17,3
Intel,o1zpnqs,"The G14 is 1.57 KG and a 1.59 \~ 1.83 cm  Are you like studying with kids?  For reference, the MacBook Pro 16 is 2.14 KG and 1.68 cm thick and Macbook Pro 14 is 1.55-1.62 KG and 1.55 cm thick",hardware,2026-01-27 10:17:03,9
Intel,o20puir,"Guy did an actual test.   https://youtu.be/jrygnUnBRNI  Skip to 12:40  Performs like a full wattage 3050, LIKE I SAID BEFORE. Or a heavily gimped 30w 4050, but anything remotely close to a full wattage 4050 it is majorly behind.",hardware,2026-01-27 14:20:03,-2
Intel,o26pjrz,"The 8060s is MUCH FASTER than the b390 lol, what are you on about, some people are so confident in being wrong.",hardware,2026-01-28 09:30:41,2
Intel,o1zqjn7,"Whatever you're right, Intel should drop pantherlake to 1k. G14 is a slim & sleek design. Honestly dont even know why its not more widly used in offices & univerties",hardware,2026-01-27 10:25:00,-4
Intel,o21aswd,"B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Can you give me a link (review will be fine) to ~3lb/sub 1.5kg laptop with full, +100W 4050 that performs on par with desktop 3060?",hardware,2026-01-27 15:58:32,2
Intel,o20n16i,"> Intel should drop pantherlake to 1k.  Intel isn't making the laptops, they're making the chips.  OEMs have decided to debut them in their premium, thin and light flag ships + a price increase from RAM shortage",hardware,2026-01-27 14:05:41,3
Intel,o23tz75,"4050 is 3 years old now, there’s not many laptops with that gpu anymore on sale.   But the 4050 at max wattage was only around 10% slower  than a 3060 12gb desktop gpu, which is also reflected in their benchmark scores.   Theres now Lenovo’s yoga pro 9i with a full wattage 5050, which is even faster than the 4050 and performs slightly slower than a 4060 desktop, it’s a thin and light premium laptop lol, is light years faster than a b390 iGPU.",hardware,2026-01-27 22:40:13,-1
Intel,o20oeoa,It makes sense to debut in premium models for oems. Cant complain if there's consumers willing to buy them.,hardware,2026-01-27 14:12:46,1
Intel,o249ukz,"> Theres now Lenovo’s yoga pro 9i with a full wattage 5050 [...] it’s a thin and light premium laptop lol  In what world is 4.5lbs light compared to 2.5-3lbs laptops with PTL? That's not even including power brick, which adds at least 1lb.  You could've chosen something like ASUS Tuf A14, but in most cases battery life will be significantly worse, despite bigger battery. Oh, and it will be loud under load.  It's also just under $2k now, so pretty big difference.  It might be a surprise for you, but what many people want from those igpus is the most performance at certain form factor while at the same time being efficient enough to last whole day (or like 7-8 hours) not having to go to every battery saving setting possible.",hardware,2026-01-28 00:00:12,2
Intel,o2ng91s,"it's very cool how optimized that game will be i think, especially for 2026 with all the components shortages",hardware,2026-01-30 18:40:46,59
Intel,o2o244k,Every gaming dev should target the B390 as the 1080p 60fps on medium/low tbh,hardware,2026-01-30 20:20:15,43
Intel,o2niprn,It seems odd that the Arc B390 was listed as the minimum required IGPU when the much weaker discrete Arc A380 is also on there. I'm sure most of the last generation IGPUs like the Radeon 880M or 890M are also powerful enough (considering they are faster than the Arc A380).  We've always known that the Forza Horizon games are very well optimized and are able to run on IGPUs just fine. Keep in mind that Forza Horizon 5 runs just fine on the Vega 3 IGPU in the 7 year old AMD Athlon 3000G.  The article is presenting the trivial fact that the B390 is listed as the minimum spec for the upcoming Forza Horizon 6 as somehow being a huge win for Intel. That's not very good journalism. It was going to run on IGPUs regardless of whether or not the B390 existed.,hardware,2026-01-30 18:51:29,20
Intel,o2o6640,Wonder how many wafer starts per month Fab 52 is up to now. Seems 18A parts are going to sell well and they have several more coming. Hope they can keep up.,hardware,2026-01-30 20:39:35,6
Intel,o2vo381,"Pricing is not one of those ""victories""",hardware,2026-01-31 23:39:16,3
Intel,o3905cc,"Unfortunately it's likely the B390 will be priced closer to Strix Halo than to Strix Point. Which is awkward since Intel only compares it to Strix Point if their presentations, not to Strix Halo.",hardware,2026-02-02 23:45:06,-2
Intel,o2reyst,Any game that has to run on a Series S will consequently have very low minimum system requirements.,hardware,2026-01-31 08:57:50,23
Intel,o2oxc4r,Low minimum system requirements were never a guarantee that a game would perform decently. Especially when they don't even list the resolution and framerate they're targeting.,hardware,2026-01-30 22:51:09,22
Intel,o2pelu4,"Gives me hope that the Fable reboot will be well optimized. Just hoping playground games nails the story, gameplay and charm that we love with the series.",hardware,2026-01-31 00:25:19,3
Intel,o2ooiyh,Do you all forget how raw Forza Horizon 5 seemed when it came out despite having relatively low system requirements? Low system requirements != good optimisation.,hardware,2026-01-30 22:06:45,1
Intel,o2qs0yz,"except you, unreal engine",hardware,2026-01-31 05:34:52,8
Intel,o2nle78,"Yeah, it feels a bit like marketing and there have been a few of those.  Right now pretty much any computer with the X9 or X7 is expensive. You can find 5050 laptops for the same price. This chipset isn’t going to do that well unless it can be found at a decent price.   Also buyers should be careful because not every laptop presented has the power and thermals set to permit full power sustained boosting at the wattages tested in the reviews.   Those laptops should get cheaper and so on of course. Right now we don’t know. A Dell XPS costs more than a new mid-range MacBook Pro though, that is steep.",hardware,2026-01-30 19:03:14,14
Intel,o2pedv9,"Sometimes even big developers put surprisingly little work into assessing system requirements. Assassin's Creed Unity lists the GTX 680 as the minimum spec GPU and the GTX 780 (same architecture, more cores) as the recommended GPU.",hardware,2026-01-31 00:24:07,8
Intel,o2nl9l7,"A380 is \~140% raster performance of 890M based on Steel Nomad scores, doesn't seem faster than 890M at least at the wattages those are ran at",hardware,2026-01-30 19:02:40,8
Intel,o2pinvg,"tbf the base requirement for Microsoft is probably the Series S, a case the B390 is probably competitive with.",hardware,2026-01-31 00:47:36,16
Intel,o3952om,"At the same time, it seemed to run at 30fps on the series X. So I’ll assume it’ll be pretty CPU heavy.",hardware,2026-02-03 00:11:57,1
Intel,o2tujqp,UE5 is the ultimate benchmark for this GPU.,hardware,2026-01-31 18:13:30,5
Intel,o2phpsz,"> You can find 5050 laptops for the same price.   For now, I wouldn't hold on and wait for better prices. Panther Lake laptops are being priced in the post RAM hike world. Older laptops are still floating around and being priced of DDR prices from 6 months ago.  If that 5050 laptop launched today, it would have to deal with $100+ dollars in extra manufacturing cost.  Also the initial launch models for new Intel CPUs always come at a premium. Budget models will come and also prices will be slashed from launch prices. Something that has already happened with older generations.",hardware,2026-01-31 00:42:24,10
Intel,o2tumme,Shouldn't the base line be the new Xbox handheld?,hardware,2026-01-31 18:13:52,1
Intel,o2u6srb,"If you're talking about the ROG Ally, then no.",hardware,2026-01-31 19:11:08,5
Intel,o2uklwi,"No, I'm talking about the Xbox Ally X.",hardware,2026-01-31 20:18:06,2
Intel,o2umfgm,"Okay, so same thing. No, that is not Microsoft's baseline.",hardware,2026-01-31 20:26:59,9
Intel,o2uqvin,"Why not? Microsoft announced it like it's an official console, shouldn't their first party studios support it then?",hardware,2026-01-31 20:49:01,1
Intel,o2uv5er,"Probably two main reasons off the top of my head: the Xbox Ally released quite late in this console cycle and it's very weak. There already exists a large library of games that would not be playable and Microsoft suddenly (drastically) lowering the minimum target would be messy for devs. Even the Series S can barely do raytracing effects, for example. [Microsoft maintains a compatibility list](https://www.xbox.com/en-US/handhelds/handheld-compatibility) much like Valve with the Steam Deck.  Maybe things change with the next gen if Microsoft brings out a first party handheld, but for now it's just a bonus if runs on the Ally.",hardware,2026-01-31 21:10:17,7
Intel,o2x762v,"What purpose is going to be achieved by supporting superficial gimmicks like ray tracing? Four generations of NVidia, three generations of AMD in, it's still incredibly difficult to find a game where the ray tracing is visible without a side-by-side screenshot comparison AND where it doesn't make the game look worse. And forcing that setting to ""on"" with certain presets doesn't exactly fix that in any way. Sure, I can continue to waste power and processing cycles on it, but it doesn't change how I still have yet to find enough games to tick BOTH of those boxes to run out of fingers on the two hands. Even after padding out the list with Quake 2 and crap Minecraft variant.",hardware,2026-02-01 05:17:24,-1
Intel,o1vq8zu,">Both Arc Pro B70 and Pro B65 are based on the BMG-G31 GPU, which is now confirmed. The memory configuration is also the same. Both cards are said to feature 32GB of GDDR6 on a 256-bit bus. The main difference is GPU core configuration. [...]  >Based on conversations we have had, it appears Intel has put the gaming variant on hold, possibly to see how the memory market develops. This does not look promising for anyone expecting a near-term launch. The longer the delay, the less sense a release makes.",hardware,2026-01-26 19:55:37,26
Intel,o1ylj6k,"Another day, another exclusive BMG-G31 leak…",hardware,2026-01-27 04:42:16,9
Intel,o1vq3yo,"Hello PorchettaM! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-26 19:55:02,1
Intel,o1w7usf,Hoenstly B770 with Xe2 at this point makes no sense. Rather if they'd do a Xe3P dGPU release late 2027 especially if memory prices collapse.,hardware,2026-01-26 21:13:41,11
Intel,o1whebt,I wonder how different Xe3P is to Xe3. Xe3P could be further along than we expect.,hardware,2026-01-26 21:55:54,5
Intel,o20cbwr,Rumour suggests Xe3P has 20-25% architecture improvement over Xe3.,hardware,2026-01-27 13:08:04,5
Intel,o28rbgk,I mean in terms of release.,hardware,2026-01-28 16:47:13,1
Intel,nzd1gfc,"that amt of hair in a clean room, steve should wear a hooded rain coat lol.",hardware,2026-01-13 15:00:01,159
Intel,nzcvzjv,I thought that was Denis lol,hardware,2026-01-13 14:32:09,58
Intel,nzd3sjp,"Yeah this is a really surprisingly open factory tour, at least compared to when Sapphire took Linus and Alex along. It feels unprecedented to see this much access and insight, but that's the goodwill nurtured by Steve paying off in spades. The lament about AMD and Nvidia ditching the sub-250 side is real, so having Arc actually be a usable option down here is going to be significant later on.",hardware,2026-01-13 15:11:40,34
Intel,nzfe7p7,"Pretty good video with a lot of detail I haven't seen in previous factory tours. I probably still won't use an Intel GPU for gaming, but for a media server it's probably sufficient.  Side note: It's funny how whenever a GN video gets posted, the same cast of characters comes out and writes essays criticizing the video, apparently without watching it. I guess that's what it means to ""make it"".",hardware,2026-01-13 21:41:25,22
Intel,nzd7t05,"**GN:** NVIDIA and AMD abandoned this segment!  #Reality:  **B580:** $249 USD  **9060 XT 8GB:** $299 USD  **9060 non-XT:** $259 USD  **5060:** $299 USD  **5050:** $249 USD  I wouldn't say this is abandoned. I will say though the actual factory tour is cool, great content. But a dumb headline/title for the video.",hardware,2026-01-13 15:31:02,103
Intel,nzf5m3z,"Amazing video, GN is so good with content like this",hardware,2026-01-13 21:01:37,9
Intel,nzd8jmh,"rtx 5050 149 mm² 128bit has similar performance to B580 272 mm² 192bit. Selling for same msrp, winning by not ""trying"". Love how 5050 is actually at msrp now cheapest b580 is $349+  ""intel's gpu division seems like the only place in tech right now where the customers arent getting shafted these days""  People just want their Nvidia gpus cheaper  Nvidia crashouts making people hype an even worse product at current prices lol  [https://imgur.com/a/8iRI87Q](https://imgur.com/a/8iRI87Q)",hardware,2026-01-13 15:34:35,12
Intel,nzgtt65,"Love the Sparkle heatsink aesthetic, just wish there were some higher end card offerings from them instead of just Intel's lower midrange GPUs.",hardware,2026-01-14 02:14:38,4
Intel,nzd585j,He probably just single handedly killed 6 wafers worth of intel GPUs.,hardware,2026-01-13 15:18:37,1
Intel,nzd6pvl,AKA 'NVIDIA and AMD are going where the real money is.',hardware,2026-01-13 15:25:50,-1
Intel,nzcwvps,Finally a video that isn’t “AI bad” or “company X bad”,hardware,2026-01-13 14:36:46,-20
Intel,nzhjk4i,"Woah, a GN video that is actually interesting and informative for once instead of just ranting about the fact hardware companies exist to make money, not please gamers.",hardware,2026-01-14 04:51:57,-3
Intel,nzdfxes,That hat is so useful,hardware,2026-01-13 16:08:37,-6
Intel,nzdz3jo,Making them isn't important selling them is.  The reality is that this segment doesn't actually exist it has no buyers in it.  Seems intel is truly doomed trying to win segments that if they even exist aren't big enough to pay back their R&D even if the dominate them.,hardware,2026-01-13 17:48:25,-15
Intel,nzdogu2,"To be fair this is not a clean room, just SMT assembly. Basically, soldering components. A lose hair might be a bit of a problem (kind of like in any factory), but his hair is tied and doesn't seem to be an issue.  At an actual clean room (where the actual silicon is processed and etched), the standards can become very crazy. Some parts of it aren't even accessible to humans, just automated lines to avoid contamination.",hardware,2026-01-13 16:47:26,94
Intel,nzd8zmt,"Gaming Jesus could walk on wafers without corrupting a single tile, His body is that pure.",hardware,2026-01-13 15:36:40,-20
Intel,nzd5ucm,"same thought, just feels disrespectful",hardware,2026-01-13 15:21:37,-30
Intel,nzdzd0h,"He desperately needs to learn how to take care of his hair. I have no idea why nerds think a dry, frizzy mess of hair is some kind of enviable quality.   I feel like I'm walking into friday night magic every time he pops up.",hardware,2026-01-13 17:49:38,-19
Intel,nzhet37,"Intel hired him on the spot when they saw the ""Live, Laugh, Liao"" sign",hardware,2026-01-14 04:19:24,8
Intel,nzd0sl2,"who knew he can into GPU manufacturing? I thought he was a video editor and first real ""why are you employed"" guy on LTT.",hardware,2026-01-13 14:56:41,19
Intel,nzipwgx,I'm pretty sure Denis has neither long hair nor beard. Though to be fair I haven't seen him recently.,hardware,2026-01-14 11:05:09,1
Intel,nzhufav,"It’s crazy. I think Steve and GN are doing the lords work with their nonstop, accurate coverage of the absolute shit state of the computer component markets and the megacorps that perpetuate it. And their seemingly endless support and advocacy for *consumers*.   But then I come in this sub and see people are actually… against this? I don’t understand it. Probably the same mfs that bought MKBHD’s wallpaper app lmfao",hardware,2026-01-14 06:14:39,9
Intel,nzd9rkp,"Wasn't that the B310? That was supposed to be the $100 bracket. To be fair that's an almost useless tier nowadays because iGPUs are capable of similar performance or even outperforming the cards in those brackets. Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.",hardware,2026-01-13 15:40:18,64
Intel,nzdtxiv,"They're talking about the A310 and A380, of which Nvidia doesn't have anything made in this decade to compete with and AMD has the 6400 that came out 5 years ago.  The only cards with modern features in that price segment that consumers can directly buy are the A310 and A380.",hardware,2026-01-13 17:24:22,38
Intel,nzgaa45,He(Lucas) stated $100 was the market that was abandoned.  10:37  > Steve: So why still making A310?  > Lucas: Because what's the competitor have? Nvidia? like... GT710? GT1030? (laughs) No way. So literally Nvidia AMD already give up the segment of this like... $100 price card.,hardware,2026-01-14 00:24:50,14
Intel,nzfp313,This guy was talking about the A310 which is a $100 GPU. Basically said that the only other options at that price bracket are either a GT 710 or a GT 1030. And from AMD you can still get an old RX 550. The A310 may be slow but it beats those two gpu's by a mile.,hardware,2026-01-13 22:32:49,11
Intel,nze3uy1,> 9060 non-XT: $259 USD >  >   LOL. Good luck finding it. It's OEM exclusive,hardware,2026-01-13 18:09:42,9
Intel,nzdcsu3,The title is a literal quote from Lucas,hardware,2026-01-13 15:54:17,25
Intel,nzdb9mw,The title is in quotes. That is from Sparkle.,hardware,2026-01-13 15:47:16,10
Intel,nzejfd4,Now do SR-IOV and 16GB RAM for under $400.,hardware,2026-01-13 19:18:41,2
Intel,nzhv1ka,B580 has 12gb of vram though,hardware,2026-01-14 06:19:46,1
Intel,nzitx93,"Sad that all of those are 8gb cards. Even if comparable or faster than the B580 aside from that, the lack of vram makes them far inferior products imo.",hardware,2026-01-14 11:38:31,1
Intel,o07mcza,I just wish all of those options were more compelling tbh. I think what Steve was trying to say was that AMD and Nvidia have just half assed that market segment at the expense of the consumer. All these being 8gb cards for $250-$300 is kinda ridiculous in 2026. I’ll take a b580 over a 9060xt 8gb or 5060 any day of the week.,hardware,2026-01-18 01:21:19,1
Intel,nzenlzf,Go to r/PCMasterrace and they will downvote you into oblivion for even MENTIONING the possibility of gaming on a 5060 let alone 5050 XD,hardware,2026-01-13 19:37:44,2
Intel,nzdm48p,The title is just playing the YouTube algorithm game. It's stupid but they have to do it.,hardware,2026-01-13 16:36:44,-7
Intel,nze22z8,"> Love how 5050 is actually at msrp now cheapest b580 is $349+  Just an FYI but B&H has the [Acer Nitro B580 for $249.99](https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html) and the [Intel Limited Edition model for $259.99](https://www.bhphotovideo.com/c/product/1869297-REG/intel_31p06hb0ba_arc_b580_limited_edition.html). So you can get B580 for MSRP, but for how long who knows.",hardware,2026-01-13 18:01:44,20
Intel,nzdnidn,"die area is not the only cost indicator. B580 actually uses N5 fab, which is likely cheaper than N4, used by 5050. In reality, B580 only has about ~15% more transistors and if we assume N5 is cheaper per transistor and N5 has higher yields (by being more mature) i'd say their die cost might be very similar.  While yes, it has wider memory, the chips are clocked lower, so they can buy slower bins, reducing per chip cost.   B580 has more power draw, which in turn costs more for power delivery and cooling.  All in all, AIB manufacturer likely has lower margin per card as it stands, so i wouldn't be surprised if intel is taking a lower margin on the gpu/gddr combo to get more market share.  Nvidia on the other hand optimized their cost REALLY well, as they have been doing GPUs for almost 30 years.",hardware,2026-01-13 16:43:04,6
Intel,nzhfs1m,"Nvidia is getting such good performance out of such a small die because they're the best. Simple as. They've been doing GPU's for decades. It's not unexpected that at this stage Intel needs to use a larger die to match the performance - it would be incredibly surprising if that wasn't the case.  But a small *part* of that die size advantage comes from that narrow 128bit bus, and *part* of B580's appeal is its wider bus and subsequently more VRAM.",hardware,2026-01-14 04:25:57,1
Intel,nzppqcd,"I see b580 for € 260. That's VAT included. rtx5050 is similarly priced.     And i found $300 lot on US amazon.  Considering other post, maybe search better than 1 place or something.",hardware,2026-01-15 12:00:19,1
Intel,nzd4pls,"Isn't the title pretty much ""AMD & NVidia bad""?",hardware,2026-01-13 15:16:07,57
Intel,nzd1pn1,Although even then the thumbnail is framed negatively.   But I much prefer these to Steves endless negativity ragebait.,hardware,2026-01-13 15:01:19,4
Intel,nzcyycp,> finally a company that isnt bad   ftfy,hardware,2026-01-13 14:47:24,-26
Intel,nzklu8k,"GN makes videos like these all the time, you just aren't watching them.",hardware,2026-01-14 17:20:56,2
Intel,nzgji2p,"Did you even watch the video? they are selling, and they are selling out, so much so that they want to ramp up production so they can push out more.",hardware,2026-01-14 01:15:59,9
Intel,nzfbt5x,I do not know if the numbering scheme from my workplace is common across the industry but the smt assembly would be in a class 5 or 6 clean room and the fabrication itself would be a 1 or 2 class clean room,hardware,2026-01-13 21:30:27,19
Intel,nzglawf,"I think it still matters to a certain point, thats why everyone in the factory is wearing a hat. The Factory boss decided to roll RNG dice and say *""Fck it, that small hat is fine, even tho wearing it is pointless now; I'll just pray nothing bad happen*"". lol  What hilarious is when you think about what going through uninformed factory-employee's head, after they saw some guy(Steve) walk-into the factory like that. Definitely a lot of ""WTF"" moment going through their mind lmao.",hardware,2026-01-14 01:26:15,2
Intel,nzdb445,"The factory boss told me not to bother tying my hair (""不用不用不用, 没事儿没事儿没事儿. 这样可以的"") when I started to put it under the hat... and after asking for a larger hat or hairnet.",hardware,2026-01-13 15:46:33,131
Intel,nze10ie,"> same thought, just feels disrespectful  It's a good thing you were there to personally witness the interaction so that we'd all know exactly how disrespectful Steve was being before holding everyone at gunpoint to force them to let him shoot the video without first fixing his hair.",hardware,2026-01-13 17:56:59,11
Intel,nzdfs8o,Nothing really surprises me with regards to gamers nexus at this point.  Edit: Downvote me all you’d like. They’ve been leaning incredibly hard into the rage bait type content of late.,hardware,2026-01-13 16:07:57,-61
Intel,nze0k30,I'm sure getting beauty tips from random redditors who have never left the basement is of utmost importance to Steve.,hardware,2026-01-13 17:54:57,18
Intel,nzi9ktg,"So, pray tell, who declared you the holder of absolute truth in terms of hair? What authority do you hold that allows you to determine what other people should do with the hair on their body?",hardware,2026-01-14 08:31:05,3
Intel,nziqu89,The other guy.,hardware,2026-01-14 11:13:11,1
Intel,nzklo11,This sub is genuinely weird. They complain about PCMR when the level of discourse here is much worse and a lot more mean spirited.,hardware,2026-01-14 17:20:09,3
Intel,nzk72ao,"They are truly rage baiting drama mongers sadly. The fact that the causes they support happen to be semi legit and consumer friendly in nature does not take away from the fact that they are essentially drama and negativity for profit at this point, and often out of touch with reality. You can be positive, polite, and have reporting standards while still calling out shitty behavior and ripping bad practices to pieces.   Companies are not your friend. That includes GN. They stoke hardware enthusiast anger for their own gain. Personally i think the harm they do to the space/peoples mentality is farm more than the amount of good most (key word most, fire risks and stuff deserve prompt drama) of their coverage does.   Even this title takes a topic and very educational opportunity to provide actual hardware coverage and immediatly try’s to stoke negative furry about some aspect of the hardware industry. Its just non stop with them. I dont think its healthy for anyone to watch then honestly, which is sad as i used to enjoy their review/news. I hope for their sake they are not actually so perpetually angry and they are doing what they do to knowingly manipulate their audience for increased views/engagement. I cant imagine living life so constantly angry at every possible point over computer hardware, as much as i am saddened by the current state hardware and love gaming.   As tough as it is with rising prices and such and sad to think of what we could have instead, modern hardware, gaming, and such is in a nearly historic good state (realistic cost to perf is probably lowest in history other than brief periods such as the 30 series launch), and is a very cost effective hobby compared with other ones for the time you get out of it. Many great games launch all the time (even if big name traditional games have largely went to trash). They are easier to run with hardware (like the ARC gpu) being quite cheap and capable. People, like GN, need to temper their overexagerated frustration with a dose of real world perspective and objectivity.",hardware,2026-01-14 16:13:54,-1
Intel,nzdm2vc,"Might be just the thing for older machines and a GNU Linux (or BSD like) migration. Or as a pass through GPU to Jellyfin, Emby, or Plex for media transcoding. Edit: or Small form computing tied together with a iGPU enhancing game performance...",hardware,2026-01-13 16:36:33,15
Intel,nze76iz,yeah there is a reason the a310 cards they are making are 4 hdmi out.  Gotta know your market,hardware,2026-01-13 18:24:10,18
Intel,nzeeiko,I assume those are being marketed to OEMs who make digital billboard systems or something. No idea why else you would want 4 HDMI ports on a card.,hardware,2026-01-13 18:56:32,7
Intel,nzddzc6,"> Wasn't that the B310? That was supposed to be the $100 bracket.  It's the A310, which is Alchemist and it's pretty much dogwater for anything beyond being a 'display out' card. The claim that NVIDIA has abandoned that segment is stupid... They've had offerings in this segment for years, plus anyone smart will just go buy a used GPU, your money goes way further. For example, the GTX 1650 performs basically 10-15% better, has better drivers, better encoding and generally is better supported. It's older, but I mean Alchemist wasn't exactly impressive either when it released and pretty much Intel has moved onto Battlemage and Celestial driver optimisations instead.   Plus let's be real here I went and searched and I found only weird places tend sell the brand new A310, the only local computer shop I found selling it in Australia for instance is a big one which is good surprisingly, but they had it for $189 AUD, a total rip tbh. A used 1650 is like $100 AUD and a used 1650 SUPER is like $120 AUD. No reason to buy an A310 tbh, pocket the cash and move on. Or if you're really intent on spending around that much buying a used RTX 2060 for like $20 AUD more, so a total of $200-210 AUD is better. Then on the AMD side you have the RX 6400 which had an MSRP of $159 USD and it's again a solid 10-15% faster, but much better off buying a used 6500 XT or 6600. Neither company has abandoned the segment, they had offerings for years and the used market basically obliterated any point to buying a brand new card like this.  >  To be fair that's an almost useless tier nowadays because iGPUs are capable of outperforming the cards in those brackets.  Yep this too. Honestly, I mean it's cool they're showing how they make cards on this factory tour, but to be like ""NVIDIA and AMD abandoned this segment"" is stupid when it comes to the A310. Almost anything these days is better than an A310.  >  Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.  GT710 hasn't made sense for like 8 years at least, even when it was relevant people laughed at it, but it did the job for 'display out' and such which was all that mattered. GTX 1630 was okay but it was supposed to be $149 USD MSRP and it came out for like $200 USD in most stores due to GPU shortage at the time, not much NVIDIA could really do about that.",hardware,2026-01-13 15:59:39,-5
Intel,nzdyz47,They haven't made anything because the market has moved on. Intel might be making these but are they selling them?,hardware,2026-01-13 17:47:52,-12
Intel,nzdhs0m,"[Is it in reference to this moment in the video?](https://youtu.be/YwrUxG26ulk?t=648) If so, he doesn't say that as a literal quote, he says ""give up the segment"". Unless there's another quote somewhere else which I missed which may be possible or maybe it was edited out or cut from the video? I can't remember everything he said tbh but there was a lot of good information in this video and I think the title is better off without it. If it was called ""Intel Arc GPU Factory Tour with Sparkle"" I would have insta-clicked to watch anyways.",hardware,2026-01-13 16:17:04,13
Intel,nzdelag,You know what you're doing with the title... It's honestly unnecessary to use it on a factory tour video tbh.,hardware,2026-01-13 16:02:26,38
Intel,nzejqso,Stop defending your clickbait.,hardware,2026-01-13 19:20:08,14
Intel,nzf28eu,do you wanna address this then? Its kinda cringe ignoring the rest of the post  >**GN:** NVIDIA and AMD abandoned this segment!     >**B580:** $249 USD  >**9060 XT 8GB:** $299 USD  >**9060 non-XT:** $259 USD  >**5060:** $299 USD  >**5050:** $249 USD  Why include that in the title then too?,hardware,2026-01-13 20:45:49,1
Intel,nzg52uc,"Obligatory ""lol stupid pcmr amirite"" comment.",hardware,2026-01-13 23:56:47,1
Intel,nzdplnj,"The RTX 50 and 40 series are using the TSMC 4N node which is a custom version of the N5 node for NVIDIA. But anyway the N5, N5P, N4, N4P, N4X are all 5 nm class node, so have around the same price for the wafer. And I wouldn't be suprised that NVIDIA is paying less for these considering the volume compared to Intel orders.",hardware,2026-01-13 16:53:27,17
Intel,nzdpf4g,Well their gpus are much more expensive than amd & nvidia who arent even trying. When they try Intel wouldnt even have a chance  Nvidia increasing their entry gpu volume  [https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026](https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026),hardware,2026-01-13 16:52:06,5
Intel,nzi4dj7,Take a positive factory tour video. How can we spin it to make outrage?,hardware,2026-01-14 07:42:00,3
Intel,nzdqnhb,Yep,hardware,2026-01-13 17:07:48,6
Intel,o0a80gr,Baby steps.,hardware,2026-01-18 12:59:19,1
Intel,nzky9ti,Recently he's been making far more rants and less informative videos.,hardware,2026-01-14 18:16:33,0
Intel,nzn1tla,those number grades are an ISO standard (14644) so they are indeed common.,hardware,2026-01-15 00:12:34,3
Intel,nzdueea,Thanks Steve,hardware,2026-01-13 17:26:35,32
Intel,nzdyjcv,But you still didn't to say hi to me at PAX West 2016 in front of the LEGO USS Missouri battleship...,hardware,2026-01-13 17:45:51,1
Intel,nzdtnpi,"makes sense, I coulda been more charitable in the way I said it",hardware,2026-01-13 17:23:03,-1
Intel,nze79m5,"relax dude, steve already replied, no need to whiteknight",hardware,2026-01-13 18:24:32,-13
Intel,nzdtsrk,"he replied in a comment to me to say that he was told to leave it alone, I guess assembly isn't as careful as the initial production is.",hardware,2026-01-13 17:23:45,7
Intel,nzdjfsy,"Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN, and their recent consumer advocacy and stepping on some very powerful toes, your comment sounds an ***awful*** lot like an astroturfing smear campaign meant to breed sentiments against Steve & GN.",hardware,2026-01-13 16:24:37,11
Intel,nze6q5q,Imagine defending the hair of a guy who looks like he judges anime conventions in his spare time.,hardware,2026-01-13 18:22:13,-12
Intel,nzdq8iu,"I get it, for those with old boxes. But intel has great transcoding according to self hosters, and the powr consumption is much better vs old i5 pairing with those dedicated cards.",hardware,2026-01-13 17:05:17,10
Intel,nzej077,day traders love having a zillion stock tickers running.  i'm sure there's more applications where a heap of monitors is useful.,hardware,2026-01-13 19:16:47,8
Intel,nzeua57,A lot of digital displays make use of DP MST to avoid the use of home run cabling.,hardware,2026-01-13 20:08:25,3
Intel,nzelrbi,The a310 and a380 are fantastical for a media server!,hardware,2026-01-13 19:29:14,12
Intel,nzdi8nv,Your wasting a lot of words defending a company about to rerelease a 4 year old GPU (3060) because they can’t get memory for the current model.,hardware,2026-01-13 16:19:11,14
Intel,nzsvw7m,The A310 was released in 2022. Do your think Sparkle would still be producing them in 2025 if they didn't sell?,hardware,2026-01-15 21:18:53,1
Intel,nzdjgo9,"Sure it was paraphrased for the title, but that's just semantics.   AMD and Nvidia ""giving up"" vs. ""abandoning"" the segment mean the same thing either way, given Lucas' intention behind the statement.",hardware,2026-01-13 16:24:44,6
Intel,nzfgwnz,He is ignoring the post because the poster didn't watch the video and is spreading BS. The segment they are talking about is $100 cards.,hardware,2026-01-13 21:53:45,15
Intel,nzdqmql,"maybe the cost for the raw wafer, but that's not all TSMC will charge nvidia for. You also need to account for yields, which could be different depending on the type of node.",hardware,2026-01-13 17:07:41,0
Intel,nzlh47j,"Lately there's been a lot of bad shit happening in the industry and a lot less good tech to talk about. Shocking, right?",hardware,2026-01-14 19:40:42,5
Intel,nzp1ajx,Thanks you 😊,hardware,2026-01-15 08:17:06,1
Intel,nze7n1w,But then I won't earn my free toaster after the 11th white knight attempt.,hardware,2026-01-13 18:26:08,-2
Intel,nzdxfeg,"> Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN  You have to be joking, right?",hardware,2026-01-13 17:40:46,9
Intel,nzdlwqs,Yeah. I’m definitely an astroturfing bot account. You got me. My profile certainly *reeks* of botting / astroturfing 😂,hardware,2026-01-13 16:35:48,-14
Intel,nzi46nl,This has to be sarcasm.,hardware,2026-01-14 07:40:11,-2
Intel,nze72qg,"Imagine being as shallow are you are while still posting on reddit behind an anonymous username.  Let's see how your hair looks, mate. You're giving off pure incel vibes here.",hardware,2026-01-13 18:23:43,12
Intel,nzdii5s,"You know I also talked about AMD right? Not just NVIDIA. Regardless, you think Intel isn't also going to have memory issues soon? They might just divert all memory they have to the SKUs that are selling.",hardware,2026-01-13 16:20:22,5
Intel,nzedwr3,>  a literal quote from Lucas  Does not line up with  > paraphrased,hardware,2026-01-13 18:53:49,16
Intel,nzdz5pq,Yields wouldn't meaningfully differ within the same family. Certainly not by enough to remotely cover for the die size difference.,hardware,2026-01-13 17:48:42,6
Intel,nzfzkw5,I'm starting to think that r/hardware is the circlejerk sub and I just haven't yet found the actual hardware sub that it's parodying,hardware,2026-01-13 23:27:14,2
Intel,nzegtti,Intel B570/580 already use GDDR6 which is what Nvidia is trying to achieve with the 3060 release. Intel presumably won't be effected.,hardware,2026-01-13 19:06:58,2
Intel,nzel47j,"...Yes, that's why I said it's a semantics issue.  The meaning is the same: The paraphrased quote isn't a statement made by GN like u/KARMAAACS implies.   What makes it worse is that Lucas said that in response to Steve's question about ~$100 A310 cards and why they're still producing them (adding that they see a healthy demand for them from their customers).   It's like he didn't watch the video and just reacted to the title.",hardware,2026-01-13 19:26:20,-10
Intel,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,111
Intel,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,41
Intel,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,96
Intel,nysrktd,I’d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  What’s the point of hardware if you can’t apply it to what you need.,hardware,2026-01-10 15:26:18,12
Intel,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,19
Intel,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,25
Intel,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,27
Intel,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,9
Intel,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,14
Intel,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,13
Intel,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,4
Intel,nyrhl2f,Website doesn’t load with adblocker,hardware,2026-01-10 10:04:14,4
Intel,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,4
Intel,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
Intel,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
Intel,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
Intel,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
Intel,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
Intel,o3eyh45,"Way too expensive. For the price of a B390 notebook, you can get a Strix Halo notebook with 40 CUs, which is significantly faster. Sadly Intel has messed up again. and in addition with 16gb shared RAM it's pretty useless imho (8GB+8GB isn't enough these days).",hardware,2026-02-03 21:25:18,1
Intel,o3eyovg,"Yeah Strix Halo is way better in Terms of bang for the buck, sadly",hardware,2026-02-03 21:26:19,1
Intel,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
Intel,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
Intel,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-6
Intel,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-10
Intel,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,35
Intel,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,1
Intel,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
Intel,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,15
Intel,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
Intel,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,30
Intel,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,42
Intel,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,23
Intel,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,9
Intel,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,15
Intel,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,13
Intel,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,7
Intel,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
Intel,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,13
Intel,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
Intel,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,13
Intel,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,17
Intel,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
Intel,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,21
Intel,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,10
Intel,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,9
Intel,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,1
Intel,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
Intel,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,7
Intel,nyt7vgu,"It seems the revived Dell XPS 16 will have the “B390” and no dGPU, as another option. LTT’s video on it said Dell is quoting 27 hours of battery life in “general tasks” and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but here’s hoping.",hardware,2026-01-10 16:45:16,3
Intel,o2dssd4,"Würde das Thinkpad X9 15p bevorzugen, hat einen SD-Karten Slot, super Lautsprecher und einen richtig großen Akku.",hardware,2026-01-29 09:36:04,1
Intel,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,40
Intel,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 “Low Power” efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobile’s 285H (not HX!), trading 2 performance cores for 2 “Low Power” efficiency cores.  I’m not 100% on this but I don’t think Stryx Halo used AMD’s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intel’s normal lineup.",hardware,2026-01-10 16:28:50,16
Intel,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
Intel,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
Intel,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,38
Intel,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,10
Intel,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
Intel,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
Intel,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
Intel,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,19
Intel,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,0
Intel,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,7
Intel,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,4
Intel,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,6
Intel,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,16
Intel,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
Intel,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,13
Intel,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-5
Intel,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,6
Intel,nytp8ri,"Huge step forward, I just wish the didn’t struggle with older and brand new games. It’s a great card if you are willing to do troubleshooting and know computers but I won’t recommend them to family yet.",hardware,2026-01-10 18:07:45,7
Intel,nz0sex5,I hope they start supporting dx11 stuff. That’s a ton of games.,hardware,2026-01-11 19:10:35,1
Intel,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,4
Intel,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
Intel,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,12
Intel,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,10
Intel,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,3
Intel,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,2
Intel,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,17
Intel,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
Intel,nyvvmm4,"> I’m not 100% on this but I don’t think Stryx Halo used AMD’s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,14
Intel,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,5
Intel,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
Intel,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,2
Intel,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,0
Intel,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,8
Intel,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
Intel,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,3
Intel,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
Intel,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,8
Intel,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
Intel,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
Intel,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,7
Intel,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,11
Intel,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,29
Intel,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,4
Intel,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,9
Intel,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,9
Intel,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,6
Intel,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,24
Intel,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
Intel,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
Intel,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,7
Intel,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,39
Intel,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,21
Intel,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,4
Intel,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,12
Intel,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-2
Intel,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,4
Intel,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,9
Intel,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,3
Intel,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,5
Intel,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
Intel,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
Intel,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
Intel,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
Intel,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,6
Intel,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
Intel,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,11
Intel,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
Intel,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
Intel,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,10
Intel,nywmg0a,"Depends on what you consider to be “high end mobile gaming”, the laptop 4060 is currently the 2nd most popular gpu on steam, and that’s the level strix halo targeted",hardware,2026-01-11 03:03:40,1
Intel,nyxpn7n,"What you’re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. That’s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,1
Intel,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-3
Intel,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,6
Intel,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,11
Intel,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
Intel,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
Intel,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,7
Intel,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,13
Intel,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
Intel,o058umn,Asus G14 would like a word.,hardware,2026-01-17 18:11:44,1
Intel,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,9
Intel,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,11
Intel,nywgma9,"QC last year had a bad product.  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200€   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,2
Intel,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,3
Intel,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
Intel,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,5
Intel,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.  They have 5 total. AMD, Nvidia, ARM, Qualcomm,  Apple",hardware,2026-01-11 13:19:34,1
Intel,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,12
Intel,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,8
Intel,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,6
Intel,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
Intel,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
Intel,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,10
Intel,nz565l5,"Don’t even get me started! My current HP laptop straight up doesn’t support any type of fan control on Linux…  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
Intel,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,5
Intel,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,3
Intel,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
Intel,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
Intel,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
Intel,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
Intel,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,5
Intel,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,7
Intel,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
Intel,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,3
Intel,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-1
Intel,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under £2k then maybe.",hardware,2026-01-10 16:20:10,3
Intel,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
Intel,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,3
Intel,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
Intel,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
Intel,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
Intel,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
Intel,nzbx72p,Medusa halo isn’t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
Intel,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,111
Intel,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,34
Intel,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,10
Intel,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,22
Intel,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,50
Intel,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,19
Intel,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,17
Intel,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,25
Intel,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
Intel,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,4
Intel,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
Intel,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,4
Intel,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,3
Intel,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,1
Intel,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-29
Intel,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-12
Intel,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-13
Intel,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-11
Intel,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-14
Intel,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,58
Intel,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,85
Intel,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,6
Intel,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,23
Intel,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,3
Intel,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,3
Intel,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-12
Intel,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-12
Intel,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,66
Intel,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,22
Intel,nxz85mh,"With everything happening around NVIDIA´s price increases and AMD´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,10
Intel,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,1
Intel,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,13
Intel,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
Intel,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,52
Intel,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only “Arc Graphics”.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arc’s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,7
Intel,ny7ef2l,It's a mid-gen refresh of battlemage.,hardware,2026-01-07 14:33:10,2
Intel,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,54
Intel,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,22
Intel,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,9
Intel,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,6
Intel,nxy6cgs,"At best, it’s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,0
Intel,ny0ce80,What do you mean that a refreshed Strix Point can’t compete with an updated architecture?,hardware,2026-01-06 14:10:24,5
Intel,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,10
Intel,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,9
Intel,nxzf0cx,I think they’re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,5
Intel,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,8
Intel,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,5
Intel,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,13
Intel,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,15
Intel,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
Intel,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,66
Intel,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,29
Intel,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,18
Intel,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,6
Intel,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,23
Intel,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,20
Intel,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,26
Intel,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,31
Intel,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,17
Intel,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,5
Intel,nxy0jsm,Eh? It's a standard 128 bit memory bus.​,hardware,2026-01-06 03:29:57,37
Intel,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
Intel,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,7
Intel,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,5
Intel,nxz7zsg,"Reddit isn’t indicative of anything really, most casual laptop buyers don’t even know what AMD is.",hardware,2026-01-06 09:07:48,10
Intel,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,32
Intel,ny7dxyt,AMD is planning on again using RDNA 3.5 on their next mobile chips as well.,hardware,2026-01-07 14:30:42,4
Intel,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,15
Intel,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,7
Intel,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,6
Intel,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,2
Intel,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,19
Intel,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,12
Intel,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,5
Intel,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xe³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
Intel,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,3
Intel,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,1
Intel,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,20
Intel,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,18
Intel,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,26
Intel,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,2
Intel,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,25
Intel,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,13
Intel,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,4
Intel,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that it’s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,13
Intel,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,4
Intel,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
Intel,ny0na4n,"It’s a super strong generational gain though, it’s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,10
Intel,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
Intel,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,4
Intel,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,50
Intel,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
Intel,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,2
Intel,nybz77q,"I don't think that will really be an issue, laptops can be configured with soldered 8 channel RAM like AMDs Z2 extreme, or they can still manage easily with regular DDR5 6400Mhz sodimms, which run at 102.4GB/s  Plus the CPUs that have intels new B390 iGPU are 4P/8E CPUs, so I doubt there will be much issues from low ram speeds. Something like the Radeon 890M have done fine with such speeds",hardware,2026-01-08 03:26:50,1
Intel,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,5
Intel,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
Intel,o2xamaf,cpu: compute processing unit,hardware,2026-02-01 05:43:22,1
Intel,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-2
Intel,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-20
Intel,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-18
Intel,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
Intel,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-8
Intel,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,11
Intel,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,4
Intel,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,78
Intel,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,16
Intel,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,8
Intel,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,38
Intel,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
Intel,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,4
Intel,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,32
Intel,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
Intel,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isn’t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
Intel,ny0wejw,"> most casual laptop buyers don’t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
Intel,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,24
Intel,ny7wgeg,This is unfortunate news  (╥﹏╥),hardware,2026-01-07 16:00:09,1
Intel,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,8
Intel,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-2
Intel,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.    > and better process node   Very much unproven.    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,3
Intel,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,9
Intel,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,3
Intel,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,4
Intel,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,1
Intel,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,15
Intel,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,23
Intel,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,28
Intel,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,3
Intel,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,3
Intel,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,7
Intel,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,6
Intel,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,5
Intel,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isn’t more performance difference.,hardware,2026-01-06 04:18:43,4
Intel,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,0
Intel,o0fmoja,"And it's not like the ""last gen"" GPU in lunar lake was bad either, so we are starting from already good and making the jump up.",hardware,2026-01-19 06:26:10,1
Intel,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
Intel,ny7dl7u,"Nvidia's graphics have shown to be more efficient for space than both Intel and AMD, so whatever they use it will likely be better than what Intel can currently put out.",hardware,2026-01-07 14:28:52,2
Intel,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,3
Intel,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,8
Intel,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide that’s suspect.  It’s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,8
Intel,ny7raf2,I think they meant that the chip is very pricey which sucks because the handheld is already low-margin otherwisr and can't be priced too high else it got undercut by its competitors.,hardware,2026-01-07 15:36:28,1
Intel,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
Intel,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,9
Intel,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,11
Intel,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,8
Intel,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,17
Intel,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,17
Intel,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
Intel,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,11
Intel,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,4
Intel,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,16
Intel,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
Intel,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,8
Intel,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,5
Intel,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-12
Intel,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-6
Intel,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,15
Intel,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,18
Intel,ny6k0v5,Except with UDNA it might be the first time over a decade AMD isn't phoning it in.,hardware,2026-01-07 11:22:51,1
Intel,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,3
Intel,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,6
Intel,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,-1
Intel,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,3
Intel,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,7
Intel,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,14
Intel,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
Intel,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
Intel,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
Intel,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,7
Intel,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
Intel,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,24
Intel,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,9
Intel,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
Intel,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,27
Intel,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,5
Intel,nxy622y,"It needs dedicated GPU hardware to run faster, but they’ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,3
Intel,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,8
Intel,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
Intel,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
Intel,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,10
Intel,ny6ka8x,"Yeah but basically unusable on pre 40 series. But at least NVIDIA gives users the choice.  AMD should just stop the BS pretending and just enable the full FP8 model across RDNA2-3 with FP16 emulation. But it prob runs so bad that they won't, far far worse than DLSS 4.5 on 20-30 series.",hardware,2026-01-07 11:24:58,2
Intel,ny6ob4j,"It would be good if that is true, but so far ive seen nothing that would inspire me confidence in AMD. And yes i remember the AMD patents you posted last year.",hardware,2026-01-07 11:55:47,2
Intel,ny7ebjk,"Yeah, it's DLSS4>FSR4>XESS (Intel)>=DLSS3>XESS (fallback)>FSR3      quality wise.",hardware,2026-01-07 14:32:39,1
Intel,nxzj5en,"First custom design OEM are fast, here 4400€ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAX… one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
Intel,ny5eqws,"When I talk about ""design targets"", I'm not referring to an arbitrary TDP. There are very specific decisions each SoC made that have tradeoffs at different power envelopes.   Also, the context was LNL which is an entirely different beast from MTL.",hardware,2026-01-07 05:25:06,2
Intel,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,4
Intel,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,3
Intel,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
Intel,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,3
Intel,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,5
Intel,ny6po5w,"If you're referring to the April dump, heck even the August dump (analysis of Kepler\_L2 patents) then that's not close to the complete picture. A lot of new patents have surfaced since that expand upon the design in many ways, but I'm waiting for the last RDNA5 to be made public before making a potential follow up post.  But regardless even if they fix HW situation completely they'll prob fail spectacularly with SW stack as they've done so far with FSR Redstone and FSR4 game adoption. Even hear a lot of people complaining about having to use Optiscaler, even in newer games.   Also NVIDIA will no doubt move the needle a lot nextgen yet again. They already did with DLSS 4.5 and DFG and something tells me that DLSS5 is gonna be even worse for AMD. They better prepare for what's to come.  Worst case it's a complete massacre. I can see the following scenario happening:  **HW:** NVIDIA invests all their silicon budget into fixing 5090 scaling bottlenecks (16 GPCs instead of 12, revamped scheduling etc...), fixes other problems with 50 series (redesign cachemem mostly) + goes Brr on ML and to some extent RT. Raster goes up 35-40%, everything else goes up multiple times.   Worst case ML HW gets bumped to 4-8X NVFP4 rate, although 2-4X sounds more likely.  **SW:** NVIDIA uses this new insane ML HW to make new DLSS models. DLSS5 goes all in on NVFP4 and is faster than DLSS4.5. DLSS5 SR and RR for 50 series + 60 series which is lightweight and fast on new GPUs (high FPS), and a new DLSS ULTRA SR for 60 series (released across stack but painfully slow for anything pre 60 series) striving for maximum Image quality. The smaller model will be better than DLSS4.5 and the big model another tier entirely (DLSS3 -> 4 leap easily on top of DLSS4.5).   They also make DRS compatible with DLSS SR and RR so users get greater flexibility here similar to DFG for framegen.   FG will also release in two versions one light and heavy. Will also work with Reflex 2. It's possible only the big model will be frame extrapolation + limited to 60 series. Should make FG result in lower ms instead of higher + overall image quality far superior and basically all issues solved up to at least 4X.   Oh and a flood of MLPs and a demo showcasing the absurd visuals the 6090 can push. Moves goalpost past ReSTIR PT and will look borderline offline render quality. Very close to Blender renderers. IDK how they'll do it but MLPs are borderline magic, so prob doable.  Thinking about it more you're prob right and even if RDNA5 HW is amazing even beats 6090 in PT, a DLSS5 feature suite this impressive + moving goalpost to MLP based neural rendering will make RDNA5 irrelevant. As always SW and marketing will kill any momentum from HW side. Really hope I'm wrong but don't think so.  Sorry for the rambling.",hardware,2026-01-07 12:05:46,2
Intel,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
Intel,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,1
Intel,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
Intel,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
Intel,nycu2b3,"I enjoy reading your optimism. I hope it all comes true, but it sounds a bit too good to be true given the recent hardware developements. The 5090 scaling issue is that we stopped resolution scaling. If you go beyond 4k the 5090 scales a lot. VR resolutions report the 5090 being as much as twice the framerates of 4090.",hardware,2026-01-08 06:52:50,1
Intel,nydd2fq,I've rewritten prev reply to provide more info.  I'll also link the scheduling patent here in case anyone reading this thread is interested: [https://patents.google.com/patent/US12153957B2](https://patents.google.com/patent/US12153957B2)   It sounds like gains in workgraphs scenarios will be be even greater.,hardware,2026-01-08 09:42:10,2
Intel,nyd5bcu,"Yeah prob not realistic. I just tried to outline a nightmare scenario for AMD. As for the RDNA5 stuff we'll see how good it ends up being.  Agreed serious issues fs. RTX 5090 scheduling is brain dead. 16 SM GPCs, one central scheduler for 170 CUs. The smaller the internal res the harder it is to keep things going. Someone smarter than me could prob make a core scaling efficiency chart for different resolutions clearly showcasing how RT > raster and derive different formulas for 1080p, 1440p, 4K etc... . There's simply no reason why it has to be this bad moving forward.       But it's also interesting to entertain that RDNA5 could be a nightmare for NVIDIA. If NVIDIA doesn’t fix scheduling AMD's nextgen could be a real nightmare scenario for them. The modular and decentralized scheduling will be a gamechanger and based on what patents have said scaling is almost perfect and can scale to [arbitrarily large configurations](https://patents.google.com/patent/US12153957B2), yes they used that wording. AT0 will function like 8 x AT4 instead of running into massive scaling issues. In fact based on what the patent has said it might be even better. Consider each scaling domain with a local cache independent of the L2, where the global command processor only acts as a distributor of work, not an orchestrator. Gains will be observed across the stack but expecting IPC gains to scale with number of CUs. Is this a big deal for RT and 4K native? Yeah but even more so for lower res gaming.    And assuming they reduce CPU overhead even further in new uarch AMD will easily take the max FPS crown although I suspect NVIDIA can finally address their driver overhead issue after booting Maxwell-Pascal. We’ll see who comes out on top in CPU overhead nextgen.  I thought most of that gain vs 4090 was due to extra BW? But yeah high end perf scaling falls apart at sub 4K internal res.",hardware,2026-01-08 08:30:40,1
Intel,nyjq3tx,"I dont think much can be done with overhead. AMDs overhead is already small, basically letting the API go directly to GPU as it is. While for Nvidia side, isnt most of the overhead related to how Nvidia handles DX12? in that case i dont see it going away for a long time.",hardware,2026-01-09 05:42:46,2
Intel,nyfqcxo,"Interesting, GN usually gets Tom to do discussions like these but instead decided to publish whatever that previous video was on 'Intel pulling an Nvidia'. I bet GN will probably have their own video with Tom, but I appreciate DF a little bit more with this discussion.  At around 21min, it's interesting to hear his talk on cross-vender SR, mentions how they'd like to work more on Nvidia's Streamline and a candid talk about DirectSR and how it isn't really the concrete solution for the work on cross vendor SR. At around 23min, Alex brought up something interesting about research they've published before on joint denoiser and SR. He kinda skirts around it, but continues on suggesting they have more plans on it. He also then continues on the state of PT, DXR 1.2, obviously it isn't a real focus with something on their iGPUs, but any future HW, will be their primary goal to tackle. Alex mentions Valve/Linux, and Tom says it isn't entirely their focus right now, at least for gaming.",hardware,2026-01-08 17:45:35,42
Intel,nyhyq7q,"Super interesting that he randomly announces that Intel will be dropping a pre built shader program for Panther Lake. And not build with the new Microsoft framework/infrastructure, but just on their own?? How can Intel randomly drop this, but nvidia and amd can’t??",hardware,2026-01-08 23:46:46,12
Intel,nyhbk2i,"Seems like we're not the only ones that think FG isn't ideal rn. I really hope Intel succeeds in their efforts to pair Framegen with reprojection, but it'll prob be NVIDIA that gets there first. Might be the killer app for 60 series, but pure speculation of course.  The stuff about using AI to smoothe frames is interesting as well.  Things prob gonna change a lot in the coming years. We'll see if it's for the better.",hardware,2026-01-08 21:55:47,17
Intel,nyfhyad,Are PC games becoming more stuttery or we're just paying more attention to it?,hardware,2026-01-08 17:08:37,26
Intel,nyfezgd,This will probably piss off MLID since he hates Tom Peterson,hardware,2026-01-08 16:55:39,13
Intel,nyikqbo,This future of gaming is ridiculous.  Aggressive upscaling (360p) and one-in-four frames actually rendered and the rest FG?   For what?  Path tracing?  Nanite?,hardware,2026-01-09 01:41:36,-7
Intel,nyhaj0j,You still watch GN? Dude only farms drama after realizing how much clicks they generate.,hardware,2026-01-08 21:51:23,42
Intel,nyhmk4k,Thank you for the summary!,hardware,2026-01-08 22:45:36,3
Intel,nyjni69,"He's talked about it before I believe in a previous interview, might have been with PC World from memory or perhaps GN. Regardless, it wasn't exactly new iirc. [Anyways this is definitely old news.](https://overclock3d.net/news/software/intel-plans-to-make-shader-stutter-a-thing-of-the-past-with-arc/)",hardware,2026-01-09 05:24:04,10
Intel,nyja51l,"What do you mean? The Shader delivery program was launched on an AMD handheld, so I presume they will use the MS advanced shader delivery infra.",hardware,2026-01-09 03:59:46,7
Intel,nyimpss,"Intel already talked about this a couple months ago. It's not an announcement here, but it seems people are more interested in AMD and Nvidia news so those threads don't get as much traction.",hardware,2026-01-09 01:52:08,6
Intel,nyflgv9,I feel like more people paying more attention since the market’s grown a lot.   I remember old games I played in the early 2000s having micro and regular stuttering depending on the game. I chalked it up to “huh guess it’s loading in data as I play” when I didn’t know much.,hardware,2026-01-08 17:24:06,45
Intel,nyfmkrk,We are paying more attention to it but I suspect as we push higher frames with new engines and techniques the micro stutter is getting worse. It just starts becoming less perceptible to people than say the micro stutter from SLI and other stuff that caused issues in the past.,hardware,2026-01-08 17:28:57,21
Intel,nyfuxga,"I think digital foundry answered that question on their podcast. Compared to 15 years ago the frame rate is higher on average for most gamers, but it also stutters more. So it's huge fps with huge drops and hangs.",hardware,2026-01-08 18:05:15,22
Intel,nyfrkdx,"Worth remembering no one even gave a shit about stuttering enough to measure it until someone at I think it was anandtech back in the late 00s was so fed up with shitty perf on his crossfire system he started doing 1% lows on benchmarks.   Back in the 90s people would run like 6x SLI voodoos and not even care that the game hitched every 10 seconds down to sub 30s fps lol...  Edit: it's also key to remember that a lot of the games that stutter on PC these days stutter in the exact same places for the exact same reasons, and worse considering the lack of CPU power, on consoles. Console gamers just don't give a fuck lol....   DF did a good video showing this truth with the Silent Hill 2 remake.",hardware,2026-01-08 17:50:48,31
Intel,nyg5v26,"With a lot of PS3 games dipping to 20s, I guess we are just paying more attention to it now",hardware,2026-01-08 18:52:01,9
Intel,nyj9uk1,There was DF Clip of this exact question.  https://www.youtube.com/watch?v=pxsfT4c-F-Q,hardware,2026-01-09 03:58:01,4
Intel,nyfihv5,They're more surgery stuttery.,hardware,2026-01-08 17:11:01,3
Intel,nyfksy4,Everyone hates MLID,hardware,2026-01-08 17:21:10,42
Intel,nyfg44c,Funny piece of lore. Why so?,hardware,2026-01-08 17:00:30,17
Intel,nykk9lw,Who's MLID?,hardware,2026-01-09 10:03:38,3
Intel,nyjx60j,If the end visuals are better who cares. we already did a lot of such things in engine just didnt tell the players about it. One in four frames actually rendering shadows is a thing for example. Heck some games go as bad as once a second shadow updates.,hardware,2026-01-09 06:38:04,17
Intel,nykocr4,"Well, I think that most people (myself included) don't really care how it's done if the end result is looking good and feel good to play, off course something like 30fps based with MFG to 120 is bs, but I quite regularly use 60 -> 120 using frame gen because It feels okay input wise to me at 60, and the added visual smoothness is quite nice. So it all depends how it's implemented and talked about.",hardware,2026-01-09 10:40:00,10
Intel,nykneij,"yeah i had to tap out a couple months back. it's a shame, they did great work, but i refuse to support ragebait.",hardware,2026-01-09 10:31:33,17
Intel,nyhh357,"Doesn't help that there isn't any particular new hardware to review or anything new besides AI, the PC industry has hit stagnation in the consumer market.",hardware,2026-01-08 22:20:10,26
Intel,nyickvj,"In other news, guy who is being served turds at restaurant loudly complains about receiving fat dooks instead of food.     *""man that guy is such a whiner""*",hardware,2026-01-09 00:58:06,0
Intel,nykpeuy,"No it didn’t sound like they were going to use the MS shader delivery program. Or no, they said they want to, but they have their own solution that they’ll launch before that.   And I also don’t think the MS solution is ready. The ROG ALLY XBOX also doesn’t have this feature already as far as I know",hardware,2026-01-09 10:49:11,2
Intel,nykoz3l,"> The Shader delivery program was launched on an AMD handheld  i assume you're referring to the steamdeck, which to my understanding was done by valve... so the point kinda stands, you just add valve to the preamble.",hardware,2026-01-09 10:45:26,2
Intel,nyg0pvl,"I dunno I remember many games, especially based on Quake engine, being buttery smooth if you could get to reasonable FPS",hardware,2026-01-08 18:30:02,18
Intel,nyjwwd6,also more cause for stutering. Back then we could compile shaders real time with no siginficant issues because they were small. Now we have to compile shaders real time that are huge to the point where we pre-compile half of them before we even start the game.,hardware,2026-01-09 06:35:52,7
Intel,nyfy60k,Console gamers *of certain genres* care.   The FGC rejected the idea of playing Street Fighter IV & MvC3  on PS3 because the Xbox 360 port had better input latency.   Even the PS4 port of USFIV wasn't liked.     Part of why I stopped going to tournaments is because SFV & Tekken 7 on PS4 always felt *off* compared to PC.,hardware,2026-01-08 18:19:10,11
Intel,nyg711t,"Not really. In many cases, console versions of games just don't stutter whereas PC games do because modern games are mainly designed for consoles and then ported to PC.  A couple of good recent examples are Wukong and Outer Words 2. Neither of those games on consoles have the horrid stutters that are prevalent on PC.",hardware,2026-01-08 18:56:57,8
Intel,nymcty0,MLID more like MID,hardware,2026-01-09 16:26:52,2
Intel,nyfkp1n,He’s been calling Tom a “snake oil salesman” for performance claims on Alchemist,hardware,2026-01-08 17:20:43,14
Intel,nzdp1mq,"Moore's Law is dead, he's a YouTuber who makes predictions and purports to have insider information from Nvidia/Intel/AMD but in reality he is better described as FanFiction for PC enthusiasts.",hardware,2026-01-13 16:50:02,1
Intel,nym0alm,"I worry about things like latency.  Or what happens when you move suddenly, or fire, and the whole image falls apart.  If the hardware can't do it all yet, then wait a few years instead of using these... methods.  And just to be clear, I think very highly of  Mr. Tom 'TAP' Petersen; I'm just not liking where this is all heading.  Are you happy with Borderlands 4 or Outer Worlds 2?  If you are then we live in two different worlds.",hardware,2026-01-09 15:30:18,-2
Intel,nyjn1ql,"There's plenty of old stuff he needs to review. I've said it before but I will say it again and beat it like a drum: He STILL has not reviewed the 9060 XT 8GB for instance. Plenty of content he could have farmed off that, especially BEFORE the RAM shortage where having an 8GB card was some sort of sin in his eyes (except for some reason he ignored it but raile roaded the cheaper RTX 5050, lol did someone say bias?). Instead, he just tore the 9060 XT 8GB down and never touched it again after that teardown. I'm sorry but there's GENUINELY stuff he could be doing instead of farming clicks about 'NVIDIA bad' and 'Intel pathetic', but that won't get him views from drama farming NVIDIA which is what he craves these days. So sad to see a big channel like his who got big off doing solid technical content become a drama-hype ""news"" channel.",hardware,2026-01-09 05:20:56,24
Intel,nyk71tg,No wonder that strategy has proved successful. There are a lot of people that only care about complaining and bitching. Which I get up to a point but it starts to feel childish and pathetic quickly.,hardware,2026-01-09 08:02:47,20
Intel,nysczdh,"I love how reddit down votes you,  reddit posts all day complaining about the industry, GN does a video on it   Redditors ""whiney cry baby engagement farmers """,hardware,2026-01-10 14:05:38,-2
Intel,nyogfa3,"Valve does have one on Steam, but Microsoft announced a store agnostic, eventually hardware vendor agnostic one launched with the Asus ROG Xbox Ally X last year.  It was supposed to be working already but there's no sign of it just like everything else Microsoft releases about gaming half-baked like their gaming UI, their attempt to unify upscaling, and Directstorage.   So I imagine they're referring to what Microsoft called ""Advanced Shader Delivery"" that they've done little with but name and announce to sell more Asus Pretend-Xbox's.",hardware,2026-01-09 22:10:03,3
Intel,nyoi6ny,I’m talking about the Xbox Ally as the other commented said,hardware,2026-01-09 22:18:25,1
Intel,nygmjs6,We considered solid 60 or 75 smooth back then. Now at least I complain as soon as I can't stay above 100.,hardware,2026-01-08 20:05:12,13
Intel,nyhs8ie,"Shader compilation stutter is a PC problem that’s been especially bad in UE4 and 5 since the transition to DX12. Other types of stutter and bad frame rates used to be equally bad on consoles, or even worse in the 360/PS3 era.",hardware,2026-01-08 23:13:28,11
Intel,nygf3pi,Outer Worlds 2 is fairly consistent for a UE5 game. To me its just very heavy,hardware,2026-01-08 19:32:17,4
Intel,nygrcy6,"funny coming from MLID, notorious snake oil salesman",hardware,2026-01-08 20:26:46,47
Intel,nyflbj9,"Eeeegh, considering MLID overall story - can see that, can see that.  OTOH usage of such terminology - snake oil salesman - reminds me of that very infamous Intel presentation...",hardware,2026-01-08 17:23:27,14
Intel,nymda4b,You say that as if we will reduce our base frame rate. But the direction of travel is to use MFG to drive the 480Hz and above monitors that will become more and more common in the future.,hardware,2026-01-09 16:28:52,5
Intel,nyqwqj9,I dont like borderlands franchise and i havent played Outer Worlds 2 yet so i cannot comment on them from personal experience. However things like Reflex/Reflex2 has actually decreased latency for me.,hardware,2026-01-10 06:52:45,1
Intel,nylgvk7,"I havent really seen it as some sort of drama farm, I think a lot of GN's videos do show how bad the current computer hardware hobby is doing and Im glad someone is focusing on that",hardware,2026-01-09 13:55:07,1
Intel,nysckbg,Whata wrong with calling companies out? Like redditors cry all day but stop at GN when the channel brings up issues within an industry.  Sorry a person isn't running their channel the way the reddit collective wants.,hardware,2026-01-10 14:03:14,-2
Intel,nysv24y,"honestly, i had completely forgotten that was a thing.   i wish i had more faith in microsoft to pull this off, but i remain skeptical in basically anything they try until proven otherwise.",hardware,2026-01-10 15:44:02,1
Intel,nyguz4e,"People on gaming forums used to go around insisting the human eye could not distinguish greater than 60fps.  It was accepted as gospel in many places/by many people.  Granted, that was in the age of CRTs and almost no one had actually seen >60, but funny to think back on.",hardware,2026-01-08 20:43:03,12
Intel,nyjx1ak,snake oil salesmen dont like competition.,hardware,2026-01-09 06:37:00,9
Intel,nyjz26x,"MLID aka ""RTX 50 super will be released november 2025""",hardware,2026-01-09 06:53:37,5
Intel,nyknw8m,this just makes me like tom more,hardware,2026-01-09 10:35:57,6
Intel,nyjwmn8,"Replace that 60 with 30. Yes, people insisted we could not see more than 30 even while playing on 85hz CRTs.",hardware,2026-01-09 06:33:40,7
Intel,nyhcljr,"Meanwhile quake players kept dropping resolution to hit triple digit refresh rates :) But yeah, it's wild that 30 was the norm for so long on console",hardware,2026-01-08 22:00:09,5
Intel,nyko1gh,That's far from the worst prediction he's missed.  Remember SMT4 for future Zen architectures? 24 and 32 cores on desktop Zen4? L4 cache for Zen 4?,hardware,2026-01-09 10:37:14,11
Intel,nyrvn0z,Which Tom?,hardware,2026-01-10 12:08:40,1
Intel,nyjbd32,"console used to lock at 60, dont know why they choose to drop down to 30.",hardware,2026-01-09 04:06:56,5
Intel,nyjivip,"Because when the hardware cant handle all the sprites, it slows everything down including game logic which was tied to frame rate back then. It happens in plenty of hardware from snes to arcade cabinet.",hardware,2026-01-09 04:52:53,7
Intel,nyjwrq4,"also happened the other way round, older games would run faster than they should because they tied it to framerate rather than delta time. This bad practice was so common you can still find studios like bethesda do this.",hardware,2026-01-09 06:34:49,4
Intel,nz6q876,"for context, this G14 was the one that gave us [the first ever Geekbench scores for Panther Lake](https://www.reddit.com/r/hardware/comments/1ocaslx/panther_lake_geekbench_leak_its_good/) over on r/hardware",hardware,2026-01-12 16:33:10,34
Intel,nz6w6zw,"The price probably wasn't going to be low enough compared with other SKUs.  In the past, Intel skuss with Iris HD etc were so expensive and were only included in $2000 ultrabooks. This is probably going to be same.",hardware,2026-01-12 17:00:15,18
Intel,nz73dw5,When I first learned about I was confused why it existed. Not surprised Asus decided to cancel it.,hardware,2026-01-12 17:33:23,11
Intel,nz6r3wa,"It makes sense, a B390 only G14 kind of defeats the whole purpose of the Zephyrus, even as a base model.     Basically brings the GPU performance down to 3060 levels (actually a little worse than that), which is roughly 4 years ago at this point.",hardware,2026-01-12 16:37:10,39
Intel,nz6q33b,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-12 16:32:31,1
Intel,nzujefe,"Ugh this would have been such a cool business laptop option, I'm finding current-gen integrated graphics to not be enough to really handle Teams + external monitor + a presentation. Sounds like this would have hit the performance mark without having to pay through the nose.",hardware,2026-01-16 02:30:35,1
Intel,nz770nj,It’s also the one that didn’t have a dedicated GPU in Geekbench,hardware,2026-01-12 17:49:53,15
Intel,nzcyo9s,"I don't think so, they're making it available on more than just the top 9 series, they also confirmed a custom chip for gaming handhelds which we'll prolly see at computex",hardware,2026-01-13 14:45:59,4
Intel,nz7invn,It probably made more sense as an low-end gaming Zephyrus when fitting it with 32GB LPDDR5x wasn't half the BOM,hardware,2026-01-12 18:42:14,18
Intel,nz7xzli,esp since on CES 2026 they launched an even more premium 14 inch laptop that actually does have the B390 (ExpertBook Ultra),hardware,2026-01-12 19:52:01,5
Intel,nz6vvy1,esp since Asus ended up launching the ExpertBook Ultra at CES  an even more premium 14 inch laptop compared to the G14 which does have a B390,hardware,2026-01-12 16:58:51,11
Intel,nz6ryqs,"But it actually can be used on battery unlike normal laptop gpus which make your laptop die in 5 seconds or performance is awful and it dies in 10 seconds, so what's the point of having a powerful GPU on a laptop if I got to plug in all the time it sucks.",hardware,2026-01-12 16:41:01,31
Intel,nz6tste,"A 3060 is still useable imo. If the efficiency is right, they can make usb-c powered gaming laptop and completely remove the dumb dc brick.",hardware,2026-01-12 16:49:18,15
Intel,nzcpz2s,"It would have made _some_ sense, but the Arc B390 must’ve been real good. But as you say, with the raise in RAM prices, you can forget about it.  I wonder whether Intel themselves would pair a mid-range Panther Lake with a top-notch iGPU… It would make sense, since the high-end CPUs tend to have dGPUs.",hardware,2026-01-13 14:00:36,1
Intel,nz6tn0v,"For the Zephyrus you can also just disable the dGPU to game only on the iGPU, and the customers of the Zephyrus are minimum looking for a decently powerful dGPU",hardware,2026-01-12 16:48:35,20
Intel,nz6snj4,I hope their plugged out power scheme is permanent and doesn't vary based on application,hardware,2026-01-12 16:44:08,3
Intel,nzaau06,"> so what's the point of having a powerful GPU on a laptop if I got to plug in all the time  I have a laptop for travelling. I travel to places that have plugs, and I don't need to use a laptop while I am actually in transit.",hardware,2026-01-13 03:03:12,1
Intel,nz8xpz8,"It still pulls 60W (and apparently 80W, briefly) at full pelt...  That is to say an hour and a half battery life when gaming should be expected. It's not magic.",hardware,2026-01-12 22:40:05,0
Intel,nz71x4x,Usb c powered gaming laptops are more common now. The new ideapad 5 pro with panther lake and 5060 (combined 110w system power) uses usb c exclusively,hardware,2026-01-12 17:26:41,12
Intel,nz753k0,"> A 3060 is still useable   Nvidia seems to agree with you, which is why they are looking to put them back into production.  /I'm sorry, I couldn't resist.",hardware,2026-01-12 17:41:14,11
Intel,nz6wes7,"Useable, yes, but what customers of the Zephyrus are looking for, no.",hardware,2026-01-12 17:01:16,12
Intel,nz6vzj4,"Yeah, and it gets pretty good battery life on that and in power saving mode/60fps screen mode. Like 8 hours. Best I've had on a gaming laptop- not a high bar, but it's nice to be able to take it into the living room and use it light a normal laptop instead of it dying instantly.",hardware,2026-01-12 16:59:19,7
Intel,nz920o1,Still better than 45 minutes  Also it can obviously be put in a lower power mode to save battery while not hurting performance too drastically.,hardware,2026-01-12 23:01:47,2
Intel,nz6x4bl,"The G14 used to have gtx1650, so not sure what do you mean.",hardware,2026-01-12 17:04:35,0
Intel,nz724pz,Which was the 5050 of that time which is significantly more powerful than panther lake's igpu.,hardware,2026-01-12 17:27:38,10
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,116
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,41
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,21
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,4
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,7
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,10
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,6
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,9
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,8
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,17
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,27
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,11
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,4
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,10
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,3
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,-1
Intel,nu2970r,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-21
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,19
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,5
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,25
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-4
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,4
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,19
Intel,nubvjrs,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,18
Intel,nt8w4et,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",hardware,2025-12-10 06:01:38,27
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,11
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,ntafnry,How does this compare to the Snapdragon X2 Elite?,hardware,2025-12-10 13:56:04,1
Intel,nuahcx5,4 pcores  8ecores 4 lpcores..,hardware,2025-12-16 06:05:52,1
Intel,ntcj3az,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:10,1
Intel,ntbju2x,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",hardware,2025-12-10 17:23:27,1
Intel,nte6d5x,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",hardware,2025-12-11 01:36:38,0
Intel,nt8wwpg,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",hardware,2025-12-10 06:08:21,9
Intel,nt9ghvf,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",hardware,2025-12-10 09:13:58,3
Intel,ntcj6hz,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:37,-3
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,16
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,16
Intel,ntdv8jz,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",hardware,2025-12-11 00:29:31,1
Intel,ntcpemm,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",hardware,2025-12-10 20:47:39,-1
Intel,ntep84w,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",hardware,2025-12-11 03:32:35,3
Intel,ntfpawi,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",hardware,2025-12-11 08:27:12,2
Intel,nthte2d,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",hardware,2025-12-11 17:03:33,2
Intel,ntckb4u,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:22:19,2
Intel,ntgubsm,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",hardware,2025-12-11 14:03:57,1
Intel,ntfhexr,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",hardware,2025-12-11 07:10:24,2
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,13
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,29
Intel,ntfm9jb,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,hardware,2025-12-11 07:57:01,5
Intel,ntco5ms,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",hardware,2025-12-10 20:41:27,-3
Intel,nthu15b,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",hardware,2025-12-11 17:06:47,2
Intel,ntd3lso,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",hardware,2025-12-10 21:57:00,12
Intel,ntfgs1k,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",hardware,2025-12-11 07:04:32,2
Intel,ntsl9xc,My old 14900hx gets 35k multi core in cinebench r23,hardware,2025-12-13 10:15:38,1
Intel,ny53e0z,better go for cb2024 cuz r23 is being less relevant these days,hardware,2026-01-07 04:09:43,1
Intel,ntgxa5r,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",hardware,2025-12-11 14:20:52,2
Intel,nt9ouc7,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",hardware,2025-12-10 10:37:37,4
Intel,nvdnrp5,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,hardware,2025-12-22 15:05:12,1
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,-3
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,0
Intel,ntd4133,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,hardware,2025-12-10 21:59:01,5
Intel,ntduxlv,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",hardware,2025-12-11 00:27:43,8
Intel,ntwk3p9,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",hardware,2025-12-14 00:54:40,2
Intel,ntmv22s,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",hardware,2025-12-12 12:29:41,1
Intel,nt8x38v,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",hardware,2025-12-10 06:09:56,14
Intel,nt9ufxf,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,hardware,2025-12-10 11:28:32,3
Intel,nt8wp7g,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",hardware,2025-12-10 06:06:35,8
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,12
Intel,ntgnaao,That looks like an AI post to me,hardware,2025-12-11 13:22:24,2
Intel,nt9re78,It has been going hayway since SME just like GB5 had issues with AES Skewing results,hardware,2025-12-10 11:01:14,1
Intel,nt92449,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,hardware,2025-12-10 06:54:12,-2
Intel,nvfvk61,Incredible hardware news. Thanks for the share.,hardware,2025-12-22 21:50:44,18
Intel,nvl1r29,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",hardware,2025-12-23 18:20:13,13
Intel,o41u1ql,"a 700W is a bit overkill, considering that you are only adding 1 b580. for the rest, it should be ok, maybe a ram upgrade would benefit from this.",buildapc,2026-02-07 08:35:10,2
Intel,o420wug,"You don't need to upgrade it, this should work fine for your use case. It looks more like a software related issue.",buildapc,2026-02-07 09:42:48,2
Intel,o424f4u,"RAM is quite expensive now, and I only have two RAM slots, so I am not sure whether I can upgrade it as of now. Would a 600W PSU be adequate?",buildapc,2026-02-07 10:17:32,1
Intel,o4249ed,"Thanks! I am considering the GPU upgrade because I couldn't even edit a DaVinci Resolve timeline of 3 minutes, only consisting of photos. I clean my computer frequently, and clear the cache, so I am unable to find the source of the issue. Do you have any ideas on what it could be?",buildapc,2026-02-07 10:15:59,1
Intel,o44vtlw,"Well, yes.",buildapc,2026-02-07 20:01:41,2
Intel,o427alb,"Sorry, I don't know DaVinci's usual performance. Your current spec should be enough for ""basic productivity work"" is all I'm saying.",buildapc,2026-02-07 10:45:32,1
Intel,o47lmh4,Thanks a lot!,buildapc,2026-02-08 06:13:05,1
Intel,o428lsq,That's all right! Thanks for the advice!,buildapc,2026-02-07 10:58:02,1
Intel,o3i20g7,9060xt,buildapc,2026-02-04 09:12:36,20
Intel,o3i2xvr,Is the 16gb not an option,buildapc,2026-02-04 09:21:32,6
Intel,o3i2hds,"If the cards you've listed are your only options, RX 9060 XT. The other options have their gaming performances kneecapped by your current CPU being too slow and stuck with PCIe 3.0 instead of newer PCIe standards.",buildapc,2026-02-04 09:17:07,11
Intel,o3i34hv,9060XT 16GB ? Would stay away from 8GB cards when buying new. If this is not an option and i'd have to choose from the 3 stated i'd go with B580 solely going by the 12GB VRAM,buildapc,2026-02-04 09:23:21,5
Intel,o3jxulf,"Unless it's very cheap, I wouldn't bother with the 5060 and an i5 10400. In most games, the CPU bottleneck will be hit. The RX 9060 XT is the best all-around option; it has a strong rasterizer, no driver nonsense, and is good in 1080p and 1440p resolutions. If you're only playing modern games and it's considerably cheaper, the Arc B580 is tolerable. For all three, a 650W power supply is enough.",buildapc,2026-02-04 16:23:55,4
Intel,o3i7cw3,"An RX 7600 XT would be a good option. So would a 9060 XT. Also, you're good bro. I'm still running an i7-10700 with the TDP set to 125W and it still handles literally anything I could throw at it. I have literally zero reason to upgrade. People in this sub are borderline obsessed with blowing all their money on new hardware every two years. They probably drag their feet when they walk and breath through their mouths. Don't let it get to you.",buildapc,2026-02-04 10:03:35,3
Intel,o3i8w0v,I was going to say 9060 xt but since you said for minecraft then I would say 100% 5060 bc minecraft is super nvidia favoured.   However for general gaming 9060 xt 8gb,buildapc,2026-02-04 10:17:38,2
Intel,o3jfdt8,I guess rx 9060xt may bottleneck with i5 10gen.   I also have i5 10400k and gpu - 1660 super,buildapc,2026-02-04 14:56:55,2
Intel,o3i25vq,Ant particular games/applications you care about?,buildapc,2026-02-04 09:14:03,1
Intel,o3i2dcp,9060xt,buildapc,2026-02-04 09:16:02,1
Intel,o3i7xeo,"If you really can't squeeze a 9060 xt 16 GB in then the B580. Or maybe a used good condition 3060 12 GB, very similar performance, but gets NVidia drivers.   Modded MC, Cyberpunk, and especially modded Skyrim really thrive off Vram.",buildapc,2026-02-04 10:08:49,1
Intel,o3i8711,"Living in 2026 with a 2020 CPU… respect the patience 😂    For 1080p high / 1440p medium in Minecraft with shaders + Cyberpunk, \*\*RX 9060 XT\*\* hits the sweet spot between FPS and VRAM.    RTX 5060 will be bottlenecked by your CPU, and Arc B580 is okay if you want 12GB VRAM and some Intel vibes.    What FPS do you guys get in Cyberpunk on a mid-range GPU + i5-10400?",buildapc,2026-02-04 10:11:17,1
Intel,o3j6bgg,The Arc is the slowest but will not have VRAM limitations.  The RX 9060XT should be the fastest.  NVidia has technology in the works to reduce the VRAM requirements of games but no official word on its progress has ever been made.,buildapc,2026-02-04 14:10:13,1
Intel,o3lj3dt,"I'd recommend 9060xt 8gb. Intel Arc cards still have driver overhead issues on older CPUs, despite being improved somewhat. Nvidia is using PCIE 5.0 x8 on their 50 and 60 class cards which will be a problem on your PCIE 3.0 system with ""only"" 8gb VRAM, and a 5060 might be overkill for your CPU anyway. 9060xt on the other hand still uses x16 interface, which means better compatibility with PCIE 3.0, and 8gb VRAM is fine for 1080p",buildapc,2026-02-04 20:48:26,1
Intel,o3na0l5,RTX 2070 Super. Not waste anything.,buildapc,2026-02-05 02:22:14,1
Intel,o3i2bm6,Talk about living in the past. We are in 2026.,buildapc,2026-02-04 09:15:35,1
Intel,o3i7wxe,I would pick 5060 from these and get DLSS and other features. I would suggest a used 3080 but your psu wouldn't handle it. 4060ti 16gb would be grand if you find it used at a good price,buildapc,2026-02-04 10:08:41,-1
Intel,o3jgqgc,"Can confirm. I paired a 9060xt 16gb with my i7-8700 and it works great for me. The 8700 is probably fairly close in performance to the 10400.  As for the vram usage, I only play Marvel Rivals 1080P and I have never seen the Vram go above 6-7GB on Ultra settings. Can't say what it would be like on other games.  I would not hesitate to get those other cards depending on the local prices. I was lucky enough to get mine for $390 with a 5% discount a month ago, but it looks like the prices are up since then.",buildapc,2026-02-04 15:03:37,5
Intel,o3ica1b,"looking at his pc specs i doubt hes playing on anything other than 1080p, which makes the 16gb cards not a need",buildapc,2026-02-04 10:48:20,2
Intel,o3i7v9o,PCIE 3.0 is not an huge issue. It is like up to 5% difference,buildapc,2026-02-04 10:08:15,8
Intel,o3i2oxw,"Yeah for now they are my current options, in that case will have to go with a 9060xt",buildapc,2026-02-04 09:19:10,5
Intel,o3i2bv4,"mainly cranked up minecraft w/ loads of mods and a mid lever shader, besides that probably cyberpunk",buildapc,2026-02-04 09:15:39,1
Intel,o3k6r2j,"probably can but that depends, its like 100$ more than the 8gb one, the b580 is overpriced and have to import it to even have remotely viable prices, looking into 3060s for now, gonna jump on one if I can find one in good condition",buildapc,2026-02-04 17:04:40,1
Intel,o3utlgf,"that's what I was thinking as well, for now Im looking into either the 16gb or 8gb 9060xt depending on the price and stock",buildapc,2026-02-06 05:54:03,1
Intel,o3iarae,Dude.  Just stop.  I am finally getting around to replacing my 10400.   Not everyone has money,buildapc,2026-02-04 10:34:41,8
Intel,o3i2g3r,my bad ;-;,buildapc,2026-02-04 09:16:47,3
Intel,o3ih62u,Can agree to disagree about that for obvious reasons but the 9900 is going to suck balls if it does run out of ram on pcie3,buildapc,2026-02-04 11:30:18,1
Intel,o3ido65,"Yeah but Nvidia uses the x8 interface, so with PCIe 3.0 you're still getting only 3.0 x8, which is equivalent to 4.0 x4 or 5.0 x2. Absolute dick move from them to cheap out on the lanes.",buildapc,2026-02-04 11:00:34,11
Intel,o3if59t,"For the Arc B580, PCIe 3.0 isn't a big issue as long as the CPUs that are stuck with it are at least as fast as the Core i5-12400F and/or the Ryzen 5 5600. Anything slower and CPU overhead issues will come up along with PCIe bandwidth issues (Arc B580 is stuck with 8 PCIe lanes only).  For the RTX 5060, being stuck with 8 PCIe lanes only and requiring a PCIe 5.0 slot for maximum performance cripples its gaming performance when paired with a CPU that's stuck with PCIe 3.0 support.  About my suggestion, I've shown some mercy since it has access to 16 PCIe lanes.",buildapc,2026-02-04 11:13:20,5
Intel,o3idgv7,"Dude, that was just for 2025 in the title. And I am on i5-4570 that needs replacing.",buildapc,2026-02-04 10:58:49,3
Intel,o3j1ccv,"Don't listen to this dude. There's nothing wrong with having older components. I'm keeping my 10700K for as long as I can. I have no intention of upgrading it until it can't keep up with the games I play. If it ain't broke, don't fix it",buildapc,2026-02-04 13:43:30,6
Intel,o3ihdt6,Oh right completely forgot about bullshit they pulled on 5060 cards. In that case only TI 16GB version is worth considering,buildapc,2026-02-04 11:32:03,5
Intel,o3inegy,I feel you. I ended up coming from an i5-4690k a few years back and then only bc my MOBO died and I had to lol.,buildapc,2026-02-04 12:17:31,3
Intel,o3jgqw1,"He was just joking that 2025 was in the title, hence the living in the past remark - he wasn’t being a jerk about the parts (I took it that way at first too)",buildapc,2026-02-04 15:03:41,2
Intel,o3k6cg6,"eh im good with no gpu too, its just i think im missing out on A LOT of games without one, gonna keep this pc for as long as I can, at least hoping 6 more years",buildapc,2026-02-04 17:02:46,2
Intel,o3ilj1t,"Yeah, but those are now priced the same as a 9070 and NOT far off from a 9070 XT",buildapc,2026-02-04 12:03:56,2
Intel,o3kbjwg,I didn't even notice lol my bad,buildapc,2026-02-04 17:26:49,2
Intel,o3dl7rj,"The CPU is quite good and can support a much faster GPU.  The problem, as you figured out, is the GPU. It's very slow.  It would also help to know how much RAM you have.",buildapc,2026-02-03 17:38:18,9
Intel,o3dqykq,"All the stuff you listed as unimportant is actually pretty fuckin important. If you cant keep your cpu cooler, you run like shit. Not enough power makes you run like shit or worse. Keep that in mind when you build your next one",buildapc,2026-02-03 18:04:15,2
Intel,o3dllgj,"That’s a low end pc, but could pretty easily be a mid range pc by plopping a 5070 or 9060xt in there.",buildapc,2026-02-03 17:40:02,2
Intel,o3dmpx8,Overheating? I would consider cooling very important,buildapc,2026-02-03 17:45:07,1
Intel,o3dnewb,"Those specs shouldn't be overheating. Is it just running warm and concerning you? Overheating is when it's actually hindering performance, and if that is happening something is wrong.",buildapc,2026-02-03 17:48:14,1
Intel,o3dphq5,"Decent 1080p build, entry level 1440p build. I can't really call it a low end pc I've friends that still rocks gtx 1650, rx 580, etc. If it runs the games that you want with 60 fps and you're good on the image quality, keep it.  My bad. I thought your GPU is a B580. I recommend upgrading your GPU to a 9060 xt, or even a 5060 if you're onky gaming at 1080p.",buildapc,2026-02-03 17:57:37,1
Intel,o3dqk81,What kind of monitor are you using?,buildapc,2026-02-03 18:02:27,1
Intel,o3du0az,What's your RAM amount?,buildapc,2026-02-03 18:17:53,1
Intel,o3dxntd,"9060 xt, it’s the best bang for your buck and you can play decent fps at 1440p",buildapc,2026-02-03 18:34:17,1
Intel,o3e2h6l,"Check your cooler and clean/replace the thermal paste between the CPU and the cooler.  I just did that on mine after several years and it dropped the average CPU Core temps by 20C .    Obviously there are other things you can do but that one takes a few minutes and costs a few dollars.  It surprised me how much better everything behaves now with the CPU in the 40's instead of the 60's+.  (it was spiking into the mid 80's, now it never does that)",buildapc,2026-02-03 18:55:51,1
Intel,o3q9rv6,32gb of RAM CL30,buildapc,2026-02-05 15:21:36,1
Intel,o3epqva,that case is awful for temperatures. truly abysmal.,buildapc,2026-02-03 20:45:10,1
Intel,o3epq8l,that case is awful for temperatures. truly abysmal.,buildapc,2026-02-03 20:45:06,1
Intel,o3qacvz,Ultra gear 165Hz,buildapc,2026-02-05 15:24:27,1
Intel,o3qait6,32gb ram cl30,buildapc,2026-02-05 15:25:16,1
Intel,o47z2wj,"No, not for double the price . Get the 5700x, you'll be fine",buildapc,2026-02-08 08:16:01,39
Intel,o47yzkp,No get the 5700x,buildapc,2026-02-08 08:15:09,17
Intel,o47z33v,"No it's not, 5700x will still be a big uplift for you  Although from a 2060super to a 9070 XT, despite the bottleneck, it should be giving you more than 10 fps uplift. You're using the exact same settings? Have you tested any other game?",buildapc,2026-02-08 08:16:04,10
Intel,o47z8l8,57x3d is about 10~20% faster than 5700x. Whether that's worth extra 200$ is up to you to decide.  Edit: https://youtu.be/uWzk6J09n_A Found a benchmark video that specifically compares performance in Arc Raiders.,buildapc,2026-02-08 08:17:31,5
Intel,o47zvze,Not really. Maybe in some edge case.,buildapc,2026-02-08 08:23:41,3
Intel,o480230,Not 1 in a million years. Specially if you gaming on 1440p,buildapc,2026-02-08 08:25:19,3
Intel,o4acrno,No. And prices won't ever go down. Ever.,buildapc,2026-02-08 17:50:16,2
Intel,o48085j,Arc raiders is very CPU heavy and the 5700x3d is a good chunk faster closer to am5 non x3d parts. If you're running like medium settings chasing high frames the extra cost is worth it imo.   3 week old testing has a 5800xt which is a bit better than a 5700X but even still the 5700 x3d takes a solid lead of 22.6% on average. With a 39% lead in arc raiders 192 FPS Vs 138. And 1% lows of 125 Vs 90. https://youtu.be/c7Au9Lb5Kw4?si=q33_25u-YlvEe3QU   Testing from launch has a wider amount of cpus. There is a 19% advantage to the 5700 x3d Vs 5700x. https://youtu.be/MU-jUtrfANA?si=yy-Fg_6TfnuGedWm  So what we see is overtime the x3d CPUs pull further ahead of there non x3d counterparts. Arc raiders in particular is a very CPU heavy game which showed a close to 40% gap and as the 5700x is slower than a 5800 xt especially in multicore workloads that can leverage the increased tdp and base clock I imagine the gap will be greater still.  Also x3d CPUs care less about memory speed and 3000 MHz is sub optimal and will further increase the gap between these processors.,buildapc,2026-02-08 08:26:54,2
Intel,o48zxz5,As an owner of 5700x3d - never,buildapc,2026-02-08 13:35:05,1
Intel,o495akz,Check out 5600X pricing as well  5700X3D was worth it a year ago when it was a bit less than MSRP. Kinda hard to justify 400 for it,buildapc,2026-02-08 14:07:54,1
Intel,o499f0b,5700x is still solid and to pay that much more is definitely not worth it,buildapc,2026-02-08 14:32:05,1
Intel,o49bup6,It used to be $100 5700x vs $200 5700x3d and it was worth double.,buildapc,2026-02-08 14:45:55,1
Intel,o49gaio,No it just isn't.  When the 5700X3D was selling for $250 USD it was worth it. In a few years just get AM6 since you are waiting.,buildapc,2026-02-08 15:10:04,1
Intel,o49rzzf,"100 to 200 I'd say yes, 150 to 300 it's a hard sell, for 400 it's probably no.",buildapc,2026-02-08 16:09:29,1
Intel,o49sxuf,i'm running the 5700x now and it really just about runs everything i need in games more than fine. it even has alot of room for Safe OC and free extra performance. i also recommend running it with two sticks of ram at 3600 to get the most out of it (very noticeable uplift from 3200 ram sticks),buildapc,2026-02-08 16:14:08,1
Intel,o49vc10,No,buildapc,2026-02-08 16:25:55,1
Intel,o4a5qor,"https://www.newegg.com/Product/ComboDealDetails?ItemList=Combo.4853122  For $430 you can get a 7600x, 16GB of DDR5, and a new mobo. This is a much better buy than a 5700x3d at $400.",buildapc,2026-02-08 17:16:18,1
Intel,o4c0uwc,"The 9800X3D is 433$, that 5700x3d is not worth it.",buildapc,2026-02-08 22:45:56,1
Intel,o4cs5n4,">upgraded to a 9070xt  >On my 2060s I would get around 50 fps on arc raiders and after upgrading I would only get about 60  >I am currently on a 2700x   This means your CPU is whats holding back performance.   To see any noticeable performance improvement, you're gonna need a CPU upgrade 100%.   Whether the 5700X3D is worth $200 more, thats up to you, but I dont think its worth it, when the 5700X or 5800XT will perform massively better than your 2700x.  5700X3D is expensive because its no longer in production, so whatever brand new stock still exists is going to stay expensive as retail supply slowly dwindles.  100% more money for less than 20% more performance is not great.",buildapc,2026-02-09 01:22:33,1
Intel,o47z7g8,Make sure not to buy the non-x version. That one is only pcie gen 3.,buildapc,2026-02-08 08:17:12,0
Intel,o487o3s,Yes actually .,buildapc,2026-02-08 09:37:47,0
Intel,o49rwue,5600x it is in %1 difference at max against 5700x in gaming workloads. If you doing multicore professional works it changes the whole context but that is it for a sole gaming oriented pc 5700x never worths over 5600x if they're in a real similar pricing.,buildapc,2026-02-08 16:09:03,0
Intel,o488dmh,"no. just go watch comparisons. for example. in iceberg's video, 7500F is fast enough to keep up with 57X3D.  Just go with AM5 like 7600 or 9600 and you will have better time overall, since it will be better at everything all around.",buildapc,2026-02-08 09:44:28,-1
Intel,o47znzp,"Key Differences Gaming Performance: The 5700X3D provides higher, more consistent frame rates, particularly in CPU-heavy games (e.g., Cyberpunk 2077, Starfield).  Cache: The 5700X3D features 96MB of L3 cache, compared to 32MB in the 5700X.  Clock Speeds: The 5700X3D has lower base (3.0 GHz) and boost (4.1 GHz) clocks than the 5700X (3.4 GHz base / 4.6 GHz boost).  Power/Thermal: The 5700X3D is a 105W TDP chip, requiring better cooling, whereas the 5700X is a more efficient 65W chip.   Which to Buy?  Choose the 5700X3D if: You are gaming, particularly at 1080p or 1440p, or using a high-end GPU, and want to extend the life of your AM4 platform.  Choose the 5700X if: You are on a strict budget, doing more productivity work than gaming, or want a lower-power, cooler system.   For many, the ~30% gaming performance increase justifies the higher price of the 5700X3D.",buildapc,2026-02-08 08:21:33,-6
Intel,o48lk74,yeah honestly that 10fps uplift seems sus... could be a driver issue or something else limiting the 9070 XT tbh.,buildapc,2026-02-08 11:47:18,3
Intel,o48bbl1,The 2700x is a massive bottleneck,buildapc,2026-02-08 10:12:15,6
Intel,o486tsw,"It's only faster if the application can take advantage of the 3d vcache. Not all games do. Even then, it's not worth double .",buildapc,2026-02-08 09:29:43,7
Intel,o495j4n,"Fellow X3D owner here, I concur  I only got it when they had a brief sale that put the 5700X3D around $200",buildapc,2026-02-08 14:09:20,1
Intel,o47znb5,That only applies to the 5700G. The 5700 non X is PCI Gen 4.   Edit: I was wrong.,buildapc,2026-02-08 08:21:22,2
Intel,o49vp47,"After using the new card a while it feels much more jittery than with the 2060s. The frames dont seem much better and the gameplay feels worse. I went through DDU, tested the gpuz, and checked ram speeds, etc and still its quite bad. I just cant figure it out, i bought a 5700x and am just waiting to see if thats the issue",buildapc,2026-02-08 16:27:41,1
Intel,o48roxh,It really isn't lol,buildapc,2026-02-08 12:37:58,-1
Intel,o480em6,"https://www.techpowerup.com/cpu-specs/ryzen-7-5700.c3305  This is the one part that I messed up in my build in august. Mono + nvme were gen4, but didn't yield proper speeds. Cezanne Vs Vermeer.",buildapc,2026-02-08 08:28:33,2
Intel,o4bhkj8,"You're capped by the CPU, that's what causes it to be jittery. The 5700x should turn 60fps into 90-120fps. For now though you can just keep cranking up the graphic settings and it won't slow the FPS down.",buildapc,2026-02-08 21:07:04,2
Intel,o4bh2pj,It really is. Upgrading from 2700x to 5700x will turn 60fps into 90-120fps in most games.,buildapc,2026-02-08 21:04:38,2
Intel,o480yj5,Holy shit you're right. Sweet fuck AMD did us dirty with that.,buildapc,2026-02-08 08:33:50,2
Intel,o48e1ma,"Yeah... They have their own shitty moves. 5800x was like 30$ more expensive, so I would have definitely bought that first if I knew it.",buildapc,2026-02-08 10:37:57,2
Intel,o3yn6fd,"Simple savings are available here. Replace the 7600X with a 7600 and use a PCIe 4 drive instead of a PCIe 5 SSD. Look for 6000 CL30 or less expensive 5600 kits instead of the slow and expensive DDR5 5600 CL46. Without sacrificing any genuine 1080p performance, you can cut 150.",buildapc,2026-02-06 20:13:42,14
Intel,o3ymh9p,"That looks pretty good!  I would say you don't need a PCI-E 5.0 drive and a 4.0 would be fine, but you might not be able to find a better deal than what you have there.",buildapc,2026-02-06 20:10:13,6
Intel,o3yunxz,Buy a used video card if you can.  That amount can get you much better on the used market.,buildapc,2026-02-06 20:51:20,3
Intel,o3ymg2a,"Nope you pretty much did it, best price to performance really. Only way to save is going used, could save a few bucks on motherboard and GPU maybe but going new is always nice",buildapc,2026-02-06 20:10:03,2
Intel,o3yrshb,There are often deals on the B580 for $250 if you’re patient,buildapc,2026-02-06 20:36:47,2
Intel,o3z6a95,"If you're near a microcenter, one of their prebuilts might be able to do better with current hardware prices. At a glance I see this with similar specs (a bit worse video card) for $700:  [https://www.microcenter.com/product/704075/powerspec-g529-gaming-pc](https://www.microcenter.com/product/704075/powerspec-g529-gaming-pc)",buildapc,2026-02-06 21:49:01,2
Intel,o3zi95m,Have you considered getting a 2017+ used office mid tower like a Dell optiplex for around 250 bucks and just throwing a 9070XT in there for the same money? You'll get more pixels and frames.,buildapc,2026-02-06 22:50:53,1
Intel,o3zknp5,swap that pcie 5.0 ssd for pcie 4.0,buildapc,2026-02-06 23:04:03,1
Intel,o41m3jm,"I don't know much about PCs and looking to either build or purchase one soon, from what I understand you NEED to have a GPU though, can someone explain why wasn't included or maybe I'm missing it?",buildapc,2026-02-07 07:19:24,1
Intel,o41thpf,You could look at some of the Newegg combos to save money on something like a motherboard and ram combo.,buildapc,2026-02-07 08:29:42,1
Intel,o4248tx,"7600x is not a good choice. [225f](https://www.cpubenchmark.net/compare/6457vs5033/Intel-Core-Ultra-5-225F-vs-AMD-Ryzen-5-7600X) is cheaper, faster, newer and comes with a cooler so you don't have to waste that $17.89",buildapc,2026-02-07 10:15:49,1
Intel,o3yriak,Could also save on an A620 motherboard. And since when what the Q300L $60?? It used to always be $40…  I was about to suggest a 650w psu but $62 is already dirt cheap so I’d stick with what he has.,buildapc,2026-02-06 20:35:21,3
Intel,o43kvte,"UserBenchmark is the subject of concerns over the accuracy and integrity of their benchmark and review process.  Their findings do not typically match those of known reputable and trustworthy sources.  As always, please ensure you verify the information you read online before drawing conclusions or making purchases.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2026-02-07 16:07:35,3
Intel,o43mlbe,"CPU - R5 5600/x or 5700x    GPU - Best you can afford, likely a 5070/9070   Everything else is fine, no need to upgrade",buildapc,2026-02-07 16:15:59,2
Intel,o43mqys,"I’d upgrade the CPU 5600/5600X/5700X/5800XT depending your price and budget. Make sure you update to a compatible bios before swapping them.   Then I’d buy the best GPU you’re comfortable spending on, the 9060XT 16GB is a good starting point.   What power supply do you have?",buildapc,2026-02-07 16:16:45,2
Intel,o43qifm,"eyo I am on an ryzen 5 5600 and an RX 6700XT. I play on 1440p, with most stuff on ultra/high and i get about 80fps on like BF6.  tbh, I would invest into an GPU and try to overclock the CPU. If thats not enough for you, you might want to try to get a used 5600, that wont be very expensive and a good upgrade.  If performance works fine for you without a new CPU, I would suggest to then just wait till AM6 and by then just upgrade completely to a new platform.",buildapc,2026-02-07 16:35:12,2
Intel,o43l22m,"I am aware, thanks bot. I used it mostly to post my specs and get a general idea of my computers performance :)",buildapc,2026-02-07 16:08:25,1
Intel,o43te8w,"Thank you, I'll give those CPUs a look, since I didn't really consider upgrading it before.",buildapc,2026-02-07 16:49:22,1
Intel,o43t6s2,"Thank you!  My PSU is the ""550 Watt Seasonic Focus GX""",buildapc,2026-02-07 16:48:22,1
Intel,o43th9e,"Overclocking is an idea. i do have a AIO watercooler on it, so i think i could get a bit more out of it!",buildapc,2026-02-07 16:49:47,1
Intel,o43zfit,"I'm a bit late to the party, but I highly recommend you visit [passmark.com](http://passmark.com), at the top there's a ""Benchmark"" button of which allows you to see different component benchmarks. In your case you'd only need the cpu and video card (gpu) options, then on the left row there's a way to compare up to 5 parts between each other. Hope this helps!",buildapc,2026-02-07 17:19:07,2
Intel,o44fyky,The 3600 is showing its age   The 5600 is a cheap upgrade that'll more consistently get you 60 FPS in modern AAA,buildapc,2026-02-07 18:40:37,2
Intel,o43vprn,I’d look at getting a 750W power supply.,buildapc,2026-02-07 17:00:43,2
Intel,o43zzn1,"Appreciate the input, I'll have a look! Thank you :)",buildapc,2026-02-07 17:21:52,2
Intel,o44nqqn,cheap upgrade sounds good. I should probably grab the 5600 then and then see whats left for my GPU,buildapc,2026-02-07 19:19:28,1
Intel,o43w17c,"This might be a stupid question but here goes:  Only if I get another GPU, or even with a new CPU?",buildapc,2026-02-07 17:02:16,2
Intel,o43wjc5,If you get another GPU.,buildapc,2026-02-07 17:04:44,2
Intel,o4401nf,"Thanks, I'll have a look at my options.",buildapc,2026-02-07 17:22:09,2
Intel,o442nh7,"You bet, good luck with the upgrade.",buildapc,2026-02-07 17:35:05,1
Intel,o44a94b,Thank you! :),buildapc,2026-02-07 18:12:38,2
Intel,nyxkxhx,"Intel GPUs have come a long way, and part of that journey was from A to B. The current lineup are th B series and are regarded as significantly better than the A series. I recommend B580 GPUs for gamers on a budget. It's a good card, and most of the a series issues are gone. Drivers work, upscaling happens, compatability is good.  tldr: Current Intel GPUs are fine, just avoid the A-series",buildapc,2026-01-11 06:49:31,8
Intel,nyxlbjn,"A580? Nah not that much, the RX 6600 and RTX 3050 are the same price and generally either have better better performance or better tech, both have better optimization, the Arc B580 though, absolutely as it has good drivers and is on the same level as a 4060 for a lower price",buildapc,2026-01-11 06:52:49,7
Intel,nyxou31,Look up the model on Gamer's Nexus. They seem to like them for the price point and pointed out that updates have fixed a lot of initial issues.,buildapc,2026-01-11 07:23:19,2
Intel,nyz9r5o,so will b570 be fine?,buildapc,2026-01-11 14:53:47,1
Intel,nyyoj98,"I wonder why no one ever recommends the B570. Sure, it might be less powerful than the B580, but I suppose that has to work better for certain CPUs that aren't that powerful",buildapc,2026-01-11 12:43:16,2
Intel,nz0htle,"Sure. It's about 10-15% slower than the B580, so take that into account when doing ""fps/$"" calculations in your head.  https://www.techpowerup.com/review/asrock-arc-b570-challenger/32.html",buildapc,2026-01-11 18:24:42,2
Intel,nz1l8yi,oh i deadass forgot that gpu existed ngl,buildapc,2026-01-11 21:22:02,1
Intel,o3htw8m,"You just gotta look up reveiws that include color accuracy. For example Rtings does them and have a decent library or reviews, including the 2715H which actually didn't score that well in color accuracy, the reviewer noting warmer than ideal color temperature.  Gaming monitors can be very accurate, if the manufacturers bother to calibrate them. That's honestly why I wouldn't ever buy a monitor without reading an in-depth review first, just reading the specifications isn't nearly enough.   Since Dell makes dedicated gaming monitors as Alienware, I doubt the Ultrasharp line will have a satisfactory response time for 120hz, but again, you'd have to see a review first.",buildapc,2026-02-04 07:56:29,1
Intel,o3k4itn,If you want color accuracy then get a colorimeter and calibrate your monitor.,buildapc,2026-02-04 16:54:25,1
Intel,o3li5uz,">I'm bothered by the fake, vomity, blue-ish color so much  Have you tried using the OSD to adjust the monitor to your liking?  You can change the color temp to something warmer and you're basically all set.  You can improve color accuracy by using a custom calibration that you can either find online, or calibrate it yourself with a good colorimeter (but these are expensive).  The best monitors for color accuracy are going to basically any OLED display at this point. But they will need even some minor fiddling in the OSD to get the best out of them since most will default to some ""gaming"" or ""vivid"" preset out of the box. Thats how my PG27AQWP-W came out as. Had to go into the OSD to change from Gaming to TrueBlack500 to get better overall accurate colors and better overall looking HDR.",buildapc,2026-02-04 20:43:58,1
Intel,o3rk439,"It all depends on what your usage and budget will be. OLEDs are a very good choice and they offer now more and more protection against burn-in. As for reading fatigue, not everyone is affected the same way. If your main usage is gaming, OLED are tough to beat but if you are doing 50/50 gaming/work, then reading fatigue can be a factor. The ASUS ROG Swift OLED PG27UCDM and the Dell Alienware AW2725Q are both good OLED choices in a mix usage. Or you can go with the Acer Nitro XV275K P5biipruzx for an non-OLED monitor. You can always change the settings in the OSD to how the monitors looks the best to you, if you don't have access to a calibration tool. Like others are saying, sometimes just changing the Color Temperature in the OSD makes a big difference.",buildapc,2026-02-05 18:55:47,1
Intel,o3rp7e1,"I thought OSD settings was a half-assed way to adjust the colors? Am I wrong? I figured the manufacturer would optimize the default color settings and any adjustments after would be cope.  I'm also not sure about 4K. I'm barely getting 120fps with DLSS & FrameGen at 1440p, so any larger would negate the benefits unless I upgrade (which is a hard ask today). Windows DPI scaling is also a toss-up even today. I heard 4K is better for text though. Is 4K a must?  (edit: My local store has a ROG XG27AQWMG if that's worth anything)",buildapc,2026-02-05 19:19:12,1
Intel,o38ukqd,"Drop a 5800XT in it and see what it does, if thats not enough, grab a 5070 (or 5060ti) and you should be good.",buildapc,2026-02-02 23:14:22,8
Intel,o38tvj0,Best option is probably a deal on a pre-built unless you live near a Microcenter and can grab a CPU/Mobo/RAM bundle.  Do you have a budget?,buildapc,2026-02-02 23:10:37,4
Intel,o38y50a,pick up a used 5700x and a 9060xt / 5060ti.   Don't go wasting money on a 5080.      And chill.... you don't NEED 144hz to survive.,buildapc,2026-02-02 23:33:51,4
Intel,o38tz9i,"You should upgrade your CPU & GPU, what is your country and maximum budget for an upgrade? What is your budget for a new build?",buildapc,2026-02-02 23:11:09,1
Intel,o38u38t,will your motherboard support a 5700X and DDR4 3200 memory (if that's not what you already have).    If so you'd be fine to get a new cpu and and GPU (provided your PSU can handle the load.... 50 series cards are thirsty.   If your MB is going to have issued (doesn't support pcie gen4x16 then I'd think about a whole new system...  But RAM cost is HUGE!!!!  so maybe upgrading everything around your existing DDR4 ram is a better option?,buildapc,2026-02-02 23:11:45,1
Intel,o38v89c,"I would say upgrade the cpu since that game is cpu intensive. Your gpu is fine honestly  5700x/5800x/5800xt are all fine options, just choose based on what the pricing is in your area",buildapc,2026-02-02 23:17:54,1
Intel,o38vorc,"Your first question to yourself should really be ""what is my budget?""  Otherwise, we'd all be scrapping our rigs and building new ones every 3-6 months",buildapc,2026-02-02 23:20:24,1
Intel,o38wnxc,First question is - what is your budget .? 5700x CPU + 9060xt GPU or 9070 non xt . Very good performance increase,buildapc,2026-02-02 23:25:43,1
Intel,o38xjdb,"Either upgrade to a 5800x/xt and grab a new GPU, grab a microcenter or newegg bundle, or find a prebuilt",buildapc,2026-02-02 23:30:30,1
Intel,o3903s8,[This video](https://youtu.be/oF3gLKmYqpE?si=If70o-_RamZOxXJS) suggests the upgraded GPU will have the desired effect even if there is some performance left on the table. This might now be a bad thing as when the (if!) price of RAM comes back down I can upgrade the rest and keep the GPU.,buildapc,2026-02-02 23:44:51,1
Intel,o38tsho,5700x3d/5800x/5800x3d would be a good upgrade,buildapc,2026-02-02 23:10:11,-1
Intel,o390dvc,"That's the CPU I have. I upgraded to it at the beginning of last year. My system is a 5800XT paired with a 5070 TI and 32gb of DDR4. It does everything that I want it it, in the resolution and fps that I want. Except for Tarkov. It looks great, but without Smooth Motion, I'm only getting 70-100 fps with maxed out settings on most maps.  I actually just recently upgraded to the 5070 TI, from a 3080. Nice jump. I think later this year, I am going to upgrade to AM5 and I really want an X3D chip, so probably the new 9850X3D. I'm also going to shoot for 64gb of DDR5, unless it's still $1k. Lol!",buildapc,2026-02-02 23:46:25,1
Intel,o391af2,"That would mean spending £200ish on the CPU, and then £1000 On the GPU and a couple hundred on the PSU. A new 5080 pre-built is around £2k.",buildapc,2026-02-02 23:51:20,0
Intel,o391l3z,UK. We have very few options for walk in PC component shopping.      Around £1000.,buildapc,2026-02-02 23:52:57,2
Intel,o39zxhb,r/usdefaultism,buildapc,2026-02-03 03:05:59,1
Intel,o3932em,No but if my 3060 isn't hitting that at 1080p what hope does it have for 1440?,buildapc,2026-02-03 00:00:56,2
Intel,o3922yf,"If the solution is a complete build I'll hold fire while I save as I don't want to buy a new machine that's less capable than I'd like.      My first PC was a budget option and I've been duct taping new components onto that ever since. Next time I want to get the top of the range out the gate.      So basically, if I can't cheaply duct tape another cheap part to this old workhorse then my budget is limited only by my patience.",buildapc,2026-02-02 23:55:37,1
Intel,o392dgj,I'm fairly certain my motherboard will take the CPU as the website says it works with any AM4 chip. Is there a possibility the marketing is being sneaky?,buildapc,2026-02-02 23:57:14,1
Intel,o392lhv,Just under £200 from Amazon. It's very edible at that price.,buildapc,2026-02-02 23:58:24,1
Intel,o38uouz,x3ds are gone for AM4 unless you wanna get taxed hard for used,buildapc,2026-02-02 23:14:59,4
Intel,o38v1bf,"x3d's are out of stock, only ones left are heavily upcharged.",buildapc,2026-02-02 23:16:51,3
Intel,o399nvv,ah yeah a prebuilt is the way then. Man are they really charging 1000 for a 5070 there? for reference i run a 12900k and a 4070 super on 650w with room to spare. My other PC is an 11700k w/ a 3060ti and thats on a 500w. The 5070 wouldn't need more than a 650w with a 5080xt cpu.,buildapc,2026-02-03 00:36:51,3
Intel,o3a2m8a,"Yeah get the prebuilt and sell your current PC, you should be able to get $500 for it",buildapc,2026-02-03 03:22:01,1
Intel,o393v8d,How much ram do you have in your current build?,buildapc,2026-02-03 00:05:19,1
Intel,o3a47cc,It is what it is,buildapc,2026-02-03 03:31:36,1
Intel,o395a0f,"That’s why I suggested a 5700x too.   As you move up the resolutions, the GPU has less of an impact on performance jumps.",buildapc,2026-02-03 00:13:03,-4
Intel,o39746j,"You can cheaply duck tape it like, for sure! Although, I think the middle ground between cheap upgrades and a New Build is the smart choice.     Upgrading your CPU to a 5600(x)/5700x/5800x/5700x3D would get your better performance on your current hardware. The 5600 is £90 right now.     Upgrading your GPU to support your new 1440p monitor with something like a 9070 XT and a new 850w PSU(unless you have a recent/decent psu). You can also get a cheaper GPU, the 9060 XT 16gb/5060 Ti 16gb are way faster than the 3060.      Doing these specific upgrades would allow you to take your GPU and PSU to your next build. With the GPU+PSU already purchased, you would just need a Case, storage, CPU/MoBo/RAM.    Buying the CPU/GPU/PSU now will get you the best performance for the least amount of money. I strongly recommend you do this, then plan to upgrade the CPU/MoBo/RAM within 2 years.    [PCPartPicker Part List](https://uk.pcpartpicker.com/list/k4bwt3)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 5600 3.5 GHz 6-Core Processor](https://uk.pcpartpicker.com/product/PgcG3C/amd-ryzen-5-5600-36-ghz-6-core-processor-100-100000927box) | £91.95 @ AWD-IT  **Video Card** | [PowerColor Reaper Radeon RX 9070 XT 16 GB Video Card](https://uk.pcpartpicker.com/product/8ZJBD3/powercolor-reaper-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-16g-a) | £599.99 @ Overclockers.co.uk  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://uk.pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | £85.47 @ Scan   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£777.41**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2026-02-03 00:02 GMT+0000 |",buildapc,2026-02-03 00:22:59,2
Intel,o393s94,"no, just checking some really old or limited old boards won't do the newest am4 cpu's",buildapc,2026-02-03 00:04:52,1
Intel,o38wxgp,What is the difference with X3D?,buildapc,2026-02-02 23:27:11,1
Intel,o396jyn,£400+ here,buildapc,2026-02-03 00:19:57,1
Intel,o3b5mj6,32,buildapc,2026-02-03 08:24:13,1
Intel,o3b66cd,Perhaps naively I'm drawn to nVidia.,buildapc,2026-02-03 08:29:37,1
Intel,o38xtoc,"tldr: Much faster for gaming, espcially cpu intensive games  They have extra 3dvcache so the cpu can run more efficiently, which translates to faster performance for gaming. Just see where the x3d's rank in terms of gaming.  https://www.tomshardware.com/reviews/cpu-hierarchy,4312-2.html'  Heres a specific benchmark for 5700x3d in arc raiders. Vastly improved 1% lows, 1% lows is the lowest fps you will see. Thats when theres alot going on in game and your performance stutters, so having better 1% lows will prevent that from happening   [https://www.youtube.com/watch?v=uWzk6J09n\_A](https://www.youtube.com/watch?v=uWzk6J09n_A)",buildapc,2026-02-02 23:32:06,3
Intel,o3cvxw2,"Yeah, you could get away with a 5700x3d or 5800x3d. Sell your 3800x. Then as long as your PSU is up for it, throw a 5070ti or 5080 in there.  I was rocking a 5700x3d with a 4080 super until just recently. I only upgraded to a 9800x3d setup because I got a killer deal on the CPU/Mobo/Ram combo. Performance doesn’t really feel much different at 1440p/UHD.  I’d check FB marketplace for the CPU upgrade.",buildapc,2026-02-03 15:40:31,1
Intel,o3d9vp9,My monitor arrived today. I'm gonna hook that up and see how I get on.      There's a place selling the Ryzen 7 5800XT for under £200 so if I feel I need some more oomph I'll try that first. Gives me a base to upgrade more if I need to and hopefully weather the RAM storm.,buildapc,2026-02-03 16:45:56,1
Intel,o3db07w,32GB is enough ram for now. Use browser extensions and browser settings to unload unused tabs automatically and save used memory.,buildapc,2026-02-03 16:51:07,1
Intel,o3g9yog,"So I'm still hitting a high enough frame rate, really. My CPU isn't even at 50%. Does this mean a higher end GPU would be fine with this chip? Or just that increasing the CPU alone isn't going to give me much of a benefit?    Also, OLED is crazy.",buildapc,2026-02-04 01:33:17,1
Intel,o3gl4sw,"You’ll feel a difference between a 3800x and a 57/800x3d. However most of that will in the form of smoothing out 1% lows.  So one of the things I try to do with my upgrades is consider current pricing vs what I can get for my existing components. There is a bit of an art form in reselling your old components before they hit obsolescence.  This is probably the strongest argument for a 9800/9850x3d upgrade. Since your current rig is still desirable, you’ll get significantly more for it now compared to when AMD launches the next gen after the 9K series.  Recommendations are also game dependent. For instance, if you mainly only play Blizzard games, an upgrade won’t really feel much different than what you have now. In my case, I play all kinds of games, some demanding some not. I’m also on a 32:9 ultrawide. Some games I play on half screen (shout out to Borderless Gaming app on Steam), some I play at full 32:9 like BF6.  I guess what I’m saying is that this is a bit of a complicated decision with multiple variables that weigh differently for each person. I hope my yammering helps you to get a complete picture and allow you to make a better informed decision.",buildapc,2026-02-04 02:36:30,2
Intel,o2zepd7,6600XT is still good for 1080p and perfectly matched with your CPU and PSU. For $400 you will not find anything significantly better (new). For used market it is hard to suggest because prices vary widely. I suggest  to save money for new build while using the existing system.,buildapc,2026-02-01 15:26:53,2
Intel,o2zaife,budget? PSU?,buildapc,2026-02-01 15:06:06,1
Intel,o2zh8lj,"Thought about that too, but playing arc is not enjoyable at all",buildapc,2026-02-01 15:39:12,1
Intel,o2zbelq,"Budget around 400, but im buying used. Psu 650",buildapc,2026-02-01 15:10:37,1
Intel,o2i3pxy,What an adventure!! Enjoy your new setup 👏👏👏,buildapc,2026-01-29 23:15:34,8
Intel,o2iaqwv,I did the same thing. A couple of years ago I finally said to hell with consoles and I built a PC. I splurged and got the 4090 and holy crap that’s the most amazing decision ever.,buildapc,2026-01-29 23:53:24,4
Intel,o2iebxr,Look at an x3d. That’s my two cents. Looking good,buildapc,2026-01-30 00:13:06,4
Intel,o2igwhz,Grats on 30. It gets worse.,buildapc,2026-01-30 00:26:43,4
Intel,o2i7t8i,I love that first line of closing thoughts and how PC gaming is a hobby in itself. I remember months ago a friend was talking mess saying you don’t have money to blow cause I’m buying expensive ass PC parts while we were shopping at the mall and looking at gold chains. To me I was like yea it’s a hobby there’s nothing wrong with paying a lot of money for a hobby you love and enjoy. I feel like it hit him pretty hard with the realization that it can be a hobby cause he didn’t say anything back but he had this look and energy of “oh shit that’s true.” To him lol,buildapc,2026-01-29 23:37:33,3
Intel,o2ib96i,"Sounds great, happy for you. Gathering up parts (on sale, open box or used) is so much more fun and better value for the money. 5070 is a good choice for your set up. Enjoy your new gaming pc.",buildapc,2026-01-29 23:56:11,2
Intel,o2ihbz4,Congrats! Happy gaming!,buildapc,2026-01-30 00:29:01,2
Intel,o2ins34,"Sounds like you really had fun with, enjoy your rig!",buildapc,2026-01-30 01:03:27,2
Intel,o2iq1l2,"Grats on the new setup, also go ahead and get project lasso and tune your games to behave better across the cores and the right speed plus priority and you'll see better stability from your system :)",buildapc,2026-01-30 01:16:01,2
Intel,o2itjn1,"I was in that phase too. At some point I noticed I would rather game more than tinker more, never overclocked anything again and just was happy with every game running as it happens to run.n  It's a lot easier with VRR nowadays.",buildapc,2026-01-30 01:35:39,2
Intel,o2k5uoe,Can't find it under 350€ used...,buildapc,2026-01-30 06:40:05,2
Intel,o2im9qm,Def more of a hobby than buying gold chains. Lol.,buildapc,2026-01-30 00:55:17,2
Intel,o2k5sr8,Bro that's smart af!,buildapc,2026-01-30 06:39:39,2
Intel,o2ts7z4,Do not buy a high end cpu but garbage gpu. The other way is better. https://pcpartpicker.com/list/wrMq6Q The 5060 8gb is utter garbage. Do not buy it. 9060xt 16gb or 5060ti 16gb MINIMUM.,buildapc,2026-01-31 18:02:41,2
Intel,o2ub6jd,"If youre flexible about turning the graphics down and on a budget like you say, then you do not need a 16 GB card for anything. I run my games mostly maxed out with a RX 7600 8GB at 1080p which is comparable to that card. Dont let people convince you to waste money. Being willing to turn the graphics down a bit is the best way to save money doing this, youre doing it right.",buildapc,2026-01-31 19:32:01,1
Intel,o2v0a7q,"for that price please look on fb marketplace, ive been seeing some good deals in that price range with 5070’s and 7800x3d’s",buildapc,2026-01-31 21:35:29,1
Intel,o2v519i,"Honestly that’s a super solid starting list. A 7800X3D + 32GB RAM + 2TB SSD is basically a dream setup for gaming.  Only thing I’d say is the RTX 5060 might be the weak spot for a $1500–$1700 budget. If you can bump that up to something like a 4070 Super / 5070, you’ll get way more value and it’ll last longer.  For the rest, you don’t need to overthink it too much — just grab a decent B650 board, a good 750W gold PSU, and a solid air cooler and you’re set.  This build would crush Siege/CS2 and handle Elden Ring easily.",buildapc,2026-01-31 21:58:39,1
Intel,o2v93oo,If you are playing @1080p i think the build makes sense but id try to get the 16gb version if possible,buildapc,2026-01-31 22:19:03,1
Intel,o2vr62m,"I JUST built a $1600 PC earlier last month, and here were the specs:  MoBo: Asrock B850M Steel Legend  CPU: Ryzen 7600X3D  RAM: Klevv Bolt V 32GB DDR5  SSD: Crucial P310 1TB  GPU: Powercolor 9060 XT 16GB Hellhound  PSU: Thermaltake GT850W   Case: Montech King 15 Pro  Fans: Thermalright TL-M12Q  AIO: Thermalright FW360   Hope this helps as some kind of reference :)",buildapc,2026-01-31 23:56:36,1
Intel,o2wiy4j,"I mean at that price range you would honestly have an easier time finding a prebuilt with those specs for cheaper/the same   $1,599 5060Ti https://www.newegg.com/icewolf-gaming-desktop-pcs-geforce-rtx-5060ti-amd-ryzen-7-7800x3d-32gb-ddr5-1tb-nvme-ssd-black/p/3D5-0074-00017   $1,630 9060 XT https://www.newegg.com/hoengager-gaming-desktop-pcs-amd-radeon-rx-9060-xt-amd-ryzen-7-7800x3d-32gb-ddr5-1tb-ssd-black/p/3D5-003E-00UF3   Or for +150 past your budget for a 5070 build with your other specs   $1,850 5070 https://www.newegg.com/p/3D5-001U-001S4",buildapc,2026-02-01 02:38:53,1
Intel,o355m1d,Bestbuy has a good selection of prebuilt PCs for your budget,buildapc,2026-02-02 12:13:22,1
Intel,o3jnboj,"I’m citing Newegg and AMD posted MSRP. If you’re finding limited parts for less than AMD MSRP during a shortage, I’d half expect you to receive a 6gb rando card. I’ll stand on my argument and sources (ya know, the ones people use worry free) best of luck shopping Aliexpress",buildapc,2026-02-04 15:35:21,1
Intel,o2ubko7,That card is definitely not garbage. This person says they're willing to turn the graphics down... why would they need 16 GB minimum? I run modern games with a card similar to the 5060 mostly on high some maxed out with no issues at all.,buildapc,2026-01-31 19:33:54,1
Intel,o3eogey,"8Gb VRAM is industry standard from 2020, next few years it will go up to 12-16. VRAM is more important than Ram. 8gb card will be obsolete in 2-3 years. 16GB card are future proof up to at least 5years",buildapc,2026-02-03 20:39:10,1
Intel,o3eqeti,"Im not sure what your idea of obsolete is. Right now I play modern games with a Radeon HD 7900 3GB card that was manufactured around 15 years ago. It handles many games on high settings without issue. I just ordered an 8GB card as an upgrade a few weeks ago and plan to use it for at least 5 years. Thats plenty future proof for me. Running every new game that comes out on max settings is not an industry standard, most people are on too tight a budget to keep up with that.",buildapc,2026-02-03 20:48:13,1
Intel,o3et6cp,"1080p or 1440p firstly? Also if you are running Brand new games - Space marine 2/ Total war/ Battlefield. A lot of those games cannot reach 30/60Fps on these systems. Some quite literally are unplayable. My best guideline is Steam requirements, 8gb is standard, up from 6gb circa 2016. I’m not saying this with absolution, I’m merely stating that the GC will ultimately become obsolete before a 16GB card does. If you were paying double to go from 8-16GB, I can see more of the reasoning there.",buildapc,2026-02-03 21:00:55,1
Intel,o3etboh,"But if you’re buying a new pc to low settings as your benchmark, by all means.",buildapc,2026-02-03 21:01:35,1
Intel,o3ew0g3,"You shouldn't need to ask if I'm gaming on 1080p or 1440p with this card. The answer to that is very obvious.   Space Marine 2's recommended GPU is 8 GB VRAM, AMD Radeon RX 6800 XT / Nvidia GeForce RTX 3070.  Total War Warhammer 3's recommended GPU is Nvidia GeForce GTX 1660 Ti/AMD RX 5600-XT/Intel Arc A750  Battlefield's recommended GPU is Nvidia RTX 3060Ti, AMD Radeon RX 6700-XT, Intel Arc B580  Battlefield is the only one of the 3 that recommends more than 8GB of VRAM, it recommends 12... I'm not sure how this is giving you the impression that 8GB cards are going to be obsolete any time soon or that these games are unplayable on a system like this.",buildapc,2026-02-03 21:14:00,1
Intel,o3f98sk,"Current gen game comes out with higher system requirements. Do you assume next gen game will be same graphics/design. Your system according to STEAM AND BATTLEFIELD (Frostbite I think) have just made your card obsolete. Can you still play it at 1080 on low at 30fps, sure. When/if you have issues with running the game then report it to battlefield, what do you think their first question will be? Now just imagine moving forwards how that will look.",buildapc,2026-02-03 22:15:31,1
Intel,o3f9uv8,"Just want to point out as well, 9060xt 8Gb is $270-300, 16GB model $350-379, if OP is asking about build advise, I’d always recommend to get the 16",buildapc,2026-02-03 22:18:31,1
Intel,o3faguq,"The reason I mentioned the old school card im still using is that its just now becoming obsolete in the last few years after using it for over a decade. Battlefield is a great example of this. This is why im upgrading to 8GB. If you say 8GB is a standard from 5 years ago, then why are so many games coming out right now recommending 8GB? Its like youre confusing recommended requirements for minimum requirements and assuming that maxing everything out in Battlefield is some type of industry wide standard, its not.",buildapc,2026-02-03 22:21:30,1
Intel,o3fd1g8,Where are you seeing this card for $350? I'll return what I ordered and go for that if I can.,buildapc,2026-02-03 22:34:26,1
Intel,o3fnnir,"Apologies, I bought mine a few months ago, listed msrp is the price referenced and what I paid at the time Luckilly, I double checked and both sets now go for 379 8gb and 419 16Gb sale price. At the end of the day. I’d argue for OP to spend the extra $50-60 for the better card. Motherboard you can skimp out on, power supply just prefer gold standard, and I’d even argue against Corsair ram sticks, may save a bit going Kingston/Gskill, you just won’t get little ship on them. It is kinda intrinsic that a 8GB card will eventually become obsolete. Again not meaning anything by obsolete in a negative way, but as games come out, OP may want to play a game that simply can’t be handled with 8gb. Elden ring also decently demanding",buildapc,2026-02-03 23:30:04,1
Intel,o2hrrbg,Your best bet is buying a prebuild (OEMs get parts cheaper). But even then the prices still suck and you'd be getting significantly less than you would have just a month ago,buildapc,2026-01-29 22:14:36,184
Intel,o2hrp5e,"It is a bad time to build 100%  DDR5 ram prices have quadrupled- the GenAI bubble is gobbling up all supply   DDR4 ram prices have tripled - demand is high since DDR5 is expensive   SSD prices have doubled - the GenAI bubble is gobbling up supply   GPU prices have increased - vram costs more due to the GenAI bubble, and they are literally making less gaming GPUs because they make more money making AI GPUs  Unless the bubble pops sooner than expected, many speculate prices will not start to drop until fall this year",buildapc,2026-01-29 22:14:18,813
Intel,o2hrv97,"I suppose it depends on your scope.   Is it a good time to build compared to a year ago? FUCK. NO.   Is it a good time to build compared to 6 months from now? Who knows. Prices could go down. More likely they'll hold steady or go up.   Ultimately you have to choose when to pull the trigger, and accept in-advance that there may be a more optimal time to buy in the future.",buildapc,2026-01-29 22:15:08,51
Intel,o2hrbsc,"not really, RAM and storage are both hugely inflated. Not sure if there's an end in sight, though.",buildapc,2026-01-29 22:12:28,77
Intel,o2ht22t,Right now is the worst time in recent history to build a new PC. Tomorrow will be even worse though,buildapc,2026-01-29 22:21:00,31
Intel,o2igi9e,It's bad. And it's going to get worse. Prices will probably take years to recover.,buildapc,2026-01-30 00:24:36,8
Intel,o2hvqyh,"I just built my first PC, I bought everything secondhand from FB marketplace except the case. I think it was $350 total, I can play everything (so far) maxed out. It took a lot of running around and some sketchy situations but it was 100% worth it.",buildapc,2026-01-29 22:34:27,12
Intel,o2hrtdb,Lots of FOMO.  But its either now or never.  Nobody can predict prices in a year or two or three.,buildapc,2026-01-29 22:14:53,18
Intel,o2hsjms,"I think right this very moment prebuilts are the best value that they have been in years. I love building PCs and I have been lucky enough to build a new one every couple of years and then passing down my ""old"" one. That's out of the window and now I'm likely going to keep my 2 current machines for the next few years because prices aren't going down anytime soon.     There are some good prebuilt options, you can even buy a used system.",buildapc,2026-01-29 22:18:28,6
Intel,o2hr6gj,"What region shopping in for parts, any chance access to a Micro Center? If can find a good deal for a combo deal with ram, could be worth for sure. Else, maybe a pre-built.",buildapc,2026-01-29 22:11:46,3
Intel,o2hru6c,"I hear ppl say this crisis could last a couple years. And when it’s over prices will drop, but sadly I don’t expect back to the levels we were last year 😔",buildapc,2026-01-29 22:14:59,5
Intel,o2ih969,It's the worst time I've ever encountered in 25 years.,buildapc,2026-01-30 00:28:36,6
Intel,o2ioi0a,"If you wanna over pay, sure. But seriously, fuck AI",buildapc,2026-01-30 01:07:26,3
Intel,o2hrybq,No,buildapc,2026-01-29 22:15:34,2
Intel,o2ht28q,I wanted to upgrade my 5 year old build.. but waited too long,buildapc,2026-01-29 22:21:01,2
Intel,o2hv8cr,"I recently bought a fairly high end PC and my final price tag ended up at roughly $4500.  It's a bad time to be a builder, but I don't think it'll get better anytime soon. The RAMpocalypse has just begun and I expect prices to rise further and I don't think we'll see a normalization of prices for a few years at least.  AI companies have bought out future stock, leaving less for the rest of us. Producers are shifting production to cater to AI companies, leaving even less for the rest of us.  It's bleak now and looking to get bleaker.. Unless the AI bubble unexpectedly pops.  Maybe you can get a decent used one? Many builders are looking to offset the price of their new builds by selling their old ones instead of keeping them as backups.",buildapc,2026-01-29 22:31:51,2
Intel,o2hxp1e,no. But it could get worse,buildapc,2026-01-29 22:44:18,2
Intel,o2ic61u,"It’s hard to say, it depends if the Ai hardware rush is a bubble or not. If it’s not, then the sooner the better to build and upgrade, if it is then waiting would be ideal.   If it isnt a bubble, then I may have built my last PC.",buildapc,2026-01-30 00:01:11,2
Intel,o2if7ne,I hate to say it but right now buying a prebuilt is more cost efficient. A lot of these prebuilts are mass produced and were built before RAM prices went through the roof. Now they can afford to sell them for the regular price without losing any money. It's only a matter of time before they start jacking up the prices of these prebuilts.,buildapc,2026-01-30 00:17:49,2
Intel,o2ihyje,"Same question, different day.",buildapc,2026-01-30 00:32:21,2
Intel,o2ikvlv,"Get costco membership, buy costco prebuilt pc, replace PSU.  Bout the best youre gonna do.",buildapc,2026-01-30 00:47:52,2
Intel,o2ipolw,"Now is one of the worst times to build a PC in a long time. Expensive GPUs, expensive RAM, expensive SSDs, and probably more as well",buildapc,2026-01-30 01:14:00,2
Intel,o2ipqht,I don’t know if there’s ever been a worse time,buildapc,2026-01-30 01:14:17,2
Intel,o2ir34j,"Nope, but doing it anyways!",buildapc,2026-01-30 01:21:48,2
Intel,o2isg2y,I just built mine. All new parts except for the GPU. I belive it was around $650 total. It runs RDR2 at 1440p 60fps which is by far the most demanding game i will ever play so. Fits my needs.,buildapc,2026-01-30 01:29:24,2
Intel,o2ivmhf,"YES.   Before you guys bully me listen to my reasoning:  Prices are only going to go up, a bit because of inflation, a bit before there will be shortages in the supply chain because of the global situation, and a bit because other chip components used for AI will create bottlenecks like it happened with GPUs and RAM.   They made it clear that ""we'll own nothing and be happy"" (DAVOS 2016) according to them. This means that they'll push for cloud gaming, PC leasing, and similar BS. We've seen this happen all over the board in many sectors. Why do you believe this will be ignored?   The ""good"" companies will try to keep up for a while but will be forced to surrender because of prices going out of control.   You thing a global Crysis or recession (as the one we're sailing right into) might bring prices down? WRONG. They'll print so much money to save banks and businesses that inflation will make it impossible to own a personal PC.",buildapc,2026-01-30 01:47:17,2
Intel,o2k5fyu,"Honestly, no. Get a prebuilt or used.",buildapc,2026-01-30 06:36:48,2
Intel,o2kgsml,"It's a horrible time compared to last year, etc, but there's no guarantee even when this ""AI"" bubble bursts that it won't get worse. They really want to move to a cloud-computing ecosystem where the consumer PC market is dead and you just buy some little piece of crap that you then pay them to compute your games/etc because you don't get the privilege of owning a powerful pc anymore. Or even if that doesn't happen, the move towards fully integrated boards where, say, Nvidia just sells a whole ass board with cpu/gpu soldered on and you basically have zero choice on your parts.  Maybe these futures don't come true, but they are certainly what some of the big players WANT to be the future. And if that's the case, then it may be a good time to build a pc because the window to even get to do so could be closing.  So if you do not have a pc, really want/need one, and can afford one, it may very well be worth eating the shitty costs to get it. Yeah it will be expensive but there is no guarantee prices go back down *even if both things I just noted do not happen.*  And the reason prices are skyrocketing is everything being bought up by datacenters, which are currently looking to be used for ""AI"" LLMs but in reality can just be shunted over to cloud-computing, mass surveillance, etc, so again there is no guarantee they don't stop hoarding tech even when the ""AI"" bubble bursts.",buildapc,2026-01-30 08:13:19,2
Intel,o2lvzsx,"No, it really isn't. And the unfortunate reality is it might not be a good time to build a PC ever again. Prices will not go down and the tech oligarchy don't want you do own anything, just rent a dumb PC connected to their data canters so you can slurp down all the AI slop they can cram into you.",buildapc,2026-01-30 14:26:24,2
Intel,o2mh04x,"It's a bad time.  But if this is your first pc, I say just go for it, work with a lower spec if your budget is limited. It's great experience having a pc.",buildapc,2026-01-30 16:04:45,2
Intel,o2htxs7,"Nope. Wait. Buy used to get by, don't get into FOMO. We've been through this before, it'll get better.",buildapc,2026-01-29 22:25:23,2
Intel,o2hsm1n,"It is a terrible time, BUT the next two years may be worse.",buildapc,2026-01-29 22:18:48,1
Intel,o2hss2c,Only if you're doing a slightly older build with used psrts. Even then probs not worth it. Prebuikt is most cost effective now because of everything thats already been said,buildapc,2026-01-29 22:19:37,1
Intel,o2ht65z,"Buying pre-built doesn't feel too bad, as at least the price has you getting mutliple things, but god I could only imagine reaching the point in a build to buy a video card & RAM.  Was looking for a 5090 yesterday and saw Microcenter had a pre-built that had one for roughly 4k, or buy the card itself for 3.9k.",buildapc,2026-01-29 22:21:34,1
Intel,o2hu3lz,"Price fluctuation is certainly an issue and, unless the AI bubble pops soon like others have suggested, there is no guarantee that prices will go down.   One thing you could do is design your ideal rig and get parts incrementally, as budget and sales will allow. Alternatively, you could build a rig that will allow you to run a majority of games you want on low to medium and upgrade as time and budget will allow. My point is, what is your timeline?",buildapc,2026-01-29 22:26:11,1
Intel,o2hwtkb,"Just ended up settling for a pre built for my girlfriend's birthday from Microcenter. Even with the microcenter bundles that save you a ton, I was having trouble building a rig for even around the same price as the prebuilt I got. The prebuilt has a 7800x3d and a 5070 ti. The microcenter bundle with a mobo, 32 GB of ram, and the 7800x3d was $750, and the 5070 ti was around $800 at that point. (They literally just raised the MSRP of that card too) So I was already looking at $1550 without a case, ssd, cooling, power supply, operating system. The prebuilt was $1700 with a 2tb ssd, aio cooler, lian li case, gold rated PSU. So as much as I wanted to build things myself it would have been stupid financially or ended up with a weaker rig at that price point. I could have maybeee gotten close in price if I had gone AM4 instead but I'd rather have the ability to upgrade in the future.",buildapc,2026-01-29 22:39:52,1
Intel,o2hxcje,"The best time to build a pc, is when you need one. Nobody can predict the future, especially when or if prices are going to return to normal.",buildapc,2026-01-29 22:42:32,1
Intel,o2i1xii,"Honestly, from a purely pricing perspective, it is almost never a good time to build your own mid-range or lower PC.  However, after the shortages caused by covid 19 and the crypto-inspired GPU shortage, and the rebound in supplies that followed, we are coming off of probably two to three years of historically low prices for ALL components.  And that is what really makes the memory price increases hurt so much.  And it has affected components that use memory (SSDs and GPUs) as well.  So it really feels like the worst time to build a PC.  Historically, there have been much worse periods in which to build a PC.",buildapc,2026-01-29 23:06:09,1
Intel,o2i6w0h,I thought December 2024 was a bad time to build mine... now I look around and am really glad it did it then. Get a prebuilt and save some money.,buildapc,2026-01-29 23:32:33,1
Intel,o2idmwp,Now is not a good time. It is the worst time. Followed by worse times to come. Which makes now a better time than then.  :(,buildapc,2026-01-30 00:09:20,1
Intel,o2idoo4,If you wait for possible sales or bundles there will be huge discounts to move inventory put for like a 5090 your F’d,buildapc,2026-01-30 00:09:36,1
Intel,o2ietob,"If price is a factor, you might still have a window to actually save money with a prebuilt (since they can buy in bulk for cheaper). But otherwise it’s awful all around, thanks to stupid AI.",buildapc,2026-01-30 00:15:45,1
Intel,o2ifery,"Honestly I would say build it now- prices on Ram, storage, and GPUs is going to be high for the foreseable future.",buildapc,2026-01-30 00:18:51,1
Intel,o2ifqhp,Every time is a good time to build a PC,buildapc,2026-01-30 00:20:34,1
Intel,o2iga3v,Terrible time,buildapc,2026-01-30 00:23:25,1
Intel,o2ih0m7,"Back in 2019 I built a PC for $1400. If i built the same PC today, it would definitely cost more than $1400. I got 32gb of ram and 1tb nvme for $300 back then.",buildapc,2026-01-30 00:27:19,1
Intel,o2ihb2a,No.,buildapc,2026-01-30 00:28:53,1
Intel,o2ihc3m,"Phone manufacturers are reporting that they are reducing the amount of ram in their next models because of the current market price changes.  So yeah, it's a bad time.  But for me? Since I started building computers like 20 years ago, the prices have been bad for the last 10 years to me.",buildapc,2026-01-30 00:29:02,1
Intel,o2ihi30,"It’s really not a good time. I built one for my partner before the ram shortage. Her kit of 32GB 6000MHz CL30 RAM was $180 CAD then, and is now nearly $800 CAD. I was planning on building one for myself too, but ended up spending the money I’d been saving on a good 4K OLED TV and sticking with my PS5.",buildapc,2026-01-30 00:29:56,1
Intel,o2ii9ax,"Everything is beyond expensive, and I don’t know if it will ever go down",buildapc,2026-01-30 00:33:59,1
Intel,o2ij1uk,"I built mine during the GPU shortage back during the Covid crap when GPU’s were overpriced. I lucked out with my RTX 3060 and got two for one. This new thing about NVIDIA upping the prices for GPU’s seems  pretty similar, to me at least. It really depends on if you’re willing to upgrade computer parts or just buying the new consoles when they come out. I personally love PC more than any console, and am comfortable upgrading when I need to. Hope this helps!",buildapc,2026-01-30 00:38:13,1
Intel,o2ij34m,"How’s your internet?  Honestly, this is as good a time as ever to try GeForce Ultimate. As long as you don’t go over 100 hours a month.",buildapc,2026-01-30 00:38:24,1
Intel,o2ijhl7,Is now even a good time?,buildapc,2026-01-30 00:40:33,1
Intel,o2ik3nb,You can still build a solid system for $1k. Ryzen 7500f + rtx 5060 ti 16gb. Will run those games very well.,buildapc,2026-01-30 00:43:45,1
Intel,o2ikluo,"Yes. If you are playing games from 2020 and before n1080p,,, you can build a dirt cheap PC",buildapc,2026-01-30 00:46:25,1
Intel,o2ikn5b,do. not. build. pc. yet,buildapc,2026-01-30 00:46:37,1
Intel,o2ilbb6,Ssd has only went up for 4tb and up.  1tb are still 100-150usd with some 2tb under 200usd.  990pro is still under 600 for a 4tb model.  Everything is a moot point.  Cpu and gpu are in good spots and the saving there make up for price hike in ram compared to building a pc in late 2024.  I however would bump that price up to 1500 if you want a console rivaling experience that will last a few years.  My 1300usd in late 2024 has been a let down compared to my xbox for smooth gameplay.,buildapc,2026-01-30 00:50:08,1
Intel,o2ilrb0,"Buying a prebuilt with the specs you want is currently cheaper.  Once the stock of parts the manufacturers are sitting on is used up, they will get more expensive.  Good luck OP, and may the odds ever be in your favor.  >Will prices of RAM/SSD/GPU, etc drop eventually?  Yes, in about 2-4 years. Maybe.",buildapc,2026-01-30 00:52:31,1
Intel,o2ilv9o,"No, but its not going to get better for long time, so actuly yes",buildapc,2026-01-30 00:53:07,1
Intel,o2ilw3o,"Generally, today is better than tomorrow, unless you want to wait 2 years on the hope it'll be better.",buildapc,2026-01-30 00:53:14,1
Intel,o2im0p7,No and yes and no one knows is the best answer.,buildapc,2026-01-30 00:53:55,1
Intel,o2im1do,Buy a pre-built from Costco. They are not the greatest specs and you didn't build it yourself but it's a great value with a warranty.,buildapc,2026-01-30 00:54:01,1
Intel,o2im91c,"Cutting edge? Probably not - but second hand? There are plenty of great deals out there. I picked up a great deal from someone leaving town. You'll easily get a good few years of use out of them, maybe have to turn fog or shadows down in your games but otherwise still top tier performance for a few years. Will it mean I have to upgrade in 3-5 years instead of 6-7? Yes, but cost wise it's just way more sensible.",buildapc,2026-01-30 00:55:11,1
Intel,o2imbz7,Is now even a good time to build a PC?     no.     Based on what various component makers are saying and the prices being as they are and will be for the next few YEARS...no.,buildapc,2026-01-30 00:55:37,1
Intel,o2in5yy,"Second hand, depending on where you are. There are always deals to be had if you snipe listings like a hawk :)  Managed to snag something a month ago that I'm happy with.",buildapc,2026-01-30 01:00:07,1
Intel,o2indqe,>Kazoo noises  *Noooooooo* 🎶,buildapc,2026-01-30 01:01:16,1
Intel,o2ine7n,"The easy answer is no.  I’ve never owned a gaming pc that went up in value over the years until this year.  SSD’s are double the price and my current ram costs 3x what I paid for it, 2 years ago.",buildapc,2026-01-30 01:01:20,1
Intel,o2iogih,Nope,buildapc,2026-01-30 01:07:11,1
Intel,o2iouu0,"definitely a bad time to build a pc like everyone else is saying. To put in perspective, a 1000$ PC pre Aug 2025 would be worth around 1300$ or more as of now.  Of course, none of these would matter if you're rich tho",buildapc,2026-01-30 01:09:24,1
Intel,o2ipzzu,Ddr4 this is the way,buildapc,2026-01-30 01:15:45,1
Intel,o2iq1i6,"Its not that bad , i think it can be worst when the stocks will be depleted, prices will go a bit higher  (they are now sellings rams that cost not much :) )   I think its either now oR Farrrrrrrrrr as much as one year MINIMAL before seeing price drops.",buildapc,2026-01-30 01:16:00,1
Intel,o2ird24,"I personally decided to go for it. You can wait and pray, but it feels like things won't settle down for a long time and might even get a bit worse. My suggestion is to try and go part by part, check deals around or maybe you can get a good pre build deal?",buildapc,2026-01-30 01:23:20,1
Intel,o2irpgx,"the best time to buy was probably a year ago. i got a prebuilt about 14 months ago because i'm canadian and anticipated tariffs being pretty bad. didn't anticipate the AI squeeze on components, so it's gotten even worse. I wouldn't recommend building right now at all.",buildapc,2026-01-30 01:25:16,1
Intel,o2irwu9,"It's the worst time to upgrade or to build a PC. Storage has increased their price by 200% I think and RAM by like 500%.  I mean, if you are willing to pay around $1000 just for the RAM itself, go ahead, everything else will be cheaper but my recommendation, look for some pre built PCs, you can still find good deals, but if you are going to buy the components one by one, for sure you will spend (or waste) a lot of money.  I was planning to upgrade my PC, but I will stick with DDR4 and my reliable 6700xt for the next two years at least, or whenever prices go down a little.",buildapc,2026-01-30 01:26:25,1
Intel,o2iryge,either pray you can get your hands on a good prebuilt  swap to console  or wait,buildapc,2026-01-30 01:26:40,1
Intel,o2is2sv,"It depends on what you think is going to happen.   Is hardware consumption from AI going to slow down? If so you should wait, let prices stabilize.   Is AI fever going to keep getting hotter? If so there may never be a cheaper moment.   Or wild card, is AI going to cause hardware and software capabilities to explode? Then perhaps buying now you'll end up with something obsolete faster than the historical trend.   No one really knows.",buildapc,2026-01-30 01:27:20,1
Intel,o2iszvv,Honestly build it right now since I heard nvidia was gonna stop or limit producing pc gpus and focus more on console and their other variant. So the prices of gpus are gonna go up. And ram prices isn't coming down any time soon either because of ai. So build one right now.,buildapc,2026-01-30 01:32:32,1
Intel,o2it62g,"I strongly recommend not to listen to any of the top upvoted comments, because dear lord are they horrible.  You can still do an 900-1000usd AM4 build with an 5600 and 9060XT and play everything you like in 1440p, let alone 1080p.  If you desire, get a 9070 or 9070XT and you can play stuff in 4k.  These are 900-1400usd builds we are talking here.",buildapc,2026-01-30 01:33:31,1
Intel,o2itgxh,"I remember earlier this year when I was trying to troubleshoot some issues that I thought were RAM at the time (they weren't) I looked up my Amazon purchase history to find the stated specs on my RAM to see that it was like $350. I'd paid a little over $100 for it just the year before.  I thought that it'd been discontinued and that was reseller market pricing. Nope. The world just went fucking insane.  Very bad time to build a computer, also a very bad time for a computer part to break on you.",buildapc,2026-01-30 01:35:14,1
Intel,o2ithce,"Unfortunately it is a pretty terrible time to build as far as price goes and that likely won't change for at least a year, unless a miracle occurs like the AI bubble popping (don't bet on it).  Unless you have access to a handful of price minimizing options (like the CPU/RAM/motherboard bundle deals at Microcenter) you're pretty much screwed on new hardware.  If you have a system already that is meeting your basic needs id stick with it until pricing improves in (hopefully) a year or two",buildapc,2026-01-30 01:35:17,1
Intel,o2iu93s,"Yea Now is already too late, get started before it gets worst, no light in this tunnel for the foreseeable future…",buildapc,2026-01-30 01:39:39,1
Intel,o2iuwo3,Costco near me has an ibuypower pre built that looks decent,buildapc,2026-01-30 01:43:16,1
Intel,o2iv468,Depends how rich you are.,buildapc,2026-01-30 01:44:26,1
Intel,o2ivdsl,"PCs have always been relatively expensive in the respective days economy.   Today's economy is fucked so therefore PC building is as well.   We have current price increases in ram (especially), SSD and GPU because of AI being targeted by manufacturers.   If you want a PC now, buy it now. Pre built if you aren't too fussed about parts and don't want to do your own build. Own build if you care about parts and want to experience building.   If prices go down in the near future (unlikely), you will have gotten whatever enjoyment for the price difference between now and then for your money....there's worse things to ""waste"" money on. If things go up or stay the same then your laughing",buildapc,2026-01-30 01:45:55,1
Intel,o2ivfo5,"The only thing that makes it a good time to build is that it may be a worse time to build in the near future. We haven’t had a real good time to build in years, though, so it’s all relative.",buildapc,2026-01-30 01:46:13,1
Intel,o2ivxi9,"If you're gaming, I would go budget. There's a lot of good performance on older hardware. There are some good prebuilds out there that you can look into that might be more affordable. That's what I'd do. Get a prebuilt with decent performance, keep it for a couple years and then re-evaluate the market later. Good luck!",buildapc,2026-01-30 01:48:59,1
Intel,o2ix6uf,"No, it’s a *terrible* time to build a new PC… But I’m going to do it anyway because I’m… Not smart.",buildapc,2026-01-30 01:56:00,1
Intel,o2ixlcu,"It's not a good time but unfortunately it's almost never a good time, it's always something. 10 years ago I was hearing ""not a good time"" because people were buying up all the graphics cards for crypto mining. Now 10 years later it's AI. Prices probably aren't going to go down any time soon, so absent going back in time and buying ram a few months ago, might as well just go for it if you can afford it.",buildapc,2026-01-30 01:58:15,1
Intel,o2ixuc8,You live close enough for a drive to a Micro-center? If so go pick up a Powerspec in your budget and enjoy gaming. You can always upgrade if prices fall later.,buildapc,2026-01-30 01:59:38,1
Intel,o2iy2t7,"Build? Maybe not.  Buy a prebuilt or mini-PC? Maybe.  There are some usable gaming prebuilts and even nice mini-PCs still on the market. If you don't mind gaming at 1080p, a careful purchase can get you in now. My bet would be that in a month or two basically everything that can run a modern game decently for under $1k will be gone, not to return until at least summer 2027.",buildapc,2026-01-30 02:00:58,1
Intel,o2iyjo0,"With the sudden increase in demand by ai and one guy straight up just buying for 40% of RAMs global supply... no.  In ~2 years more factories should come online, flooding the markets dropping prices. That's a long time to browse for good deals, you probably can find something at least reasonable before then.",buildapc,2026-01-30 02:03:36,1
Intel,o2iyv48,"It's REALLY not.  Give it a year. Asus is entering the RAM market, and lots of those ordered data centers might not come through.",buildapc,2026-01-30 02:05:23,1
Intel,o2iyza4,"It’s a good time to build a PC when you want a PC and can afford it. Don’t get caught up in following prices because the reality is no one here knows where they will go from here. What’s the worst case, the ram you buy today is $50 cheaper in 6 months?",buildapc,2026-01-30 02:06:01,1
Intel,o2iz08l,"No, I absolutely won't be replacing my PC - and if part of my current rig breaks I'll just aim to repair it  I would expect prices to drop eventually, but the question is whether it's 1 year, 2 years, or 5 years",buildapc,2026-01-30 02:06:11,1
Intel,o2izi81,"Many of the key components of a PC build have increased in price, some drastically so.  It is truly a horrendous time to be considering building a new PC.",buildapc,2026-01-30 02:08:59,1
Intel,o2izz7y,"Well, it ain’t the right time to build. But a good time to buy other components that are currently cheap. And leave the expensive ones out of your budget to go down",buildapc,2026-01-30 02:11:38,1
Intel,o2j0e20,"Now is even a bad time to build a PC, unless you already have RAM and storage parts lying around.",buildapc,2026-01-30 02:13:56,1
Intel,o2j0jfi,It's up to you really.  You can buy the parts but they won't go down any time soon meaning 2026.,buildapc,2026-01-30 02:14:45,1
Intel,o2j0r67,I sold ram for the same amount of money i used to buy a pc before lol. This is the worst time posible except the gpu crisis i think,buildapc,2026-01-30 02:15:55,1
Intel,o2j1tdt,"Even though it is a bad time to build, I built anyway. I was waiting for gpu prices to come down for years. It never really did. Now ram prices have skyrocketed. I think if you wait for the perfect time to build, you are going to be waiting forever.",buildapc,2026-01-30 02:21:46,1
Intel,o2j1vn4,Bruh. Now is a good time to get a used gaming PC on Facebook marketplace. Tons of cash strapped non-pc gaming young people are seeing the uptick in demand and giving it a shot at a sale. I’ve seen TONS of really good builds for <$1k lately,buildapc,2026-01-30 02:22:07,1
Intel,o2j2oa6,Now is probably the WORST possible time in the last decade to build a PC. Go to Micro Center. Or don’t build one at all. In my opinion,buildapc,2026-01-30 02:26:29,1
Intel,o2j4eab,It is not.,buildapc,2026-01-30 02:36:04,1
Intel,o2j6ry2,"Its the season for prebuilts or used parts. I just rebuilt my PC having replaced my GPU (9070xt) and ended up continue panic buying enough parts to have an entirely new PC.  However, the value of patience cannot be understated, especially with the used market. You can't jump on small savings relative to recent price increases, that's just a recipe for regretting a few months/1 year down the road when prices either stabilize or drop completely. (e.g. snatching at 20% off SSDs when prices have more than doubled).  For example, I ended up obsessively tracking new used listings and managed to get DDR4 32*2 GB 3200mhz sticks for 130usd.   Going jank is another option. I went with a Chinese MODT TOPC board with a 13950HX for 300usd and it performs beyond my wildest dreams if I were to go with standard off-the-shelf parts. Sure these may fail in a year, but I'll take the savings and insane performance (ended up spending only ~1300 for basically top of the line for 1440p) while it works!",buildapc,2026-01-30 02:49:06,1
Intel,o2j7k9c,It is definitely not a good time to build a pc. RAM prices being as high as they are and the issues with Nvidia cutting production on their GPUs not to mention they’re investing more money and time into AI.. yeah it might be awhile before building a pc is worth it again. And that’s unfortunate because AMD has stated they’re doing the same thing so it’s a shit show.,buildapc,2026-01-30 02:53:27,1
Intel,o2jdbrk,"Hard disk, SSD, and RAM prices are now more costly as the so-called AI boom takes the lion's share of parts production.  Better bet is to find a secondhand gaming PC where somebody's quitting to play games because they don't have time to.",buildapc,2026-01-30 03:26:15,1
Intel,o2jfk1s,"No, clearly not. It hasn't been a good time for a while. This pattern I see, when a hobby goes mainstream, capitalists find a way to cash in on it as much as possible, which ruins it for everyone. In the end people stop buying into said hobby, and we move onto something else. They did it with gaming, they are doing it with PC building, they will do it with anything and everything.",buildapc,2026-01-30 03:39:09,1
Intel,o2jfpvq,"It's a terrible time to build a high-end PC, but still a fine time to be a gamer. If you're just looking to play great games, any old ass-tier PC will play pretty much every 2D game that exists, plus tons of older AAAs and less demanding 3D indies.  Basically, if your hobby is gaming, you're great! If your hobby is playing the latest AAAs, you better hope you're rich.",buildapc,2026-01-30 03:40:07,1
Intel,o2jiu1v,Terrible time to build a pc im holding onto my DDR4 builds. They run great no need to upgrade for a while,buildapc,2026-01-30 03:58:41,1
Intel,o2jiuiw,"I fell for the trap of FOMO in 2021 when prices were peak, then 2022 price went down by a lot. I’m wondering about the same as you but don’t want to buy high again",buildapc,2026-01-30 03:58:45,1
Intel,o2jk5ka,"I would look at bundles, and the used market for RAM. I am building a new PC and a bundle made it bearable but still expensive.",buildapc,2026-01-30 04:06:54,1
Intel,o2jp1u1,"If you know you're going to build it in the next year, no matter what. Get the CPU, Mobo, PSU, case, and potentially GPU now. (Some GPU's are still at the prices they were last month and earlier this month, some have gone up).   Just wait out for a good RAM deal until then.",buildapc,2026-01-30 04:37:43,1
Intel,o2jq1vy,no lol,buildapc,2026-01-30 04:44:06,1
Intel,o2jsgzq,Doubt prices will ever come back down tbh. People are still going to buy the components snd the companies will still rake in the cash.   Its a bad time yeah. But they might be cheaper right now than they'll ever be again.,buildapc,2026-01-30 05:00:02,1
Intel,o2jvahc,"I just built this last weekend on a long awaited AM5 build (9800x3d, 64GB of Team Group DDR5 6000), and fortunately already had a 4TB Evo 990    Still need a good GPU and kinda kicking myself over passing on the 5090s I've seen (and had one in hand) but been kinda scared off by the burned power connector issues, too...  Maybe a 6090 in a couple years.   Other than that a smooth build except for the whole AM5 ""memory training"" thing but up and running and still spent about 50% more than what I would have spent on that RAM last year ($600 vs $400 or so). I decided it wasn't going to get any cheaper...",buildapc,2026-01-30 05:19:45,1
Intel,o2jve88,"It’s a terrible time to build. Then, if the AI bubble does burst… global economy with crumble into deep recession to where you’ll wish you had spare cash vs a pc.  Will prices drop eventually? Yes. Is that this year, next year, 5 years from now? Absolutely no one can say.  New chip manufacture facilities are coming online, AI demand won’t be infinite because we don’t have infinite power.  When component costs get like this, prebuilts start to gain in value. You could look at those, or just deal with using GeForce now or something for a stop gap. Also, combos deals where companies are trying to offload less popular stuff might help you. Microcenter and the like.",buildapc,2026-01-30 05:20:29,1
Intel,o2jvknf,No but it will get worse,buildapc,2026-01-30 05:21:47,1
Intel,o2jx7x0,"Horrible time to build, buy used one instead",buildapc,2026-01-30 05:33:38,1
Intel,o2k0yv7,It's not ideal but your finances are your business. How bad do you need it and how bad do you want it. No one can answer this for you all your gonna get from this post is seething about AI and corporations.,buildapc,2026-01-30 06:01:33,1
Intel,o2k8azw,"No, it’s a terrible time, but next month is likely to be worse, and the month after that worse again. I’m doing a build now even though it’s bad because things are likely to get worse before they get better.",buildapc,2026-01-30 07:00:02,1
Intel,o2ka313,"at a $1000 budget, if you're building with all new parts, I think you're looking at a 1080p gaming machine",buildapc,2026-01-30 07:14:44,1
Intel,o2kb2lo,"My old PC killed itself a month ago and I have to bite the bullet on a new build. So yeah, avoid it if you could.",buildapc,2026-01-30 07:23:06,1
Intel,o2kde1x,Only way to get these prices down is to stop using ai.,buildapc,2026-01-30 07:43:15,1
Intel,o2kle16,"Of course it s a good time. But not new pc. Old /used pc. A DDR4/3 pc should be a decent bang for the buck. Or you could go even older, i just payed 5$ for a Pentium 4, playing Half Life 2, Warcraft 3 and Gun 😇",buildapc,2026-01-30 08:55:23,1
Intel,o2kph4s,"No.  And also yes.  The AI-pocalypse is ruining this industry, but some of the forecasts I've seen predict that this will carry on through 2027.  If you're willing to wait until 2028 or potentially beyond, go for it.",buildapc,2026-01-30 09:32:30,1
Intel,o2l3shn,No if you compare to 6 months ago.  Almost certainly yes if you compare to any time in the near future because prices aren't going to drop any time soon.,buildapc,2026-01-30 11:36:36,1
Intel,o2l59ob,No it's not a good time. Memory and GPU prices increasing due to AI demand,buildapc,2026-01-30 11:47:59,1
Intel,o2l85x9,I feel so bad for people who finally decided to build a PC only to have their dreams crushed due to reasons that are completely out of their control,buildapc,2026-01-30 12:09:02,1
Intel,o2l9oiv,"No, but yes, ram sucks right now, the market is so bad, but gpus and ssds (which are already bad) are about to get so much worse, so if your building, do it now.",buildapc,2026-01-30 12:19:43,1
Intel,o2lf24q,best time now. soon people will own nothing and be happy,buildapc,2026-01-30 12:54:48,1
Intel,o2lh17f,"It's definitely not a good time.  Will it get better? We can't really predict it, people like to say in a few years prices will go down a lot, after they AI bubble explodes, but we don't really know how the changes in production and the changes in the companies priorities will affect the landscape long-term.  If you have the money now and want the PC now, I'd just build it; otherwise you'd have to wait probably until late 2027 according to the most optimistic but realistic predictions I've seen.  However, I'd look into used PCs or parts, maybe you can get a good deal there",buildapc,2026-01-30 13:06:43,1
Intel,o2lowpf,Worst time ever.,buildapc,2026-01-30 13:50:07,1
Intel,o2lro5l,"My 10 year old DIY NAS died and I finally bit the bullet and spent WAY more than I wanted to for outdated parts to get it back up.  I don’t even have all the parts yet but I found better deals afterwards, just not sure if I want to deal with the hassle of returns to multiple vendors…",buildapc,2026-01-30 14:04:28,1
Intel,o2lsz93,"Depends what for, I’ve gotten a PC towards the end of a generation (PS3->PS4 era), and my PC which was high end at the time was basically obsolete for PS4 era games so it barely lasted me a year and a half.   I’d suggest getting a very mid range PC now, so that when this new generation comes out which everything points to a 2027 window you won’t have spent 2-3k on a PC that will be obsolete.",buildapc,2026-01-30 14:11:12,1
Intel,o2lt1xs,I bought a fully built PC last week for $1500 (on sale from $1900) with Ram costs going up it would have been close to $2000 if I had built it with the same specs.,buildapc,2026-01-30 14:11:35,1
Intel,o2luenx,Yes,buildapc,2026-01-30 14:18:29,1
Intel,o2lx9ui,"no. If this keeps going on, personal computing will be dead.",buildapc,2026-01-30 14:32:49,1
Intel,o2m8r0t,Elon is building Robots now instead the Model S and Model X.,buildapc,2026-01-30 15:27:33,1
Intel,o2mafd0,"Honestly it’s not as bad of a time as people make it seem, but it is getting worse by the day.  Take advantage of sales and deals online, or find certain parts on local used markets.  Newegg constantly has 2x8 gb ddr5+mobo combos for 249ish, or 2x16 gb ddr5+mobo combos for 500ish, and even 2x8 ddr5+mobo+ram combos for 450ish and up.  You can also get a good quality 750w-1000w psu for 100 and under, as well as a nicely built good looking case for under 80.  I know a few days ago I threw together a list on Newegg, for 1050ish you could get a budget am5 build with 16gbs of ddr5, 1tb storage, and a 9060xt 16gb - plenty for 1080p and 1440p",buildapc,2026-01-30 15:35:11,1
Intel,o2mg01c,If you want it buy it. Nobody can predict the market,buildapc,2026-01-30 16:00:16,1
Intel,o2mqakp,"i would personally just decide which parts you want, and keep an eye on the prices over the course of the next 6-12 months, especially on aftermarket markets like facebook.",buildapc,2026-01-30 16:46:07,1
Intel,o2n4fwz,No.,buildapc,2026-01-30 17:49:29,1
Intel,o2njxzi,Its best to hold off a little while if you want to build.  Currently there's insane demand for memory and storage due to the AI bubble.   While at the same time the producers of these parts were actually at a downswing in how much supply capacity they have.  Within a year you'll likely see this resolved as the producers will be able to make much much more to meet the demand,buildapc,2026-01-30 18:56:48,1
Intel,o2owzas,It's always a good time to build a PC. It's a terrible time to buy parts.,buildapc,2026-01-30 22:49:16,1
Intel,o2pewgb,"If you don't want to build a new one, you can buy a refurbished one on the cheap still, especially if you aren't attached using Windows.",buildapc,2026-01-31 00:26:55,1
Intel,o2pt6df,"Its fine.  DDR4 is fine.  1TB NVME SSD + 4TB HDD is fine, just swap games on and off NVME.  Microcenter actually has some affordable pre-builts still.  I'd go that route personally.",buildapc,2026-01-31 01:48:52,1
Intel,o2pzny9,No.,buildapc,2026-01-31 02:27:10,1
Intel,o2qgqw7,No,buildapc,2026-01-31 04:13:37,1
Intel,o2qmrnq,"Been wanting to build a PC for around a decade, finally did it for the first time a month or so back. Yes the PC part market is absolutely fucked but you can do what I did and Frankenstein used parts into a used office PC. Hp pavilions, dell optiplex, whatever Lenovo ones are all dirty cheap used on eBay for around/under $100. Add a GPU and upgrade what else needs upgrading and you're gold. I got out much cheaper than building new, threw Bazzite on there and I have no regrets been having a blast. This gets my foot in the door to upgrade one part at a time and still having a very capable 1080p 40-100fps htpc in the meantime.",buildapc,2026-01-31 04:55:40,1
Intel,o2qnz75,I'm still sore at spending 1500 on my rig. But 3080 does it for me still. (bought 3080 used),buildapc,2026-01-31 05:04:25,1
Intel,o2qy0gc,"Maybe now will be the last time you are able to, so go for it",buildapc,2026-01-31 06:23:13,1
Intel,o2rbffj,"It will likely be several years before prices begin dropping. The big tech companies have already put out official statements on supply being locked up for a year or two.  It is not an ideal time to build a PC, but if you are going to need one it is better to buy now than a year from now.  The only sort of PC that I would soft recommend is buying an AMD APU like the 8700G. It has enough integrated graphics performance to play modern games on 1080p at lower settings, and you could overclock it a fair bit if you bought a cooler. It would let you play for now, and you wouldn't need to fork out for a GPU right away. Once you have money for a GPU down the road, it works fine as a CPU. You can look for used RAM and SSDs for cheaper, or settle for a small SSD and rotate through games more frequently. You could pick up a 1TB hard drive for dirt cheap and then just copy games over to your SSD when you want to play them.",buildapc,2026-01-31 08:24:19,1
Intel,o2reme1,"Price is now unreal for ram memory.. If you dont have ddr4 already.. Dont by ddr5..  for 1k$ you cant build sh1t for future proof.. My setup with 5700x and 9060xt with 32gb of hoyer x fury is around 1500-2000$(depends where you buy) and this build is counted as oldy.. Cuz od am4 and ddr4.. Pc prices for good future proof are around:  Ryzen 7 9800x3d around 480$ Ddr5 kingston are around 600$ for 32gb kit, for 64gb around 1100$ Rtx 5070ti around 1100$ Mother board x870 chipset around 250$ Good case around 100$ Watercooling 150$ Corsair shift 80 gold 1000w around 220$ M2 samsung 990 evo plus 2 tb around 220$ M2 990 evo plus 4 tb around 420$ Total: around 4000$  I meam this build is near high-end.. If price did not skyrocketed, this pc build would cost around 2400-2600$",buildapc,2026-01-31 08:54:33,1
Intel,o2sx0w4,"Short answer no, long answer no.  Use market and part exchange with your friends and family is the only thing making sense currently.",buildapc,2026-01-31 15:32:43,1
Intel,o330rt9,"I agree about the price of hardware but companies price based on what the market will bear.  Having built many gaming systems I can tell you that this is a never ending hobby that will consume all your lunch money.  My suggestion is to get a great base motherboard/cpu, a decent amount of ram, and adequate disk space.  You want a board that will have sufficient bus speed and room enough to add a good video card.  Buying used is a great option to save money.   Check the specs for the games you currently want to play, not for every game.  I would discourage buying a pre-built system.  Manufactures take shortcuts and value engineer to reach a certain price point.  You really won't find out what the PC's shortfalls are until after you purchase.  Building your own system gives you the option of swapping out and replacing/upgrading as you see fit.  When you replace a component, don't hold onto it -- sell it.  The longer you keep an unused part, the harder it is to resell.  The ultimate gaming system will be ongoing.  You'll be upgrading video cards, power supplies, memory, and motherboards as the technology improves.   Don't forget that you'll need low ping times and a high performing router if you want to compete with other online gamers.  The world's best performing computer won't help you if your ping time is 100ms!",buildapc,2026-02-02 02:11:51,1
Intel,o34svha,The best time to plant a tree is today. The second best time was yesterday…,buildapc,2026-02-02 10:25:19,1
Intel,o2ht0f2,Microcenter got decent deals,buildapc,2026-01-29 22:20:46,1
Intel,o2hsu8e,"1k will get you a very low end gaming PC. Raising the budget to 1,200 + tax will get you a better system. I just saw such a build recently. Look around and you will see builds around your price range.",buildapc,2026-01-29 22:19:55,0
Intel,o2htgv0,I don’t think there’s ever been a good time to build a PC…you just have to do it.,buildapc,2026-01-29 22:23:02,0
Intel,o2htkjp,No obviously not.,buildapc,2026-01-29 22:23:33,0
Intel,o2hw1jl,Bro no,buildapc,2026-01-29 22:35:56,0
Intel,o2ik4vw,"Im not building or buying shit until my processor is “minimum requirements” for anything I want to play, and maybe not even then - Ive been playing the same few games since before the pandemic  Its not just the AI bubble Trump is fucking the market with tariff nonsense.    Anybody remember when RAM doubled overnight after that typhoon in Vietnam like a decade back?",buildapc,2026-01-30 00:43:56,0
Intel,o2ht1ef,I did exactly this.  I noticed the pre built prices were lagging behind individual component price adjustments.,buildapc,2026-01-29 22:20:54,46
Intel,o2l5obf,"Depending on your needs you still find good bundles too. Microcenter is good for this if you live near one, but recently I picked up a U7 265k, Strix Z890 mobo, 32GB DDR5, case + AIO bundle on Newegg for $850 so there's some good online deals too.",buildapc,2026-01-30 11:50:58,6
Intel,o2m9wyy,Costco has some good deals,buildapc,2026-01-30 15:32:51,1
Intel,o2isg8y,Or he can just make an AM4 build with an 9060 or 9070 and you guys could stop giving horrible advise?,buildapc,2026-01-30 01:29:26,-4
Intel,o2hu5ro,Fall?! You’re far too optimistic.,buildapc,2026-01-29 22:26:30,463
Intel,o2hu0ha,"People are hoping prices ""start"" to drop around the fall.  1. That's best case scario. We may see rising prices for years given what companies have said about ram/gpu manufacturing and not wanting to go heavy into making new factories.  2. Even if they ""start to fall"", they are going to be rising all year up to that point. It might be 4+ years or more before ram/gpu prices even come back down to the levels they're at right now.  I think bottom line is nobody can tell the future. Buy what you want, when you need it. Assume it's going to cost more than you'd like no matter when you buy.",buildapc,2026-01-29 22:25:46,58
Intel,o2htrb5,Hell even hdd's are jacked from AI was planning to fill out the rest of my nas after I only got 1 drive black Friday but even recertified enterprise drives are at least up a 100$ if you can even find any now :(,buildapc,2026-01-29 22:24:31,16
Intel,o2ilhn9,"There're a lot of different reasons (and, let's be real, excuses) for components' prices to just keep rising too.   Bubble keeps inflating and / or plateauing for our lifetime: prices go up. Even, or especially, if it just plateaus.   Bubble pops despite trillions put into its life support: market crashes, prices go up.   This just becomes the new norm: production companies shift to selling components specifically made for only industry use, so consumer product prices go up.",buildapc,2026-01-30 00:51:04,7
Intel,o2ievhh,This is the same thing I heard with the 3000 series and those prices never went down,buildapc,2026-01-30 00:16:01,11
Intel,o2hunvj,">Unless the bubble pops sooner than expected, many speculate prices will not start to drop until fall this year  Some are saying they won't fall at all this year or the year after (if the bubble doesn't pop.)  As AI companies have bought out future stock from producers and more of them are shifting production to cater more to AI rather than private consumers. And there are many more data centers being built.  We might not see a normalization of prices for a few years until producers can ramp up production, if they even want to. If they increase supply they are investing in making the price of their product fall.",buildapc,2026-01-29 22:28:59,5
Intel,o2igczq,"It doesn't matter when the bubble pops, we would still have to wait for the contracts to expire.",buildapc,2026-01-30 00:23:51,3
Intel,o2jmfks,"These prices are never going down. There is no competition to the handful of companies that manufacture the chips needed for RAM and SSDs, now that the prices are up they'll stay up, because there is no choice for consumers to go anywhere else.",buildapc,2026-01-30 04:21:06,5
Intel,o2jr0u4,"More like the end of the decade. Spinning up more production fabs that's a long time, 3 to 5 years.   Open AI alone bought up 40% of Samsung and SK Hynix's production capacity through at least 2026.",buildapc,2026-01-30 04:50:23,3
Intel,o2j06k7,So is it a good time to sell my DDR2 and DDR3 that I have in a drawer?,buildapc,2026-01-30 02:12:47,2
Intel,o2iptax,"More than quadrupled on the higher end. I paid $195 for 64GB ddr5 6400 in June and Newegg has the exact same kit for over $1,000 right now.",buildapc,2026-01-30 01:14:43,3
Intel,o2ib2ig,I heard mid-2027,buildapc,2026-01-29 23:55:10,1
Intel,o2ic7uc,"What about business branded solutions, like with ECC and what not?",buildapc,2026-01-30 00:01:28,1
Intel,o2jc26t,Yep. The G.skill DDR5 RAM I bought in November 2024 for $112.99 at Micro Center is now $519.99. Holy-O-Jesus.,buildapc,2026-01-30 03:18:59,1
Intel,o2jy9w6,Bubble 😂,buildapc,2026-01-30 05:41:22,1
Intel,o2k3983,DDR3 RAM are going up now as more people buy older computer because newer parts are getting too expensive to build a Minecraft gaming machine,buildapc,2026-01-30 06:19:27,1
Intel,o2kbuuv,"Nope. If you don't build your pc now, you won't be able to till 2028 or even 2029. Prices ain't coming down anything soon, especially not in this fall lmfao.",buildapc,2026-01-30 07:29:52,1
Intel,o2kxiva,I agree it is a bad time but is it going to get better any time in the future? Only thing that realistically can change is OP's economic situation.,buildapc,2026-01-30 10:44:41,1
Intel,o2lwk6j,"Fall this year? My man, the way this industry works is to allocate future production. They are committed through at least 2027. And in the AI arms race, the easiest way to cripple your competition is to buy as much as you can, even if you don't need it, just to deprive 'them' of the tech. You and I are peons and of no concern to multi-billion $ enterprises. The gaming PC is rapidly going the way of the dinosaur.",buildapc,2026-01-30 14:29:15,1
Intel,o2n6j5s,"The AI bubble will not pop anytime soon... Its just getting started imo. It will stabilize once proprietary ""AI"" pc builds start selling in stores (e.g. AI pre builds in costco)",buildapc,2026-01-30 17:58:34,1
Intel,o2opu56,"Almost a month ago I decided to build a PC out of the blue and it was the best decision I did. Yes, I had to bite on the ridiculous ram price, but that's life. Reused my NVME that costed me 50 bucks and now it costs almost 200. 3 days after getting my 9070 xt for 667 bucks, they announce the 3070 ti is getting killed and prices sky rocketed.",buildapc,2026-01-30 22:13:12,1
Intel,o30zgcw,In many cases SSD prices have tripled just look at San disks NVMe drives for proof of that,buildapc,2026-02-01 19:47:43,1
Intel,o2iyd7z,"How can that sustain itself even for two years? Are they thinking everyone will just use weaker components?   Won’t they just destroy the home Pc market? Is the plan really for consumers to access high power compute via tokens, to an AI company?   I just don’t believe everyone will accept this? But maybe I’m wrong.",buildapc,2026-01-30 02:02:35,1
Intel,o2ju5b3,'normal' decent prices will never return. The golden age of Pc gaming as we know it was well and truly over IMO. They may drop but things will always be overpriced forever going forward.,buildapc,2026-01-30 05:11:39,1
Intel,o2ireqk,2000usd PCs are  like 200-300 more expensive. You guys are 12/10 overdramatic.  You can still do an AM4 build -  brand new  - for 800 and play everything you want in 1080p 60fps on max settings.  There is a reason Steams Hardware Survey disagrees with reddit to an extreme.,buildapc,2026-01-30 01:23:36,-5
Intel,o2hsjtk,"I get it. This makes total sense, I am just beating myself up. I had been patient about ensuring I wasn't impulse buying a $1.5k item, and it turns out I waited too long LOL.  But to you're point, it's all relative.",buildapc,2026-01-29 22:18:29,19
Intel,o2j2xns,"I agree. I built 11 months ago to replace a build from 2014. Going off Newegg, where I got most of it.    MOBO: X-870-A AM5. Paid $300, now $290   RAM: G.Skill Trident Z5 64gb DDR5. Paid $195, now $900   SSD: WD_BLACK 2TB SN850X. Paid $160, now $560   CPU: Ryzen 7 7800X3D. Paid $450, now $400   Case: Fractal Design Focus 2. Paid $70, now $75   Cooling: be quiet! Pure Loop 240mm AIO. Paid $80, now $90   GPU: PowerColor Red Devil AMD Radeon RX 9070 16GB (Amazon). Paid $650, now $700  What I built for ~$2k a year ago would cost me over $3k now, and almost all in the RAM and SSD - wild. And I was worried back then that I may have overpaid. Glad I did, though - she runs fast and quiet.",buildapc,2026-01-30 02:27:56,5
Intel,o2j83nj,"Honestly, I don't think memory prices will come down for a long time, if ever. The industry is an oligopoly, and even if the bubble bursts they won't let a good crisis go to waste.",buildapc,2026-01-30 02:56:28,1
Intel,o2hrupc,"Would it be better to buy a used PC for the time being? Ideally I am fine doing that and revisiting building one in a while, or slowly upgrading.  I'm not certain the best path forward.",buildapc,2026-01-29 22:15:03,9
Intel,o2k7ws5,"If they could get their RAM and storage overseas, then it's cheaper in 3rd world countries like in ph",buildapc,2026-01-30 06:56:51,1
Intel,o2iifb7,Parts list?,buildapc,2026-01-30 00:34:53,9
Intel,o2kpda6,"Define ""everything"" and ""maxed out"" please. I can understand being able play to Stalker 2 in 1080p (or even Cyberpunk)  but I doubt it's with max settings and 60 fps.",buildapc,2026-01-30 09:31:30,3
Intel,o2hrjg6,"I spend my time between Brooklyn NY and Stamford, CT. I just checked and saw one near my in Brooklyn I could go to!",buildapc,2026-01-29 22:13:30,2
Intel,o2hsnki,Combo deals have increased like $300 from when I bought my comparable combo.,buildapc,2026-01-29 22:19:01,1
Intel,o2ie3ij,Prices didn't drop after that fiasco during COVID.  Definitely not dropping after this bubble pops.,buildapc,2026-01-30 00:11:50,4
Intel,o2il9rd,"Eh, I've got some newegg invoices that are 10+ years old that would make the me of today shudder.",buildapc,2026-01-30 00:49:55,2
Intel,o2mgl7j,We fkd bro,buildapc,2026-01-30 16:02:53,1
Intel,o2itcyx,Nah mate. You will build another and definitely upgrade what you have regardless.   If AI companies continues to require all PC hardware be made for them. We will see changes to the PC consumer. What changes exactly time will tell but if you've done a recent build and worst case scenario happens it means you will be set for longer,buildapc,2026-01-30 01:34:36,1
Intel,o2il6l2,"Been building my own pc’s for about 20 years.  Love to do it and handpicking your components is great.  In the early days I could save some serious money but the margins have been shrinking every year. I built 3 pc’s in 2022 that should hold up until Windows 12 rears its head. May upgrade the video card on my gaming rig in the meantime.  Windows 12 will probably FORCE us into new hardware which for me would probably be motherboard, ram, cpu and an ssd. I’d expect a prebuilt will cost less by that time.  Sad because building your own is so rewarding.",buildapc,2026-01-30 00:49:26,1
Intel,o2mp7mv,"Same. Can't wait for prices to go up even more, and depending on how much money you have a few hundred extra isn't really THAT much",buildapc,2026-01-30 16:41:18,1
Intel,o2hyu7s,"Honestly, no real timeline.   It’s year after year new awesome games come out that I miss out on so I finally started to look into it- and boom! Worst time ever to buy hahah. I just don’t know much about building or PCs so I figured to ask.",buildapc,2026-01-29 22:50:07,2
Intel,o2imjk8,That is what I've been doing just buying parts one at a time when i see a deal or discount. I can wait on ram and gpu..,buildapc,2026-01-30 00:56:46,2
Intel,o2jm1wc,WHAT THE FUCK DID YOU JUST SAY TO ME? DO YOU REALISE WHICH SUBREDDIT YOU ARE IN?,buildapc,2026-01-30 04:18:43,1
Intel,o2ipzn6,"Dude, how can you even suggest that.  Why would we use a service from a company that is enabling the RAMpocalypse to begin with?",buildapc,2026-01-30 01:15:42,2
Intel,o2klmtl,"They want us to tunnel into that and lose ownership of our PCs.  That is terrible advice or do you like clicking on ""This Computer""",buildapc,2026-01-30 08:57:35,2
Intel,o2jmds3,In other countries SSD prices have hit 2GB 400$ it's the future.. consoles will increase in price too you know. A console is just a PC with autism they are still the same on the inside.,buildapc,2026-01-30 04:20:47,2
Intel,o2reovm,"As someone mentioned, buy prebuild.. Ultra cheaper",buildapc,2026-01-31 08:55:12,1
Intel,o2i0t1n,"Getting into the hobby is the worst I have ever seen it.  If you have a built PC that you can use some parts, like the PSU and the GPU, it isn't terrible but it still sucks.",buildapc,2026-01-29 23:00:13,3
Intel,o2jwc3z,"There have been constant good times to build a pc. Literally all of 2025 things were cheap. Mid last year any thing you wanted would be in stock as well if it was a 5090 you wanted.   Right now, 64gb a ram is the cost of a mid level rig 4 months ago. The sad part is at least 2026 is likely an entire year of sucky to build in. No 5xxx super series gpus were announced as its data center focus time. Who knows how bad it will stay.",buildapc,2026-01-30 05:27:12,1
Intel,o2hxkf1,Just experienced this getting a prebuilt from microcenter. Had my eye on one of the prebuilts there with a 5070ti in it. They just increased the MSRP of the 5070ti up to $900 but that prebuilt didn't increase in price at the same time.,buildapc,2026-01-29 22:43:39,11
Intel,o2hxq4n,"Yeah I bought a prebuilt with a 9800x3d  and 5070 ti. I normally build all my own stuff but it didn't work that way this time. I plan on selling the 5070 ti and putting my 4090 in after I replace the garbage power supply. Everything else seemed ""good enough"" and not complete trash so that's good haha",buildapc,2026-01-29 22:44:26,23
Intel,o2if56q,"Same here, this is the first time I've ever suggested buying prebuild (started with building 10 years ago). I wasn't planning on upgrading this year, was on 2080S, but saw a 5080/9800x3d PC for £2100 at the end of last year and knew if I waited any longer I'd regret it. Wish I'd known someone with a Costco card during the Black Friday sales though...",buildapc,2026-01-30 00:17:27,1
Intel,o2j4no9,"Same here. I built a 5060TI self-build for the same price as a pre-built 5070TI at Microcenter (1800). For me, I was not price sensitive and building was the purpose of the hobby, so pre-build didn't make sense. But fuck me if that didn't hurt.",buildapc,2026-01-30 02:37:30,1
Intel,o2lg6mk,"That's a really good deal. I wish they had more locations. MicroCenter is in Miami and I'm in northern Broward. It's like 45 miles away from me. Doesn't sound bad, but what makes it bad is the traffic and the really dangerous drivers. :(",buildapc,2026-01-30 13:01:37,3
Intel,o2mi3f1,"They indeed do at times that are worth it. Unfortunately not as great as it was a month ago like where my buddy got a gaming PC with a 7900X, RTX 5080, 32 GB RAM, and 2 TB NVMe for just $1999. It was an unbelievable deal.",buildapc,2026-01-30 16:09:39,1
Intel,o2iuurj,"Nope. This isn't bad advice. I don't think you realize how bad the prices are, even on older platforms.  I have a coworker and I was about to give him an AM4 motherboard, RAM, and a PSU for free (I literally just have these parts lying around). He would be coming off of a 9900k. I calculated the cost to get other components, including a 5700X3D CPU and a 5060 Ti 16 GB (selling at around $520 just yesterday) and I found out way better value could be had if he spent a few hundred more on an AM5 prebuild where he'd have a bigger performance boost (it's hard to justify spending $1200-1300 when you include new storage and other accessories on an AM4 upgrade with a 30% performance boost on Tarkov). And again, all of this cost after I give him a bunch of parts for free. X3D CPUs are stupid expensive on AM4 used. You should look at the prices. A 5800X3D is often times being sold at prices higher than a 9800X3D brand new.",buildapc,2026-01-30 01:42:58,11
Intel,o2icac6,I heard it was Fall 2028,buildapc,2026-01-30 00:01:51,173
Intel,o2hwi76,fr fall is pretty wild to speculate. That's fast.,buildapc,2026-01-29 22:38:17,30
Intel,o2immev,I hate how this^ post has two meanings that are both plausible.,buildapc,2026-01-30 00:57:11,2
Intel,o2j9ni4,"Yeah, likely more like end of year or next year with the way China is ramping up prod.",buildapc,2026-01-30 03:05:19,1
Intel,o2k1471,I think it’s over unless a competitor makes a new gpu line.,buildapc,2026-01-30 06:02:42,1
Intel,o2k7sww,The prices will drop before GTA VI that's for sure,buildapc,2026-01-30 06:55:58,1
Intel,o2kbcla,Think he means the fall of rome 2.0,buildapc,2026-01-30 07:25:28,1
Intel,o2pxrzd,Fall of this year!? Well bless your heart lol,buildapc,2026-01-31 02:15:55,1
Intel,o2irxz2,"Why is every top upvoted comment on reddit pure moronic misery?  People are asking for genuine advise, they do not want to have pure egoistic misery dumbed on them for no reason.  I know, this is kind of the point of reddit, but come on....",buildapc,2026-01-30 01:26:35,-7
Intel,o2hu9fn,"Key words are ""**speculate** prices will not **start** to drop until fall this year"" absolutely! 😅",buildapc,2026-01-29 22:27:00,18
Intel,o2itp90,"And this is all assuming they come down at all. I fear it'll be like crypto or covid all over again where prices just stay high because why not? A lot of people will still pay it. GPU prices never really came back down after crypto, they only got more expensive.",buildapc,2026-01-30 01:36:33,8
Intel,o2kh2nh,"They'll have to lower prices if people stop buying their products. Unlike high-end GPUs where its a dedicated product for a smaller userbase, flash memory is used in just about every tech product around. Hardcore gamers might be willing to pay inflated prices for their GPUs, but your average person won't be so willing to pay double for a new PC when they can just keep using their old one. And the last thing these manufactures want to do is sit on tons of stock that isn't selling and rapidly depreciates in value.",buildapc,2026-01-30 08:15:50,1
Intel,o2iyvn9,"I was just telling somebody the last time I upgraded my RAM was late 2022. I spent 84.99 (total) on two sticks of G.Skill DDR4 16GB 3600 from Newegg. Same exact kit now on Newegg is $254.99. That wasn’t even any kind of new hotness when I bought it, now it’s practically antiquated. $254.99. It’s scalper pricing.",buildapc,2026-01-30 02:05:28,1
Intel,o2j1ffv,The home computer market is a rounding error compared to enterprise.,buildapc,2026-01-30 02:19:38,6
Intel,o2j49an,"There is always someone willing to buy. And home PCs are a rounding error when compared to everything else, especially enterprise products.",buildapc,2026-01-30 02:35:18,2
Intel,o2j1cp3,"No way. I don’t even get max settings on Cyberpunk at 60 fps at 1080p and I’m using a Ventus (no frills) RTX 2060 12GB. That GPU alone right now is $695 on Newegg.  With Windows 11 you’re also going to need 32GB of RAM and the cheapest no-name 32 GB kit (DDR4 2400 at that) is $120. We’ve already busted $800 and haven’t even looked at a CPU yet, let alone the rest of the PC.",buildapc,2026-01-30 02:19:13,3
Intel,o2m4m9m,"Its crazy to think that i bought my ddr5 2×16gb ram for roughly 110€ about 18 months ago, i feel incredibly lucky that i didnt wait any longer to finally build my first pc. On the other hand i also waited and slowly saved up for about 5-7 years, allways looking if there were good deals. One can only hope that times will get better soon. Im honestly worried that thr whole ai thing will not crash and we will have to wait a couple more years until things stabilise again in the electronics market.  Sometimes its worth checking local electronic shops, they sometimes have really good deals, noone knows about, because we young people order everything nowadays. I have seen some crazy deals on prebuilds for like 600-800€ down from 1.5k€",buildapc,2026-01-30 15:08:21,1
Intel,o2mieeg,"Generally, your instincts are correct as prices usually go down. It's just crazy times we live in",buildapc,2026-01-30 16:11:02,1
Intel,o2ixtuq,I bought a 1k pre built new with a 9060 16gb. Not sure of your expectations but it plays almost all games in high quality.,buildapc,2026-01-30 01:59:34,1
Intel,o2hstcz,"Used PC's are banger deals if you're careful.   I sold a Ryzen 5700 system with a 2080 init for the price of the dang ram less than a month later. So yea, theres deals to be had.",buildapc,2026-01-29 22:19:48,38
Intel,o2hsquq,Yes used is the way to go.,buildapc,2026-01-29 22:19:28,6
Intel,o2hvh8d,"Are you in the USA? If so, might want to check local brick and mortar stores like Walmart or Staples. They sometimes have deals on parts that you can scrounge together. Or display units for prebuilts. Biggest price sinks right now are RAM, storage, and GPU.",buildapc,2026-01-29 22:33:04,4
Intel,o2ipwqo,"As a teen, I would aways buy used parts because I was too impatient to save for newer parts.   I still do it to this day.  I've had a couple times where it did not turn out well for me but for the most part, if you are careful and fast enough, you will be able to score some really good deals.     I was recently able to build a pc with parts I got from a deal for a whole pc and some other listings I found for around 1800$CAD that easily would have cost about 2700 to 2800$ tax in new.",buildapc,2026-01-30 01:15:15,2
Intel,o2irrjd,I really wanted to finally get another PC to upgrade from an i5-7400 and 1050ti (my old one was from 2017 and I built it during the crypto ram price bubble) but parts got stupid expensive again. I seem to time it poorly every time.  Just bought a second hand rig for £470 with an i5-12400 and 3060ti. Seems to be playing everything on high/ultra super nicely so far! And not the end of the world if in a few years time I want to upgrade again! Though in my case probably another 8 years.,buildapc,2026-01-30 01:25:36,2
Intel,o2hsvel,If you live near Microcenter it's not too bad. The used prices are kinda crazy right now.,buildapc,2026-01-29 22:20:05,1
Intel,o2j8418,"You might find a good deal on a used PC, but they're also more expensive than normal,  and they might have undisclosed defects and expired warranty.  I haven't seen anybody mention it so I'll suggest you consider the upcoming Steam Machine, we should have some more info soon.",buildapc,2026-01-30 02:56:31,1
Intel,o2ju2ob,"I just bought one today after checking different options. I made part lists, tried builder sites, and looked at prebuilt. The used ones were cheapest but you might have fewer options on parts. Prebuilt is next cheapest, I was able to get a slightly better graphics card and processor with a prebuild over building my own for the same price, and that's not even counting shipping for ordering parts separately and having a warranty. I feel like right now a prebuilt is the best option. Or used/refurb if you find the right one.",buildapc,2026-01-30 05:11:07,1
Intel,o2kbszb,Used is good but you really have to be careful and know exactly what youre getting. Make sure everything works,buildapc,2026-01-30 07:29:24,1
Intel,o2j4u4m,"If you only want to game, your best option right now is to buy a PS5 (Pro), save the rest of your budget, and buy a new PC in a few years when prices have come down. A used PC will have a hard time beating a PS5 for the same price.",buildapc,2026-01-30 02:38:29,0
Intel,o2jtqgq,Feels very bad buying slightly older tech for more than what it cost when it was new a year or two ago,buildapc,2026-01-30 05:08:46,1
Intel,o2jed1k,"I second this, interested to see cause im going about the same route",buildapc,2026-01-30 03:32:12,1
Intel,o2hsrkx,"Nice! There's some pretty good bundle deals might check for. Might set the store location to check stock.   https://www.microcenter.com/search/search_results.aspx?fq=category%3aComputer+Build+Bundles%7c775&sortby=pricelow&storeid=115   Might also check there for any GPU deals maybe better pricing(also open box deals), and others.    PCpartpicker is also a great tool for price checking at a few retailers.   For storage, and GPU, also seen some deals at in store only not advertised online at Staples and Walmart of all places(but its slim pickins now that people have found out).   All and all, micro center sounds like a good start.",buildapc,2026-01-29 22:19:34,3
Intel,o2ime4m,Ur better of getting an am4 system. Ddr4 3600 used is still cheap if u browse Facebook marketplace. I got 16gb corsair rgb for 70$ a week ago.,buildapc,2026-01-30 00:55:56,1
Intel,o2hvtcl,"Yep, ram pricing the main culprit.",buildapc,2026-01-29 22:34:47,1
Intel,o2hwzo4,"Basically the ""discount"" you get is not paying super jacked up ram prices.",buildapc,2026-01-29 22:40:43,1
Intel,o2ihcv2,"Nope, they'll just keep charging high because they can",buildapc,2026-01-30 00:29:09,6
Intel,o2kcrk0,Uh. Yea they did. To levels seen before? No. From peak? Yes.,buildapc,2026-01-30 07:37:47,4
Intel,o2os8lt,It did though. Do you not remember graphics cards being 3x?,buildapc,2026-01-30 22:25:13,1
Intel,o2jlxr4,Yes..,buildapc,2026-01-30 04:18:00,1
Intel,o2itml6,"I hope you’re right, I love this hobby and in the last 15 years it’s been shit on over and over again, from crypto to Ai. I wish these Ai models never happened.",buildapc,2026-01-30 01:36:07,1
Intel,o2jyfn9,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 3 : No piracy or grey-market software keys**  > This includes suggesting, hinting, or in any way implying to someone that piracy, or violation of license agreements is an option.   > If a license key is abnormally cheap (think $5 - $30), it is probably grey market, and thus forbidden on /r/buildapc.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1qqmqb6/-/o2jlbyg/%29.%0D%0D---%0D%0D)",buildapc,2026-01-30 05:42:33,1
Intel,o2jpy2r,A cOnSoLe PlAyEr?! In mY bUiLd A pC sUbReDdIt?!?!?! bLaSpHeMy!!!!!!!!,buildapc,2026-01-30 04:43:26,1
Intel,o2izrmd,"Because building a PC right now is crazy? Personally, I’m just sitting the next year out with my 3080. But there are worse things in the world than paying $200 for a year of 4k 120/240hz with 10-15ms of added latency. Much better than paying $1500 for old tech.",buildapc,2026-01-30 02:10:27,1
Intel,o2kggdq,"I upgraded last year from a i7700k, new CPU = new motherboard & cooler = new ram = new PSU, upgraded GPU…same case & fans.",buildapc,2026-01-30 08:10:18,1
Intel,o2i1nmz,"Indeed. Also sometimes there may be great deals. For $1999 in early to mid December, my buddy got a brand new IBuyPower gaming PC with a 5080, 32 GB RAM, and a 7900X. Even at that time, good luck getting that at such a price. I was looking the other day and now that same PC is like $2500 if I recall correctly.",buildapc,2026-01-29 23:04:41,4
Intel,o2ll2jv,"why are there SO MANY prebuilts with horrible psus even tho theyre built by alleged ""experts""? even the prebuilt im trying to buy has an E tier PSU that im gonna have to ask them to remove so i can place one of my own",buildapc,2026-01-30 13:29:36,6
Intel,o2jkqzx,Throw the power supply in head of trump?,buildapc,2026-01-30 04:10:37,-4
Intel,o2in17o,Wrong fall 38',buildapc,2026-01-30 00:59:23,46
Intel,o2iqlav,Closer to the Fall of the US.,buildapc,2026-01-30 01:19:04,6
Intel,o2lhz2k,You all may be too “glass half full” kind of people. I was going for Fallout 76,buildapc,2026-01-30 13:12:13,1
Intel,o2isjme,Fall 6050,buildapc,2026-01-30 01:29:57,3
Intel,o2l32fp,"GTA VI isn’t being released on PC, so that’s unlikely.",buildapc,2026-01-30 11:30:49,1
Intel,o2j0zyh,"Yes, you should expect this from Reddit.   But also every report indicates that these AI companies have bought up all the RAM for the next couple years. RAM prices ain’t dropping in fall.",buildapc,2026-01-30 02:17:16,6
Intel,o2hve0r,"I've lived through multiple computer price boom and busts.  There is literally no way of predicting where prices will go.  They could stay elevated for 3 years, they could crash in 3 months (not likely, imo).  I remember when I built my first PC I bought RAM right before prices jacket up in 2017, and they stayed elevated for several years IIRC.",buildapc,2026-01-29 22:32:38,14
Intel,o2lctky,"Not sure where you live but GPU prices returned to pretty much the same in the UK.  During covid/crypto the RTX 20 Series was betweeen £1-2k, and I know people who were paying more. I remember seeing the RTX 30 Series for like £4k+ in 2021.  Meanwhile in 2023 I managed to get a 4070 for £550.",buildapc,2026-01-30 12:40:40,0
Intel,o2j3tcs,That makes sense.,buildapc,2026-01-30 02:32:51,2
Intel,o2j2clr,The only way you’re getting a PC that’ll run anything you want at 60 FPS/1080p/Max settings for $800 right now is if you pay a truck driver $800 to have one fall off the back of his truck.,buildapc,2026-01-30 02:24:41,5
Intel,o2ilzoo,I bought the last 3 gen4 nvme ps5 drives that were the only reasonable option to buy ssd storage rn. An equivalent nvme was double the price,buildapc,2026-01-30 00:53:46,3
Intel,o2kknj7,"I just got my friend a used i5 12400 with 3060, 2.5 yr old PC for under $500 last week.",buildapc,2026-01-30 08:48:42,1
Intel,o2js0zm,Mr.pierogi do you not grasp the grave mistake you have made? This is nothing to joke about I will have my AI overlords feed you to the hungry machines at once!,buildapc,2026-01-30 04:57:02,1
Intel,o2isf7b,Same thing on my end. 9800x3d/5070 TI/2 tb ssd/32gb ram $1899. I just jumped on it because they are going to catch on at some point and raise the prices on all the prebuilts,buildapc,2026-01-30 01:29:16,1
Intel,o2lmivk,Because it makes it cheaper for them. They must have a partnership with apevia or something. I know it wasn't ideal but it's the way I went in the current market. I bought a super flower 1200w 80+ titanium to replace mine because I DON'T fuck around with power supplies,buildapc,2026-01-30 13:37:27,6
Intel,o2ltah5,"NZXT has good components for their pre-builts, at least when I got mine.   Might want to take a look at their offering.",buildapc,2026-01-30 14:12:47,1
Intel,o2mqbsy,Average person don’t know any better. Just another cost cutting measure.,buildapc,2026-01-30 16:46:16,1
Intel,o2iojd7,"Yall really think these companies will remain solvent by 2028? They're hemorrhaging billions of dollars a quarter with little revenue. Vc funds are already drying up, and the talk of ad inclusion is bc shareholders want returns sooner rather than later.   There's just no way they have enough capital to remain afloat till then.",buildapc,2026-01-30 01:07:38,37
Intel,o2iz0w8,"Nah, Fallout '76",buildapc,2026-01-30 02:06:17,8
Intel,o2iolye,Fall 3800 for sure.,buildapc,2026-01-30 01:08:02,6
Intel,o2iuav6,Fall of society,buildapc,2026-01-30 01:39:55,8
Intel,o2irouy,I heard it was Fall of Reach,buildapc,2026-01-30 01:25:10,2
Intel,o2k3akj,You mean 2138.,buildapc,2026-01-30 06:19:45,1
Intel,o2lzlxv,Was it the bite of 87?!,buildapc,2026-01-30 14:44:23,1
Intel,o2jbxbc,We might be done as a society by then because of their greed.,buildapc,2026-01-30 03:18:12,1
Intel,o2jrn54,Don’t tempt me with the RTX 6000 series GPUs 😭,buildapc,2026-01-30 04:54:28,1
Intel,o2l71my,"In my experience, by the time you’re ready for a new computer, the market will have levelled out and you’ll know when it’s time to start fresh. When prices go silly like the COVID bitcoin mining era, the market basically makes your decision for you. It levelled out for a couple years, by then my computer was around 7 years old. I built a new one 2ish years ago, I’m sure in 5 years when I’m ready to start fresh RAM won’t cost $750.  At least, I hope. If not, we’ve got bigger fish to fry than PC builds lol",buildapc,2026-01-30 12:00:59,1
Intel,o2ogsqk,"MSRP has gone up every series since the 1000 series, as have third party/reseller prices. Pretty sure the 5000 series is the first series where MSRP actually went down (just barely) compared to the previous series, but MSRP prices hardly exist. Just because cards aren't at their peak crypto absurdity prices anymore doesn't mean the prices in general still haven't gone up series after series on average.",buildapc,2026-01-30 21:29:50,2
Intel,o2t8wyk,Nice. What mobo did you use? i was looking for a 119$ msi mobo. Too bad it doesnt have wifi.  I already have my gaming pc but i was building one for my mom with a 400 budget. i just need the mobo.,buildapc,2026-01-31 16:30:04,1
Intel,o2l8ovj,I'm selling a r5 3600 + 2070 8Gb + 16 Gb FFR4 + 1.5Tb (nvme + 2 SSD) + Bequiet case for 420 euros and people are still bargaining whereas I can de-activate the classified ad and sell it 500 euros in 2 months.,buildapc,2026-01-30 12:12:48,0
Intel,o2jsb80,Machines? Hungry? Impossible!,buildapc,2026-01-30 04:58:56,1
Intel,o2lndex,"as you shouldnt. And mine isnt an apevia, but it is in fact TR2 S from Thermaltake, which also happens to be one of their Worst psus, while not even being relatively cheap. Im still baffled tbh, but i would chalk this up to incompetence",buildapc,2026-01-30 13:42:01,1
Intel,o2jxftm,"Seems vastly unlikely to me that any of OpenAI, Anthropic, Microsoft, Amazon, xAI, or Google will declare bankruptcy before 2028. Capex vs revenue run rate is not that extreme by historical standards, compute can be used for inference as well as training, and most of these companies are pretty diversified (public and private sales, B2B as well as general users; not just language assistants, but driverless cars, drug discovery, material science, miltech, etc).  There will definitely be middle rank AI companies that get eaten but the general AI bubble narrative doesn’t hold water.",buildapc,2026-01-30 05:35:14,9
Intel,o2j0qjt,"It's the dot-com bubble all over again. Just earlier today my dad was reminiscing on it. Billions poured into telecom companies that were supposed to be the backbone. It probably won't crash like the dot-com bubble, but it will come down, hopefully hard.",buildapc,2026-01-30 02:15:49,17
Intel,o2k5rt9,"If the AI bubble does pop, we’re all going to have way bigger problems than the price of PC components. AI investment is the only thing keeping the economy growing, so it the bubble pops, we’re in for the mother of all recessions.  The AI industry is great. If it succeeds like the proponents say it will, most of us will lose our jobs, but if it fails like the skeptics say it will, most of us will lose our jobs. ¯\\_(ツ)_/¯",buildapc,2026-01-30 06:39:26,6
Intel,o2ipesb,The parts manufactures .,buildapc,2026-01-30 01:12:29,6
Intel,o2juvog,"Dunno about that.    OpenAI is apparently about to get $100 billion from Amazon, Microsoft, Nvidia, Softbank, etc. ahead of an IPO late this year.  Wouldn't surprise me if certain nation-states also bought into it.  Google and Meta generate enough cash outside AI to keep throwing money at it for years.  xAI has...whatever deal with the devil Musk made to keep it afloat.",buildapc,2026-01-30 05:16:49,4
Intel,o2muanr,trump will put some other guy and interest rates will go negative so that banks will give infinite money to VCs to blow in even more harebrained AI investments,buildapc,2026-01-30 17:03:57,1
Intel,o2jtgzc,They’re getting bailed out by daddy trump cause some how these scam jobs apparently became 50% of the stock market over night,buildapc,2026-01-30 05:06:56,1
Intel,o2ixl0w,I’m hearing the chip producers have just retooled for industrial production vs consumer production… But I hear a lot of shit so I dunno.,buildapc,2026-01-30 01:58:12,1
Intel,o2ix9sa,"Ai is here to stay, it doesn't matter which company wins they will be building data centers for the next decade.. it's not a bubble that will pop.    You don't just make ai and then forget about it because it wasn't profitable.  That's like creating electricity and then saying naw... that's too much work.    Look at Tesla.  It's been around like 15 years and it's still barely breaking even.    Rich companies with all the money in the world will fund it till it's profitable because it's too great of an invention to let go and whichever of these rich companies ends up with control over it will be in charge for the foreseeable future.    Lack of profits will never, ever kill AI.",buildapc,2026-01-30 01:56:28,-3
Intel,o2jyd4q,That's the price for 32GB RAM in fall.,buildapc,2026-01-30 05:42:02,1
Intel,o2m4dm6,Last year I told myself by the end of 2026 ill need to upgrade my PC.  Not happy about the ram prices but it is what it is.,buildapc,2026-01-30 15:07:14,1
Intel,o2lb89l,"Honestly, that's kinda overpriced even in this market. 3600 is very slow and that 2070 is quite old. Keep the 2 SSDs and sell it. The seller gave my friend additional brand new headphones and keyboard along with it noting how diligent and nice we were.",buildapc,2026-01-30 12:30:10,1
Intel,o2pburn,Darpas EATR robots are fiending for a meal they have already eaten 99% of the human population and their batteries are running low,buildapc,2026-01-31 00:10:20,1
Intel,o2ltyjd,Yeah it's ridiculous. Let's put high end components in a PC and put a time bomb in it with them when power supplies aren't even that expensive. I spent $189 on mine and it's an A+ on the PSU chart.,buildapc,2026-01-30 14:16:12,1
Intel,o2kh7dh,"The companies themselves may not collapse, but the more diversified companies (Google, Microsoft, Amazon) will toss AI aside once the bubble pops, just like they do with every other tech fad once the money is gone. They have plenty of other revenue sources that they can rely on to weather through a storm.  The AI-exclusive companies on the other hand (OpenAI and Anthropic) are bound to collapse with the bubble, since they basically exist solely from investment funds and hemorrhage money like there's no tomorrow.",buildapc,2026-01-30 08:17:01,1
Intel,o2kh6gp,"This is why they are all investing and moving money into each other's companies. They don't want to crash. If everyone is linked together, the govt will bail them out anyway. They make their billions easy.",buildapc,2026-01-30 08:16:47,3
Intel,o2kmeu8,"I worry it's going to be like the 2008 financial crisis.  That same level of dumb bullshit, the same level of ""we got fleeced, bail us out or everything crashes!""",buildapc,2026-01-30 09:04:46,2
Intel,o2j02kb,Presumably they will raise prices in other areas to fund the AI for as long as possible.  So the people will end up paying one way or another.,buildapc,2026-01-30 02:12:09,4
Intel,o2kh91z,"Artificial Intelligence itself certainly isn't going anywhere, but Generative AI (which is what 90% of people are talking about when they refer to ""AI"" and is what the current bubble is built on) does not make money and will never make money. The operating costs are tremendous, and if they try to impose higher costs, they lose the majority of their users (which comes from everyday people using free models).",buildapc,2026-01-30 08:17:27,2
Intel,o2l60aq,"The bubble can still pop, and it can still wreak havok on the global economy.  It's not like the dot com bubble killed the internet or the 2008 crisis killed owning property.  AI is definitely here to stay, and I am convinced we're also in a bubble. Those two are not mutually exclusive.",buildapc,2026-01-30 11:53:25,3
Intel,o2lxlp2,I know but I sold 2 with comparable specs in the last 4 months at this price :p,buildapc,2026-01-30 14:34:26,1
Intel,o2pmczd,"Sounds like I just have to wait them out then! Can they catch me before they run out of battery? Next time, on r/BuildaPC Abridged!",buildapc,2026-01-31 01:08:36,1
Intel,o2luax7,their clever ruse is to make your pc blow up and hope youre gullible enough to (for some reason) buy another one from them,buildapc,2026-01-30 14:17:56,2
Intel,o2l8dh8,"Google, Microsoft, and Amazon are not going to drop AI. You think Google is going to say its fleet of Waymos rolling out in cities around the world was empty hype? That the medical and material science research tools coming out of DeepMind were just fads? Meanwhile Satya Nadella has promised $80 billion on data centres and compute.  OpenAI and Anthropic are also not going to run out of funding any time soon. OpenAI just raised at $500B valuation, Anthropic has Amazon and Google as backers and a queue of sovereign wealth funds waiting for equity. They have years of runway, and revenues are increasing fast; OpenAI went from $6 billion in 2024 to $20 billion in 2025. Anthropic went from $1 billion to $5 billion in the same period.  All of this will resolve in the next 2-3 years anyway. But it’s interesting to see how many people are classing AI as “just the next hype thing” alongside the metaverse and bitcoin when it’s something utterly different — a proper general purpose technology covering dozens of different sectors and markets and the culmination of an 80 year research project.",buildapc,2026-01-30 12:10:33,2
Intel,o2pndtl,Oh god oh no they are connecting into a human centipede like structure using the charging cables coming out of their mouth to charge into eachothers behinds to charge up the batteries of the strongest robots now having enough energy to EAT YOU NOOO,buildapc,2026-01-31 01:14:37,1
Intel,o2luvm2,I knew the risks going into buying what I bought and I'll have to gamble with it. I checked all the connections and everything was surprisingly well put together and managed. I'm hoping that with my new PSU everything will work out. I'm trying to get my buddy to upgrade his 3090 to the 5070 TI that came in it so I can make back some of the money I spent.,buildapc,2026-01-30 14:20:52,1
Intel,o2q43f0,"Should've been more specific, AI as a whole isn't going anywhere, it's Generative AI that has no real future. AI for research and FSD clearly has a future and has tremendous value as a tool, but Generative AI such as LLMs bleed money without any real forms of profit.",buildapc,2026-01-31 02:53:30,2
Intel,o2vst3w,"Insulated Scissors, go! Use Cut!",buildapc,2026-02-01 00:05:54,1
Intel,o2lv9jb,"That 5070 ti was in the back of my head, i thought ""theres no way buying a prebuilt with a garbage psu AND weak GPU costs less than building everything on your own"", i thought you'd try to sell it, but that seems like a more coherent idea lol",buildapc,2026-01-30 14:22:46,1
Intel,o2saz7b,"Good clarification. Overall I still disagree (eg, because a lot of valuable STEM AI tools are built on generative AI backbones) but it’s definitely the area where there’s the greatest danger of a bubble.",buildapc,2026-01-31 13:29:48,1
Intel,o0rbmt4,"You could spend all that on ram alone, honestly your system seems pretty balanced and not in dire need of an upgrade unless you’ve got fuck you money",buildapc,2026-01-20 23:21:26,1
Intel,o0rcynh,I sold my 570x/5900x/32 GB CL16 for 250 flakes back then.,buildapc,2026-01-20 23:28:39,1
Intel,o0rc58e,That’s not my system bro that’s what I’m eyeing,buildapc,2026-01-20 23:24:13,1
Intel,o0rch8a,"Ahh sorry I misread, it’s definitely balanced. For the money could you get a 9060 xt instead of the b580? It performs slightly better and has more vram if you get the 16gb version",buildapc,2026-01-20 23:26:00,1
Intel,nx5hhax,It's a good gpu for its price. The driver issues that ruined A-series seem to be gone. Just be sure to enable ReBAR: BattleMage needs to to perform best,buildapc,2026-01-01 22:20:13,5
Intel,nx5gj70,[https://www.youtube.com/watch?v=00GmwHIJuJY](https://www.youtube.com/watch?v=00GmwHIJuJY)   [https://www.youtube.com/watch?v=npIpWFSfmv4](https://www.youtube.com/watch?v=npIpWFSfmv4)  Not sure what kind of options you have where you are. 3060 is pretty long in the tooth unless your getting a smoking deal. No low priced 9060s or or anything around where you are?,buildapc,2026-01-01 22:15:11,3
Intel,nx6wr9i,At 250 its a crazy good deal. Its not top of the line but its winning competition is the fact that it beats out every other card in its weight class at its price point.,buildapc,2026-01-02 03:18:01,2
Intel,nx7n3le,It’s great,buildapc,2026-01-02 06:21:14,2
Intel,nxbtc77,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance?",buildapc,2026-01-02 21:45:50,1
Intel,nxbtinv,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance? Especially if the driver issues are no longer present with the more recent updates",buildapc,2026-01-02 21:46:42,1
Intel,nxbt4xl,"A bit out of my price range, even for a used one. I’m only 19 and I’m building this by myself with my own money as my first ever build. From where I’m from, the 3060 12 gb is a bit manageable price-wise, I just wanted to check out other options.",buildapc,2026-01-02 21:44:51,1
Intel,nxo75v2,"i remember hearing that the overhead issues are less of/not an issue now, due to driver updates",buildapc,2026-01-04 18:49:23,1
Intel,nxbtrn7,[similar to a 3060ti](https://www.techpowerup.com/review/intel-arc-b580/32.html) but with newer features,buildapc,2026-01-02 21:47:56,2
Intel,nxbzpsr,Fair. If your on a tight budget. Consider some of the RDNA3 AMD cards as well. I mean if your looking at a 3 generation old 3060 anyway. Something like the 7600 XT 16gb might be on the market where you are at a good price. Its probably 20% or so faster then a 3060.,buildapc,2026-01-02 22:17:35,1
Intel,o264q60,"They're same thing, I don't even see point of pro tbh unless you need it for some reason.  The difference is Intel vPro® Essentials & Intel SIPP (Stable Image Platform Program) that basically it.",pcmasterrace,2026-01-28 06:28:30,2
Intel,o25cjn8,I think one of the key differences is that one has the word Pro in it while the other doesnt,pcmasterrace,2026-01-28 03:22:52,2
Intel,o26kebw,the difference is money. if you ask intel they'll answer you with that:  https://preview.redd.it/mbnrjbrwy1gg1.png?width=1080&format=png&auto=webp&s=9d19e620addc72804f671b1c9706b6e77f93f7b0,pcmasterrace,2026-01-28 08:42:47,1
Intel,o28cw15,"The real answer is that it supports a different driver suite, and probably will have better support for photoshop and other softwares.",pcmasterrace,2026-01-28 15:44:59,1
Intel,o3jm6lp,I thought it would be like more memory n stuff but thanks for clarifying,pcmasterrace,2026-02-04 15:29:58,1
Intel,o3jm3qs,😂,pcmasterrace,2026-02-04 15:29:35,1
Intel,o2dmp7c,I love the krabs meme,pcmasterrace,2026-01-29 08:38:20,1
Intel,o3jmc9l,Oh k,pcmasterrace,2026-02-04 15:30:42,1
Intel,o35ui6v,"Hello :) Intel Arc isn’t the same as Iris Xe. Arc is newer, and it’s usually noticeably better for graphics.  Out of these three, I’d pick the **HP ProBook (Core Ultra + Arc)**, it’s the most “future proof” option, and ProBooks are generally better built. About the HP hinge issue, it’s mostly a problem on some cheaper consumer models, it can still happen, but ProBooks are usually a safer bet. If you want the simplest “no worries” specs out of the box, the **MSI** is also a solid choice because it already has **16GB RAM and 1TB storage**.",pcmasterrace,2026-02-02 14:42:59,1
Intel,o360vpg,Thanks man ;),pcmasterrace,2026-02-02 15:15:23,2
Intel,o3v74f5,"i personally use a xeon e5 2650 v4 with  the motherboard being the x99 pr9-h eith a gtx 1070, but i dont personally play arc raiders",pcmasterrace,2026-02-06 07:51:07,2
Intel,o3v9h80,">CPU/MOBO/RAM was a flash sale on the AliExpress app that I snagged for the criminal amount of $31 shipped.  Yeah, the ram alone is 2/3 of the budget otherwise.",pcmasterrace,2026-02-06 08:12:58,2
Intel,o3xryq9,"I just got the case, I flipped the right most top fan to be intake to better CPU temps",pcmasterrace,2026-02-06 17:43:23,2
Intel,o3v8par,Please dont use a xeon for gaming. This one has a low single core score but you need a high single core score for gaming.,pcmasterrace,2026-02-06 08:05:43,1
Intel,o3vlnt2,"What performance are you actually getting with this? I have an i5 6600k and RTX3070 and wasn't entirely happy with performance in arc raiders being that my CPU was the minimum spec. I don't see how an even older weaker CPU would be a fun experience, although maybe we have different standards?",pcmasterrace,2026-02-06 10:09:46,1
Intel,o3v81y1,With the insane prices of RAM now I would think prices on these LGA2011-3 chips are going to sink even further. That said most new games really want threads and these Xeons really keep up.,pcmasterrace,2026-02-06 07:59:41,1
Intel,o3w8cow,Highly recommend if you’ve been looking for a shooter,pcmasterrace,2026-02-06 13:05:10,2
Intel,o3yx1kw,"Arc raiders is already in my gaming hall of fame. Which is Desert Combat (BF 1942 Mod), Halo 3, and PUBG. It’s top top top tier fun",pcmasterrace,2026-02-06 21:03:11,2
Intel,o3v8e6r,It's the first game in years that has really pulled me in. Before that I'd play games but it'd usually be on my old favorites and not much of anything new.  I had played The Finals for a bit so it was very familiar control-wise. That's free if you want to check out performance and see if a similar playstyle seems fun. The Finals is more twitchy and also TF2 like though.,pcmasterrace,2026-02-06 08:02:50,1
Intel,o3vam1f,It sucks for us but it really sucks for PC gamers in less developed nations. This kind of setup is very prevalent in South America and Eastern Europe. Now they have no path for a cheap rig anymore. RAM and NVMe prices have shut them out.,pcmasterrace,2026-02-06 08:23:43,1
Intel,o44vs23,I considered it with a regular dual fan GPU dumping hot air in the case but with the blower style GPU cooler the CPU tops out at 65C when doing both CPUz stress test and furmark. GPU hits 98C during that but it’s expected.,pcmasterrace,2026-02-07 20:01:27,1
Intel,o3v9fej,"It's not 2017 anymore. Games use threads like crazy. I used a 6650XT with one of these CPUs before selling it in another rig and played NBA 2K25 (same as on console) at 120FPS+, and well as the Finals on 720P low at 120FPS+. I'm getting 85FPS+ 1080P (FSR to 720) on Arc Raiders with this setup.   Plays games perfectly.   Here's this exact CPU with an RX 6600. Has the same dips my 5950X has when new arc or players pop in.   [https://www.youtube.com/watch?v=DuDCxnhdH\_k&t=86s](https://www.youtube.com/watch?v=DuDCxnhdH_k&t=86s)",pcmasterrace,2026-02-06 08:12:30,1
Intel,o40nlzf,"I don't know what made you think this CPU was less powerful in anything but a single threaded benchmark. Arc Raiders uses 16+ cores. The Passmark for this CPU is 17.2kpts and the 6600K is 6.3kpts. In the 5th photo in my post I had put a passmark score for the two minimum, this CPU and the two recommended. This game will use every thread it can.  Here I'm on the practice range, graphics set to 1080P with FSR set to quality (\~720P). In fullscreen mode the CPU usage goes back and forth between 40-52%. GPU isn't even near full utilization so this is taxing the CPU and not the GPU.   Take a look in the bottom of the steam overlay. That CPU percentage is treating hyperthreaded cores as real cores and you're able to see all 28 threads from the 14 core CPU. 57% CPU usage is 114% real core usage, or 15.96 fully loaded cores worth to get 112fps. In game it's more around 75-90fps 1080 FSR quality mode.  https://preview.redd.it/tedxnm2ijzhg1.png?width=1919&format=png&auto=webp&s=1b8ed94ec92b99d4c5b52ca3cc3d2770d628df18",pcmasterrace,2026-02-07 02:56:42,1
Intel,o3vejau,"i personally use normal desktop ram not ECC, Im 100% upgrading to am4 and not staying on the xeon platform for too long, enough said i got my pc for 400 bucks a month ago used, think i got ripped off lol",pcmasterrace,2026-02-06 09:01:30,1
Intel,o3kg9aq,congrats! don't forget to enable resizable bar in the bios settings,pcmasterrace,2026-02-04 17:48:34,809
Intel,o3kfzfi,Nice!! And congratulations on your sobriety.,pcmasterrace,2026-02-04 17:47:20,169
Intel,o3kgxcc,"Treat yo' self! Enjoy every byte of it, you've earned this victory lap big time.",pcmasterrace,2026-02-04 17:51:37,85
Intel,o3khug8,![gif](giphy|26tnjjQQRqPbwDxdK|downsized),pcmasterrace,2026-02-04 17:55:46,37
Intel,o3kguzr,one of the best cleanest look gpu out there. Intel was cooking with this one,pcmasterrace,2026-02-04 17:51:19,52
Intel,o3kgfzh,"Sobriety milestones deserve rewards, and this one’s fire. keep stacking wins, u’re killing it",pcmasterrace,2026-02-04 17:49:24,84
Intel,o3kgz5x,Hell yeah. The most deserving reason. Congrats!,pcmasterrace,2026-02-04 17:51:51,14
Intel,o3kgh5t,"I want one so bad. I just can't justify it with my current rigs.  Also, congratulations on the milestone!",pcmasterrace,2026-02-04 17:49:33,11
Intel,o3kih9t,"Nice, just put one in a build for my daughter. I really think its the best option sub $350.",pcmasterrace,2026-02-04 17:58:38,8
Intel,o3ki0uv,Congrats on sobriety! I'm now going on to 7 years clean of drugs in April. A rocky journey that turned into being the best choice I could make,pcmasterrace,2026-02-04 17:56:34,14
Intel,o3kk7dz,VRAM on that card is worth more than the card itself now haha.,pcmasterrace,2026-02-04 18:06:29,12
Intel,o3kh8id,Nice and Congratulations!  I hope you really enjoy it!,pcmasterrace,2026-02-04 17:53:02,4
Intel,o3khcex,Congratulations!,pcmasterrace,2026-02-04 17:53:31,5
Intel,o3ki5uy,May need yo update your bios to the newest version. I just rebuilt a PC with the same GPU and Arc Raiders kept crashing. BIOS update and now it runs really well even at 3440x1440,pcmasterrace,2026-02-04 17:57:12,4
Intel,o3kooch,"Brutal karmafarm bait. Like peak ""gem"" memed to hell era of reddit ass post.",pcmasterrace,2026-02-04 18:26:37,10
Intel,o3kimto,Upvote bait,pcmasterrace,2026-02-04 17:59:20,16
Intel,o3ki3ki,"Congratulations man, this card is insanely good for the price and what it is, and with XeSS3 coming soon, it will get even better.  Enjoy your games and be proud of yourself, for you made this far.",pcmasterrace,2026-02-04 17:56:55,3
Intel,o3kjj8p,Bro got an arc for his soberity ARC.,pcmasterrace,2026-02-04 18:03:26,3
Intel,o3kjque,"Hell yes! You've earned it (just so you know, you were worthy of it before anyway but probably feels good now)",pcmasterrace,2026-02-04 18:04:25,3
Intel,o3kk7so,Congrats and happy gaming!,pcmasterrace,2026-02-04 18:06:33,3
Intel,o3knj2g,Nice! I'm proud of you op! Enjoy the gaming.,pcmasterrace,2026-02-04 18:21:27,3
Intel,o3knnmd,Congratulations and stay vigilant! Great card too,pcmasterrace,2026-02-04 18:22:01,3
Intel,o3knwkd,"Glad you still here with homie, congrats on the card!",pcmasterrace,2026-02-04 18:23:07,3
Intel,o3lrija,From 1 ex addict to another congratulations! It works if you work it and 1 day at a time we got this ;),pcmasterrace,2026-02-04 21:28:31,3
Intel,o3kprsv,/r/titlegore,pcmasterrace,2026-02-04 18:31:32,8
Intel,o3lsx7i,"You can go get pats on the back in addiction subs, no need to grift here.",pcmasterrace,2026-02-04 21:35:15,8
Intel,o3klaxj,"I mean, nice gear but... Where's the correlation dawg?  ![gif](giphy|3ohhwz39TKZHkWSsEM)",pcmasterrace,2026-02-04 18:11:28,10
Intel,o3kpe0u,Sir this is a Wendy’s.,pcmasterrace,2026-02-04 18:29:49,7
Intel,o3khvfv,Unreal! Game on,pcmasterrace,2026-02-04 17:55:54,2
Intel,o3khzwv,"Niceee man, glad for u",pcmasterrace,2026-02-04 17:56:27,2
Intel,o3kid7j,"Built my partner a build with this just before the rampocaplye - been excellent so far, good choice! Hope you enjoy! As others have said, make sure resizable bar is enabled - and have fun building the mini desk GPU model that it comes with haha  Edit - and congrats on the milestone",pcmasterrace,2026-02-04 17:58:07,2
Intel,o3kietp,Lucky! What mobo+CPU you running it with? I wound up going over what I needed on the mobo just because I love ocing+future use although am4 so meh,pcmasterrace,2026-02-04 17:58:19,2
Intel,o3kj3bl,"Oohhh, good card!!",pcmasterrace,2026-02-04 18:01:24,2
Intel,o3kj5cs,Well done. Smash those goals and now smash your framerates.,pcmasterrace,2026-02-04 18:01:40,2
Intel,o3kj7qt,"Hell yeah, congratulations and keep up the good work.",pcmasterrace,2026-02-04 18:01:58,2
Intel,o3kje6o,Don't know you OP but I'm glad you came out on the other side! Enjoy the card and keep going strong!,pcmasterrace,2026-02-04 18:02:47,2
Intel,o3kjfcm,Go brother! Enjoy it man,pcmasterrace,2026-02-04 18:02:56,2
Intel,o3kjo9d,"Recovery comes in many forms, at every stage, in many ways. This is just another sign that you’re on the right path.   Congrats, brother. From one addict to another, I’m so proud of you.",pcmasterrace,2026-02-04 18:04:05,2
Intel,o3kjp6a,Grats on four years! Awesome choice; I'd love to upgrade my 2060 to a B580 at some point.,pcmasterrace,2026-02-04 18:04:12,2
Intel,o3kjs2t,Sober with you today! Congrats on four years and I hope you enjoy the new card.,pcmasterrace,2026-02-04 18:04:34,2
Intel,o3kkvhr,This one internet stranger is proud of you. Enjoy the hell out of that card!,pcmasterrace,2026-02-04 18:09:33,2
Intel,o3ktbsc,"I hope I can get my hands on one of these soon (before they go up), and use my 4060 for Lossless Scaling.",pcmasterrace,2026-02-04 18:47:35,2
Intel,o3kuru1,"One of us, one of us!",pcmasterrace,2026-02-04 18:53:55,2
Intel,o3lowcp,Hell yea man proud of you for staying sober!!! Keep at it,pcmasterrace,2026-02-04 21:16:02,2
Intel,o3m0yjd,Did you buy it to quit gaming addiction as well?),pcmasterrace,2026-02-04 22:13:54,2
Intel,o3pp8ku,Oh hell yeah. Nothing was better for my addiction issues than getting an i9 14900 cpu requiring me to get a damn minor in engineering (fixing the vdroop compensation and tuning load lines with CEP enabled) In order to have the chip function as efficient as possible and taking my mind away from the substance abuse issues to becoming obsessed with something less destructive (other than to my power bill) way to go on 4 years sobriety!,pcmasterrace,2026-02-05 13:32:11,2
Intel,o3kivki,Never forget to reward yourself,pcmasterrace,2026-02-04 18:00:26,2
Intel,o3kspog,TIL Intel has its own high end GPU line. I'm amazed I never saw it before. Can any owners confirm how it runs? The marketing sure makes it feel like if you don't have Nvidia or AMD you might as well not have a GPU considering how important AI driver optimization is now,pcmasterrace,2026-02-04 18:44:51,2
Intel,o3kkwyf,Intel is making a real shot at the gpu market.  Im happy to see it.  Congrats on your upgrade,pcmasterrace,2026-02-04 18:09:44,1
Intel,o3kkzhk,"To give my honest input, hopefully you mostly play recent mainstream games where Intel cares to prioritize bug fixes and improvements. Old games that use DirectX 9 (or even older) can result in quirks but it depends on the game, e.g. Half-Life 2 works, but The Settlers 2 may have quirks, etc. Nvidia's drivers have decades of driver workarounds for more games than most of us know, which can make it relevant for some people.  So as advice to help you feel good and confident about your purchase without possible regrets, test all the games (new and old) that you play or would play rather sooner than later. If they run well and all is OK, then you are golden. Just mentioning this in case you may end up running into niche problems and are past the return period or something. Much success and have fun.",pcmasterrace,2026-02-04 18:10:02,1
Intel,o3klbmh,"Congrats , keep up the good work and enjoy that Beast.   Similar story, myself. Years of addiction and multiple overdoses. This year makes 6 years sober and counting.",pcmasterrace,2026-02-04 18:11:33,1
Intel,o3kldai,Bro is winning!,pcmasterrace,2026-02-04 18:11:46,1
Intel,o3klkkc,Nice! Gz!,pcmasterrace,2026-02-04 18:12:40,1
Intel,o3klqio,Congratulations,pcmasterrace,2026-02-04 18:13:25,1
Intel,o3km863,Have fun!,pcmasterrace,2026-02-04 18:15:37,1
Intel,o3kmbgh,"Congrats on the sobriety, we appreciate you here in the group and know you will really enjoy the new card. 🙏🏽",pcmasterrace,2026-02-04 18:16:01,1
Intel,o3knbjp,"I just watched the tour Gamers Nexus took of their factory.  Really good, I really enjoyed how up front they were.",pcmasterrace,2026-02-04 18:20:32,1
Intel,o3kq9xx,Congrats. Enjoy your new baby.,pcmasterrace,2026-02-04 18:33:49,1
Intel,o3kqf5y,Nice!! I was thinking of getting one of these for my recent build but they were sold out everywhere! Always congrats on the milestone. Keep at it and enjoy ya gaming!,pcmasterrace,2026-02-04 18:34:29,1
Intel,o3kqkud,Congrats!,pcmasterrace,2026-02-04 18:35:13,1
Intel,o3ktzi3,"I applaud your tenacity! Also I hope that card serves you well, this is Intel’s time to shine. If they don’t take advantage of the current market now, they might as well go out of business.",pcmasterrace,2026-02-04 18:50:28,1
Intel,o3kukhe,Hell yeah brother 💪,pcmasterrace,2026-02-04 18:53:01,1
Intel,o3kve0d,"Massive respect, I'm glad you made it. Enjoy your new hardware and your sobriety.",pcmasterrace,2026-02-04 18:56:38,1
Intel,o3kvygl,"Congrats!  If no one has told you recently, I'm proud of you.  I've watch addiction destroy the lives of people I love, I can' put in words how happy it makes me to see someone recovering from addiction and doing well.",pcmasterrace,2026-02-04 18:59:10,1
Intel,o3kwrlu,Absolute winner,pcmasterrace,2026-02-04 19:02:49,1
Intel,o3kyqq2,"How do you felt during the od? It is not painful or bad, right?",pcmasterrace,2026-02-04 19:11:50,1
Intel,o3kytjz,Congrats! Here's to 4 more.,pcmasterrace,2026-02-04 19:12:11,1
Intel,o3kyybw,As someone with a substance history let me say this from the bottom of my heart: Props and congratulations to you! You earned it. Keep going strong ma man / or woman! Cheers from Germany,pcmasterrace,2026-02-04 19:12:47,1
Intel,o3l05z6,Congrats on the card and the sobriety!,pcmasterrace,2026-02-04 19:18:23,1
Intel,o3l09sf,Now you're addicted to frames 😭,pcmasterrace,2026-02-04 19:18:52,1
Intel,o3l0t6b,I'm so jealous.   https://preview.redd.it/8k8gi9y63jhg1.jpeg?width=735&format=pjpg&auto=webp&s=7f203e0dde1f78242d6e37ad7d3470dbf7dad12b  Grats though.,pcmasterrace,2026-02-04 19:21:23,1
Intel,o3l1j6w,"Great card for the price. Can play anything on the market at good frame rates and quality settings. Can struggle a little with some older titles, but even there with the driver updates Intel has put out it's doing pretty well. Enjoy!",pcmasterrace,2026-02-04 19:24:47,1
Intel,o3l2vnq,Looks like expensive chocolates,pcmasterrace,2026-02-04 19:31:05,1
Intel,o3l3f39,Congrats man!,pcmasterrace,2026-02-04 19:33:39,1
Intel,o3l3iy6,Welcome to a new addiction.,pcmasterrace,2026-02-04 19:34:10,1
Intel,o3l4o00,congrats ! i am 1  year 9 months and 4 days sober . nut still broke as fuck paying debts ...,pcmasterrace,2026-02-04 19:39:33,1
Intel,o3l4uxu,As another ex-addict who had pawned my xbox multiple times...congrats!,pcmasterrace,2026-02-04 19:40:27,1
Intel,o3l5c2w,Congrats bro,pcmasterrace,2026-02-04 19:42:42,1
Intel,o3l8oem,"Congrats, dude :)",pcmasterrace,2026-02-04 19:58:21,1
Intel,o3lal5n,Congratulations 🎉 keep it going …,pcmasterrace,2026-02-04 20:07:48,1
Intel,o3lcb45,Glad you're still here! Treat yo self!!,pcmasterrace,2026-02-04 20:15:59,1
Intel,o3lfrfb,Proud of you!  Enjoy the card!,pcmasterrace,2026-02-04 20:32:29,1
Intel,o3lgi0h,How good is this graphics card?,pcmasterrace,2026-02-04 20:35:58,1
Intel,o3ljniy,I'm curious how Intel GPUs run,pcmasterrace,2026-02-04 20:51:06,1
Intel,o3ljoam,"Hell yeah bro, good work.",pcmasterrace,2026-02-04 20:51:12,1
Intel,o3ln0eg,congrats,pcmasterrace,2026-02-04 21:07:03,1
Intel,o3lvq06,"Congratulations man, sometimes it’s nice to treat yourself. You deserve it.",pcmasterrace,2026-02-04 21:48:35,1
Intel,o3lvqpp,B58 for the win 😤(bmw driver),pcmasterrace,2026-02-04 21:48:40,1
Intel,o3lvxdd,What a weird fuckin post title,pcmasterrace,2026-02-04 21:49:32,1
Intel,o3lz41n,"I got 11 years clean now with a 9950x3d+5090 custom loop , traded one addiction for another, keep pushin' it's worth it, having nice shit is a lot better than a needle in your arm. It will all fall into place if you keep doing the work",pcmasterrace,2026-02-04 22:04:50,1
Intel,o3m0uz9,I know how hard it is.. You deserve to treat yourself with this card and please do not look back too much but only ahead. Even if it means nothing from a stranger: I am proud you were able to overcome your addiction...,pcmasterrace,2026-02-04 22:13:24,1
Intel,o3mccuh,Congratulations on being sober first of all ! Enjoy your new hardware man you really deserve it,pcmasterrace,2026-02-04 23:12:56,1
Intel,o3mf7xq,Congratulations friend. You deserve it. What an accomplishment!,pcmasterrace,2026-02-04 23:28:43,1
Intel,o3mgqy6,You do deserve this my friend. Great work.,pcmasterrace,2026-02-04 23:37:17,1
Intel,o3mhfwx,"From one recovering pc nerd to another, hell yea bro, I hope u like it I was eyeing the intel gpu’s hard. Might still grab one to have extra vram for local ai",pcmasterrace,2026-02-04 23:41:10,1
Intel,o3mli96,"Wellllllll at least my drug addictions didn’t kill me. Ah, what the hell!",pcmasterrace,2026-02-05 00:03:31,1
Intel,o3mt404,"You’re a legend, glad you are still here, enjoy the GPU!!!",pcmasterrace,2026-02-05 00:45:19,1
Intel,o3mxs50,You earned that. Congratulations and great job.!,pcmasterrace,2026-02-05 01:11:51,1
Intel,o3n4epy,I heard about Intel gpus but never got to see performance on them.,pcmasterrace,2026-02-05 01:50:11,1
Intel,o3n9io3,Congrats brother.,pcmasterrace,2026-02-05 02:19:22,1
Intel,o3ndhx5,"Congrats bro! I assure you that you've saved more money than that card cost, let alone the time better spent ;)   Have fun!",pcmasterrace,2026-02-05 02:42:02,1
Intel,o3nnqcw,You should be proud!!! <3,pcmasterrace,2026-02-05 03:42:20,1
Intel,o3ns4nm,Congratulations,pcmasterrace,2026-02-05 04:10:36,1
Intel,o3nuego,hope you're having fun with it :D,pcmasterrace,2026-02-05 04:25:42,1
Intel,o3nv3f4,.,pcmasterrace,2026-02-05 04:30:19,1
Intel,o3nz5oz,🥳,pcmasterrace,2026-02-05 04:58:05,1
Intel,o3o5hgi,Hell yeah ! I have a B580 as well. It's wonderful ! Congrats friend !,pcmasterrace,2026-02-05 05:44:43,1
Intel,o3ocjgp,Love to see it!,pcmasterrace,2026-02-05 06:43:27,1
Intel,o3olizn,Legion go 2,pcmasterrace,2026-02-05 08:05:08,1
Intel,o3oo9ps,Still into self-harm I see,pcmasterrace,2026-02-05 08:31:15,1
Intel,o3opymu,Congrats on the sobriety and a fine budget workhorse graphics card.,pcmasterrace,2026-02-05 08:47:23,1
Intel,o3osa7q,"Congrats man, you deserve it, best feeling ever",pcmasterrace,2026-02-05 09:09:56,1
Intel,o3oulxw,yes keep a competitive company running.  do not let's them die.,pcmasterrace,2026-02-05 09:32:51,1
Intel,o3p0e87,Nice! Have fun!,pcmasterrace,2026-02-05 10:28:02,1
Intel,o3p0udk,![gif](giphy|26gsobowozGM9umBi),pcmasterrace,2026-02-05 10:32:12,1
Intel,o3p15q1,Bought this for my wife’s build and it’s a nice card! Enjoy! Congrats on the sobriety,pcmasterrace,2026-02-05 10:35:10,1
Intel,o3p5dh1,"wait what, Intel does video graphics? since when?  how do these compare with the titans?",pcmasterrace,2026-02-05 11:12:48,1
Intel,o3p80kj,"Fuck yea, brother!       As someone who has BPD, among other mental health disorders, I understand how difficult it is to pull yourself from the darkness and stay on the straight and narrow. I'm glad you're still with us. You're doing a great job!  Enjoy.",pcmasterrace,2026-02-05 11:35:15,1
Intel,o3p8e5e,"Nice and congrats! I love seeing posts like this rather than “look at my $10,000 gaming machine and behold!”",pcmasterrace,2026-02-05 11:38:23,1
Intel,o3pgblk,"Congrats, friends. You earned it.",pcmasterrace,2026-02-05 12:36:55,1
Intel,o3pkiwb,"Congrats on beeing sober, mate!",pcmasterrace,2026-02-05 13:04:10,1
Intel,o3pkwf1,"Brother play Red Dead Redemption II and The Mafia Trilogy,if you haven't already. Just believe me",pcmasterrace,2026-02-05 13:06:30,1
Intel,o3pnicx,Congratulations on your victory lap! :) My Arc A750 is looking up to you! 😁,pcmasterrace,2026-02-05 13:22:13,1
Intel,o3py55e,"Ayeee! You beat the statistics and made it, congratulations!!!! od is the number 1 cause of death for males under 50 …and 90% of people that have used will relapse, treat yo self! What are you going to play first?",pcmasterrace,2026-02-05 14:21:44,1
Intel,o3qb6xm,Congrats mate,pcmasterrace,2026-02-05 15:28:30,1
Intel,o3qz5xt,Mad respect for people that manage to get sober!,pcmasterrace,2026-02-05 17:19:48,1
Intel,o3rcwp0,Oh hell fucking yeah dude. 😁,pcmasterrace,2026-02-05 18:23:00,1
Intel,o3rduow,Intel has really stepped it up. Have fun!,pcmasterrace,2026-02-05 18:27:22,1
Intel,o3rpjir,Congrats on being sober!!!,pcmasterrace,2026-02-05 19:20:47,1
Intel,o3s2y1u,VRAM is the new crack,pcmasterrace,2026-02-05 20:24:14,1
Intel,o3s4yd9,You deserve it!! Have a great life and great fun gaming.,pcmasterrace,2026-02-05 20:33:45,1
Intel,o3s9x36,Good for you man,pcmasterrace,2026-02-05 20:57:39,1
Intel,o3sgqbz,"Congrats on 4 years, enjoy the card!",pcmasterrace,2026-02-05 21:30:30,1
Intel,o3sm9ag,I,pcmasterrace,2026-02-05 21:57:13,1
Intel,o3sq6ok,"it's amazing to see that recovery really is possible. and yeah, you most definitely deserved a treat, man. great job!",pcmasterrace,2026-02-05 22:16:28,1
Intel,o3sxfpx,Congratulations on many levels!,pcmasterrace,2026-02-05 22:53:27,1
Intel,o3sy7f2,Sober 10 end of the month for me. Congrats. Proud of you.,pcmasterrace,2026-02-05 22:57:28,1
Intel,o3t4bji,least reddit title.,pcmasterrace,2026-02-05 23:30:57,1
Intel,o3w0vd2,Lemme make the upvotes 4.8k,pcmasterrace,2026-02-06 12:15:57,1
Intel,o3zvrep,Let's fuckin go homie. Big ups and proud of you 👏,pcmasterrace,2026-02-07 00:07:58,1
Intel,o40moss,Congrats man for real. I’m going on 5 years myself and it’s important to acknowledge and treat yourself. Keep it up man it’s an everyday battle but you’re stronger and better for it.,pcmasterrace,2026-02-07 02:50:58,1
Intel,o42vdg6,"Congratulations bro, being sober is the best feeling ever!5/5/26 will be 9 years for me. Enjoy your PC, I hope it brings you immense joy! 😁",pcmasterrace,2026-02-07 13:52:01,1
Intel,o44sqfk,![gif](giphy|KlCpW0I2Ptd3a),pcmasterrace,2026-02-07 19:45:27,1
Intel,o3lfufj,Don't need your life story to justify buying a budget GPU...,pcmasterrace,2026-02-04 20:32:53,0
Intel,o3kq82j,A bag of smack would be much more satisfying,pcmasterrace,2026-02-04 18:33:34,-4
Intel,o3kmvbh,How many fps in quake champions ultra settings on 2560x1440p?,pcmasterrace,2026-02-04 18:18:31,-1
Intel,o3khg5g,Also ASPM if possible (this also requires a tweak in Windows). This drops idle power consumption significantly.,pcmasterrace,2026-02-04 17:54:00,171
Intel,o3khymz,"do not forget to update bios before installing card  i have a MSI X470 Gaming Plus, i had the bios from 2019 installed and i could not flash the BIOS using the motherboard's flash utility cause cause the flash utility boots using CSM and the B580 does not support booting with CSM enabled",pcmasterrace,2026-02-04 17:56:18,30
Intel,o3o3mvn,"And to have a processor that supports many modern ones, as many do. It's a great 1440p card and great for AV1 transcoding.",pcmasterrace,2026-02-05 05:30:34,2
Intel,o3qn7f0,thanks! i will keep it in mind! im gonna install the gpu this weekend,pcmasterrace,2026-02-05 16:23:56,2
Intel,o3omnar,What does this do,pcmasterrace,2026-02-05 08:15:40,1
Intel,o3knql5,I see what you did there,pcmasterrace,2026-02-04 18:22:23,9
Intel,o3lbvgu,"Enjoy every byte, you’ve earned this victory meal",pcmasterrace,2026-02-04 20:13:56,1
Intel,o3n3szi,![gif](giphy|TaBRY9MzqfoDHfDrcx|downsized),pcmasterrace,2026-02-05 01:46:43,5
Intel,o3n3z9w,"Fr. Based on looks alone, I would buy the gpu (if I could afford it)",pcmasterrace,2026-02-05 01:47:44,3
Intel,o3pn1i7,Frames are only thing that matters and sadly it is quite bad one.,pcmasterrace,2026-02-05 13:19:26,1
Intel,o3tbmzc,"Eh, looks kinda boxy.",pcmasterrace,2026-02-06 00:12:21,0
Intel,o3l1cti,Oh we are doing gpu fashion now?,pcmasterrace,2026-02-04 19:23:57,-41
Intel,o3kly2w,PC gaming is the ultimate distraction. Better to be addicted to frames than anything else.,pcmasterrace,2026-02-04 18:14:22,15
Intel,o3l5bw5,I’ve enjoyed mine and the best part is performance has been steadily improving as driver support has made leap and strides from intel,pcmasterrace,2026-02-04 19:42:40,2
Intel,o3km4j7,"I guess he wanted to share that he had recently recovered, and that pc building is evidence that he’s doing a lot better now",pcmasterrace,2026-02-04 18:15:10,3
Intel,o3qfayw,Strangely relatable...never really thought of owning an Intel CPU this way 🤣. But boy do I sure have it locked in now after x-teen revisions and stress tests!,pcmasterrace,2026-02-05 15:47:39,1
Intel,o3kyllc,I jerk off daily,pcmasterrace,2026-02-04 19:11:11,7
Intel,o3l25px,"I have this card and I'm satisfied. I have no problems with playing games I want, and I actually used the previous flagship intel card (a770) for machine learning for my engineering degree (python + tensorflow)",pcmasterrace,2026-02-04 19:27:42,2
Intel,o3qfqca,2080 ti tier performance.,pcmasterrace,2026-02-05 15:49:37,2
Intel,o3li5jk,You sad pathetic fuck,pcmasterrace,2026-02-04 20:43:55,-11
Intel,o3kkqe7,"Oh thanks for that tip. I got one of these last week and did the rebar setting, but it’s definitely using more power than my old card. Will have to look into ASPM later today",pcmasterrace,2026-02-04 18:08:55,33
Intel,o3lcc5m,"Does this work for all cards (AMD, NVIDIA, etc)?",pcmasterrace,2026-02-04 20:16:08,2
Intel,o3lgaub,"The difference between 10W and 30w is an interesting thing to be concerned about.  Unless you're running on batteries, what's the big woop? It's negligible in power cost.",pcmasterrace,2026-02-04 20:35:02,1
Intel,o3kwzmw,Same. Was tough to learn lol,pcmasterrace,2026-02-04 19:03:50,2
Intel,o3lwue1,Nothing gets past you,pcmasterrace,2026-02-04 21:53:54,3
Intel,o3l23qs,"Well, I’m out. There is nothing fashionable about my GPU except the frames.",pcmasterrace,2026-02-04 19:27:27,21
Intel,o3pn3s6,These are the modern PC users. About looks. Not actually gaming. This sub is full of weirdos having 5k PCs and they play less than 2h weekly.,pcmasterrace,2026-02-05 13:19:48,0
Intel,o3wmrjh,I completely agree. Addicted to frames or min maxing fitness as well lol.,pcmasterrace,2026-02-06 14:24:18,1
Intel,o3o73iz,"XeSS 3 is coming soon, which with some tweaks you can get working but apparently it's a huge jump compared to 2.",pcmasterrace,2026-02-05 05:57:29,1
Intel,o3wm6h7,"Luckily I always had strict power limits to where with old microcodes I never had my voltage spike dangerously high but I also used to use the i9 on a b760 board that wouldn’t really allow really high vid requests, now that I’m on a z790 board on 0x12f I can get 38,000 points In r23 cinebench at 200 watts.",pcmasterrace,2026-02-06 14:21:15,1
Intel,o3leuv8,"That great! I checked the specs and it's better than any of my cards. I wish I had more than 4GB VRAM in 2026 lol.   How does it handle DLSS? In hindsight I think I got it confused with the many years of PhysX being the coolest thing, because THAT was very proprietary.",pcmasterrace,2026-02-04 20:28:11,2
Intel,o423wni,thank you 🙏,pcmasterrace,2026-02-07 10:12:32,1
Intel,o3kpohs,For me it dropped 10W while on 3440x1440@144 but at 60Hz it’ll do sub 10W idle for the whole card.   Without ASPM you are looking at 40W idle.   For AM4 this isn’t so common in the BIOS but luckily my Gigabyte B550 has this option exposed.  https://www.intel.com/content/www/us/en/support/articles/000092564/graphics.html,pcmasterrace,2026-02-04 18:31:08,13
Intel,o3lrs3n,It’s the difference between the fans spinning and the fans not spinning. Also it’s probably what the PC will do most of the time and if you pay for your electricity then it’s saving money just by changing a setting.,pcmasterrace,2026-02-04 21:29:49,9
Intel,o3l20p9,"i did not expect to not be able to access BIOS settings with CSM enabled  * i have to use CSM with my RX 580 cause i modded the vbios (fan profile stuff; also cooling modded) and even though secure boot is disabled it still has to pass some sumcheck handshake BS  change card back to toggle CSM, lets see how bad this is without rebar, have to change card back to flash bios, redo all my bios settings...",pcmasterrace,2026-02-04 19:27:03,1
Intel,o3l2tak,Nah bruh you don't get it fashion is pain hence why my cores have bad cooling so I know it is cooking in the looks,pcmasterrace,2026-02-04 19:30:46,-12
Intel,o3lnh3n,"DLSS is Nvidia proprietary. There is an Intel equivalent called XeSS and it actually worked quite well when I tested it in few of the games I played (for e.g. cyberpunk, expedition 33). It is comparable to Radeon's FSR, there are plenty of YouTube videos comparing the 3 standards.  In practice it depends on the game but IMO DLSS is a little bit better than the other two, and with wider support. But for me raw non AI performance and cheaper price (and dislike for NVIDIA practices) made the trade-off worth going with Intel",pcmasterrace,2026-02-04 21:09:15,2
Intel,o3l2pr4,Is this only for Intel cards or does this work for any GPU brand?,pcmasterrace,2026-02-04 19:30:18,3
Intel,o3m9g4v,Thanks! I habe a b580 and noticed it always ifles at 50c😂. Should I use the L0 or L1 option? Theres also an option that says L0 ans L1...,pcmasterrace,2026-02-04 22:57:14,2
Intel,o3ppt4t,How much do you predict a gpu would save power bill wise if zero rpm fan mode were enabled vs disabled ? I always disable it because I figure it’s easier to leave the fans running at 20% 90% of the time then 0% and ramping the fans up dozens of times per day on and off and save the fan bearings long term but maybe I’m wrong.,pcmasterrace,2026-02-05 13:35:27,1
Intel,o3mnc9f,"That makes sense.   I have one of the most expensive 5090s available, and if I leave it in adaptive power mode so that the fans can stop spinning I get hardcore driver crashes.   GG NVIDIA and ASUS",pcmasterrace,2026-02-05 00:13:40,1
Intel,o3l49cw,Redoing all the bios is one of my biggest pet peeves.,pcmasterrace,2026-02-04 19:37:38,2
Intel,o41qebx,"you made it look like a choice, but there is no choice really is it. B580 is worse than both a 5060ti or a 9060xt, so if you're looking for actual computational power I wouldn't put it as Intel being an actual contender, since it has no ""flagship"" options. Maybe for the price point Intel may be worth compared to it's counterparts from AMD/Nvidia, but even then the software the other two bring, and most importantly nvidia make the card better",pcmasterrace,2026-02-07 08:00:02,1
Intel,o3l5den,AMD and Nvidia have their own thing going in the driver. Iirc currently it's only Intel who is following established industry standards in this regard.,pcmasterrace,2026-02-04 19:42:53,3
Intel,o41svu0,Nvidia cards literally have a switch on the GPU itself for quiet mode and performance mode.,pcmasterrace,2026-02-07 08:23:46,1
Intel,o3ogwv5,The linked support page mentions L1 is required to be enabled.,pcmasterrace,2026-02-05 07:22:12,1
Intel,o3pqkq8,Spinning up and down was a concern with old harddrives but with modern bearings and thus with modern fans it really isn’t anymore.  Usually the bearings have an hour rating and every time the fan isn’t spinning it won’t wear on the bearing.    If you don’t change the fan curve then the main power saving isn’t the power the fans use but the reduced GPU power that allows 0 rpm in the first place.,pcmasterrace,2026-02-05 13:39:50,1
Intel,o3l544j,screenshots of all setting,pcmasterrace,2026-02-04 19:41:40,3
Intel,o3lgkqt,"[""Established industry standards"".](https://xkcd.com/927/)",pcmasterrace,2026-02-04 20:36:20,0
Intel,o3ptr2u,"So a gigabyte gaming oc 9070xt would you recommend I use zero rpm fan mode? The temps are so low with this card the gpu never goes above 51c under load 81c hotspot and idles with the fans on at 28c with no fans l think it idles around 40c, I care less about saving on my power bill and more about making the card and its 3 95mm fans last the next 10 years if possible, I do keep it undervolted with -0.100mv negative offset and  power limits -20% (8 % less performance and when over 140-160fps I don’t notice the difference without telemetry)",pcmasterrace,2026-02-05 13:57:39,1
Intel,o3l5hn9,A must.,pcmasterrace,2026-02-04 19:43:25,1
Intel,o3lpsy0,"ASPM is in the spec since PCIe 2.0, so for 19 years now. I’d call that established.",pcmasterrace,2026-02-04 21:20:22,10
Intel,o3pxaye,I personally do it mostly to silence the system when idle.    You can always replace just the fans (or ziptie Noctua 92mm fans with a adapter) if you want to use the card longer. So I wouldn’t worry about wear too much.,pcmasterrace,2026-02-05 14:17:13,1
Intel,o47e45l,"Intel is the better option, more cores and threads.",pcmasterrace,2026-02-08 05:11:19,1
Intel,o46odv2,Amd option is better for upgrade path I say. Intel is already changing chips again so it’s going to be dead by that time. The monitor also can be a acer for Best Buy I saw for 155 that’s usually 390.,pcmasterrace,2026-02-08 02:16:51,0
Intel,o472oxi,"AMD. You get 32gb of RAM instead of 16gb with the intel option. With the prices of RAM atm, the 32gb is a huge W",pcmasterrace,2026-02-08 03:48:57,0
Intel,o40wfym,That MB has some coil whine. It's a noisy boy. But it doesn't have any performance issues,pcmasterrace,2026-02-07 03:54:24,1
Intel,o413cth,When a deal looks 2 good you should do research  https://preview.redd.it/bwwl7mcb50ig1.png?width=1388&format=png&auto=webp&s=bb0b40a6ea413168a43e16d994d675c480640156,pcmasterrace,2026-02-07 04:43:33,0
Intel,o423xkg,9600x is not a good choice. you bought it nevertheless so live with it. it looks like you're familar with amd cpus so you didn't consider the [225f](https://www.cpubenchmark.net/compare/6457vs6199/Intel-Core-Ultra-5-225F-vs-AMD-Ryzen-5-9600X) which performs similarly but is cheaper and newer,pcmasterrace,2026-02-07 10:12:47,0
Intel,o3paah8,Nice build. X99 platform seems to have aged well.,pcmasterrace,2026-02-05 11:53:16,40
Intel,o3pap2q,"I didn't know DDR3 was supported by this processor. I have the 2699v4 in an old server but I'm using DDR4. the upgrade from a V3 to a V4 was well worth it back then, so maybe it's something to consider for your build.",pcmasterrace,2026-02-05 11:56:20,26
Intel,o3pfc8j,Quick tip for people considering x99: the e5 16xx are slightly nicer for gaming while costing less than the 269x cpus.    There are some 26xx that outperforms though and it depends on your locations deals.   Nice build though. Super solid for 1080p gaming. X99 has really held up.,pcmasterrace,2026-02-05 12:30:06,16
Intel,o3pg2n9,Good job man... That's a nice build... And I see Stellar Booty... xD,pcmasterrace,2026-02-05 12:35:12,6
Intel,o3pbgh9,how fast is the xeon compared to consumer cpus like i7-5775c,pcmasterrace,2026-02-05 12:02:01,5
Intel,o3psosk,"There was definitely a bell, this old ass ram just wasn't listening",pcmasterrace,2026-02-05 13:51:44,3
Intel,o3psnm8,"Huh, didn’t realize they made x99 boards that support ddr3. Did it also have re-bar modded in, or did you have to modify the bios yourself?",pcmasterrace,2026-02-05 13:51:34,2
Intel,o3puxm2,Damnnn that CPU is sleeping but the FPS hits hard!,pcmasterrace,2026-02-05 14:04:14,2
Intel,o3pvu2w,"Wait, don't you need direct access memory to run an A770 well? And the earliest is 10th gen?",pcmasterrace,2026-02-05 14:09:12,2
Intel,o3plgnr,btw how much slower is this (fps wise) compared to if you used 64gb of ddr5?,pcmasterrace,2026-02-05 13:10:01,1
Intel,o3pnpxo,My 28 core xeon was $20+ shipping and came with thermal paste. The good kind as well.  Tripple digit amount of RAM in 8 slots and PCIe lanes for days...,pcmasterrace,2026-02-05 13:23:26,1
Intel,o3po1l6,"Ah such a cool budget build, reminds me of my teenage days, good times",pcmasterrace,2026-02-05 13:25:18,1
Intel,o3pycib,Meanwhile I paid 150€ for 24GB sodimms for my new 7800X3D.🥲,pcmasterrace,2026-02-05 14:22:49,1
Intel,o3pzz5x,Ayy welcome to X99. Have you tried flashing with undervolted BIOS?,pcmasterrace,2026-02-05 14:31:30,1
Intel,o3qfgpr,"What were your settings in Clair Obscur ? I play in 1440p as well but on a 3060ti, I didn't remember the A770 to be that fast. Was there any upscaling/frame gen ?   I'm on mid/high settings and use DLSS quality and get about 70 fps depending on the area.",pcmasterrace,2026-02-05 15:48:24,1
Intel,o3rntrm,What is the second game?,pcmasterrace,2026-02-05 19:12:49,1
Intel,o3sdmcp,I paired 9070xt with xeon E5 2667 v3 and. 64gb ecc ddr4 and i can run everything max at 1440,pcmasterrace,2026-02-05 21:15:35,2
Intel,o3tc0we,Congrats on the average Facebook Marketplace build.,pcmasterrace,2026-02-06 00:14:33,1
Intel,o3pwj08,YES to all of this.,pcmasterrace,2026-02-05 14:12:58,1
Intel,o3poha5,https://preview.redd.it/8cfy8ot1hohg1.jpeg?width=2982&format=pjpg&auto=webp&s=b9432080e13f409e6d60c58c2adad504b278488a  Same Case i think?,pcmasterrace,2026-02-05 13:27:48,0
Intel,o3q06g4,"I think it's funny you can put an old computer in a new case to make it look ""modern"". Honestly, why not?  I have an i7-4700 something in the garage. I think I have an idea for a new build. :)",pcmasterrace,2026-02-05 14:32:36,0
Intel,o3pewxu,This is the list of known DDR3 compatible X99 CPUs  Support DDR3:   v3 CPU's (Turbo boost unlock available)  E5-2629 v3 - 8/16 - 2.4/3.2Ghz  E5-2649 v3 - 10/20 - 2.3/3.0Ghz  E5-2666 v3 - 10/20 - 2.9/3.5Ghz  E5-2669 v3 - 12/24 - 2.3/3.1Ghz  E5-2673 v3 - 12/24 - 2.4/3.1Ghz  E5-2676 v3 - 12/24 - 2.4/3.0Ghz  E5-2678 v3 - 12/24 - 2.5/3.3Ghz  E5-2696 v3 - 18/36 - 2.3/3.8Ghz    v4 CPU's (No turbo boost mod)  E5-2686 v4 - 18/36 - 2.3/3.0Ghz   E5-2696 v4 - 22/44 - 2.2/3.6Ghz,pcmasterrace,2026-02-05 12:27:07,11
Intel,o3pfl0w,You can unlock the v3 for overclocking though. Definitely a benefit if your cooler/psu can take 140w++.,pcmasterrace,2026-02-05 12:31:47,7
Intel,o3q8q3q,"Ok, I’m glad I wasn’t the only one a bit curious about that. I have a server running E5-2683v4’s and it also uses DDR4",pcmasterrace,2026-02-05 15:16:27,2
Intel,o3ph50j,"Definitely. If you can get some cheap DDR4 ECC (They are not compatible with DDR3), they make a good build.",pcmasterrace,2026-02-05 12:42:27,5
Intel,o3pfhak,For gaming? Probably i7 4790 plus a few extra cores good.   Maybe ryzen 1st gen level.,pcmasterrace,2026-02-05 12:31:04,7
Intel,o3pdqty,It's  more equal to a Ryzen 3600  https://preview.redd.it/btazuo5r4ohg1.png?width=523&format=png&auto=webp&s=2e0c773f6907cd320b2025053b2b45ddf56bd4d4,pcmasterrace,2026-02-05 12:18:55,3
Intel,o3t3w5n,Outside of its frequenzy range.,pcmasterrace,2026-02-05 23:28:32,1
Intel,o3pu41h,The newer x99 China boards already have rebar in the bios. The only change with the bios flash was to enable turbo boost unlock.,pcmasterrace,2026-02-05 13:59:38,3
Intel,o3pwpl7,"Yep, thanks to feature unlocks made in the bios, that ship on China x99 boards. this can be enabled.",pcmasterrace,2026-02-05 14:13:59,3
Intel,o3pmyc9,Can't really compare since a kit of DDR5 64gb would cost more than the total build. there's gaming screenshots attached.,pcmasterrace,2026-02-05 13:18:56,3
Intel,o3s9ibx,-50/-50/-60 is the undervolt I used.,pcmasterrace,2026-02-05 20:55:39,1
Intel,o3qqyxz,He is using frame gen on almost all the titles.,pcmasterrace,2026-02-05 16:41:26,2
Intel,o3pi1ej,True. Forgot about that. DDR4 RDIMMs were super cheap last year. Now it's like $20 for 8gb. Kinda sad I sold my 24gb extra early. Could get like $60 now...,pcmasterrace,2026-02-05 12:48:21,2
Intel,o3pg8lw,"Yeah, if I remember it scores slightly faster than a 1800x in single core",pcmasterrace,2026-02-05 12:36:22,2
Intel,o3pekqp,"In multicore yes, because it has a shitton of cores to compensate lower clocks, but the moment it has to play a semi-modern game that only uses 1-4 cores it falls apart very quickly.",pcmasterrace,2026-02-05 12:24:46,9
Intel,o3qxwgl,"Thanks, somehow I missed the only part I cared about.",pcmasterrace,2026-02-05 17:13:52,2
Intel,o3pgyzi,Sounds about right. The cache helps for gaming too.,pcmasterrace,2026-02-05 12:41:20,3
Intel,o3pllj3,"It is interesting to see as my 5600G scores 11285 in R23 multi and 1434 in single, so the massive Xeon is 10% slower multi and almost half the performance in single core with using double the amount of power. Still very capable for a decent amount of loads, but as you say it tanks for gaming. Great it has all the RAM bandwidth you could ever desire as well as PCIe lanes up the wazoo, but for most consumers we never use those. I would like to try one out just for fun, see what it actually is like using it, but I can't really find a use case for it.",pcmasterrace,2026-02-05 13:10:49,5
Intel,o3pi4qb,Just for you.  https://preview.redd.it/0bvrvpb4aohg1.png?width=517&format=png&auto=webp&s=091df8782bf7b23e094027eee1343b0c26025eed,pcmasterrace,2026-02-05 12:48:59,0
Intel,o3s3uae,"Medium, and xess on balanced is over 60 fps at 1440p. Framegen just smooths it out.",pcmasterrace,2026-02-05 20:28:29,2
Intel,o3pnxa2,"Single core is it's biggest problem, and actually the architecture itself. People don't realise that these old high end chips are great for budget builds, but they consume a lot of power for the performance, and the instruction sets won't allow for some programs to run, orthey will run but slow due to some instructions have to be simulated.   Around 2020 I used to recommend cheap X99 bundles on Newegg or Ali to people on a budget, but knowing that for the same 55 dollars you can pick up a Ryzen 5 3600 or just spend less on RGB and get a 3700X for 100$ than I can not see the purpose of these old Xeons and 1-6th gen Intel builds.",pcmasterrace,2026-02-05 13:24:37,4
Intel,o3pntyx,"This was to build something cheap with readily available parts showing it can game. 32gb of DDR4 costs the more than what i paid for the CPU/MB/Ram here.  I have access to much faster components, this was more of a ""curiosity"" build.",pcmasterrace,2026-02-05 13:24:05,1
Intel,o3s4csp,"Thanks, it made me question my rig and it's performances",pcmasterrace,2026-02-05 20:30:53,2
Intel,o3pq6rj,"Give me a title to run, remember, i'm also handicaped with an A770.",pcmasterrace,2026-02-05 13:37:36,3
Intel,o3psys8,"Exactly. I use a parts bin system as a multimedia PC and that one really struggles with missing instruction sets. I hardly ever hit that wall, but if I do there is nothing you can do to solve it. It really is the price of DDR4 that is killing the 3600(X) route, otherwise that would be such a great bargain right now.",pcmasterrace,2026-02-05 13:53:18,1
Intel,o3psgbc,"Thatś why I am happy I got my 5600G built done with 32GB when it was under €100. I could have made the build even cheaper, but it was about the cheapest I could get for the performanceis gives me. With the same SSDs, 770, I would have ended up around the same price as what you paid. I went with different priorities.  Now the same RAM kit is €285, clearly that plan would instantly fail pushing towards Xeon systems hard. Don´t get me wrong, I have a soft spot for builds like this, but unless the market is wonky, they kind of never make much sense unless you are in an area where prices are terrible anyways. I think it looks really good for a curiosity buid as well. Plus, gaming on great hardware fun, but you expect it to perform wellfor the price you paid for it. There is nothing better than messing around with the underdog and seeing how much you can squeeze from it.  I wonder where you even found DDR3 64GB for $40 as those prices also went up a lot in the lastfew days/weeks. I see similar kits nearing €200 already and that is just insane.",pcmasterrace,2026-02-05 13:50:26,2
Intel,o3pt3qg,Facebook market place. He even gave me 2 sticks of 8gb ECC because he couldn't be bothered listing them. lol     My 4090 is looking at me wondering why i'm tinkering with this old stuff,pcmasterrace,2026-02-05 13:54:03,2
Intel,o3pv7nh,That is a great deal to find. That makes a lot of difference in the total cost. Getting a spare set because he couldn't be bothered is icing on the cake.,pcmasterrace,2026-02-05 14:05:46,2
Intel,o3bgzd3,"With the sheer amount of fans in there, that ain't a pc, that's a Cyclone Dryer like the ones you see at waterparks😅 Nonetheless, awesome first build dude! At least you won't have to worry about temps lol",pcmasterrace,2026-02-03 10:14:43,7
Intel,o3bzykl,looks similar to my PC haha that I just built last month  https://preview.redd.it/qb7bu4r40ahg1.jpeg?width=4096&format=pjpg&auto=webp&s=078bfeedf7ab1e1ef8e2bc7f73bf505434a66b3d,pcmasterrace,2026-02-03 12:48:01,2
Intel,o3bixy9,I call cap on this being your first build,pcmasterrace,2026-02-03 10:33:12,2
Intel,o3bgzdf,"Very clean, nice",pcmasterrace,2026-02-03 10:14:43,1
Intel,o3bkmgw,Is it loud?,pcmasterrace,2026-02-03 10:48:39,1
Intel,o3bsod9,"Woah what a beauty, I also recently built mine, turned out similar but not even close to looking as good as yours  https://preview.redd.it/xb4nhr3wq9hg1.jpeg?width=3060&format=pjpg&auto=webp&s=e23e80b371a86c4e22feeba853c9afbd283e5aa6",pcmasterrace,2026-02-03 11:56:14,1
Intel,o3bzzpm,https://preview.redd.it/lf7o3v360ahg1.jpeg?width=3072&format=pjpg&auto=webp&s=db595158bc17a8bff93edcb748966ec71fef596f,pcmasterrace,2026-02-03 12:48:13,1
Intel,o3bnsxk,It is my first build. I have been doing my research for more than a year now and i bought 1-2 parts each month over a period of time due to budget constraints. Then finally this month i had all the parts and time to build this PC.,pcmasterrace,2026-02-03 11:16:38,10
Intel,o3c0281,https://preview.redd.it/xoqm0yw80ahg1.jpeg?width=4096&format=pjpg&auto=webp&s=094ba895fa1769ef4c9da72493c3f9c7075d26e9,pcmasterrace,2026-02-03 12:48:40,2
Intel,o3caq25,Then you did a really good job. People build pcs for years and can’t do it that clean. Here’s mine btw (also first build). Can’t see a lot but I’m too lazy to go take another one lol  https://preview.redd.it/mx7v0fbfbahg1.jpeg?width=4032&format=pjpg&auto=webp&s=ec80b48e1d0e4c2d00421bf92edd720527815064,pcmasterrace,2026-02-03 13:51:40,5
Intel,o4930it,"I keep two boxes - mobo & GPU. All small boxes, unused cables, checks and warranties, ties, screws and thermal paste syringes are stored inside those two. Case box is too big to keep it and both PSU & CPU cooler boxes are neatly folded inside mobo box.  That's just two boxes filled with stuff but I have everything I'd ever need in case of RMA or if I'm reselling my stuff.",pcmasterrace,2026-02-08 13:54:08,1
Intel,nz9lefm,you must have some huge hands judging from the apparent size of that box,pcmasterrace,2026-01-13 00:45:35,2
Intel,nzakbj4,"Holy shit my hands look like a giant Hold on ill get you a pic with me actually holding the BOX, not the gpu.  https://preview.redd.it/kkndv19xh1dg1.jpeg?width=4000&format=pjpg&auto=webp&s=924d6b75377cecc6a98ac1bae1a765b4349dc025",pcmasterrace,2026-01-13 03:55:11,3
Intel,o43yp8l,new looks empty with that 2-slot gpu.  and the rear exhaust doesnt really work. aio will dump all the air before anything reaches there.,pcmasterrace,2026-02-07 17:15:28,1
Intel,o447asj,Server time,pcmasterrace,2026-02-07 17:58:05,1
Intel,o2o4wia,"That’s not a bad build for your needs whatsoever. You will struggle to reach 240fps in anything outside of GD, but I’m guessing that’s why you got the monitor. You will be able to play most modern games at 1080p 60FPS, setting obviously won’t be perfect. Editing may be a bit laggy though",pcmasterrace,2026-01-30 20:33:27,2
Intel,o2o9ckb,"If it works and you’re complacent with your gaming sessions, no need to upgrade. Once something starts to slow down (esp when you optimize your pc) then maybe start upgrading.",pcmasterrace,2026-01-30 20:54:47,2
Intel,o46lhyy,I have 1650 playing gta 5 it reach 70 and when playing icarus it reaches 80 to 90,pcmasterrace,2026-02-08 01:58:44,1
Intel,o46m43c,"As long as you’re getting good performance, the power usage is normal, the load is normal. You are absolutely fine. It just means you have adequate cooling.   Most GPU temps are highly dictated by case airflow.",pcmasterrace,2026-02-08 02:02:34,1
Intel,o46mc1y,Are you plugged into the gpu? Are you sure the gpu utilization is your graphics card and not the Igpu?,pcmasterrace,2026-02-08 02:03:57,1
Intel,o46q4it,Are you really a gamer if you can't roast a marshmallow on your gpu?,pcmasterrace,2026-02-08 02:27:49,1
Intel,o4763hx,"GPUs typically have a thermal limit of around 85-90c, with thermal shutdown at around 95c.   Anything below that is perfectly fine.",pcmasterrace,2026-02-08 04:12:12,1
Intel,o4cmfyi,"Honestly, that either means you aren't actually using it, or your case airflow is amazing. Too low temp is only an issue well below 0ºC, which is not happening here.",pcmasterrace,2026-02-09 00:49:55,1
Intel,o46o5bj,I am running a 5600x sadly. Very underpowered CPU compared to the GPU,pcmasterrace,2026-02-08 02:15:21,0
Intel,o476f2l,"Thats a perfectly fine CPU.   There's also not much you can upgrade to, other than 5700X or 5800XT and the improvement isn't really going to be huge.   You'd have to upgrade to AM5 on a 9700X or 9800/9850X3D to see any significant performance improvements.",pcmasterrace,2026-02-08 04:14:27,1
Intel,o40yfmz,"I'm guessing it either has to do with the anti cheat those 2 games use (don't quote me because I'm not actually aware what/if anti cheat they use.)  If my understanding is correct, the kernal is what allows the hardware to communicate with the software, so I believe that would mean this has to be some kind of software issue. I believe that could also mean its a problem with your OS and it coincidentally only occurred on game 1 & 4 meaning it would have nothing to do with the developer of Arc if you tried them in the order you listed them. How long did you run the two games that worked?  EDIT: After a quick research, kernal security check can absolutely be triggered by faulty hardware. Id double check everything is connected properly and that there are no blinking lights on the mobo.",pcmasterrace,2026-02-07 04:08:09,1
Intel,o40ywxa,"Windows Memory diagnostic alone is not enough to conclusively say your RAM has no issues. KERNEL_SECURITY_CHECK_FAILURE essentially just means that an application attempts to read information from your system that it's not allowed to and/or is corrupted and Windows, not knowing what to do as a result of this, basically just terminates itself.  I'd try removing 1 RAM stick and see if this works to fully rule out faulty RAM. Do these 2 games have any anticheat or have you modified them in any way?",pcmasterrace,2026-02-07 04:11:33,1
Intel,nyiyx49,I'll believe them (either Intel or AMD) when I see the benchmarks.  Until then this is all just pointless noise.,AMD,2026-01-09 02:57:02,212
Intel,nyj1h7b,well yeah you can't compare them because strix halo is on a signficantly larger die wheras panther lake is more comparable to something like the hx370.   If amd is able to get strix halo at a competitive price then sure it will compete but the issue is that with such a large die I don't think it is possible for them to compete in price with panther lake,AMD,2026-01-09 03:11:08,57
Intel,nyj2ydj,I’ll never understand why AMD is not committing to design RDNA4 based APUs and at this point I just take RDNA 3.5 as a joke because they can’t even support FSR4 on it officially nor the RX 7000 cards.  It’s like they are losing on purpose,AMD,2026-01-09 03:19:12,62
Intel,nyja8ig,"AMD has this “it’s good enough for a while and we’ll release something great that people will forget this happened”  Vega lasted in mobile for nearly 5 years and got RDNA2 designs. Now, it’s RDNA3.5 being built for mobile platform and betting on that to be good enough until RDNA5/UDNA bridge die designs releases (unverified rumor)  AMD also has this weird obsession with competitor naming. Sure, it’s meant to confuse buyers but it’s hurting them than helping, maybe it does help in terms of inventory.  They’re not intel-like of stagnation. They’re competing but not for us in the consumer market and we’re just getting scraps until enterprise trend die down (currently AI trend/bubble).  Well, it’s understandable as Zen designs are really focused in Epyc and scale down to Ryzen SKUs.  And the 400 series is a bad refresh when Ryzen 6000 mobile is the definitive refresh they have done, Zen 3+ and move to RDNA2. AMD could’ve done similar commitment but it’s not currently.  Also, AMD forgor Strix Halo laptops are still nowhere to be found aside from 1 or 2",AMD,2026-01-09 04:00:20,12
Intel,nykgnvm,"Except that the B390 will be far more common as it will be seen in far more laptops. Yes, the 8060S & 8050s can be found in some laptops, but for the laptops you'll find in places like Currys, Best Buy or Mediamarkt, the B390 will be the most powerful iGPU you'll likely find & it'll happily outdo a Radeon 890M",AMD,2026-01-09 09:30:29,6
Intel,nyj02vs,Amd really doesn’t gaf about anything other than data centre these days,AMD,2026-01-09 03:03:22,11
Intel,nyjy5yc,Idgaf when Strix Halo products are nowhere to be seen (notebooks),AMD,2026-01-09 06:46:12,6
Intel,nyj9p6p,"AMD is at the point where Intel was before they went down the route and are recovering, history repeats before it's too late.  Not going to believe either until we get actual benchmarks and results.",AMD,2026-01-09 03:57:09,8
Intel,nynggco,"AMD is playing the same intel book a few years ago. Except now instead of 14nm+++++++, it is RDNA 3.5555555.",AMD,2026-01-09 19:23:52,3
Intel,nz0u8e4,Meanwhile AMD keeps putting out new chips with years old GPUs.,AMD,2026-01-11 19:18:48,3
Intel,nyjg6co,Core Ultra 2 is already a better mobile soc.  I don't know why AMD thinks 12-16 cores is more important than battery life when it comes to laptops.,AMD,2026-01-09 04:35:45,10
Intel,nykbcoh,The Intel igpu has a better upscaler by far. FSR3.1 is a third class competitor in comparison. People have been crying out for AMD to release FSR4 for RDNA3.5 but AMD has some seriously stupid execs in charge.,AMD,2026-01-09 08:41:31,4
Intel,nyjsbo1,It does sound complacent but ultimately the proof is in the pudding.,AMD,2026-01-09 05:59:29,2
Intel,nyku0xe,"If it is not even fair to compare (because Strix Halo is WAY more watts) then why is AMD comparing them? B390 will exist, Strix Halo virtually does not in laptops.",AMD,2026-01-09 11:27:40,2
Intel,nykofoz,Lot of markets and Intel did bribe the oems for decades and still do,AMD,2026-01-09 10:40:44,3
Intel,nyjsl7o,Benchmarks first,AMD,2026-01-09 06:01:32,1
Intel,nyn2qhh,Beware hubris.,AMD,2026-01-09 18:22:54,1
Intel,nynbvp5,"There are already benchmarks, look them up.",AMD,2026-01-09 19:03:00,1
Intel,nynoj5f,Don't they already have an integrated GPU that's on par with an RTX 4060? According to Framework?,AMD,2026-01-09 20:00:51,1
Intel,nyorjcc,"strix halo is nice and all, but too prohibitively expensive to be considered for many people  arc b390/b370 will be available in much cheaper products for which amd doesn't have a proper answer to atm. amd's next lineup can't be lazy if they want to stay competitive",AMD,2026-01-09 23:05:05,1
Intel,nyqdhsx,"The GPU doesn't matter if you don't have proper drivers and they are so far behind still Intel.   Great progress, but the drivers are still going to be the thing that makes people say no.   If intel keeps on chugging away and they work with all the DirectX games backwards and going forward.   I'm talking past DirectX games, you can't just worry about the new games there's games that are older that don't run well.   It took AMD many many years to get decent drivers, Intel I don't know if they're just focusing on hardware and not the drivers, but that so far is what's been holding it back.   Hopefully they can release a true dedicated GPU back in rival something that's out there at a much better price that will bring at least some competition back until the AI scam is over.",AMD,2026-01-10 04:30:09,1
Intel,nzbi19w,Panther Lake is using a superior process technology. So they are right. But it doesn't matter as customer will choose what's better. But until AMD has something out that uses 2nm then yes they will be behind probably,AMD,2026-01-13 08:12:42,1
Intel,nzgbs70,I hope Intel stays competitive and AMD also brings its best to the table.,AMD,2026-01-14 00:33:01,1
Intel,nyj94g8,They should be worried about DLSS 4.5 though. Fix stuttering on FSR 4 and improve image quality,AMD,2026-01-09 03:53:50,1
Intel,nyj4pf7,I just wish Intel would make a very cut down panther lake offering to be the successor to the N1xx/N3xx line of efficient chips that have found their way into mini PCs.,AMD,2026-01-09 03:28:57,1
Intel,nyoak0m,amd unfazed? I bet they are talking big shit again then will get absolutely demolished (as it happened with vega too),AMD,2026-01-09 21:42:52,1
Intel,nyjappo,"Yeah it's old architechture, that's the point, Intel moved to the lastest node and barely manages to eek out a win. A win is a win nonetheless, but AMD still have plenty to dials to turn up.",AMD,2026-01-09 04:03:08,-6
Intel,nyju3k9,"I don't understand the fuss about iGPUs? Like why do they assume the average Joe would care about an IGPU at all? That's maybe 5% of the market and even then...most of them would get a dGPU anyways.  And apart from the GPU, what's special about the CPU? Combining (Lunar Lake) efficiency with (Arrow Lake) power? Sorry but my Ryzen AI 7 350 does that already. The top of the line x9 388h is about ~10% faster in single core aka the only thing that matters and will probably be in 2500€+ laptops whereas my 7 350 is in 500€ laptops.  I tried a 285h laptop besides the AI 7 350 and not only did it run hotter and less efficient, it also felt less snappier.    And the AI 7 350 was designed as a Lunar Lake competitor anyways so it was never worse in efficiency and ahead of Arrow Lakes like the 255h in that regard.  So I don't see why anything should really change...?",AMD,2026-01-09 06:13:30,-11
Intel,nyj661m,"AMD doesn't want to bring RDNA 4 to APUs, so as not to give FSR4 to users other than those with dedicated GPUs.",AMD,2026-01-09 03:37:14,-6
Intel,nyjyeyc,a brand new product on a newer node is better than an older product on an older node?! who knew?,AMD,2026-01-09 06:48:14,-2
Intel,nyj7q2f,"And the price. I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand. Hope Intel gets back into the game and if Intel can slash their prices it might end-up being the right choice.  At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.",AMD,2026-01-09 03:45:51,89
Intel,nylsap7,"Both are right/wrong.  Intel made their comparisons to Strix point because they’re in the same power class. Panther lake is much faster than Strix Point at the same power level (according to Intel, AMD doesn’t deny that) at 45W.  AMD says it doesn’t matter because their Strix halo (up to 120W) is faster which is pretty obvious.   It’s not technically lying, AMD is just referencing an entirely different class of product.",AMD,2026-01-09 14:52:54,5
Intel,nyjgluo,Plenty of folks tested it at CES.  Intel was confident enough to let reporters run benchmarks and it's basically around 4050 level.  You should be able to run most games at 1080p at medium-high settings in an Ultrabook form factor.,AMD,2026-01-09 04:38:25,19
Intel,nyj2b71,"Well lunar lake is a monster and competes directly with the z2e both on performance and efficiency, so no reason to think panther lake will be worse.  Even if it falls short of Intels claims it will still be the leader until next year.",AMD,2026-01-09 03:15:43,21
Intel,nyjcak9,The noise is doing a great job advertising for them. A war between them with fighting words will get them tons of free advertising.,AMD,2026-01-09 04:12:29,1
Intel,nylt6fh,And a much higher power budget.  AMD says they win because their 120W chip is faster than Intels 45W chip.   No surprise to anyone.,AMD,2026-01-09 14:57:05,25
Intel,nyjcmeq,"I generally agree but i suspect that the 388h is using a much larger gpu than people suspect. I think its probably ~165mm2 in size, not 55mm2. i suspect the 55mm2 die varient is for the 4xe version, and the 12xe version is 3x that size.  I also dont understand why strix halo is so expensive. It would be interesting to see bom and packaging costs.",AMD,2026-01-09 04:14:25,8
Intel,nyje36x,"It sounds like RDNA4 just doesn't scale at all. All the rumors point to them going straight from RDNA3.5 to RDNA5 in APUs, just skipping RDNA4 all together.",AMD,2026-01-09 04:23:13,37
Intel,nyjuufi,RDNA 3.5 was the only reason I didn’t invest in a STRIX HALO mini PC. The price is too much for outdated unsupported tech.,AMD,2026-01-09 06:19:22,20
Intel,nyji62l,"Might be the same reason they stuck with Vega for so long in APUs.  At the current available desktop memory (DDR4 at the time) an architecture change wouldn't have made a huge difference.    Once DDR5 came out for laptops, we finally saw RDNA 2+ APUs (Ryzen 6000 APUs).  I'd bet once DDR6 starts appearing on laptops we'll get a similar iGPU architecture leap.",AMD,2026-01-09 04:48:17,9
Intel,nynizjw,"AMD probably just didn't bother making a new APU design when they didn't have new CPU core to go with it. Medusa Halo is rumored for 2027 with Zen 6 and UDNA/RDNA5, so the Point version will likely release then too.",AMD,2026-01-09 19:35:27,1
Intel,nyiu18v,AMD has and always will be their own worst enemy,AMD,2026-01-09 02:31:09,78
Intel,nyiyjxo,"It's not like they aren't developing something this whole time, releases are planned many years in advanced. Intel will have some rope and then will get inevitably leap frogged",AMD,2026-01-09 02:55:05,8
Intel,nyj83ic,It's not like Intel isn't doing the same. Panther Lake and ARC are holdovers of things developed under Gelsinger.,AMD,2026-01-09 03:47:56,5
Intel,nyj28hs,"> AMD is going to f--- around and let Intel catch up, in CPUs and GPUs.  this is what we actually need: competition. AMD kicked intels butt, now intel is kicking back. it's a win for us either way.",AMD,2026-01-09 03:15:19,13
Intel,nyiwa7d,I hope Intel will catch up and encourage AMD to compete. Having cleat leader in CPUs or GPUs is bad for consumers.,AMD,2026-01-09 02:43:07,9
Intel,nyjspa3,"I mean, we know that AMD is innovating. They literally showed Zen 6 at CES. Its just not ready yet for mobile, and Intel caught up. Same thing happened with Alder Lake, where intel released that before Zen 4 was ready.",AMD,2026-01-09 06:02:25,6
Intel,nykmhz9,"So AMD having much faster iGPUs for decade or more did not do much.  But now Intel rolling out something at unknown price/power package will absolutely decimate AMD.  Regardless of what will happen, ""AMD's fault"" indeed. (amazing silicon designers and experts at everything posting for free on reddit have convinced me)",AMD,2026-01-09 10:23:35,5
Intel,nynjyg2,"Laptops haven't really been AMD's focus, and apart from Zen1, AMD's focus has been mostly on data center, with desktop being the natural offshoot.",AMD,2026-01-09 19:39:58,1
Intel,nyjozjz,Yeah they ain't immune to being complacent.  And bad press doesn't make Intel stay bad.,AMD,2026-01-09 05:34:41,0
Intel,nzb2682,"News at 10: ""Companies prioritise profits""",AMD,2026-01-13 05:54:06,1
Intel,nynkzx7,"so either AMD doesn't have the capacity to produce them, or OEMs aren't interested, neither option is a compelling reason for AMD to focus on mobile",AMD,2026-01-09 19:44:43,1
Intel,nyjj3pe,"We’d wish they were, but they’re not. Intel was struggling on all fronts due to their fabs. Amd is actually moving super fast in data centre so both epyc and instinct which is where they believe their money will be. They just don’t care to do anything in the consumer market.",AMD,2026-01-09 04:54:22,7
Intel,nynm27a,"Halo is so much faster that the upscaler difference doesn't matter at all. Of course, it is probably also bigger.",AMD,2026-01-09 19:49:33,0
Intel,nyoktyw,"Yes, Strix Halo",AMD,2026-01-09 22:31:24,1
Intel,nyjieva,And having fsr4 supported in mobile at all,AMD,2026-01-09 04:49:53,5
Intel,nzat1bf,FSR 4+ may be great but game support (number of titles + GPUs supported) is embarrassingly low,AMD,2026-01-13 04:48:21,1
Intel,nyjm8fi,Dlss 4.5 not that great in my opinion. It fixes some ghosting but creates more shimmering because it has so much sharpening. I had to dial back to 4.0.,AMD,2026-01-09 05:15:20,0
Intel,nyj9k89,wildcat lake.  only issue it seems to be using 2 P cores and 4 LPE cores instead of E + LPE,AMD,2026-01-09 03:56:23,2
Intel,nyjicwx,70% faster being “barely eke out a win”? Go ask why amd is stuck with 18 month old architecture despite intel managing to replace arrow lake after 12?,AMD,2026-01-09 04:49:30,11
Intel,nyjvty1,"You’ve got it backwards. The majority of laptops use IGPUs. IGPUs being as powerful as integrated graphics allows for cheaper thinner devices that are more power efficient. The entire intel CPU/IGPU performs on par with a 4050 at 60w at only 45w. When you factor in the 10-15w the CPU takes with the 4050 and you’re looking at similar performance at like half the power.   Being power efficient opens up a lot of form factors to be able to game with such as thin and light laptops, tablets, or gaming handhelds",AMD,2026-01-09 06:27:13,10
Intel,nyl2kgg,"> I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand.   That's why it's silly to be a ""fan"" or ""supporter"" of one company or the other.  They don't care about you, they care about making money and when they have a dominant position they will exploit it.  > At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.  Outside of that one device (Ayaneo maybe?), you're right.  But now AMD is also releasing an 8-core version of Strix Halo with the full 40 GPU CUs, which should be cheaper.  I expect that we'll see that in more handhelds at the high end.  Realistically speaking, it's easy to make the case that on a 7""-9"" screen the 40 CUs is way overkill.  There's still room for a middle ground that Intel could easily fill.",AMD,2026-01-09 12:30:34,31
Intel,nyksz11,"Absolutely consumers win when competition is hot, AMD has a bit too much of a lead ATM so they are cashing in and getting lazy. That said I am glad they are having their day, only because a few years ago they were on the brink of bankruptcy and I really want to see them on a fairly level playing field with Intel... If we end up with 2 juggernauts training blows, having big resesrch budgets, etc we'll get lots of innovation and competitive pricing.",AMD,2026-01-09 11:19:09,7
Intel,nykkoyj,"You had to shovel $1k for a 8 core CPU for about a decade, before AMD came.  So ""it just hiked the price"" is BS.  AMD cannot keep prices low while TSMC, effective monopolist, keeps posting record profits quarter after quarter.",AMD,2026-01-09 10:07:29,39
Intel,nyjf6qu,AMD is basically just waiting for their chance to do the bad things. They are a corporation after all.,AMD,2026-01-09 04:29:52,37
Intel,nyo271t,"Help us Cyrix, you are our only hope...",AMD,2026-01-09 21:04:11,3
Intel,nykfx0k,AMD has always been like this. The OG Athlon FX line from ~22 years ago were $1000 CPUs.,AMD,2026-01-09 09:23:31,-3
Intel,nyjjlvs,Wasn't it equivalent more or less to the 4050m as it was power limited to 30 watts?,AMD,2026-01-09 04:57:40,13
Intel,nykm4lu,"It's the price of the final product that will matter.  And given that Intel has lion's share of the mobile market, I don't see why the would not ask outrageous $$$ for it.  It is ""impressive"" only in the ""for iGPU"" context.   Based on the benches shown, laptops were consuming around 60W.  While AMD""s 370 HX has been shown to be able to game at below 20W, so uh.  Let's bait for wenchmarks in any case.",AMD,2026-01-09 10:20:17,5
Intel,nyl1xad,"Sure, sure.  I'm going to wait for proper benchmarks done under lab conditions and documented by more than ""Intel let me run this game with the FPS counter on.""  I mean, the general impressions for Intel are quite positive and if they're true then I hope it spurs AMD to do more.  I'm just not going to blindly accept ""first impressions"" as a replacement for proper testing.",AMD,2026-01-09 12:26:10,1
Intel,nym6wt0,*With 64gigs of ram at 9600mhz,AMD,2026-01-09 16:00:23,0
Intel,nzialor,"Tbf, it's like dgpu winner is claimed by who has the strongest one, so in that way it's kinda fair.  But how's the availability? Is halo in laptops actually? What's the pricing?  And what's the bang per buck on point and this?",AMD,2026-01-14 08:41:12,1
Intel,nyoht8c,"https://x.com/jaykihn0/status/1812898063502938260/photo/1  ""PTL-H 12Xe pictured."" so according to that the 55mm2 die variant is xe12",AMD,2026-01-09 22:16:37,5
Intel,nynhnmj,"12 Xe cores is 60% of the 20 cores in the B580 and that's 272mm², but of course that also has GDDR memory controllers, and such that aren't needed on a GPU chiplet, but it's likely that the die for the top SKU is quite a bit bigger than 55mm², I'd say between 90 and 130 mm².",AMD,2026-01-09 19:29:20,3
Intel,nykps9b,">I think its probably ~165mm2 in size, not 55mm2.   Which might open an ""dGPU sized iGPUs"" race.  NV could be the main victim here Surely AMD can oversize its iGPUs too.  I actually thought that AMD was forced to do so, by Filthy Green's GPP effectively banning AMD dGPUs. Typing this from G15 AMD Advantage Edition TUF.",AMD,2026-01-09 10:52:22,1
Intel,nyjhzne,What matters is the intel chip regardless of actual die size runs on quad channel LPDDR memory instead of the octa channel of strix halo and is fitting into mid and small size laptops 15-45w.,AMD,2026-01-09 04:47:09,-7
Intel,nykcsfk,Then how is the exynos 2600 using rdna4 fron samsung if it doesn't scale?,AMD,2026-01-09 08:54:32,14
Intel,nyktb13,RDNA5 doesn't exist. The actual name for the next generation architecture is UDNA1.,AMD,2026-01-09 11:21:54,-3
Intel,nzasd0f,"Same here. No new FSR tech and ROCm was just as poor. It works now but Vulkan is often better.. Very disappointing. For AI, NVidia is so far ahead.",AMD,2026-01-13 04:43:51,1
Intel,nym1e1u,If intel can extract more out of LPDDR5x with B390 then I don't see how AMD can't. Just too stingy to give more die area to cache?,AMD,2026-01-09 15:35:23,4
Intel,nyjxf3k,So basically RDNA4 is just another RDNA1,AMD,2026-01-09 06:40:05,1
Intel,nytzgu6,Kepler said in another subreddit that Medusa Premium and Halo is launching in 2028. You're only getting the crappy RDNA3.5 iGPUs for the third time.,AMD,2026-01-10 18:54:53,1
Intel,nyj1177,"Intel: but the enemy of my enemy, is my friend.  Intel 🤝 AMD  we're cooked guys /s",AMD,2026-01-09 03:08:41,22
Intel,nykbk7y,"Yeah the people saying AMD is stagnating are just wrong. AMD is kicking all kinds of ass... They just don't care much for the consumer market currently.    The other issue is that there's no point releasing a new line of products when no one can afford anything because nand flash is so expensive.    Companies CAN afford this because they need to ride the ai wave, but consumers can't because the average PC cost almost doubled.",AMD,2026-01-09 08:43:25,4
Intel,nyjeicz,"Wow! Excellent. Hopefully we'll see the products coming to market soon, and hopefully the 2 P cores won't matter as much since we're seeing a major lithography improvement. Intel is really impressing me lately.",AMD,2026-01-09 04:25:42,2
Intel,nyltrkg,This. Plus even lunar lake outperformed Strix Point in many scenarios already. Intel is at least one generation ahead here,AMD,2026-01-09 14:59:54,2
Intel,nyk0tw7,"It's not the 4050 at 60W. The laptop they compared only allows for 30W to the 4050. Nvidia's specs for the 4050 is 35W minimum, so I don't know how Dell even got to 30W. Below a certain wattage, gpu performance decreases exponentially because a minimum level of power is required to even have the gpu turned on.   Panther Lake is built on Intel 18A, which is supposed to be much better than the 'ancient' TSMC 5nm the 4050 is built on. The 4050's cpu is also Arrow Lake, which is less efficient than Lunar Lake. Again, that skews the agenda.   You can already game on thin and light devices with discrete graphics. Laptops like Asus's G14 is only 3.3lb, but sports a 4060 which is like twice as fast as intel's new igpu. The dgpu turns itself off when on battery, and the integrated graphics takes over. Anything more intensive should be used with a charger plugged in.   In short, paying for a big igpu doesn't make much sense for anyone interested in performance. And gaming handhelds? Does anyone really care about those useless bricks for investment into integrated graphics? It's not like the cost of Panther Lake is going to be cheap when its laptops start at $1300. With that kind of money, you can get 2025 Asus Zephyrus G14 with a 5060 and blow its shit out the water. Or for those on a budget, 5050 laptops have been seen for $600.   Integrated graphics have come so far, pairing Intel's newest 18A Panther Lake with an RTX 4050 could still make a lot of sense.",AMD,2026-01-09 07:08:24,0
Intel,nyjxwil,"Ehh not the mayority but *all* computers use iGPUs. The thing is, for the average Joe aka 95% of the market, there won't be a difference in the usage between an Intel Iris or RTX 5090 dGPU. And the efficiency would only come into place if they would game on battery (who does that anyways) or create/edit videos (again, virtually no one would do that without a dGPU). And even then, having your laptop drained in 2 hours 15 minutes instead of 2 hours is not ""gamechanging""   So it does not affect the efficiency at all during webbrowsing, watching videos, creating documents etc.  Thats also the reason why Intel has the non X 5,7,9 which will properly be by far the more demanded version as, again, the average Joe does not care the slightest about iGPU.  And apart from the iGPU, PTL is just a tiny step up from the Ultra 200 series...",AMD,2026-01-09 06:44:02,-2
Intel,nyqzsy8,"It's so weird that people treat their computer parts with a cultish following. Most of the people I know don't think about their cards at all and are just happy to play whatever games.   Super weird to be ""team red"" or ""team green"".   I can't imagine describing myself as a ""my computer chip manufacturer fan"". Cringe lmao.",AMD,2026-01-10 07:19:21,2
Intel,o0v4u9k,"How are you determining that AMD has a ""lead""?  In terms of Marketshare, Intel absolutely dominates x86, especially mobility (laptops), and if you walk into a Best Buy and ask 10 random customers ""Would you ever consider buying an AMD laptop?"", five of them would ask ""What's AMD?"", another three would say ""Isn't that a budget brand?"" (Their ""awareness"" of the zeitgeist of PC hardware is stuck in 2005), and maybe, and that's a big maybe, two out of ten would have a favorable opinion of AMD hardware so long as they've been paying attention for the last few years.  In terms of budget and expenditure, in 2025, Intel spent $17 billion in R&D versus AMD's $7.4 billion, outsizing it by a large amout. Total sales for 2025 are projected at approximately $53 billion for Intel and $33 billion for AMD.  I see this all the time and I've seen it for the past 8 years.... the only place that AMD has a ""lead"" is in the mind of PC hardware enthusiasts.... because it's not in the numbers as seen above (sales, expenditure, market share, etc), and it's not in the ""mindshare"" of your average consumer.  Hypothetically speaking, we could say that in a duopoly, as with Intel/AMD in x86, the BEST situation a consumer could hope for would be an even 50%/50% split in marketshare.... this would bring about the fiercest competition and would hopefully lower prices, increase innovation, etc. (and yet another reason why any fanboy who wants their favored company to dominate is LITERALLY cheering against their own interests as a consumer) Even with AMD's seriously impressive turnaround, their capture of marketshare, their ability to compete with two of the largest companies in the world while having considerably fewer resources, they still have a very, very, long way to go before approaching 50% of the x86 market across all segments.  In fact, to get any closer to that idealistic 50/50 split, AMD would have to continue winning and Intel losing for for many more years.  In other words, fears that AMD is ""becoming what Intel used to be"" and ""getting lazy"" are not an accurate reflection of the reality.",AMD,2026-01-21 14:59:24,1
Intel,nykyb9y,"What no AMD deserves much worse. How can a company fuck up so much and still survive. I would rather good competition rather than competing for the sake of competing. Marketing is bad, products are bad and they keep shooting themselves in the foot. Id rather Qualcomm or some other ARM company compete with X86. Its ARM or RISCV time to shine.",AMD,2026-01-09 12:00:44,-14
Intel,nylsh5z,Ever head a look at their profit margins?,AMD,2026-01-09 14:53:45,2
Intel,nykktsq,"Bullshit.  For starters, pricing is not a ""bad thing"".  Bad thing is, pick any piece from blue/filthy green's arsenals:  1) Strongarming OEMs 2) Strongarming Journalists 3) Proprietary standards",AMD,2026-01-09 10:08:42,14
Intel,nylsknw,Waiting? They’ve been busy doing that for years now,AMD,2026-01-09 14:54:13,0
Intel,nyosqz5,"I WISH they were still around. I think their IP got sold to Via, who's not doing anything with it.  At this point, we might only get competition in the desktop x86 space if the government forces Intel and AMD to license x86 and x86-64 to some other chip designer (like Qualcomm or Mediatek) or Windows on Arm and Linux on Arm start getting wide application support, including office software and games.",AMD,2026-01-09 23:11:26,2
Intel,nykpg6r,"""Being good"" was never about price.",AMD,2026-01-09 10:49:30,2
Intel,nyjpruz,"They said it rivals a 4050 at 60W. The 4050 maxes out at 100W on paper but it's actually at 80W that it hits its peak performance. a 60W 4050 is about 85% of it's max performance.  So performance wise panther lake should be about on par with a full powered 3050Ti laptop.  That plus more advanced ray-tracing cores, it's running doom dark ages really well, AMD is still stuck at RDNA 3.5 and ray-traced games suck on the 890M.  I wish intel released a 24 Xe Core Variant with a 256-bit bus, double the cores and bandwidth. That would compete with the 5060/5070 laptop GPUs.",AMD,2026-01-09 05:40:22,16
Intel,nyl1feq,"Not synthetic benchmarks, we want to see benchmarks in games.",AMD,2026-01-09 12:22:47,6
Intel,nyld8l0,"You're either intentionally mis-stating this, or truthfully aren't aware, BUT, you can game sub 20w on any igpu. What actually matters is the performance scaling.  Also, just to clarify, while the 890m CAN game between 6-20w its performance is essentially identical to the 780m, z1e, etc. It only gets impressive at power draws 30+ (signed, a very happy 7840u handheld owner)  So, what we need to know is how well the new Panther Lake chips scale",AMD,2026-01-09 13:35:33,1
Intel,nyjhpru,Lunar lake came out after strix point. It was squarely a competitor to the 890m. Amd just officially released the cut down strix point as Z2E later.,AMD,2026-01-09 04:45:20,-3
Intel,nzinuqp,"No, not really. Making up a ""winner"" is stupid and nothing but fanboy behavior.  A faster dGPU is generally better because you’re generally not power limited on a desktop. It doesn’t really matter whether or not you have a 5060 or 5090 or whatever.  In mobile systems it’s a huge difference.  There are entirely different power classes that don’t compete with each other. A thin and light notebook with a 15W CPU cannot have a 100W CPU in it.  Panther lake aims towards unplugged performance which is the 45W power class and the same as Strix Point.  Strix Halo is a much higher power class that requires a laptop to be plugged in permanently for the full performance.  It’s for mobile desktops that are usually plugged in but can be mobile for some time with heavily degraded performance.  It’s an entirely different class of product and you won’t find (many) devices where Strix Halo and Strix Point/Panther Lake compete with each other.",AMD,2026-01-14 10:47:20,1
Intel,nyojqy6,"Yeah, i am gonna contend that either that is wrong, and is the 4xe version, or that intel basically lied on their benchmarks.  If none of those two things are true, Intel's new graphics architecture will absolutely dominate in the next round of discrete graphics GPUs.  with B580 intel needed \~80% more silicon to match nvidia performance. Now they need \~10-20% less die area. meaning their performance per transistor basically doubled gen/gen. which is unheard of. Even maxwell (largest architectural uplift in the history of GPUs in the last 10 years) did not achieve anything close to that. And it was a massive overhaul with huge changes.  So . . . there is something big i am missing . . . or intel is going to dominate in all things graphics going forward.",AMD,2026-01-09 22:26:03,1
Intel,nyjiq2t,"Let's not perpetuate this ""octa channel"" DDR 5 nonsense; it's a quad channel chip, and the Intel one is a dual channel",AMD,2026-01-09 04:51:54,15
Intel,nyk3wyi,"If you're going to be pedantic about it at least be correct please, they both use 16b LPDDR channels so the actual counts are 8 channels for Pantherlake and 16 for Strix Halo.  You're fighting a losing battle either way, the industry has long since settled on 64b as the standard channel width for marketing, independent of the actual number of address/command buses.",AMD,2026-01-09 07:35:06,1
Intel,nyki3az,Where did you find information of it being rdna4?,AMD,2026-01-09 09:43:42,6
Intel,nykhvc9,It's a custom implementation. IIRC it's not even RDNA4 but some Samsung derivative that probably has a ton of changes in silicon design to drive power down.  The short story is that AMD didn't bother to do low power optimizations in the architecture and silicon design. RDNA5 should change that.,AMD,2026-01-09 09:41:40,8
Intel,nynjaao,"Mark Cerny talked about RDNA5, AMD's leaked documents have talked about both UDNA and RDNA5",AMD,2026-01-09 19:36:51,9
Intel,nykyia5,This is nonsense RDNA5 does exist. I used to work there.,AMD,2026-01-09 12:02:08,-5
Intel,nyu9kyt,"Kepler's track record with AMD stuff isn't great, but everything is possible",AMD,2026-01-10 19:43:39,1
Intel,nyj1emc,">we're cooked guys /s  No sarcasm there lol  All tech companies are colluding right now, seeing as american business laws don't matter anymore",AMD,2026-01-09 03:10:44,15
Intel,nyj8sm2,"Yes, a frienemy.",AMD,2026-01-09 03:51:55,0
Intel,nyleo84,"Oh, you didn't say AMD was ""slacking off"" and letting intel ""catch up"". Figures.",AMD,2026-01-09 13:43:25,5
Intel,nylti7r,But that’s also more because of the increased demand from AI than anything else.,AMD,2026-01-09 14:58:39,1
Intel,nyjxwti,"It's 6C/6T and 2x Xe3, so don't expect a whole lot of performance. This is Intel's Mendocino.",AMD,2026-01-09 06:44:06,1
Intel,nylwgde,The slide specifically says 60w sustained for the 4050. I couldn’t find your claimed 30w anywhere. If I’m wrong I’d be interested to see where you got the 30w number from because that would be shady by Intel,AMD,2026-01-09 15:12:36,2
Intel,nyp8y9d,"I posted elsewhere about the Framework Desktop with the Ryzen AI 385 and 32GB of RAM.  That's a pretty sensible config for a small gaming device, though it has 32 CUs instead of the full 40.  Still, that puts it ahead of anything in it's class other than the 395+.",AMD,2026-01-10 00:37:45,7
Intel,nym6sbh,"Yes, have you compared it to that of the competitors?  In general, pricing is not the issue to me.   Dirty play like blackmailing OEMs, proprietary standards and other misuse of the dominant market position is. (on top of being illegal)",AMD,2026-01-09 15:59:50,15
Intel,nytz6q1,"Let me guess, you're too young to recognize he's talking about before your time.",AMD,2026-01-10 18:53:34,6
Intel,nylxgon,"Yeah while AMD can't even announce their product AT A CONSUMER ELECTRONICS SHOW! and instead ONLY TALK about AI and government work......   AMD sucks just as bad as Nvidia just as bad as Intel, it's just a constant moving circle jerk as to whom is the least evil.",AMD,2026-01-09 15:17:16,7
Intel,nyr90di,How about this one: strongarming game developers into NOT including DLSS?,AMD,2026-01-10 08:43:32,0
Intel,nytewb4,"Nobody is interested in x86, otherwise Via would have been bought up.  We are in the age of the cloud and all software is custom made. Hence RISC+",AMD,2026-01-10 17:18:43,2
Intel,nyk5oi5,"Also Intel has XeSS which is very helpful for handhelds since they can't manage higher wattages, although not all games provide XeSS as an option",AMD,2026-01-09 07:50:42,6
Intel,nykdl7b,Is there really a 4050 or are you referring to the 4050m even when you don't add the m?,AMD,2026-01-09 09:01:51,2
Intel,nyobdr7,"they said 60W, but if you look at the laptop they used, it's 30W, probably 60W whole system",AMD,2026-01-09 21:46:41,1
Intel,nysouji,Nvidia paid intel 5B dollars ....Read whatever u can ... But it was to stop intel giving high power gpu to mainstream... .,AMD,2026-01-10 15:11:53,1
Intel,nylhxw8,"Perf + price + (to a lesser extent, but it still matters) power consumption together is what matter.   None of the 3 is decisive on its own.",AMD,2026-01-09 14:00:38,3
Intel,nylu1zs,"I think you’re getting downvoted because it was sold by reviewers as a Z1E competitor as that’s what was available in regular devices.  Hx370 was only used in niche manufacturers, like GPD only when it launched.   You’re right though, it was supposed to be a competitor for hx370, but was held back by drivers and other things until mid to late 2025, which corresponded to Z2E (cut down hx370) release.  Fast forward to now, and it competes/beats both",AMD,2026-01-09 15:01:17,1
Intel,nyplsu3,"i mean you can legit put it over the intel provided slides and its pretty much a dead on match. the PCH is smaller in the presentation photos so they can make it look pretty, but the real chip is an exact match to that leak https://cdn.videocardz.com/1/2025/05/INTEL-PANTHER-LAKE-DEMO-1200x675.jpg  ~~this generation is seeing quite a significant leap forward in manufacturing technology (Gate All-Around/RibobnFET & Backside power delivery/PowerVia) these usually do result in big gains and that does make it a bit harder to compare to prior nodes. not to mention~~ **[GPU is TSMC N3E still quite a bit denser than N4 class tho]** its not the exact same uarch as B580, while still a derivative of battlemage, there does seem to be some (rather significant) improvements between xe2 and xe3 https://gamersnexus.net/gpus/intels-new-gpu-xe3-architecture-changes-handheld-gaming-cpus-xess3   but where a lot of the fps gain will be from is N-Frame and Pixel generation intel want to promote those numbers over native performance. AMD cant do with RDNA 3.5. id expect 8060s to be a much more powerful igpu but it is lacking what is essentially lossy compression for realtime graphics. that is a pretty big deal and i do think it will be what causes Strix halo to be a product that just ages poorly, costs too much for what it is really (308mm^2 io/igpu chiplet cant be cheap on 4nm) its really amds pipe cleaner for future packing tech.  people said they same when the zen chiplets were rumored to be the size they are. you save a lot of area not needing memory controller on that chip would be my guess. d2d bonding is very space efficient compared  for some perspective strix halo igpu block + media engine block is about 120mm^2 on N4P(143.7216MTr/mm^2) of the iod, the rest is i/o and the npu.  the 12 Xe3 chip is 55mm^2 on N3E(216MTr/mm^2) (so we could napkin approximate about 80mm^2 if it was on N4P)",AMD,2026-01-10 01:48:30,9
Intel,nyjjq7p,"Technicality is technicality, if the channel width is cut down by half but channel number is doubled then they still doubled the memory channel. People just need to know memory channels are not all equally wide just because that’s all they know being PCMR enthusiasts.",AMD,2026-01-09 04:58:28,-1
Intel,nykilu3,"https://www.thelec.kr/news/articleView.html?idxno=50232. Seems your out of the loop, you think amd can't scale rdna4 but samsung can?",AMD,2026-01-09 09:48:29,8
Intel,nykicmf,"Rdna4 is objectively faster than rdna3/rdna3.5 at the same clock, power' cu count and bandwidth  something 100% desirable for apus. Stop making excuses for amd and their bad decisions. Everything b your claiming samsung did for rdna4 to scale is something amd could have done aswell and has done as amd has made changes to rdna3(rdna3.5) for apu specifically the same can be done for rdna4.",AMD,2026-01-09 09:46:05,4
Intel,nyui8sr,Sure. we’ll see,AMD,2026-01-10 20:27:04,1
Intel,nyj73k6,Have the business laws mattered since the dawn of post-dialup internet?  I don't think they have. Where's our fucking bell-style breakup? 41 years ago was the last *real* monopoly breakup... and they let it come right back.  EU does half measures and they don't come to the rest of the world. It's a travesty that we don't have nationwide GDPR or force allow sideloading on ios.,AMD,2026-01-09 03:42:31,5
Intel,nyjduwz,An enerend of sorts,AMD,2026-01-09 04:21:49,0
Intel,nylwn40,"Sure. But they're still not stagnating. Since December 2023, when Mi300X and Mi300A were released, they released Mi325x, Mi350x, Mi355x and soon, Mi400x.    The latest gen is running HBM3e and 3nm CDNA4. Those are some immensely advanced products.    On the Epyc side, they've got the 9965, a 192 core 384 thread monster that Intel can't even attempt to compete with.    Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.    AMD has gone from 32 cores first gen in 2017 to 6 times that in 2024.    It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.",AMD,2026-01-09 15:13:28,2
Intel,nylaft6,"The modern Atom is fine by me, the N100 had more performance than a 6500t so this one should have more than enough compute for many different use cases whilst retaining low load efficiency. This product could potentially obliterate even the newest and best SBCs for home lab use cases, even regarding efficiency.",AMD,2026-01-09 13:19:41,1
Intel,nymqm53,"[Intel Performance Index](https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-core-ultra-processors-series-3_1/) Search 4050. [Dell 14 Premium is this laptop, with a TGP of 30W](https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor)  [In PCWorld's test, they got 48 fps for Cyberpunk](https://www.youtube.com/watch?v=NdLYuQQPo5c). My 4060 gets 73 fps using 60W using high settings and 2880x1800 DLSS instead of XeSS. That's a game where Intel gpus performs well above average. A 4060 optimus laptop uses around 3.5W an hour at idle without the screen turned on. With the screen and igpu powering it, it's about 8W. Having discrete graphics in modern systems doesn't really impact battery life anymore.   So yeah, Intel was intentionally being misleading, hoping people wouldn't actually bother to check their figures. Panther Lake's massive igpu still doesn't make sense for anyone who cares about performance. Maybe a little bit for battery life, if it's more efficient to drive high resolution displays, despite its large size being wasteful. Most igpus go into office pcs. In terms of gamers, Steam's hardware survey suggest that desktops and gaming laptops with dgpu are the biggest share.",AMD,2026-01-09 17:28:27,2
Intel,nym76rq,"Yes, I did. AMD could hold prices low, they choose not to.",AMD,2026-01-09 16:01:38,-3
Intel,nzqu8wh,What AMD did a decade ago has no bearing on how they operate today. Corporations have to alter their behavior quarterly in order to maximize their legal obligation to constantly increase shareholder profits.,AMD,2026-01-15 15:45:18,0
Intel,nymy6xm,9850X3D lol gottem,AMD,2026-01-09 18:02:35,8
Intel,nyp6syu,you didn't just try to suggest that CES is for....consumers..... did you.... seriously?,AMD,2026-01-10 00:26:18,0
Intel,nyrhvdt,"Ahaha, lovely lie. And even if true, how would that change a lit of ""bad things"" lol.",AMD,2026-01-10 10:06:50,1
Intel,nytjk6m,"Seems like things are going that way. I guess all we can do is wait and see if the Arm takeover gets so complete that Intel and AMD have to join in, and then suddenly have to compete with Qualcomm and Mediatek.  If that ever happens, hopefully we'll see more competition.",AMD,2026-01-10 17:41:05,1
Intel,nykh31e,linux and optiscaler is the way,AMD,2026-01-09 09:34:24,3
Intel,nylckas,"4050m, although it really wouldn't matter either was as the 4060 and 4060m are functionally identical in regards to performance (+5-7% for desktop) so if there were a full size 4050 we'd expect it to be the same or even less of a difference",AMD,2026-01-09 13:31:44,1
Intel,nyljjoq,"That's all fair, I'm looking at it from a handheld perspective. Performance at power draws that are actually feasible in handhelds has been stagnant since handhelds have really gotten popular. That is, it has if you want more than an hour of battery life",AMD,2026-01-09 14:08:59,0
Intel,nylvmjs,"Actually, the supposed driver issue was only a MSI Claw specific issue and not a general lunar lake issue.  https://www.notebookcheck.net/Intel-Lunar-Lake-iGPU-analysis-Arc-Graphics-140V-is-faster-and-more-efficient-than-Radeon-890M.894167.0.html  Here's a review from September 2024 using a LNL Zenbook S14 with 28w TDP. It had no issues generally outperforming the HX370 in the Zenbook S16. As usual the PCMR-esque dominated crowd on here paid no attention to laptops (which is the real life volume) and only looked at some handheld (which is a niche irl) so they thought that supposed ""lunar lake issue"" was widespread.",AMD,2026-01-09 15:08:44,3
Intel,nyjl1zy,"Each DIMM of DDR5 has 64 bits total of bus width, same as DDR4, 3, 2, and 1. And I do understand what you're talking about (not to mention that Strix Halo can't even take SODIMMs), but nobody else talks like that. When you call it an ""octa-channel"" chip, what people read is that it has as much bandwidth as a Threadripper Pro, because that is how [AMD is marketing those chips themselves](https://www.amd.com/en/products/processors/workstations/ryzen-threadripper.html).",AMD,2026-01-09 05:07:19,8
Intel,nyko7q2,"Yeah, and AMD says Strix Halo has four channels in their customer facing spec as well, because they have 128b and 256b buses respectively. They use the 64b channel convention as it is a customer facing spec, that doesn't meant they actually have that many channels in hardware.   The Pantherlake datasheet isn't public yet, but you can see plainly in the [actual spec sheet](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/memory-controller-mc/) for Arrowlake H that it supports 8 channels of LPDDR5X (additional [spec](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/supported-memory-modules-and-devices/) for channel width). Pantherlake will be the same.   The equivalent AMD doc is not available for Strix Halo but you can see the 16x16b spec quoted by Chips and Cheese [here](https://chipsandcheese.com/p/evaluating-the-infinity-cache-in#:~:text=Strix%20Halo%20has%2016%20memory%20controllers%20and%20CS%20instances%2C%20each%20handling%20a%2016%2Dbit%20LPDDR5X%20channel).  You cannot gang these channels into a dual channel mode, that is not how modern memory works, and there is no allowance in the LPDDR5 spec for 64b channels. The 16b channels have separate command/address buses and burst for a sufficient length (32n) to fill a cache line with each access.  To be clear I think standardising on 64b ""channels"" for marketing specifications is a good thing, it allows quick mental calculation of memory bandwidth without having to get into the nitty gritty. But if you're going to be pedantic and use the actual channel count, it's best to be correct.",AMD,2026-01-09 10:38:46,5
Intel,nykiuny,"Im not original guy you responded to I just wanted to know, becouse I couldnt find it on google. thx",AMD,2026-01-09 09:50:43,10
Intel,nykjg5k,"I'm not making excuses just explaining the rationale, which I don't agree with BTW.       Yes I know AMD are some lazy mofos. RDNA 3.5 till 2029 for iGPU is cheapo strategy as usual.",AMD,2026-01-09 09:56:09,4
Intel,nyli3fq,https://en.wikipedia.org/wiki/Hyperbole,AMD,2026-01-09 14:01:27,2
Intel,nym2zdk,">It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.  >Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.  Since we're talking about the server side now, Haswell-EP went from 18 cores maximum to 22 core Broadwell-EP to 28 cores on Skylake-SP. Cascade Lake-AP (rare bespoke sku) went up to 56 cores per socket. ""Haven't moved an inch"" is inaccurate.",AMD,2026-01-09 15:42:41,3
Intel,nym7k8z,"In CPUs they’re losing market share to arm, the Datacenter GPUs are mostly bought by companies who can’t afford NVIDIA",AMD,2026-01-09 16:03:20,0
Intel,nymvxz5,"I think you were looking at the old core ultra series 1 testing not the current CES testing. For their claim they used the following settings:   Intel B390: Processor: Intel Core Ultra X9 388H (Panther Lake) PL1=45W; tested in Intel reference platform; Memory: 32GB LPDDR5 9600; Storage: Samsung PM9A1 512GB; Display Resolution: 2880x1800; OS: Windows 11 26200.6725; Graphics Driver: Intel Arc Graphics Pre-Production driver; NPU Driver: Pre-Production driver; BIOS: Pre-Production BIOS; Power Plan set to Balanced, Power Mode set to ""Best Performance"".  NVIDIA RTX 4050: Processor: Intel Core Ultra 7 255H (Arrow Lake); tested in Dell 14 Premium with Nvidia GeForce RTX 4050; Memory: 32GB LPDDR5 8400; Storage: Samsung 9100 Pro 1 TB; Display Resolution: 2k IPS; OS: Windows 11 26200.7171; Graphics Driver(s): dGPU: 32.0.15.8180 (GeForce 581.80) & iGPU: 32.0.101.8250; NPU Driver: 32.0.100.4404; BIOS: v1.4.0; Power Plan set to Balanced, Power Mode set to ""Best Performance""; Dell Optimized = Ultra Performance. Battery Size: 68Whr",AMD,2026-01-09 17:52:34,2
Intel,nyma4vw,"Why would AMD ""hold prices low""?  Gross margins are below 50% (48, as in 2022), while NV has it at 70%.  We know they are worse in PC/GPU market and better in datacenter.",AMD,2026-01-09 16:14:51,8
Intel,nypf46n,What did you think the acronym CES stands for?,AMD,2026-01-10 01:11:10,7
Intel,nyrmvf6,It would further validate what HisDivineOrder said which is that AMD is just another corporation.  Which they are.,AMD,2026-01-10 10:52:27,0
Intel,nyl953n,"Really wish Optiscaler had a better installer, something akin to Reshade. The whole manual process for each game makes it annoying to use.",AMD,2026-01-09 13:12:04,3
Intel,nyks16k,Intel still takes a heavy penalty on Linux in graphics vs. AMD. Hopefully that improves as well.,AMD,2026-01-09 11:11:24,5
Intel,nyjobcx,Well there’s more than one type of memory 🤷‍♂️ PCMR crowd just defaults to DDR DIMMs but the world of mobile is mostly LPDDR from phones tablets to handhelds and most small laptops,AMD,2026-01-09 05:29:47,1
Intel,nynuctv,"Nope, I was looking right at the current testing. I do have to make a correction though: Panther Lake's cpu is built on Intel 18A, and the gpu is built on TSMC N3E  Let's summarize. In a head to head battle, Intel claims the 45W Panther Lake Core 388H with its ""massive graphics"" is 10% faster than a 30W 4050 paired with a 30W Arrow Lake 255H. Panther Lake's cpu is built on the most advanced silicon process node 18A, designed to compete against TSMC's N2 (2nm) which is set to release in products in the second half of 2026. Panther Lake's gpu is built on TSMC N3E, a significantly more efficient N3. The 4050 is built on a custom 2020 TSMC 5nm variant, and Arrow Lake is built on TSMC N3+6nm. Arrow Lake is designed for specifically for high power use vs the low Lunar Lake and Panther Lake.   The future of integrated graphics is truly bright. I can see it being exactly where it is now. Vital for battery life in office laptops and actual gaming laptops with discrete graphics. Big igpus? Mostly irrelevant and a waste of money.",AMD,2026-01-09 20:27:44,2
Intel,nyqxy0s,"Yes acronym has ""consumers"" in the name, but it's not directed or intended for consumers, it's intended for the big industry, the maker's manufacturers, the ones creating services, and the subtle parts of the distributors and such, it was and has NEVER been intended for the end users, the broad consumers.  Maybe bloody well look up what CES is and what it's for before asking a silly question.",AMD,2026-01-10 07:03:08,2
Intel,nys7tqn,"No, it would not. There is a difference between a shoplifter and a serial killer, even though both are criminals.  Filthy Green plays in a league of pieces of shit of its own.",AMD,2026-01-10 13:35:08,0
Intel,nynpovt,True but you put command once in your game and you forget about it,AMD,2026-01-09 20:06:13,0
Intel,nysa4r8,"This is so delusional  You're talking about AMD that made Int8 version of FSR4, which is THE hardest part, and then keeps it away from users to sell more RDNA4 cards.",AMD,2026-01-10 13:48:55,-1
Intel,nyov5va,"Missing the point. It's about accessibility and ease of use not how often you need to do it. If a tool to bring similar functionality as Nvidia isn't at a similar level of accessible and easy to use as the manufacturer apps, then it's relegated to enthusiasts only.   Reshade is one of the most popular modding tools for post-processing shaders because it's so easy to install, use, and manage for multiple games on the same system.",AMD,2026-01-09 23:24:12,1
Intel,nytis7z,"AMD had no reasons for such lock-in, it makes sense only for companies dominating the market, to push people to refresh.  I have not seen palatable proof that FSR4 could be ""easily backported"" but isn't.",AMD,2026-01-10 17:37:25,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,5
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,4
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nzdyg7l,"Is PowerColor a good brand of GPUs?  I’m planning on upgrading my gpu from my almost 7 year old nvidia rtx 2060 to an PowerColor rx 7800xt Red Devil, and am bit worried if they’re a reputable brand.   Was holding off the upgrade due to not wanting to chase percentages, and now that I fully embraced Linux (Fedora 43 KDE) I wanted to get something that has better compatibility with the OS as I did encounter some issues due to NVidia drivers.  Edit: forgot to mention that I have a compatible system with 600w power supply",AMD,2026-01-13 17:45:26,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  • MB Gigabyte|GA-970A-UD3P AM3+R   • VGA Sapphire|100365BF4L R9 270 2GD5   • PSU Roswell|RX850-S-B 850W RT (has been replaced)   • CPU AMD|8-Core FX-8350 4.0G 8M R   • SSD 256G|Samsung MZ-7PD256BW R   • MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencé à rendre l'âme fin 2025 et je dois donc me dépêcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prémonté étant donné que je suis peu doué là dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose à part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxu74dg,what are the best settings for my rx 9070 xt steel legend on adrenalin? should I prioritize lower temps or higher performance? and will the performance between settings be negligible playing in 3440x1440p? I'm currently running the default option under Performance>Tuning,AMD,2026-01-05 16:15:52,1
Intel,nxurns3,"When I'm playing a game, my screen suddenly goes black and I have no way to shut down my PC; I have to restart the power supply. Does anyone have any solutions, please?",AMD,2026-01-05 17:50:56,1
Intel,nxv130z,"Are there plans for chipset refresh for Zen 6 or 7 or there will be only firmware and BIOS  updates  for existing ones? I heard Zen 6 should have better memory controller , with higher 1:1 RAM speed support (perhaps 8000MT/s + ) etc. , but of course still same AM5 socket.",AMD,2026-01-05 18:33:10,1
Intel,nxv5q21,"Hey guys.  Whats the best way to get a smooth 60fps lock on a 120hz display?  I use MSI Afterburner and the adrenaline app, neither felt as smooth as native 60hz.  On nvidia i used the half vsync feature and that worked for me but AMD doesn't have an equivalent option.",AMD,2026-01-05 18:53:49,1
Intel,nxzp3f0,"weird issue as off 2 days ago: RX 7700 XT with 25.12.1 driver on W10 - when powering on the system, the secondary screen (HDMI) is not receiving any signal until the HDMI cable is unplugged and plugged back in. No recent updates installed.",AMD,2026-01-06 11:41:16,1
Intel,ny1f10g,"7800x3d SUSPICIOUSLY LOW TEMPERATURES   I just finished building my computer and tested it in two games, at 2k resolution and the highest settings: The Last of Us Part Two and Battlefield 6. My 7800x3d is showing temperatures below 50 degrees Celsius, even though I'd read on forums that it can get hot. I checked it on the cooling display, HWMonitor and in MSI Afterburner. Is it possible for air cooling to be this efficient, or do I need to configure something in the BIOS to get the processor to run at full performance? Bf6 runs with 180fps and TLOU have around 100fps.  I have rtx 5070 and 32gb ddr5. Cooler: Phantom spirit Evo vision with stock paste.",AMD,2026-01-06 17:15:25,1
Intel,ny4j9i2,"New to AMD and plan to keep the same cpu cooler, I have a NH-D15. I bought this cooler back in 2021-22. Would I need a new mounting bracket to accommodate this change?   I have upgraded to 7 9800X3D, Mobo is a Tuf gaming B650E-E if this information is needed. Any help appreciated!",AMD,2026-01-07 02:15:55,1
Intel,ny70dtx,"Hi there, hope everyone is doing fine and started new year on a good note :)      Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically (https://www.powercolor.com/product-detail214.htm)  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?     Thank you !",AMD,2026-01-07 13:16:10,1
Intel,nyc3nmx,"looking for help understanding core parking on the 9950X3D, does it outright disable the other cores while gaming? or do other applications running use the non X3D cores?",AMD,2026-01-08 03:51:56,1
Intel,nyci0og,"I'm building my first ever AMD PC, and my second ever PC (My old one had a 2080 super, 10th gen i9 and sadly died a few months ago). I did not know that you were supposed to buy certain ram depending on what CPU/motherboard you used. I'm either going to be buying a 9850X3D or a 9800X3D, and the motherboard I have currently is the MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard. My ram is the G.Skill Trident Z5 RGB 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory. Should I return the ram and get the AMD EXPO equivalent? Will I lose performance if I keep it? Will I lose stability if I keep it? Will it even work properly?  Some extra info:   I can afford to return it and buy the equivalent for an extra $100 or so. My GPU is the Sapphire Pulse 9070 XT, I'll be gaming at 1440p, my I have an WD\_BLACK SN8100 2 TB SSD, and a Corsair RM850x (2024) 850 W Fully Modular ATX Power Supply.",AMD,2026-01-08 05:22:26,1
Intel,nygdcim,"Is a reasonable upgrade for my system possible?    Hello there,  i would like to know if there is any reasonable upgrade possible on my AM4 System...   I would like to play Call of Duty Warzone on a 1080p Monitor with 180 fps but ALSO use ~ 500 tabs at the same time.  Currently my System runs the 500 tabs but only gets ~ 120 fps in cod.   Since AM5 is very expensive currently due to RAM prices, i do not see any reasonable chances for a Upgrade and therefore am looking for advice :)    My current System:  AMD Ryzen 9 5900X - 12x3.7GHz  => OVERLOCKED at 4.7GHz with 1.304v (undervolted for that speed -> Temps below 80°C)   32GB DDR4 3600MHz Team Group T-Force Vulcan Z - DDR4 (2x16GB) => UPGRADED to 64GB (4x16GB)   AMD Radeon RX 7900 GRE 16GB => slight OC possible BUT Temps tend to go above 80°C, at higher OC even above 90°C... (possibly i could add more cooling to the Tower?)   * Systemtreff Gaming Mid Tower AirForce GT1   * Systemtreff ITS-Raven - Prozessor - Luftkühler  * Gigabyte B550 Gaming X V2 - AM4  * 850W MSI MAG A850GL PCIE5 80+ Gold  => UPGRADE MSI MPG A850G  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0   In the future i might want to play COD2026, which could receive a huge engine upgrade... and i also will run ~500 tabs at the same time.",AMD,2026-01-08 19:24:33,1
Intel,nyj771y,"Hi there, hope everyone is doing fine and started new year on a good note :)  Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically ([https://www.powercolor.com/product-detail214.htm](https://www.powercolor.com/product-detail214.htm))  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?  Thank you !",AMD,2026-01-09 03:43:02,1
Intel,nyld64a,"What does the 18th byte do?  On my system it changes on a daily basis. Display port radeon software rx 580  Also Current Link Settings - 2.7 Gbps x 4. Seems I have a bandwith issue, should be more as i have a DP 1.2 standard gpu port cable etc  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,89,ff,ff,00,00,00,00,00,00,00,00  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,80,ff,ff,00,00,00,00,00,00,00,00",AMD,2026-01-09 13:35:10,1
Intel,nylqaba,"AM4 CPU Compatibility question.     I currently have an HP system with a Ryzen 2700.   I'm thinking about picking up an AM4 motherboard for a ""new"" build. To be precise, an ASRock B550M-ITX/AC.  I can get a Ryzen 2600 on the cheap and swap out the 2700 into the new mobo. That way I can hand the HP system to my wife as an upgrade.  But. According to the ASRock lists. These older CPU's are not compatible. Just the 3000 series and up.  My question is, what makes these older CPU's incompatible on the same socket? I see some Chinese boards that support the 1000 to 5000 series Ryzens.  Right now, I can get the ASRock new for a decent price. Given the DDR5 debacle, I still have enough DDR4 sticks laying around that makes sticking to AM4 an easy , affordable choice.",AMD,2026-01-09 14:43:10,1
Intel,nymut0x,"HELP - GPU not detected after Ubuntu boot repair and CSM toggle  SYSTEM SPECS CPU: AMD Ryzen 5 9600X GPU: AMD Radeon RX 9070 XT Mobo: AsRock B650M PG Riptide Main Storage: Crucial T500 M.2 NVMe (Windows 11) Secondary Storage: ADATA SATA SSD (Old Ubuntu install)  THE ISSUE My PC was working fine until I tried booting into an old Ubuntu installation on my secondary ADATA SATA SSD. Now my RX 9070 XT is not detected at all in BIOS or Windows Device Manager, and I only get display output from the motherboard.  WHAT HAPPENED To see the old SATA SSD in the boot menu I had to enable CSM in the BIOS. Booting into that drive resulted in a black screen. I then used a Live USB to run the boot-repair utility with the recommended repair settings. This seems to have installed the GRUB partition onto my Crucial T500 M.2 drive instead of the SATA drive. Now I can boot into both OSs, but the GPU is completely invisible to the system.  CURRENT STATUS Windows Device Manager only shows the Integrated Graphics. When I try to install AMD drivers, the installer fails because it cannot detect the GPU. I was briefly able to get graphics output from my GPU by unplugging the PC, flipping the PSU switch to off, and holding the power button to empty the capacitors. I then booted it up with the HDMI cable plugged into my GPU and I saw the asrock logo but it was stuck there for 2 minutes and I impatiently turned it off. I'm also considering trying this again and letting it run it's course  PLAN AND QUESTIONS I am planning to disconnect the SATA SSD and try to wipe the Ubuntu boot entries from the M.2 drive to see if the GPU reappears. Has anyone experienced a Linux bootloader repair or CSM toggle hiding a GPU from the BIOS? Specifically, could the Crucial T500 and ADATA drive conflict be causing PCIe initialization issues after the CSM change? Should I try clearing the CMOS first? Any help would be greatly appreciated. Thanks!",AMD,2026-01-09 17:47:28,1
Intel,nyp2whf,"I can buy a Ryzen 7 5700 (without X, the one that is 5700g without a built-in graphics chip) for 120 euros (\~$140) or 5800x for 188 euros (\~$218). Is it worth the extra? My GPU has 16GB, so PCIe shouldn't have too much of an impact.",AMD,2026-01-10 00:05:44,1
Intel,nz62itx,"So I have a question guess I want a 2nd opinion, with the new information of AMD potentially bringing back some old AM4 chips.   I recently bought a ryzen 5800x for my little brother to upgrade his 3600 thing is my brother is currently at the military until May so his upgrade isn't super urgent. Should I return it and potentially hold out on the chance that AMD brings back the 5800x3d?!",AMD,2026-01-12 14:39:27,1
Intel,nzq963u,# OpenGL to DX12 Wrapper on Windows? Do any of you guys know a program that wraps OpenGL API to DX11 or 12? Something like Dgvoodoo2.,AMD,2026-01-15 14:01:35,1
Intel,nzrzrx7,I have seen alot of posts about changing the Curve Optimizer to All cores -30 or -20 and it gives you same the performance and lower temps?  is -20 or -30 better?  and how lower does the temp go?,AMD,2026-01-15 18:51:03,1
Intel,nzssz2e,"Hello guys, hope u're doing well today.   I currently have a perfectly working system, Ryzen 7 9800x3d, X870E aorus pro ice, and 2x16gb trident z5 royal neo 8000mhz cl38.   got a pair of corsair vengeance rgb cudimms, 2x24gb 8000mt cl38, as an upgrade.   after I got them I found about the cudimms and the ""incompatibility"" with ryzen which makes them run as a normal udimms in ""bypass"" mode.   searched a lot on the internet about performance or benchmarks, couldn't find any.   since I'm thinking of returning them if they don't work, and I can't if I open them, I'm looking for someone that has tried cudimms on 9800x3d and could share his experience, that would be awesome and I'll be really thankful.",AMD,2026-01-15 21:05:23,1
Intel,o007h0r,"Question about dealing with overheating  Playing a game I'm getting 95ºC on my GPU and the performance is tanking. I know it's overheating. However, when I stop playing, the temperatures go down, I try playing again an hour later, now I'm getting 75ºC and the performance is just as bad as before. Has it not really cooled down somewhere inside?   It's a 5 year old 6900XT and I'm trying to see if I can reapply the thermal past, but until there I'm trying to see if there's anything else I can do to keep it from getting to the point of overheating, and I don't know what to do to really reset the test except waiting for the next day before I try again.",AMD,2026-01-16 22:23:58,1
Intel,o03mdoz,"Hi all,  Trying to use the curve optimiser for my 7800x3d so going Advanced CPU configuration>AMD Overclocking>Precision Boost Overdrive then setting it to advanced to use the curve optimiser. However every time I go back into the BIOS it's changed from advanced to enabled.  Does this mean it's not saving my settings or is it just a visual error - wondered if anyone else has had this?  The other reason I ask is ran three cpu cinebench tests with the curve optimiser set to -15, -20 and -25 and all returned results within 0.1% of each other and same temps too so doesn't seem to be working as I expected.  I'm pretty new to this so any help is appreciated thanks :-)",AMD,2026-01-17 13:18:40,1
Intel,o073hc7,"I have been looking at the user manuals for motherboards like H13DSG-OM which support eight MI300X Instinct GPUs.  *None* of them explain whether the midplane (like BPN-GPU-GP801) **must** be fully populated with eight GPUs, or if it will boot and operate with fewer GPUs installed (like four, two, or only one).  Does anyone know if any of the OAM motherboards (of any manufacturer, not just Supermicro) for MI300X support such partial installs?",AMD,2026-01-17 23:41:57,1
Intel,o075i10,"I have a 9800x3d , is it worth having a negative curve of -30 and +200 on core clock for games?   I tried cpu intensive games and I didnt notice really any fps difference at all",AMD,2026-01-17 23:52:44,1
Intel,o08w39x,"PLEASE HELP. I just finished a build for my friend and I'm getting frequent no display out on power on. This is only fixed by a restart but I'm PASSING all my POST lights. The only thing I can think of would be a bios update but I'm not sure, these are all brand new parts.",AMD,2026-01-18 05:58:32,1
Intel,o0dmvvr,"AMD Ryzen 9 9950x  ASUS Strix X670E  my current clock speed is over 5500 MHz (showing in NZXT cam, but Task Manager shows slightly lower), default is 4300MHz. Power hovers around 40-60W while browsing.  i have never touched anything regarding overclocking so dont know whats happening, didnt realize how long this has been happening.  I just tried disabling PBO in bios but that didnt change anything.  Any suggestions? Finding conflicting information online",AMD,2026-01-18 23:18:01,1
Intel,o0l1emh,"# Is the 6000 series cards stuck with 25.3.1 forever now for stable VR?    I keep trying the updated software for my 6800XT, and every time it still has the stuttering/ghosting issue that doesn't occur with the 25.3.1. So I keep rolling back (properly with ddu) but now games aren't just whining about the old drivers (ninja gaiden) but Battlefield 6 flat out won't allow me to play (without some registry hack) So with AMD not doing any work on this card's drivers anymore, does that also include this issue that is THIS old being completely ignored?",AMD,2026-01-20 01:12:52,1
Intel,o0n18nb,"Should I disable my iGPU?  I have a 9600X and 9060XT and wondering if there is any benefit to leaving the iGPU active. I noticed that it allocates 512 MiB of system ram as vram, but I wonder if I could miss out on something if I disable it.",AMD,2026-01-20 09:31:19,1
Intel,o0q9hqx,"Hi everyone, the refresh rate of my Lenovo Yoga Slim 7 13ACN5 drops suddenly when I connect it with the Gigabyte M027Q28G. I can't change the refresh rate back then in that case.  I use my laptop basically as a desktop so I only use the display of the Gigabyte monitor. The issue only happens when I connect my laptop with the Gigabyte M027Q28G, when I try to connect my laptop to other monitors the issue doesn't even appear and the refresh rate is stable throughout the entire usage of the monitor.  When my twin brother hooks up his laptop to the monitor he doesn't suffer from the refresh rate drop issue at all so the issue only appears when I hook up my laptop to our Gigabyte M027Q28G.  When I'm using the monitor I connect it with a HDMI cable to my laptop through the Baseus Joystar 7-in-1 USB-C Hub as my laptop has only USB-C ports so no HDMI port or a DisplayPort port at all.  I've tried the following things in order to try solve the mentioned issue:  * Reinstall the GPU drivers of my laptop. * Pull the HDMI cable out of the HDMI port on my USB-C hub and then plug it in again.  Regrettably these things I tried didn't solve the issue so now my question is how I can solve the refresh rate drop issue when my laptop is connected to the Gigabyte M027Q28G.  Thanks for your help in advance lastly! :)",AMD,2026-01-20 20:17:48,1
Intel,o18qds7,My amd icon in system trey has a ! mark next to it i don't have any notifications i restarted and it wont go away,AMD,2026-01-23 14:21:49,1
Intel,o1g8osf,"**Is it worth waiting for the 9950x3d-2?**  My current system is a ryzen 5950x w/ 64Gib RAM on an x370 mainboard (Asus Crosshair VI Hero).   The mainboard is 7 years old (bought back then with a ryzen 1800x) and I really want to replace these components in my main PC.  The sooner I am thinking, the better, since I am using it 99% for work (Software development, virtual machines, local LLMs, databases, etc.) and everything needs to work - always.  Now the question is: Wait out the upcoming 9950x3d2? Or just go for the existing one?  I would pair the processor with a Crosshair X870E Hero.  Is it worth the wait considering my main use for the rig? My feeling says ""no"", but I am curious what others think about this :-)",AMD,2026-01-24 16:39:29,1
Intel,o1l4dza,"Hey everyone!  I’m thinking about upgrading my GPU and found two used options: 🔹 PowerColor Red Devil RX 6800 🔹 Sapphire Nitro+ SE Edition RX 6800  I currently have a Ryzen 7 5700X and I’m wondering if these cards are a good pairing with that CPU. I want to avoid any heavy bottleneck, and ideally get good performance for 1440p gaming.  Here are some pictures of the two cards: (attach your photos)  My questions: 	1.	Will the RX 6800 pair well with the Ryzen 7 5700X without significant bottleneck in most modern games? 	2.	Are there any major downsides to buying either of these used cards (e.g., reliability, VRM cooling, Coil whine, etc)? 	3.	Is one version (Red Devil vs Nitro+ SE) generally a better choice for long-term use?  Thank you!",AMD,2026-01-25 08:34:41,1
Intel,o1l4fkp,"Hey everyone!  I’m thinking about upgrading my GPU and found two used options: 🔹 PowerColor Red Devil RX 6800 🔹 Sapphire Nitro+ SE Edition RX 6800  I currently have a Ryzen 7 5700X and I’m wondering if these cards are a good pairing with that CPU. I want to avoid any heavy bottleneck, and ideally get good performance for 1440p gaming.  Here are some pictures of the two cards: (attach your photos)  My questions: 	1.	Will the RX 6800 pair well with the Ryzen 7 5700X without significant bottleneck in most modern games? 	2.	Are there any major downsides to buying either of these used cards (e.g., reliability, VRM cooling, Coil whine, etc)? 	3.	Is one version (Red Devil vs Nitro+ SE) generally a better choice for long-term use?  Thank you!",AMD,2026-01-25 08:35:05,1
Intel,o1q6ffb,"hello. I recently built a computer with a 7900 XTX and a 9800 X 3-D. I’m getting really good frames, especially compared to the laptop I was using, but I’m really curious to see how far I can push this system. So far, I’ve been dissuaded by the bigger warning on the AMD overclock section on the app, and I’m not sure how to do it or what to do. my temps are fine both CPUNGPU are around 50°C under load. My power supply is like 200 W more than I need if that matters",AMD,2026-01-26 00:31:18,1
Intel,o2bf2cw,"Hey all! I recently started venturing into Linux gaming and everywhere it says that AMD GPUs are way better than Nvidia's on that OS. That said I currently have an RTX 4060 and I love the performance since I game only on 1080p for now. I want to get around the same amount of performance or better if its possible for the price. Also, I have a Ryzen 5 5600 CPU so it would be nice to have an all AMD system.   Is there a good AMD equivalent I can get? Thanks!",AMD,2026-01-28 23:58:34,1
Intel,o2dekrv,"I need help! My rig was having issues mounting drives, and crashing, and hang ups, so I figured my 13900k was finally dying (as expected from Intel). So I bought a 14900k and that wouldn't boot even with the latest bios. I had an Auros Z790 elite AX motherboard with 96gb Corsair vengeance 5200mhz DDR5 ram, and RTX 4090. So I said screw it, I'm gonna rebuild my PC and switch to AMD.   So I bought a MSI x870e Tomahawk Wifi Mobo, and Ryzen 9950x3d. I assembled everything, and it posts first try. Great. So I install Windows 11 and get to configuring things, removing bloat, etc, and I start having freezes. And my screens blinking off and a message telling me there was a failure and that it needs to put the graphics into safe mode.   On top of that, I was having a lot of random hiccups and lag. I checked with LatencyMon and was having all sorts of DPC latency with my Nvidia drivers. So I uninstall the drivers with DDU and install an older driver (566.36) and cool, things seem more stable. Except they're not. Now I'm getting high latency from other drivers like storage and network. So I'm thinking okay maybe it's the ram. So I run memtest86 overnight only to find my PC shut off at some point. I figured the ram must be faulty. So I took out one stick and tested it with the other and the test completed. I'm thinking okay, ram stability could explain a lot of things, so I've found the issue. So I stay with one stick and up the DRAM voltage to 1.35 to see how it goes.   I'm still getting intermittent latency. Some programs crashing, and on top of that, my PC won't shut down now. When I press shut down, my screens turn off and it seems like it's off, but my fans and light and everything else are still running. So I have to hold the power button to shut it down.   I am suspicious of my PSU because I think it could cause some of these issues, but I've had this PSU only since 2023 and it's a Corsair HX1000i.   The only things I kept from my old build were the PSU, GPU, and ram. And considering both systems were having issues, it makes sense that maybe one of them is acting up. But I don't really know if that explains all the problems I've been having.   I've updated the Bios, I've reinstalled windows, I've tweaked power plans and bios settings, reseated hardware, and I feel like I'm just on a wild goose chase.   I'm hoping maybe someone else here has had a similar experience and can help, or if anyone has any suggestions, I'd appreciate it.",AMD,2026-01-29 07:24:47,1
Intel,o2m6wri,I/O Clock stuck at 1200 please help,AMD,2026-01-30 15:19:05,1
Intel,o2mii1p,I/O Clock stuck at 1200 please help,AMD,2026-01-30 16:11:30,1
Intel,o2phbk0,Already bought a asrock b650 to pair with my 9600x and I was unaware of the burnt cpu issue and I cant really return it. Am I safe to use it with the newest bios?,AMD,2026-01-31 00:40:15,1
Intel,o39emib,"Hello,  I'm thinking of swapping from Nvidia gpu family to AMD to pair with my 7800X3D (love it) but I rely on Nvidia Broadcast a lot to remove voice echo from discord since I use a tabletop Mic and speakers (I hate wearing headsets). Broadcast completely eliminates this issue like a miracle so I'm wanting to hear if anyone else does a similar setup with success on AMD full rigs. Thanks in advance!",AMD,2026-02-03 01:04:28,1
Intel,o3cf6l0,"I’m currently on an **i5-12600K** with an **RX 7900 GRE**, and I have a chance to grab a **Ryzen 9 9900X** through a **Micro Center deal** (full AM5 + DDR5 switch).  Main use:  * 1440p high-refresh gaming * Gaming + YouTube / browsing on a second monitor * Some multitasking, nothing crazy  I know the 12600K is still solid, so I’m trying to figure out:  * Is the upgrade actually noticeable in real use? * Worth the platform switch, or better to wait / go a different route?  Would love to hear thoughts, especially from anyone who’s upgraded from Alder Lake or gone Zen 5.",AMD,2026-02-03 14:15:59,1
Intel,o3jaj5t,"I'm at my wits end. Started having issues with my 7800x3d build a few weeks ago randomly.  It will randomly shut down, or if I shut it down, then it will boot maybe 1/100th of the time. It just sits there with the DRAM and CPU lights lit up like its memory training, I've left it overnight and nothing. I can't even get into the bios, it won't post at all but everything spins up. My mobo is a MSI b650 tomahawk. Seems like the shutdown last night was the last one, hasn't come back up at all.  I have tried:  Resetting CMOS  Flashing new bios  Reseating everything and checking CPU pins (all clear)  Booting with a single stick of ram, booting with the other stick of ram only  booting with no GPU.  Nothing works.",AMD,2026-02-04 14:32:07,1
Intel,o3m4ym3,"I have a TUF Gaming X570 Plus (Wifi), BIOS 5021. It does not have the 2023 Microsoft certificates that will be needed for secure boot starting this June. Does the latest beta (5044) carry these certificates?",AMD,2026-02-04 22:34:08,1
Intel,o3zq2ta,"I am an aspiring PC junkie. I am still on the AM4 motherboard and want to max out my CPU so i can keep the PC for at least 7 to 8 years. I wanted a Ryzen 5950x but recently came across the 5900XT. I am the type to have pride and want the best of the best. Even with money not being an issue, the price difference, it's almost like AMD is forcing you to buy the 5900XT. Is there any other difference than just a 100mhz? Is it still worth the extra 100$?  I do content creation, editing, streaming, and gaming.",AMD,2026-02-06 23:35:01,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89°C. If you're seeing temperatures over 100°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 °C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nxtun1i,"check if memory and/or fabric clocks spike at the same time (max values basically), if they do it is a sensor bug.",AMD,2026-01-05 15:16:47,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nyidpo3,"The ram by itself is fine, though you'll probably need to manually set it to something like 6000 CL32. You can get the expo sticks, but the only thing that'd really change for you is the preprogrammed profile to allow you to get it with just a single setting.  The 7200 should run out of the box, but the CPU will switch into a different memory mode where it runs its memory controller at half clocks that lowers performance until about DDR5 8000, so that's why you'd go to the 6000 instead",AMD,2026-01-09 01:04:11,2
Intel,nyv1n08,good point,AMD,2026-01-10 22:03:25,1
Intel,o3l30vk,"its definitely going to improve a noticeable amount on all 3 cases, but if you're having some issues with the second monitor a GPU upgrade would probably help more",AMD,2026-02-04 19:31:46,1
Intel,o3l04ky,Could be a PSU issue. Check the connectors from tge PSU to the mobo for any damage or   that they're seated correctly.,AMD,2026-02-04 19:18:12,1
Intel,nxn91rb,"it's not constantly hitting 100 °C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxrzsmj,"Thanks bud. It's the actual AMD card branded 6950xt, some people mis name it as a founders edition. I'll.try reach out to AMD today.",AMD,2026-01-05 06:52:53,1
Intel,o3l2dsb,Turns out my MOBO was bunk. Swapped it out and all Gucci.,AMD,2026-02-04 19:28:45,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,147
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. I’m loving it,AMD,2025-12-22 10:10:46,32
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,4
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,3
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,4
Intel,nvahjmg,Only compares 8GB cards from teams red and green since it’s only considering <$300.,AMD,2025-12-22 00:43:27,10
Intel,nw2aruf,😮🫳🍿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-14
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-16
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-8
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,o0nywwz,Does the 9060xt with fsr have higher performance than 5060 oc edition No Ti with dlss? And by how much?,AMD,2026-01-20 13:46:49,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-13
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-32
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,8
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,o18s8r6,"Honestly? Pretty decent deal for a 8GB model, considering how the 16GB cost around 450$ these days, almost double the price.",AMD,2026-01-23 14:31:18,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,51
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,9
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,20
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,-1
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-26
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,2
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,0
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,4
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,26
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. It’s also half the vram.,AMD,2025-12-23 20:25:01,7
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-6
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,6
Intel,o18rwrw,"I only play Space Engineers, Cyberpunk 2077, Helldivers 2 and pre-2020 videogames, new games are meh for me.  All of them at high with ultra textures and a few middle settings at things i never notice mid-game at 1440p 27"" IPS monitor, 60fps (don't care for more, maybe one day i'll try 75fps).   Runs very cool and quiet, never hitting above 65°C on closed room 20°C ambient temperature, my case sounds the same as a 20"" metal fan at speed 1, which is the same white noise that helps me sleep at night, an upgrade compared to my old RX 6600 during the summer lol.  Not sure if this helped or way too late, just wanted to comment anyways.",AMD,2026-01-23 14:29:36,1
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because there’s people like me to whom prices in dollars means nothing (or who don’t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).🙂",AMD,2025-12-22 12:11:20,-8
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-8
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,8
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,4
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-11
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMD’s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. You’re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,9
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,7
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  I’m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,102
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,304
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,88
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,25
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,49
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,19
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,87
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,35
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,14
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,53
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,31
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,9
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,29
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,19
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,8
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,9
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,14
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,6
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,23
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,6
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,10
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,21
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,60
Intel,ntb9myj,Please add the broken noise suppression to “Known Issues”.,AMD,2025-12-10 16:33:11,5
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,5
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,4
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,5
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,4
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,6
Intel,ntb4cu0,Where‘s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,14
Intel,ntaofpr,I’m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,20
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,4
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,5
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,4
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,7
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,6
Intel,ntawnis,Still no fsr 4 support for rdna3 🙄,AMD,2025-12-10 15:29:24,10
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,13
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,o14ytde,"u/AMD_Vik Hey there! Just want to let you know that I've seen a lot of people have problems lately (including myself) with Direct-to-Display/Directmode implementations for DisplayPort/HDMI wired VR Headsets. DirectMode cannot enable reliably even on SteamVR native HMDs, and the only way to get them running right now is by installing a older driver version, enabling directmode there, and then updating to a newer version with the directmode already set -> so directmode itself still works fine, but there seem to be problems toggling into this. I also saw this happening with other VR compositors like PimaxPlay though radeon users are generally not as common there.  Happens with every driver released after and including 25.4.1 on my machine, and driver older than 25.3.1 can toggle into Directmode just fine. It is still broken as of 25.12.1 and 26.1.1  at first I thought this was a somewhat individual issue but as I've looked into it I saw more and more people have that problem with RDNA3 and RDNA4 GPUs across the board for basically any VR headset that uses Directmode.  I remember you mentioning on the AMD forums once that you want to be hit up about VR related issues - but really I didn't know how else I would reach out to you.",AMD,2026-01-22 23:12:00,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,6
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,4
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,9
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,13
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,4
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ny008s3,"AMD sucks: FSR 4 is locked to RDNA 4, while NVIDIA’s DLSS 4.5 runs even on RTX 20-series GPUs. My next GPU will be NVIDIA only, and I advise everyone against buying AMD. It’s a greedy company with no respect for customers — you buy a graphics card last year, and the next year it’s already outdated",AMD,2026-01-06 13:00:32,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,5
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,2
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,3
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? 😂😂,AMD,2025-12-10 18:46:48,4
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,4
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,3
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,4
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,2
Intel,ntaitkk,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still present… :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeon™ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,nymudet,"cześć, jest sen instalacji jak używam 7900xtx i nie używam żadnych wspomagaczy ?",AMD,2026-01-09 17:45:31,1
Intel,nzbmbw2,"Hi devs!   I would like to bring again to your attenction this thread: [AMD Software: Adrenalin Edition 25.9.2 Release Notes : r/Amd](https://www.reddit.com/r/Amd/comments/1nk9qgo/comment/nfb2o2j/)  Is there any chance you can bring us 60Hz mode for ex WMR drivers, now working directly in steamVR with Oasis Drivers?",AMD,2026-01-13 08:53:49,1
Intel,o0k4npb,So uh... I just had an application crash on Blue Gate while playing Arc Raiders... I don't think it is fixed...    9070xt with a 7800x3D,AMD,2026-01-19 22:19:09,1
Intel,o3z0v4r,"u/AMD_Vik Hey Vik, I have a question/request that might seem a little strange and niche. Is there any way to have the Windows update AMD OEM drivers available for manual download? If this isn't your department, would it be possible for you to pass it on or redirect me to someone in that department?   I recently did a fresh Windows install, and during a Windows update, the AMD 32.0.21030.2001 driver was installed and worked incredibly well regarding audio and image quality (video & overall). Audio quality, specifically, is still the best I've heard from any AMD driver to date.  I thought maybe the fresh install itself might have been the reason for the better audio quality, so I installed an adrenaline driver that I'm very familiar with, and was disappointed because the audio was noticeably worse. I've now tried every driver besides 25.12.1 & 26.1.1, and the audio quality is substantially worse compared to 32.0.21030.2001.",AMD,2026-02-06 21:22:10,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,48
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,10
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,8
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,54
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,21
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,18
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,23
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,135
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,7
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,27
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,5
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,14
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,6
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,7
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,4
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,17
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,9
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,28
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,34
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,17
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,4
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,7
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,29
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,16
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,60
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,17
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,19
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,10
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,12
Intel,ntcro7s,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,2
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,3
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,28
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,7
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,8
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,5
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,18
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,5
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,23
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,won’t happen,AMD,2025-12-10 16:18:18,3
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,4
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,5
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,o155b38,Thank you for reaching out.   That's really weird - I don't suppose you have any links to posts about this for us to skim through?  I'll follow up with my colleagues about this tomorrow,AMD,2026-01-22 23:45:38,3
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,2
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,4
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,7
Intel,nteeogf,la même. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,6
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,2
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,2
Intel,o3zmuas,"hey little confused about this one.  - Can you elaborate on your audio config? Is this directly driven through dGfx or system IO? - Is this on a mobile system like a laptop or handheld device? if so, can you tell me which model? - Are these subjective assessments of audio quality or are you factoring hard metrics like latency etc?  if you remove adrenalin, WU should step in and serve you their latest WHQL driver. If you have a mobile system, it may grab an OEM-specific package (providing hidden bits for their eDP etc).",AMD,2026-02-06 23:16:19,1
Intel,ntazsxe,It’s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,11
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,19
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,14
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,20
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,19
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,7
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,4
Intel,nzzy88u,Oh thank god it's not just me.,AMD,2026-01-16 21:39:12,1
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,3
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nv8ptlv,přesně zustávám na 25.9.1 všechno jiné crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,5
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,27
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,10
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,7
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when there’s already community workarounds,AMD,2025-12-10 15:19:15,11
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that something’s terribly wrong with something in your software that’s breaking vsync. that’s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,4
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,22
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",AMD,2025-12-10 22:41:15,2
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,24
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,15
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,8
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,7
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,5
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,11
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,16
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,4
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,o15856l,"Most of the conversations happened across various discord servers, so I cannot directly link them, but I also found a few posts here on reddit that seem to point towards the same direction which roughly match with the timeframe.   [https://www.reddit.com/r/Vive/comments/1nl0540/direct\_mode\_not\_working/](https://www.reddit.com/r/Vive/comments/1nl0540/direct_mode_not_working/)  [https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr\_crashing\_on\_amd\_drivers\_after\_2451\_direct/](https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr_crashing_on_amd_drivers_after_2451_direct/)  It's a bit odd because if you set the flag beforehand with a older version and then update to newer versions, Directmode still works fine, but if you fully remove the drivers with AMD's driver removal utility or Display Driver uninstaller (which propably deletes some cached driver file for directmode, is my guess), Directmode disables fully and can't turn on anymore at all.      As far as headsets are concerned, I know at least Valve's implementation seems to suffer from this (I know about cases involving the Valve Index, Bigscreen Beyond and Vive Pro 1, all using Valve's implementation), but somebody in the pimax subreddit community discord seemed to have the same problem, which he also was only able to fix by rolling back the drivers.  I could propably reach out to a few more affected people over discord and encourage them to get active here, if you'd like.  But I do think that it is a fairly reproducable problem, at least on my end, it happened across two different systems.",AMD,2026-01-23 00:00:34,3
Intel,o16r403,"Hi - I am having this problem. I wanted to try the special ROCM driver so dutifully did a clean install as suggested.  Tried to launch SteamVR and it errored. Realised headset had reverted to being a monitor, running at low resolution (less than recommended) Increased the resolution to recommended (combined resolution of the two panels), and SteamVR started detecting the headset - though promoted to enable direct mode. Pressing the button to enable it restarts SteamVR, but then prompts again, and HMD is still a monitor.   Interestingly reverting to the RDNA 4 release driver, it instantly worked - Direct Mode was enabled without me having to toggle.  So it is like SteamVR is successfully toggling something, but the newer AMD drivers ignore whatever it is doing/don't act on it. As soon as older driver is installed, it does act on it, and the HMD disappears from Display Settings/stops being treated as a monitor.  This wouldn't affect the majority of SteamVR users who are streaming to a Quest, and it wouldn't impact people who upgrade to a newer driver without a cleanup.  It would affect people doing fresh installs with a recent driver, and people who do clean driver installs/use DDU.",AMD,2026-01-23 05:17:40,1
Intel,o178o4d,"I installed the latest driver from a couple of days ago, and my Beyond is a monitor again 😭   I didn't explicitly choose to do a clean install, but maybe because I chose to install the AI Package, it decided to do a clean install for me automatically?",AMD,2026-01-23 07:37:51,1
Intel,o31fiek,"u/AMD_Vik Hey! just following up on it!   Did you guys manage to replicate it? Just curious if there is anything happening in that area.  I know development and testing takes time, so I completely understand if nothing happened yet, just curious if you guys had problems replicating the issue.",AMD,2026-02-01 21:06:13,1
Intel,ntpptsg,Yes that’s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,3
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,nylsudn,"Good morning, when will the new AMD Software driver be available?",AMD,2026-01-09 14:55:31,1
Intel,o417fmw,"Sorry for the delay, I had to leave for work.   Absolutely.   I'm on a desktop pc. I have powered speakers connected via usb to a pcie USB card installed in the pc, and I have a separate DAC (for my headphone setup) connected via USB to the motherboard.   All of the differences I'm hearing are from subjective assessments while testing & using the drivers.  All of my listening has been and is with Windows audio set to 16/44.1 for each device. I don't use any of the AMD audio outputs that populate the audio control panel, so I disable them. I don't use any software players; Roon, Jriver, etc. I just listen to music with Qobuz (Wasapi 16/44.1) and Spotify. Additional listening is done with Youtube and Twitch via Firefox. Just everyday simple stuff.  I hear you, the reason I haven't tried running WU again to maybe get that specific driver again is mainly because I don't want to update my system to a build that could potentially cause performance degradation or instability. That's why I would like to be able to download and install/uninstall that specific driver manually, similar to the Adrenaline drivers on the AMD website.  If you would like or need any more information, just let me know, and I'll be happy to share.",AMD,2026-02-07 05:14:00,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,9
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,4
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,16
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,4
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,5
Intel,ntctlcs,🌍👨‍🚀🔫👨‍🚀,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,13
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,o012dhm,Disable mpo,AMD,2026-01-17 01:17:11,1
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,14
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,9
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,5
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,3
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,5
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well 😭🙏. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,4
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,34
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,24
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,6
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,3
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,7
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,10
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,o17bv58,"Scrap that!   I ran SteamVR which caused Steam to crash but restarting it as prompted, my Beyond 2E disappeared as a monitor again - weird!",AMD,2026-01-23 08:06:21,2
Intel,o31hwxw,"Hey there,  Filed this to triage and debug some time ago. A few colleagues outside of that domain have attempted to repro on their personal systems with applicable headsets like the Index and Vive but haven't had any luck so far - the toggle's working for them just fine.  Should be picked up by the T&D team fairly soon, will see what they find.",AMD,2026-02-01 21:17:49,2
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,2
Intel,nym5qu9,"I think our SVP noted in a recent interview it'll be later in Jan, the date they provided was the 21st, though I'd treat this as a tentative timeline just in case anything crops up",AMD,2026-01-09 15:55:10,5
Intel,o44mqlk,"Two close colleagues have very kindly gotten back to me on their saturday to chime in with the following insights:  - One of them suggests that USB audio *should* be unaffected by the adrenalin package. - The other expands on this to mention ""If the audio is choppy, I'd suggest using latency mon to see if there's any long running DPCs or interrupts"", linking to the following  https://resplendence.com/latencymon",AMD,2026-02-07 19:14:23,1
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,6
Intel,ntcauqa,Hell yeah 👍🏻   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,4
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,3
Intel,nthi3lk,I might be an ass but I’m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,2
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,9
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,AMD,2025-12-10 19:55:01,7
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,7
Intel,ntaufrk,Oh great will also test after work it’s been headache since last driver update,AMD,2025-12-10 15:17:58,6
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,3
Intel,nwtxl54,yeah something’s definitely wrong. i’m assuming you’ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,7
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,6
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,😭😭😭😭,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,5
Intel,ntu98tq,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,2
Intel,o12cvk0,issue persists in the newest 26.1.1 update....,AMD,2026-01-22 15:53:23,1
Intel,nym5z84,OK thanks.,AMD,2026-01-09 15:56:13,1
Intel,o46e0sj,"First and foremost, thank you for taking the time to listen/read and look into this, I appreciate it greatly. I'm not sure why, but it seems your reply about the issue possibly being related to ISR isn't showing up on here for some reason. Could just be the site being dumb.  So, the audio differences I'm hearing aren't related to functionality, or the ability to playback audio without interrupts (dropouts, clicking, popping, choppiness). That has been flawless, for me, which is great. I'm referring to audio fidelity differences between drivers.  For instance, I uninstalled the driver I've recently been testing (25.12.1) via DDU and listened to some music with just the Microsoft display adapter being used as the driver. The audio sounded pretty normal again, and what I would consider correct/good. I then ran windows update, and it thankfully installed the 32.0.21030.2001 driver. Listened to some music, and as I heard before, it's still the best sounding driver I've used. The only other driver I've used that comes close is 25.1.1. That one has some slight issues with audio fidelity and image quality, but it was close enough to correct at the time.   The only thing I can think of that sounds similar and would describe what I'm hearing with these newer drivers is when you have a bit-depth and sampling rate mismatch between your music and output device. I have my audio devices set to 16/44.1 in windows, but with these newer drivers (25.9.1-25.12.1), the audio sounds as if the bit depth and sampling rate are set to higher values, even though they aren't. It's extremely strange, and sort of difficult to explain, but that's what I'm hearing.   The frustrating and unfortunate reality that might be coming into play here, is that differences could also vary between systems. So, a bad sounding driver on my system (25.9.1), might possibly sound fine or even good on a different system with different components.  It could also be caused by or related to interactions between code or the registry entries of different programs on the system interacting with drivers, but that's above my level of understanding. I'm just a dude who is noticing stuff (audio & video) and would like others who are smarter and more skilled to look into why there are differences in the first place and what could potentially be causing them.    By the way, you guys have been doing a great job with the video/image quality of these recent drivers. They are substantially better and closer to reference than everything previously released. 25.9.1 and 25.11.1 are the best drivers I've tested/used, in regards to video & image quality.",AMD,2026-02-08 01:11:28,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,4
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,6
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,7
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,3
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,ntciae3,Thank you for letting us know 👍,AMD,2025-12-10 20:12:10,7
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,4
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,4
Intel,o49igbf,"Hope this gets fixed eventually, it's still an issue for both my 9070xt and 7800xt machine",AMD,2026-02-08 15:21:22,1
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,o12dr67,that's... unexpected. Can you tell me what hardware this is with?,AMD,2026-01-22 15:57:19,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,5
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,2
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,2
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,4
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,4
Intel,nwvgoe4,"i don’t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,o12hpew,"can you explain how one would reproduce this corruption who has never used second life and has no $ to spend in game? I am trying to reproduce the corruption you are describing but it seems that it has to do with in-game purchases or ""face layers"". can you explain how to apply these layers to the player?",AMD,2026-01-22 16:15:12,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, I’ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,2
Intel,o131pj2,7900 xt!  I did a DDU and installed the newest driver too so I feel like it isn't carried over unless it was something from my settings ...,AMD,2026-01-22 17:45:13,1
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,6
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,3
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,o1w6e2g,just replying that I did find a regression point for the corruption in second life related to a change in the OGLP api :)   hopefully the change will be coming down the pipeline soon!   Id like to give a shout out to the user Eliza for being AFK where i first was able to reproduce the issue and i used their avatar to bug  check! o7,AMD,2026-01-26 21:07:09,2
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,3
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,o133wr0,"Okay, this is going to be tricky. I was under the impression this was completely eliminated, as we can no longer hit this internally.  Assuming that only your mouse input is blocked, I'll need your help capturing a usermode dmp of the RadeonSoftware.exe process via task manager.  This will involve setting some keys in windows registry. Are you comfortable with this?",AMD,2026-01-22 17:54:58,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,2
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,4
Intel,nx0meck,"i’m using a 1440p freesync monitor, i basically always have fps counter on in all of my games so i can verify that vsync always works. even works without the freesync monitor. frame rate only ever goes uncapped when i disable vsync. is it only an issue at 4k?",AMD,2026-01-01 02:07:41,1
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,o13ecrh,"at this point you kind of need to edit the windows registry to make windows usable: so I'm mostly familiar with the process.     though I'm a bit worried about the ""no longer hit this internally"" part since this has been a thing for quite some time now...",AMD,2026-01-22 18:40:53,1
Intel,nuyojnk,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,AMD,2025-12-20 01:45:48,1
Intel,nwyqsn1,it's tagged as a milestone for feb,AMD,2025-12-31 19:33:08,2
Intel,o13jkce,"it was newly introduced in the 25.20 branch, eliminated at the tail end of that branch's production lifespan (25.12.1), though it seemed to still repro for several people, including AMDers. We've never observed it with 25.30 release candidate builds, which is why I'm surprised.  [Following this resource](https://learn.microsoft.com/en-us/windows/win32/wer/collecting-user-mode-dumps), can you set the DumpType DWORD at       HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\Windows Error Reporting\LocalDumps\  ...and set its value to 2, leading to the generation of full usermode dmps.  This is a non invasive configuration for the most part, and isn't something you should have to revert, but you can also save this location in the registry editor for ease of access.  When that's defined, you can reboot the system.  The next time you reproduce the issue with Adrenalin In-game Overlay hogging mouse input, see if you can pull up task manager (ctrl shift escape). The 'cursor' will be on the process list header, if you hit tab twice, it should focus on the hamburger menu element. from there you can arrow down to the Details header, and hit space or enter.   With details open, you can hit tab a few more times to cycle back to the process list elements. if you start typing 'RadeonSoftware.exe' (should just need to key in ""rad""), it'll pull that process element into view. I'm hoping you have a context menu key rather than a function button on your keyboard, if you hit that, you should be able to arrow down to the 'Create memeory dump file' option.  When you have that in hand, zip it up and fire it over to me via a method of your choice. I'm partial to https://send.vis.ee  hit me up if you need a hand with whatever",AMD,2026-01-22 19:03:54,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,83
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,74
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,18
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,13
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,17
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,7
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,6
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,27
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,5
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,3
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,7
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,5
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,17
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,5
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,4
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,8
Intel,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,1
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,1
Intel,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,21
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,7
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,83
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,26
Intel,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,4
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,5
Intel,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,1
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,103
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,2
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,7
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,9
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,6
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,6
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,12
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,5
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,18
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,3
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,6
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,7
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,2
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,5
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,5
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,2
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,5
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,4
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,9
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,12
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,15
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,18
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,17
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,22
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,13
Intel,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,6
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,4
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,6
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,16
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,12
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,7
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,5
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,5
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,5
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,6
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,7
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,7
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,-1
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,4
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,2
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,48
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,14
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,6
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,24
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,2
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,5
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,6
Intel,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,4
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,4
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,3
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,2
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,5
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,22
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,8
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,6
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,10
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,5
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,5
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,5
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,3
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,o3db89o,Happy New Year ;)  Were you able to check on this issue yet?,AMD,2026-02-03 16:52:08,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,2
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,o3jnizi,we've done so several times but have not reproduced the issue so far. I'll follow up with the T&D team about this ticket when I'm back at work next week,AMD,2026-02-04 15:36:18,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,163
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,82
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,45
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,5
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,5
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,3
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,5
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,45
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,16
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,3
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,9
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,3
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,4
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,7
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,5
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,3
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,23
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,16
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,7
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,36
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-7
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,81
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,11
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,22
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,16
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-14
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-16
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,5
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,6
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,8
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,57
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,19
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-15
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,49
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,2
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-2
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,5
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,10
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,12
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,10
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,5
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,30
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,5
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-1
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,5
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,23
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,13
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,12
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,4
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,25
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,114
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,16
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,30
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,124
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,69
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,8
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,37
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,36
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,61
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,23
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,21
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,3
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,25
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,6
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,4
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,6
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,10
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,12
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,15
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,4
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,2
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,2
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,5
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,-1
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-5
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-22
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-4
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,46
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,27
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,15
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,6
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,35
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,12
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,8
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,3
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,9
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,4
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,28
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,8
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,6
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,11
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,4
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-6
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,9
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,4
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,1
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,12
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,3
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-15
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,11
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,0
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-2
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,31
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,6
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,45
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,29
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,5
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,13
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,3
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,7
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,8
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,9
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,2
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,0
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,3
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,9
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,19
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,5
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,6
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-14
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,31
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,30
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,3
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,4
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,8
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,16
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,10
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,0
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,12
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,10
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,7
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,8
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,7
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-1
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,2
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-1
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,1
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-4
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,11
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-14
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-13
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,5
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,2
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,7
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,2
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,28
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,4
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,39
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,12
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,22
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,17
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,12
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,5
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,16
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,2
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,7
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,2
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-9
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-8
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-1
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-4
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,3
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,7
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,19
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,6
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,9
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,9
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-1
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,9
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,3
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-3
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-2
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,13
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,2
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,5
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,o419xp2,"only +23% in Linux feels like bad drivers, when the B390's getting 2x performance on Windows",Intel,2026-02-07 05:33:34,19
Intel,o41k2xt,Good. AMD has been rehashing same crap lately. This is why competition matters,Intel,2026-02-07 07:01:00,15
Intel,o426cd8,AMD 14nm moment,Intel,2026-02-07 10:36:23,7
Intel,o42ncwr,"Nice, I'm tired of AMD rebranding old APUs and charging for them as if they were brand new.",Intel,2026-02-07 13:01:43,3
Intel,o420dq7,What about 8060s? Though?  Who fucking cares about 890M? It’s made for business laptops…,Intel,2026-02-07 09:37:29,0
Intel,o3zmils,I want Intel to win but sadly they only win against the HX370 which is a generation behind the 395+. So intel is still behind. They better price it well below the HX370 to regain market share. AMD lost the plot in terms of pricing and HX370 handhelds are well over 1300. If they can partner with MSI and produce a handheld for under 1000 they have a winner. Anything above that is DOA.,Intel,2026-02-06 23:14:29,-24
Intel,o41ay0s,considering sr-iov and passthrough on linux are completely busted in the xe drivers for the B50 and that has been out for months it wouldn't surprise me if these are rough as well.,Intel,2026-02-07 05:41:39,6
Intel,o421h3i,RDNA 3.5++++,Intel,2026-02-07 09:48:29,13
Intel,o45uzpk,"Yes since they have this new loser as the head of client they don't build anything new. They just recycle old shit with new marketing name. I am sure he will kill the business in 2 years. Saving grace is AMD is untouchable right now for desktop, and Intel sadly is full of useless VPs that spend their days in meetings.",Intel,2026-02-07 23:14:29,1
Intel,o42u6kw,"8060s still comfortably beats out the B390, but it's designed for a different power class and draws a lot more power.",Intel,2026-02-07 13:44:59,6
Intel,o45a3to,"The 890M is in the Z2 Extreme. It's not just for laptops, it's their prime handheld GPU chip.  The 8060S only starts beating the B390 around 33 W, making it unfit for a handheld device where battery life is very important",Intel,2026-02-07 21:19:03,2
Intel,o3zrsvz,the 395+ is a different segment of product imo. They start at $2200+ while the intel chips start around \~$1200 so far. The battery life also isn't even close.,Intel,2026-02-06 23:45:02,38
Intel,o3zrwwh,Panther lake and 395+ can’t be compared the 395+ is a much bigger die and with double the bus they’re completely different tiers,Intel,2026-02-06 23:45:41,25
Intel,o40cvka,"Strix Point and Strix Halo are the same generation, one is just much bigger and more expensive and for an entirely different performance tier. You're coming into a 5050 vs B580 comparison and saying that a 5090 is faster than the B580. Duh, of course it is.",Intel,2026-02-07 01:50:11,10
Intel,o40lyhs,The price of panther lake is gunna be expensive too,Intel,2026-02-07 02:46:26,-3
Intel,o424dp4,"Exactly. I was really disappointed with their igpu tactics. Rehash, rehash, rehash.   I do think we need DDR6 for next major igpu scaling, as memory bandwidth is a problem too.",Intel,2026-02-07 10:17:09,7
Intel,o40emvu,Which laptop is releasing at $1200 with the B390,Intel,2026-02-07 02:00:58,2
Intel,o45uscn,"Yes agreed, but if the machines offering them are in the same ballpark then it is a failure on intel. A machine sporting the B390 should be cheaper than an equivalent machine with the HX370. We need handhelds with B390 below the 1000 mark for example.",Intel,2026-02-07 23:13:16,1
Intel,o40fa3x,"[https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780](https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780)     there's this $1400 one. there's also only like 5 available series 3 laptops. if this 16"" is starting at this price, I'd expect the 14"" laptops to be a little cheaper.",Intel,2026-02-07 02:04:52,11
Intel,o41rwbo,This seems like a significant outlier from what I can see and it's unavailable so unclear if it's an error.    I also can't see this on the hp website.   Guess we'll have to wait and see. From what I've researched the x /h processors are significantly more than the base,Intel,2026-02-07 08:14:15,2
Intel,o416ch9,it's unavailable .....possibly it's a subsidized product.....,Intel,2026-02-07 05:05:43,-3
Intel,o428rdn,"yep, this  basically all the new B390 laptops lists around a same ballpark price as the 395+ ones  just over 2k USD",Intel,2026-02-07 10:59:32,2
Intel,o41au1b,"it ran out of stock a couple days ago. There's a few open box ones you can still pick up though. most of the panther lake laptops just aren't released yet. They might go up in price due to ram, but everything else will as well unfortunately.",Intel,2026-02-07 05:40:45,5
Intel,o45ul90,"That is my point. Not worth the price if it barely beats the HX370. And just the fact we need to have this conversation shows how Intel Marketing miserably failed. If the B390 devices are cheaper, please make sure you plaster it everywhere and communicate it well. Even the people interested in it like me do not know or believe it.   Again if anyone at intel reads this, you better price this lower than the HX370 or you have no chance. I do not like AMD but sadly they have the best overpriced products right now.",Intel,2026-02-07 23:12:08,1
Intel,o45vxqk,this sounds about right.  Intel just said that the margins are PTL are below corporate average. which means yields on 18a are abysmal.  Expect prices to be high and supply to be low until yields improve (which the CFO head said wont be till 2027. But they should gradually improve through 2026.  AMD and intel are in a race. Intel need to improve 18a yields. AMD needs to get zen6 out. Whoever gets there first will be the winner. Until then it kinda looks like the laptop market is just a continuation of what it was before. AMD gets a slight pickup from the 400 series. Intel gets a slight bump from PTL.,Intel,2026-02-07 23:20:02,1
Intel,o431u16,well why don't they restock it ??,Intel,2026-02-07 14:29:14,1
Intel,o35qnw3,"Honestly, I wish Intel would give these 'Intel Graphics' model numbers. An Intel Graphics on a N series SKU is going to be far weaker than a B370/B390 masquerading as 'Intel Graphics'. That or just give these ARC iGPUs that don't meet the RAM speed a number like ARC B370e/B390e instead.",Intel,2026-02-02 14:22:42,29
Intel,o37ku63,Is panther lake getting an N series release?,Intel,2026-02-02 19:33:06,2
Intel,o397w1s,Wildcat Lake for the N Series,Intel,2026-02-03 00:27:12,1
Intel,o3ch73d,"That looks like it's filling in the ""ultra 3"" tier, not the N/celeron/pentium tiers.",Intel,2026-02-03 14:26:50,1
Intel,o3nclgj,"Wildcat Lake will succeed N200 Series, but will be named under Core 3 Series brand, likewise Panther Lake for the Core Ultra 3 Series.",Intel,2026-02-05 02:36:55,1
Intel,o1zvssn,iGPUS are replacing the low end discrete graphics. That's incredibly impressive,Intel,2026-01-27 11:10:17,38
Intel,o23z1r4,Pat did a good job.,Intel,2026-01-27 23:05:06,6
Intel,o1zd4re,Will panther lake pc CPU's have this kinda powerful igpu?,Intel,2026-01-27 08:20:52,12
Intel,o2245xf,"Have any of the ""regular office worker""-level CPUs been seen yet?  I am hoping we can see 8 to 10 hours of battery life under real usage.  We get 12 to 14 with MacBook Air 13's, so we would like to at least see a full work day with the Intel CPUs.  We were really hoping we would have seen that with Lunar Lake, but we usually see closer to 4 to 7 hours under real usage.",Intel,2026-01-27 18:05:29,2
Intel,o1zd2wu,"Stop with these fake benchmarks.  You will see when the benchmarks are not cherry picked, it will be no where near a 4050 lol. They had to gimp the poor 4050 all the way down to 30w, which I don’t even know how it even operates as such low wattages for the b390 to have a chance.  The full wattage 4050 (100w+) is about as fast as a 3060 desktop, this tiny iGPU in real world gaming tests will be at most best case scenario as fast as a full powered 3050, which is still  a massive 50-60% slower than a 4050 laptop.",Intel,2026-01-27 08:20:23,-21
Intel,o2004r5,"Because the low end discrete options are non existent or outdated, or bad value. The door is open for anyone to fill in that low end spot.",Intel,2026-01-27 11:45:12,22
Intel,o20fvx6,"It has to be cheaper first.  Current pricing of laptops with these chips is looking really high.  Granted they are mostly 'nicer' laptops, but still, really steep.  Check out the XPS pricing for example...",Intel,2026-01-27 13:28:05,5
Intel,o1zymzc,Impressive if the cost is good. Not so much if they're priced too high like Strix Halo.,Intel,2026-01-27 11:33:27,7
Intel,o206dww,yeh its amazing but nvidia and amd did it to themselves...  rtx 3050/4050/x should not be priced as they are currently.,Intel,2026-01-27 12:30:17,6
Intel,o2c33xf,Papa’s here,Intel,2026-01-29 02:07:51,1
Intel,o1zdgl7,You mean desktop? PTL is mobile only.,Intel,2026-01-27 08:23:53,31
Intel,o20iisi,"For desktop, we're stuck with Raptor+Arrow Lake until Nova Lake, which uses entirely new P and E cores even from Panther Lake.    So no.",Intel,2026-01-27 13:42:14,4
Intel,o23m337,Laptops are pc so yes,Intel,2026-01-27 22:02:53,1
Intel,o1zdh08,No lol,Intel,2026-01-27 08:23:59,-5
Intel,o25o55z,"Lunar lake already gets over 12h battery     I wish people would stop comparing it to Macs, people in businesses and people who use specific programs or even games will never use Macs",Intel,2026-01-28 04:31:07,2
Intel,o1zlgrm,It trades blows with 65 watt 5050.   And it does that while using less than that full package. (including cpu)    No one is saying its better than 130 watt gpus.,Intel,2026-01-27 09:38:47,19
Intel,o20j2fp,[https://www.youtube.com/watch?v=jrygnUnBRNI&t=1s](https://www.youtube.com/watch?v=jrygnUnBRNI&t=1s)  Shows it's like 3050Ti performance.,Intel,2026-01-27 13:45:07,3
Intel,o22e0if,"Low-end cards don't make sense for anyone involved. the fixed costs are far too high, you have no margin, and it just looks stupid compared to a slightly more expensive card on which you can put twice the GPU.",Intel,2026-01-27 18:47:06,2
Intel,o21y41l,"True, but it's probably the ram situation. Unless intel is asking for a lot to recoup the 18A investment... Theoretically the PTL SOCs themselves shouldn't be the pricing issue",Intel,2026-01-27 17:39:38,2
Intel,o203kv0,"This. If 8060S Halo had been available.. at all, but also.. in sub 1600 laptops, or 8050S in sub 1200, it would have been wonderful. Same is true for PTL with B390 + 9600 MT/s RAM. I hope they will be competitively priced (also, hope the RAM pricing doesn't hit them too bad).",Intel,2026-01-27 12:10:46,12
Intel,o206nt4,There is laptop models priced at $1300 with the iGPU you want.  I don't know what the chip pricing is - we never really do with mobile - but it seems low enough that regular priced laptops can include a high end Panther Lake chip.,Intel,2026-01-27 12:32:08,4
Intel,o2076gf,"To be fair...they are priced very cheaply. There was a $600 5050 laptop on sale last year even.  Nvidia volume pricing on 50 tier mobile GPUs seems to be very attractive, it's up to manufacturers how much extra they want to tack on but you can build a very cheap, very capable laptop using this GPU.",Intel,2026-01-27 12:35:40,5
Intel,o1zho72,Until some chinese manufacturer makes a hacky desktop motherboard that is,Intel,2026-01-27 09:03:10,24
Intel,o2oxene,Is the rumored drop by end of 2026 accurate to your knowledge?,Intel,2026-01-30 22:51:31,1
Intel,o226459,Our fleet are the Dell Pro Plus 16-inch.  We can probably eek out 10 hours if we really baby it.,Intel,2026-01-27 18:13:53,1
Intel,o22efrd,"what's ""real usage""? i can do mildly CPU-intensive work on my 14"" premium pro whatever laptop and it won't last more than 10h.",Intel,2026-01-27 18:48:53,1
Intel,o25or77,>people in businesses will never use Macs  Wrong.,Intel,2026-01-28 04:34:57,2
Intel,o1zmcv6,"It does not lol….  Once you see the real gaming benchmarks it will be wayyy slower with its low powered small gpu cores. Idk what all you people are smoking, heck the 120w 8060s with its massive die is still 13% slower than a 4060 laptop in a 20+ game average tested by Jarrod’s tech.  It’s at best case scenario in real uncherry picked games 3050 performance, considering the 140v is about as fast as a 1650, even though that was leaked to be as fast as a 3050 in cherry picked benchmarks.",Intel,2026-01-27 09:47:05,-16
Intel,o20pa10,"Exactly, I hate these cherry-picked overhyped tests and claims.   It’s a small iGPU, you can’t possibly expect 4050 and definitely not 5050 discrete level performance lol, and I get downvoted so hard even though I’m right… reddit moment.",Intel,2026-01-27 14:17:11,0
Intel,o220te4,"If that is true, then we should see some spiking up in costs across the space in which case you'll get a collapse in shipments since relatively few people can afford 2k laptops and I'd guess people are more sensitive to a laptop priced at 1200 vs 1500 compared to something priced 2000 vs 2300 since the most price elastic part of the market is going to just give up before they even get to 2000.",Intel,2026-01-27 17:51:18,1
Intel,o20rncp,They put too much CPU in them to get them cheap. Though my 8060s z13 with 64gb ram was $1650 (open box Best Buy a while back) that was a lucky sale unlikely to happen again. I’d rather have a super thing and light XPs 13 with panther lake though and probably will unload the z13 when I can get the XPs.,Intel,2026-01-27 14:29:00,4
Intel,o25nygi,"the die is like twice the size it will never be cheap, its a scam chip for people buying stuff for LLMs not for gaming/content creation",Intel,2026-01-28 04:29:55,1
Intel,o20esgd,"If you mean the MSI model I'll definitely be skipping that one. Not a fan of MSI.   $1300 certainly isn't terrible. Better than Strix Halo offerings I've seen, but I hope that's not going to be the low end. I'll keep an eye out. Given the price of all electronics these days I won't get my hopes up. I'd love to be pleasantly surprised though.",Intel,2026-01-27 13:22:05,0
Intel,o1zso7u,Nova lake will use that as well,Intel,2026-01-27 10:43:44,3
Intel,o2p97ux,"Not a rumor, that's Intel's word.  Which is certainly shaky these days.  Though they do seem to be delivering things on time better nowadays, so I'm cautiously optimistic.",Intel,2026-01-30 23:55:45,2
Intel,o24tg9c,"The 16 might have bigger batery than the 14.  I have a 14 really happy, big upgrade from old Laptitude with 11th gen. Hated the Keyboard at first",Intel,2026-01-28 01:41:28,2
Intel,o25p6pg,yeah totally the majority of big businesses with 1000+ employees has a lot of Macs and totally not windows laptops,Intel,2026-01-28 04:37:39,2
Intel,o1zqarn,It was shown to be running RDR2 at 75 FPS on High settings at 1080p and Forza Horizon 5 at 90 FPS on Ultra at 1440p. All while consuming 50 - 70 watts package power.,Intel,2026-01-27 10:22:46,10
Intel,o1zrsjo,"Can you confirm what suggests cherry picking in this review? Looking at it they seem like a fairly standard range of tests, 3D mark and a bunch of popular games.",Intel,2026-01-27 10:36:02,4
Intel,o23avv5,It's the RAM that is going to make them expensive I think... and unfortunately a lot of these with the good chip will have soldered RAM since it's basically a requirement unless they want to use CAMM2.,Intel,2026-01-27 21:13:08,1
Intel,o41aset,there's also the new HP Omnibook X 16 for $1449,Intel,2026-02-07 05:40:23,1
Intel,o208h65,please intel 🙏,Intel,2026-01-27 12:44:13,1
Intel,o22tcmj,"dang. that's real usage, alright.",Intel,2026-01-27 19:54:09,1
Intel,o25pe7y,Majority?  No.  But that isn't what you said.  Stop being a dumb troll.,Intel,2026-01-28 04:38:59,6
Intel,o1zt1i8,"That’s with upscaling turned all the way to the max and frame gen on lol.   You people are so gullible, it’s best case performance is 3050 perf, which is still decent. Intel deliberately says as fast as a 4050 because they gimped the 4050 to 30w ffs, a full wattage 4050 is in its own league compared to this iGPU, that’s around 8060s perf.  Inside thin laptops where a majority will have this chip it will be even slower because it’ll be wattage capped so real world perf will be BEST Case 3050 perf or worse.",Intel,2026-01-27 10:46:51,-6
Intel,o1zuofi,"They gimped the 4050 to 30w, intel’s official “tests” and even then, in their own cherry picked benchmarks it showed the gimped 30w 4050 was still ahead in a few games lol.    This guy I replied to stating it’s on par with a 65w 5050, is stupid and gullible. At 65w the 5050 laptop loses a few percentage of perf, compared to if it was max 100w+. It is no where close to a 5050, at any wattage, whether the lowest 45w or not. Some people are so gullible.  Like I said the timespy score is inline with a 3050, the 5050 laptop has around 4060 desktop perf for crying out loud. This tiny little iGPU with its low combined package tdp is no where close to a 4050, and especially not even in the same universe as a 5050. The 5050 laptop is literally faster than the desktop 5050 because the laptop has gddr7 and desktop version only has gddr6.",Intel,2026-01-27 11:00:54,-2
Intel,o207hvy,Nope. No upscaling -   [Forza Horizon 5 1440p High 100+ FPS](https://youtu.be/AX_rvgsYHJE?si=YxbVFJj2NfiUpD2D&t=633)  [RDR2 1080p High/Ultra mix 70+ FPS](https://youtu.be/AX_rvgsYHJE?si=F4oig0C6V8SoFY1F&t=689)  With Upscaling -   [Spiderman 2 1080p High XeSS Quality](https://youtu.be/AX_rvgsYHJE?si=QatQtMt2flikdvMB&t=738) 75+ FPS  [CP2077 1080p High Xess Quality ](https://youtu.be/AX_rvgsYHJE?si=6s7MvEGrUMjWa3-r&t=753)80+ FPS,Intel,2026-01-27 12:37:47,7
Intel,o1zxp2x,"The article doesn't do this though, and even discusses it?   >Slower versions of the Nvidia GeForce RTX 4050 Laptop GPU up to 60 W are also in the range of the B390 — you need a fast version with a 90 W TGP, for example, as in the Lenovo IdeaPad Pro 5, to have a clear advantage.   The article is in the context of laptop components so obviously they aren't running at desktop TDPs, but your commentary seems... misplaced? Especially given that these aren't official Intel benchmarks.",Intel,2026-01-27 11:25:55,9
Intel,o23w1on,"In the very review this thread is on, they have the 4050 60W as 40% faster. The review itself is very sketch, having the 4050 30W as the same fps as the 4050 60W in Baldur's Gate 3. It's not trading blows with the 5050 65W. It's getting creamed by the 4050 60W.   That means if you have a laptop with a B390, it would/could actually make sense to pair it with the lowest end dgpu from 2023 if one valued performance. The RTX 4050 6GB which isn't even being made anymore.",Intel,2026-01-27 22:50:14,1
Intel,o202y5v,"You will see in real world gaming tests, it’ll lag behind a 60w 4050 laptop by quite a lot, mark my word.",Intel,2026-01-27 12:06:11,-2
Intel,o20bodn,"Ahh, yes, real world gaming tests. As opposed to these tests of games by an independent third-party that (now I've had a bit more time to look) align with all the other reviews coming out and which were also conducted by third-parties.  People that get super emotional about certain brands are weird. Don't be those people.",Intel,2026-01-27 13:04:10,8
Intel,o238d3x,"dudes dense, id stop bothering. i have a lunar lake device, the GPU is nuts for its power envelope.",Intel,2026-01-27 21:01:52,2
Intel,nzwgm1n,Dual gpu? Huh? Haven’t seen one of those since gtx 690. It used to be useless because memory wasn’t shared. I wonder how it will work this time,Intel,2026-01-16 11:14:50,8
Intel,nzxbywf,"lol. Six months late and $300 (60%) over the announced price.  If the dual GPU version was really available for $999 right now (as announced), then Intel would make significant inroads into the local AI market.  As it is, buying this for $800 over a used 3090 is a really hard sell. Compared to a B60, the 3090 is readily available for $1100, and provides the same VRAM, double the compute and memory bandwidth, better perf/watt, and CUDA support.  With the dual GPU cards even at $2k each this does have one single niche - being able to get 144 GB of VRAM in a server at under 1500W for under $10k -  which is legitimately useful for LLM inference.  It's really sad that Intel didn't put in the investment a year ago to have a lot of capacity to produce these now. For the prices to be so high they seriously must be making like 10 chips a week.",Intel,2026-01-16 14:30:14,4
Intel,nzwvdau,NAND SHORTAGE WHO?,Intel,2026-01-16 13:00:20,3
Intel,nzva35v,"> Unlike Sparkle, Maxsun has two cards in its arsenal: the regular 24GB VRAM model with a dual-fan design, and the dual-GPU 48GB model with a blower-style fan  seems nice",Intel,2026-01-16 05:12:03,4
Intel,o04avmj,One card $799? Two cards $1598...,Intel,2026-01-17 15:32:16,1
Intel,nzz5o69,It's no different. Not even an SLI/Crossfire bridge.,Intel,2026-01-16 19:25:21,4
Intel,o135x8c,"To use this card you need your motherboard to support bifurcation. Without it only sees 1 GPU and 24GB VRAM  That wasn't the case with the likes of GTX690, R9 295 etc which the system saw them via SLI/Crossfire and could work on any mobo without supporting bifurcation.",Intel,2026-01-22 18:03:51,1
Intel,nzz8xk5,Those were also kinda useless as they only really handled sync as far as I understood,Intel,2026-01-16 19:40:31,2
Intel,nzzefbl,"What do you mean? They were a proper data bus, iirc.",Intel,2026-01-16 20:05:59,3
Intel,nzzoc4y,"i'm no expert but i dove in to Wiki      it was a [1GB/s to 3.25GB/s](https://en.wikipedia.org/wiki/Scalable_Link_Interface) interface. For refference PCI-E 2.0 was 1GB/s and 3.0 was 2GB/s BUT per lane so 16 and 32GB/s.  For refference HDMI 1.4 has 10.2Gbit/s  So as i understood it most of the data bandwidth were used for tossing the image output back and forth as only a single card were the one outputting the image (assuming you weren't running tripple monitor widescreen setup on SLI GPU's....) and our image data could more or less saturate the link  someone correct my math if i'm totally bonkers, but 2560x1440p = 3.686 million pixels. each using 24 bit (8-bit color on 3 chanels) which is 88Mbit/s. if you output 60 of those that is 5.3Gbit/s. 1GB/s = 8Gbit/s. Okay okay you wouldn't always throw a full image so lets call it 2.5Gbit. You were still using quite a lot of the bandwidth on the image alone, so from my understanding most of the interface communication was more related to ""who did what"".  and in AFR you would be handing over a full frame.",Intel,2026-01-16 20:52:43,1
Intel,o0g6w5o,"It was used purely to transfer rendered images, correct.",Intel,2026-01-19 09:27:32,1
Intel,o0g7wxp,"phew! my understanding was correct! And this is why the new nvlink is different. It's not just image data transfere. But also a LOT faster. NVlink 4.0 is 900BG/s. that is 300x faster than the FASTEST sli bridge recommended for 4K monitors. and that link is 15x faster than PCI-E 5.0 at a full 16 lanes. 15 times! the old sli was only the speed of a single or two pci-e lane. not 225 lanes equivalent speed  edit. One is numbered as unidirectional and the other bidirectional, so half the numbers above. But that is still VERY fast, even if it's a bit slower than the memory bus speed.",Intel,2026-01-19 09:37:24,1
Intel,o0g86uf,"It wasn't used differently on 20- and 30-series, and it's discontinued for the 40- and 50-series.",Intel,2026-01-19 09:40:01,1
Intel,o0g9x44,what do you mean by that?,Intel,2026-01-19 09:56:26,1
Intel,o0g9zvv,I'm saying NVLink did the same job as an SLI bridge for SLI setups. There's a reason it was discontinued,Intel,2026-01-19 09:57:10,1
Intel,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer can’t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,10
Intel,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,13
Intel,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
Intel,nz4rv7t,"“Takes on strict halo” at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,5
Intel,nzi9w6l,"Amd hasn’t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen “AI 7” 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
Intel,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
Intel,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,9
Intel,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,4
Intel,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4 times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,14
Intel,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
Intel,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
Intel,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,16
Intel,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
Intel,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
Intel,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
Intel,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
Intel,nz9efnf,"We’re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
Intel,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
Intel,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,4
Intel,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
Intel,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,14
Intel,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
Intel,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,3
Intel,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
Intel,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
Intel,o21p2wl,https://youtu.be/X9eYQTkzxqU?si=Uzdz-E3QJzzVM42N  Not the only one comparing. Intel actually outperforms at lower wattage.,Intel,2026-01-27 17:00:16,1
Intel,nz610q1,">GPU's will take all the bandwidth you can feed them.   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,2
Intel,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
Intel,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.   https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
Intel,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
Intel,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
Intel,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
Intel,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
Intel,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
Intel,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mm² to 300mm² when compared",Intel,2026-01-13 18:08:18,1
Intel,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
Intel,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
Intel,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
Intel,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,5
Intel,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
Intel,nyet7we,"I wonder if Valve would be considering Panther Lake for a Steam Deck 2. It sounds like a generational leap from RDNA2 (which is what they were looking for), and can be cheap enough for valve to slightly subsidize the cost.   I guess the main thing is the power envelope. I think Valve is only interested in making handhelds that deal with a 15W TDP or lower",Intel,2026-01-08 15:18:52,30
Intel,nyepcic,"Series 3 seems like the biggest Intel W in a long while.   Plays AAA games at 45-60fps with upscaling, and all other games at native at 60+. Not to mention this is 50% faster than AMD's equivalent HX370 while being massively more power efficient.   There's also already laptops listed on sites like Best Buy for reasonable prices (sub $1300). Compare this to the AI 395 from AMD that can't be found for less than $2500 while being significantly less power efficient. Granted, that APU isn't really comparable.",Intel,2026-01-08 15:00:39,31
Intel,nyf9pbk,"the handhelds with these chips are going to slap. Also people gotta remember that using upscaling on smaller screens especially 7-9 inch screens is a lot more tolerable.  and these benchmarks are of triple A titles, monster hunter wilds is a dogshit unoptimized game, and people will be playing a mixture of game from older titles, indies, emulation etc.",Intel,2026-01-08 16:32:58,10
Intel,nyhyhe2,"So remind me, the ARC B390 is not a discrete GPU?  although the ARC B580, and B570 are discrete GPUs?  and the ARC A380 is a discrete GPU?",Intel,2026-01-08 23:45:30,1
Intel,nyejwbi,"That should say ""playable at 540p""",Intel,2026-01-08 14:33:53,-5
Intel,nyf0j6v,"Linux driver support is the only problem at the moment. Lunar lake is already competitive with Z2 extreme in gaming in windows, but not even close in Linux. Hopefully this changes by the time handhelds with panther lake come around",Intel,2026-01-08 15:52:10,24
Intel,nz26s22,There's an Xbox project within the coming years apparently.,Intel,2026-01-11 23:05:33,1
Intel,nygbkt8,I think Valve is looking for an ARM chip. Intel could dip their toes in this market before Qualcomm catches up to Apple.,Intel,2026-01-08 19:16:45,-5
Intel,nyf1d4m,"Yeah, amd went to extremely greedy!",Intel,2026-01-08 15:55:52,6
Intel,nyg345w,AMD should have made a RDNA4 igpu... RDNA5 igpu is gonna be a huge boost.,Intel,2026-01-08 18:40:16,7
Intel,nyg3cne,AMD should have made an igpu from rdna4...  but nope... the engineers are too busy making AI gpus.,Intel,2026-01-08 18:41:17,5
Intel,nyfhqqg,"In a year or two,when they can get panther lake for cheap, this could be resolved. They could even get more efficiency with a refined 18a for the CPU and whatever node is available for the GPU. With the current RAM prices I wouldn’t expect any major console style updates until 2027 anyways",Intel,2026-01-08 17:07:42,5
Intel,nygdjk9,Do you think there's any particular reason why they would want to go with ARM? Quite sure Intel proved here that efficiency is essentially equal between both ARM and x86 here,Intel,2026-01-08 19:25:24,5
Intel,nyhii5z,It's hilarious to think Steam would work with Qualcomm.,Intel,2026-01-08 22:26:42,4
Intel,nyhzkyv,"Very unlikely. Proton and DXVK work alright with x86 but adding ARM conversion on top is, uh, a **very** poor experience. In fact that first generation of Snapdragon laptops had among the highest return rate of laptops I have ever seen. Amazon [literally warns potential customers](https://www.tomshardware.com/laptops/snapdragon-x-powered-surface-laptop-7-gets-frequently-returned-item-warning-on-amazon) about it.  Valve definitely wants a wider adoption rate of SteamOS and ditching x86 is not going to help that. They are **not** Nintendo that can ask devs to target their architecture. It's going well so far because for most games you can just make a standard Windows version, slap Proton and it works within 10% of native performance under optimal circumstances. Anything that can increase incompatibility rate (and it VERY well can, ARM **does not** support AVX2 natively for instance meaning [a lot of games that might have issues](https://www.reddit.com/r/macgaming/comments/1dekmtz/avx2_game_list/) even starting or underperform).",Intel,2026-01-08 23:51:14,3
Intel,nyjz9hz,i'm so tired of people throwing ARM around for everything. superficial much?,Intel,2026-01-09 06:55:17,3
Intel,nyfnu8h,"Intel and AMD especially have this habit honestly, they hit the lead and stagnate *bad* and the catch up for whatever company stagnated takes a hot minute, though hopefully this level of pushing boundaries in the handheld PC space keeps both of them moving at a steadier pace for a while rather than one team moving way the fuck forward",Intel,2026-01-08 17:34:32,5
Intel,nyincie,why didn't they do one? Why use tech that is so old?,Intel,2026-01-09 01:55:33,3
Intel,nyi9wg9,Yep AMD ces presentation dry as a bone until that rack announcement when ceo turned giddy.,Intel,2026-01-09 00:44:04,2
Intel,nygxlyp,"Even more efficiency that doesn't exist yet. Snapdragon elite can do some heavy workloads for several hours at a time, but it's not powerful enough for heavy gaming. Valve says they know what they want, it's just not ready yet. I think as great as Panther Lake benchmarks are, the biggest complaint of the Steam Deck is still the 2ish hours of battery life in more demanding titles.  With the existence of other handhelds already, I don't think Valve is trying to aim for the beefier spec department here, if it means the same battery life.",Intel,2026-01-08 20:54:40,-2
Intel,nyidygm,"I'm not sure if this is sarcastic or if you missed the memo, but their new VR headset coming later this year is powered by a Snapdragon 8 gen 3.  [https://store.steampowered.com/sale/steamframe](https://store.steampowered.com/sale/steamframe)  A Qualcomm based steam deck isn't out of the question.",Intel,2026-01-09 01:05:28,2
Intel,nynct2x,"Valve's stand-alone VR headset uses Snapdragon, and x86 emulation, so will be interesting to see how well it performs",Intel,2026-01-09 19:07:13,2
Intel,nylpddj,"It depends. For a PC, yes. For a handheld, I don't think so. For the future of enterprise notebooks, probably, especially since Apple has been doing it for a while.",Intel,2026-01-09 14:38:38,1
Intel,nykkj1q,Save cost. AMD's assessment is that no one in mobile cares about gaming and if they do they should just get Strix Halo or build a PC.   cheapskate AMD as always.,Intel,2026-01-09 10:06:00,3
Intel,nyndytz,"probably didn't want to bother redesigning the APU without also having a CPU upgrade, it's expensive after all, and takes resources from other projects",Intel,2026-01-09 19:12:31,2
Intel,nyoo6vt,"""Even more efficiency that doesn't exist yet.""??????????????",Intel,2026-01-09 22:48:02,1
Intel,nyk1lrj,"But Qualcomm drivers on Linux are even worse than Intel lol. The custom AMD APU on the Steam Deck had the advantage of great Linux driver support for gaming(not just being able to support the GPU hardware and benchmarks, but run games at good performance which Intel still can't match). No other company has both high performance GPU and good Linux graphics drivers",Intel,2026-01-09 07:14:57,3
Intel,nyo6aao,isn't the APU a modular unit where they could just put the new one into the same die space?,Intel,2026-01-09 21:23:15,2
Intel,nyp4s4e,"Not my words. I'm taking Valve's. They said they know what they want, and if it was Panther Lake, we'd know already.",Intel,2026-01-10 00:15:40,1
Intel,nyo9nvh,"The Strix Halo is that basically, but the normal Strix Point APUs (e.g. HX 370) are not. The Strix Halo follow-up, Medusa Halo, is slated for 2027, and to use Zen 6 and RDNA5. While the Strix Halo could benefit from FSR4 if it got an RDNA4 update, it's still way stronger than the B390, even at similar power.",Intel,2026-01-09 21:38:47,2
Intel,nytzm0l,Medusa Halo = 2028,Intel,2026-01-10 18:55:32,1
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,3
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nxsj613,100°C,Intel,2026-01-05 09:52:28,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nyktz5u,"surprisingly fast, it has its own suite on windows, but i want to use it in linux, trying to figure out how as im not that good ta linux.",Intel,2026-01-09 11:27:16,1
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,6
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,5
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,2
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,3
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxvcb13,"It's the Flex ATX form factor. I think the most powerful one that is also reputable is the Enhance ENP-7660L, which is 600w.",Intel,2026-01-05 19:23:31,1
Intel,nzf8l4a,Around 65c-75c under load,Intel,2026-01-13 21:15:43,1
Intel,nxs9peb,"a lot of commercial software also just does not give a shit about improving, and I don't mean that as a defense for apple.  after effects is just ass for a 2025 product. basic filters are still using legacy code and memory management is horrible. like you're not going to clean 64gb of memory until I hit 96gb utilization, then you're going to slow to a crawl and maybe crash because of threadlock? why even bring back MT rendering? 3rd party scripts people wrote in their basements outperform this stupid thing. spoofing multiple instances and then stitching the results works better than just running the software, its baffling.  anyway yeah, there's a lot of good to x86 and not having to reinvent the wheel, but god damn if so many companies are using it as an excuse to resell garbage.",Intel,2026-01-05 08:22:17,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nyqxyvg,"> Memory - The Core Ultra X9 will feature soldered Dual Channel LPDDR5x 9600 MT/s memory up to 96GB   96GB of RAM? So it's $20,000?",Intel,2026-01-10 07:03:20,15
Intel,nyodemu,"Great, want one!",Intel,2026-01-09 21:55:54,2
Intel,nywvp1h,any plan for Wildcat Lake variant for very cost effictive mini PC solution?,Intel,2026-01-11 03:56:05,2
Intel,nyxxa7v,I hope it costs under 1600.,Intel,2026-01-11 08:40:19,2
Intel,nzlnhoq,Hadn't noticed this on the first read: there are separate SKUs for HDMI vs DisplayPort. Will all of the models (X9/X7/U7/U5) be available with both options?,Intel,2026-01-14 20:09:33,2
Intel,nyovlyi,It will be very interesting to compare it with the Nuc 15 pro. I am currently reviewing this model with a U5 225H processor. And I know what this processor is capable of in combination with ARC 130T.,Intel,2026-01-09 23:26:34,1
Intel,nyr50we,any word on availability?,Intel,2026-01-10 08:06:45,1
Intel,nz7zsaj,5x4 nope… stick with the 4x4 that’s been around for a decade at this point.  Any increase in size just dilutes the meaning of NUC,Intel,2026-01-12 20:00:12,1
Intel,nzknjzr,Man I wish they'd move on to 10Gb NIC already. I would be all over these for lab and canary. Using a thunderbolt dongle is miserable.,Intel,2026-01-14 17:28:45,1
Intel,o11azs2,"Am I seeing correctly ...your specs say Bluetooth 6, but your event photo shows Bluetooth 5.4 😬",Intel,2026-01-22 12:34:06,1
Intel,o2iliz1,"This looks like a nice NUC for Home Assistant, Frigate, Openvino to run local LLM and Detection because of the Nvidia 4050'ish performing iGPU and AI focused optimizations.  Something I can leave on like an appliance for the cameras and IoT smart home integration.  But if it's priced too high and I can build a desktop with a dedicated GPU for the same money that will be easily upgradeable using standard ATX parts... it might not be worth it.",Intel,2026-01-30 00:51:16,1
Intel,o3haoyd,Unobtanium. No memory.,Intel,2026-02-04 05:17:08,1
Intel,nyryfbb,"Ok atleast this doesn't have co pilot button,plus one to that",Intel,2026-01-10 12:30:39,1
Intel,nzad5zk,"Nothing to share about that. I see some news about that platform, but nothing has been shared with me internally to say one way or the other. For these ASUS NUC models, they are all listed from the Intel Core Ultra 5 and higher.",Intel,2026-01-13 03:15:32,3
Intel,nzlkg2h,"I suspect this would wind up being ""NUC 16 Essential"" to replace the Twin Lake-equipped NUC 14 Essential.",Intel,2026-01-14 19:55:40,2
Intel,nyudelu,"It's mentioned in there, but late Q1 - early Q2 is our current target.",Intel,2026-01-10 20:02:42,4
Intel,nzjltxw,"5x4 yes, high TDP no",Intel,2026-01-14 14:32:17,1
Intel,o13l2qw,"Ah, I see the issue. I took these snaps from one of our product videos at the show. The display was featuring all of our products, and the one that lists BT5.4 was for the ExpertCenter PN55 (you can see the copilot button on it to compare). The other information, including the spec one-pager below lists BT6.",Intel,2026-01-22 19:10:41,1
Intel,nyv67be,"If I were to buy one, are these like just hardware or do they include an OS with all the drivers installed out of the box?",Intel,2026-01-10 22:26:12,1
Intel,nyx7821,"Thanks, is that for global availability or just US? Also any word on what the lowest spec SKU will start at?",Intel,2026-01-11 05:05:43,1
Intel,o1l1llm,"Apologies, this is not a Nuc question, but any idea when the Zenbook Duo's with Panther Lake might ship? And any thoughts to a version with 64GB RAM? I use After Effects a lot, and AE is RAM hungry.  I was going to get a Mac, but I just saw these Zenbook Duo's today and I could be tempted... they look quite cool.",Intel,2026-01-25 08:10:12,2
Intel,o15ze6h,Talk about a poorly timed photo 🧐,Intel,2026-01-23 02:30:04,1
Intel,nyvi99v,"They will be available in both types of models. I don't have a full breakdown on which hardware will be included with the complete Mini PC (e.g. with memory, storage and OS), or the Barebone kit (no memory, storage, or OS), but you'll be able to purchase it in either configuration.",Intel,2026-01-10 23:28:53,4
Intel,nzacphp,"I would expect US availability to be around global availability, but that different barebone kits sometimes are configured a bit later.   To your second question, pricing information isn't available at this time, but if you're just asking the specs, I would follow what I posted above.   However, if you go to our global product page, you'll usually find a download link for our spec datasheet (sometimes this doesn't show on mobile + plus the document isn't available yet). This will be a better way to see this. I'll ask our team when the datasheet will be available.",Intel,2026-01-13 03:13:08,2
Intel,o16s6n3,"We do what we can. Thanks for noticing, however.",Intel,2026-01-23 05:25:17,1
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,46
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,11
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,6
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,18
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,13
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,9
Intel,nw63y3k,CXL killed octane it’s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,5
Intel,nvbtvc9,If the b770 is 5060ti levels even €500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it would’ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,5
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
Intel,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
Intel,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
Intel,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
Intel,o29lgdq,"i've got an i7-14700kf on an asus rog strix b760-a with a corsair h100i elite capellix xt (240mm aio) and when i run after effects my cpu temps shoot up to 90 degrees, it probably wouldve gotten higher but i closed it cause it felt too high for what i was doing, i'm looking to undervolt my cpu but i dont know anything about it. i just want something safe and simple (im not looking for an extreme undervolt, just one that would lower my temps & possibly keep the same performance)",Intel,2026-01-28 18:56:54,1
Intel,o3j0qr3,"Killer Ethernet keeps asking me to update, i already uninstall, reinstall, safe mode and reformat my laptop and keeps asking me to update.   [Killer Ethernet](https://imgur.com/a/G3YuvJN)",Intel,2026-02-04 13:40:14,1
Intel,o3v6d3v,"Hi all, if i were to buy a series 2 intel CPU laptop with accompanying NPU and iGPU (likely arc 130 or 140). I wondered if anything mentioned on CES 2026 about the Panther lake software improvement is being guaranteed to get to the older CPUs via software/driver updates. I want to buy a cheaper older laptop now while still getting any supposed/promised gains from Intel than buy a panther lake laptop amid the nonsense ram pricings",Intel,2026-02-06 07:44:05,1
Intel,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
Intel,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
Intel,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!  You can find and apply for all of our jobs online at [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We don’t currently accept submissions via social.  Good luck!",Intel,2026-01-07 00:05:20,2
Intel,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
Intel,nya3rq0,Hi u/ConspiracyPhD **Post** a question on [Intel® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud) forum for further investigation.,Intel,2026-01-07 21:48:00,1
Intel,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for Intel® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
Intel,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is built‑in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intel’s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If you’re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to Intel® Platform Trust Technology (Intel® P…](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
Intel,o2kc1kd,"Individual\_War\_129, we do not provide typical temperature operating ranges for each processor or each core, as it can vary based on the system design and workload. Processors have internal protections to prevent against excessive temperatures. Operating ranges below the protection points are highly dependent on system configuration and workload.  In case you haven't come across it yet, you may check the articles below:  [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [What Is Undervolt Protection and How Does It Affect Overclocking in Intel® Extreme Tuning Utility (…](https://www.intel.com/content/www/us/en/support/articles/000094219/processors.html)  [Thermal Design Power (TDP) in Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html)",Intel,2026-01-30 07:31:28,1
Intel,o3o6m00,"nfsanton, please be advised that the product you are reporting is an OEM (Original Equipment Manufacturer) device. As such, our support may be limited, since we do not have full visibility into the specific technologies, settings, or customizations implemented by the system manufacturer on your device.  For laptop systems, we strongly recommend installing and using the drivers provided by the system manufacturer, as these drivers are customized and validated to ensure full compatibility with your hardware.  That said, you may also choose to use the Intel generic driver if needed, which is available here: [**Intel® Killer™ Performance Suite**](https://www.intel.com/content/www/us/en/download/19779/intel-killer-performance-suite.html). Please note that functionality and behavior may vary when using generic drivers on OEM systems.  You may also find this public article helpful: [Intel® Driver & Support Assistant (Intel® DSA) Keeps Showing Available Driver Update Notificati…](https://www.intel.com/content/www/us/en/support/articles/000090127/software/software-applications.html)",Intel,2026-02-05 05:53:34,1
Intel,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
Intel,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
Intel,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
Intel,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like it’s up and running. Could you try accessing it again using your Intel account?  [Intel® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
Intel,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
Intel,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
Intel,nz3b0gd,"u/ConspiracyPhD Please check your inbox, I’ve sent you a personal message. I’ve already coordinated your concern with the respective team, and as per their instructions, you’ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
Intel,ntkfg69,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Intel,2025-12-12 01:18:37,20
Intel,ntmjjev,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Intel,2025-12-12 10:55:28,6
Intel,ntlssa2,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Intel,2025-12-12 06:40:14,3
Intel,ntxw9ob,"How many concurrent users will this serve, 30 devs would be nice",Intel,2025-12-14 06:36:27,1
Intel,ntsaqxn,:),Intel,2025-12-13 08:27:35,2
Intel,nvplqob,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Intel,2025-12-24 13:00:31,6
Intel,nvpsi8z,Mac Pro Trashcan 2.0 is crazy,Intel,2025-12-24 13:45:33,6
Intel,nvt0j8h,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Intel,2025-12-25 01:15:41,3
Intel,nvte7q0,No one knows. Synthetics seem to put it roughly at a 3050m.,Intel,2025-12-25 02:59:28,2
Intel,nwfd6hr,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Intel,2025-12-28 20:51:38,1
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,55
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,10
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,10
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,6
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,5
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,3
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,7
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,7
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,5
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nv5fgk8,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Intel,2025-12-21 04:48:32,8
Intel,nv5nfm2,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and it’s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Intel,2025-12-21 05:49:09,4
Intel,nv5h74y,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Intel,2025-12-21 05:00:55,3
Intel,nv5hr1e,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality it’s overall better and better IMC as well. It’s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Intel,2025-12-21 05:04:58,2
Intel,nv70zt0,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Intel,2025-12-21 13:28:34,2
Intel,nvehvk5,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30°c cooler in game!!! I would average 80-85, now it’s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and I’m extremely happy with the outcome!",Intel,2025-12-22 17:37:19,2
Intel,nv6um6s,I also PL1/2 at 150. My temps stay under 60c when gaming.,Intel,2025-12-21 12:41:07,1
Intel,nv7icoh,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Intel,2025-12-21 15:16:34,1
Intel,nvb3hok,Im using a duel air tower for my 14900k game temps are at 60 to 70,Intel,2025-12-22 02:56:21,1
Intel,nvkr6z0,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Intel,2025-12-23 17:28:35,1
Intel,nv62yz6,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Intel,2025-12-21 08:13:54,-2
Intel,nv6s3tx,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Intel,2025-12-21 12:20:08,5
Intel,nvx118g,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Intel,2025-12-25 20:10:57,2
Intel,nv6848y,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Intel,2025-12-21 09:06:11,0
Intel,nwbwttq,"Dropped the voltage further down by -0.10000 and now I’m getting 58°c core temp and max 65°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings I’m able to still maintain a framerate that matches my screen refresh rate.",Intel,2025-12-28 07:43:31,1
Intel,nv7zbxm,This post is about the K and KS. Both have the same iGPU,Intel,2025-12-21 16:46:01,3
Intel,nv7imu3,even better. i take it they extended tbe warranty period for those products?,Intel,2025-12-21 15:18:10,1
Intel,nvgy84t,For 5.5 39k in CB23 is a little low,Intel,2025-12-23 01:34:28,0
Intel,nvx2yr5,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Intel,2025-12-25 20:22:49,1
Intel,nyq5yj3,"Yeah your method is better, it’s what I do.",Intel,2026-01-10 03:42:43,1
Intel,nv84jg4,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Intel,2025-12-21 17:12:59,1
Intel,nv7jm5d,Yes because of the degrading issue.,Intel,2025-12-21 15:23:39,2
Intel,nvhfrt3,Lol stock is 5.8ghz lol and most stock after the update get 35k,Intel,2025-12-23 03:22:04,1
Intel,nvhi35n,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Intel,2025-12-23 03:36:52,1
Intel,nxyvuyn,"That is a little on the lower end, my 12900K gets 30K. Also in single core a little below 285K, multicore just below 9900X, top of 13700K. Back to it :)",Intel,2026-01-06 07:14:02,1
Intel,nvhitar,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Intel,2025-12-23 03:41:34,2
Intel,nvhk1pd,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Intel,2025-12-23 03:49:33,2
Intel,nvkpd9m,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Intel,2025-12-23 17:19:30,2
Intel,nvkq3xk,Talk to him not me... The guy said 35k is the score 😂😂😂😂😂,Intel,2025-12-23 17:23:13,2
AMD,o42yfzn,"As a 6950 XT owner this is VERY annoying. I'd rather not have to mess with my drivers just to get FSR4 working modded in... but AMD just wants to leave anything pre RX 9000 behind, even their brand new RDNA 3.5 iGPUs it seems. I've done the driver mod on my 6950 XT before in Cyberpunk and while FSR4 is a little more heavy than XeSS, it also looks a decent lot better. Then XeSS is much heavier than FSR3.1 anyway so like, it just makes sense to go with FSR4. AMD needs to release it, no question.",hardware,2026-02-07 14:09:59,82
AMD,o42thn0,Amd dug their own grave with FSR  First it was you dont need upscaling. Then they pretend a sharpening filter was good with fsr1. How open source and working on multiple vendors is good. Then they said you don't need AI to do good upscaling with fsr2. Then they said you don't need dedicated hardware for FSR3 (frame generation).  Every single time they kept digging a bigger hole. All this time people got locked into the Nvidia ecosystem with Dlss.  And now AMD have a hardware locked ai based upscaling and frame generation.,hardware,2026-02-07 13:40:50,213
AMD,o42sd8n,"Put pressure on them, it's the only way to get them to change their shitty decisions.",hardware,2026-02-07 13:34:01,49
AMD,o42vtel,"The real travesty is AMD gaslighting  buyers into thinking RT and AI upscaling were fads, only to finally imitate Turing 5 years too late, and to then blame the fans for listening to them.",hardware,2026-02-07 13:54:37,127
AMD,o4373qj,Why bother with marketing and image when you are already at capacity with your sales and you probably plan to reduce supply in the near future?,hardware,2026-02-07 14:58:12,9
AMD,o42lni5,AMD really screwed over anyone pre 9xxx series. Crap upscaling really makes games look so much worse.,hardware,2026-02-07 12:49:46,88
AMD,o42ypyk,"AMD's marketing might actually improve if they replaced their marketing executives with LLMs, or at least save some payroll expenses.",hardware,2026-02-07 14:11:34,22
AMD,o44xbvi,"This is the same company that said that it was ""impossible"" to put a 5000 series ryzen on a B350 motherboard. Lo and behold.",hardware,2026-02-07 20:09:44,9
AMD,o435ru2,Aw man turns out AMD is a corporation after all.,hardware,2026-02-07 14:51:00,21
AMD,o4830in,"Glad they're keeping the pressure on her another dumb AMD decision, but they missed a key point which makes AMD's stance even worse - they are still releasing rDNA 3.5 graphics cards in their APUs. Those same products which are in most need of FSR for gaming.",hardware,2026-02-08 08:53:21,5
AMD,o42m57s,Yeah we really know at this point it's getting tiresome and ppl can already use it with mods anyway. I mean it's not even like RDNA4 is that much better off most games need Optiscaler just for basic FSR4 support so what does it even matter? AMD always relied on the open source community and they already said FSR4 will most likely go open source soon as well.  Seriously company's are not your friends and both Nvidia and AMD are just looking in to making a quick bug with AI atm so dunno what ppl are even expecting.,hardware,2026-02-07 12:53:19,19
AMD,o48kj9r,I wouldn't be even fine with an experimental version. Needs to be full,hardware,2026-02-08 11:38:05,2
AMD,o43llnk,Moved from 7900XTX to a 5090 because of this and many other reasons like games not supporting AMD feature.  But what pissed me the most is how bad the frame gen from AMD is to my eyes,hardware,2026-02-07 16:11:06,5
AMD,o42muev,"The difference in tone and style between when Nvidia does something and AMD is hilarious. Cant take them seriously anymore since the RTX 5000 and AMD 9000 series launch and their obvious bias and borderline misleading coverage when it come to the ""fake msrp"" saga",hardware,2026-02-07 12:58:15,18
AMD,o44mymn,Only in this sub could you get people accusing the people of creating a negative AMD video of being biased for AMD.,hardware,2026-02-07 19:15:32,4
AMD,o45nxnr,"It's pretty bad that my first thought was ""which one?""",hardware,2026-02-07 22:33:56,2
AMD,o439mr3,"AMD make quite good GPUs, but they have spent years in denial that the software is a vital part of the consumer GPU product, and an increasingly large part at that.  The Radeon division treat this side of the business as a chore their mom is making them do on a sunny Saturday morning when they'd really really rather be doing anything else - to be done late, slowly, and in as slipshod a manner as they think they can get away with.",hardware,2026-02-07 15:11:38,2
AMD,o43bjul,The only thing holding AMD up is good processors (only x3d) and cheap graphics cards.,hardware,2026-02-07 15:21:32,-3
AMD,o42lapc,"Here they are with the video to assure their fans they don't give unfair reviews by subtly hinting ""AMD bad""  Edit: I fucking love getting downvoted for this, I am awaiting the comments pointing out my obvious Intel PFP and then proceeding to call me a troll that shouldn't be given attention all while still posting the comment!",hardware,2026-02-07 12:47:08,-41
AMD,o437mew,"As a 6700 XT user, I honestly do not care yet. Do not get me wrong, I am glad big YouTubers are turning up the heat and keeping AMD honest. But my OptiScalar test 2 to 3 months ago was disappointing. FSR 3 Quality was performing close to, or even better than, FSR 4 int8 Performance. quality difference was not worth it  At this point, I would only care about FSR 4 on RDNA2 if AMD can squeeze out an extra 10 to 20 percent. I am not even sure that is a realistic expectation anymore.",hardware,2026-02-07 15:00:59,-5
AMD,o44kgx3,AMD can't release it probably because it can't run on RDNA 3.5 APUs,hardware,2026-02-07 19:02:56,-2
AMD,o43ctih,Same with features for rtx2080ti they don’t release. Rebar for example,hardware,2026-02-07 15:27:57,-2
AMD,o43nzd6,>As a 6950 XT owner this is VERY annoying. I'd rather not have to mess with my drivers just to get FSR4 working modded in... but AMD just wants to leave anything pre RX 9000 behind  And who now has any faith that RDNA4 GPUs won't get left behind when UDNA comes out...?,hardware,2026-02-07 16:22:49,65
AMD,o45ivbt,"Don't forget that the transformer model (4 and 4.5) was even released on the 20 series.  Which supposed to lack that kind of HW performance to do them right, they just said hey, this isn't the best on them and you may need to tweak settings to get the best things out of it, and that if you wished 3.x can still be used just toggle it. And that 4.5 will have more perf issues.  I cannot imagine why AMD won't do the same, like seriously what is going on there, esp if they gave them the extra vram to make them last longer supposedly and now drop them on features that could actually help them last longer.",hardware,2026-02-07 22:05:56,8
AMD,o45oqg9,"RDNA4 1, 2, and 3 owners are just going to have to accept that you bought in at the time that AMD had its head in the sand about hardware for RT and ML upscaling. The cards can't do it because they don't have the hardware, and the cards don't have the hardware because AMD didn't take it seriously on the consumer side. The only options for RDNA2 would be to take a big performance hit to the point where it might not even be worth turning it on, or accept an image quality hit to the point where it might not be worth modifying the model when FSR3.1 exists.",hardware,2026-02-07 22:38:24,-6
AMD,o4303lm,Meanwhile Intel has been quietly doing a good job with XeSS,hardware,2026-02-07 14:19:25,126
AMD,o43qut6,They also promised fsr 3.1 development would continue in parallel with fsr4.,hardware,2026-02-07 16:36:54,19
AMD,o43ddnt,"> Every single time they kept digging a bigger hole. All this time people got locked into the Nvidia ecosystem with D  and you know what the real kicker is ?      If AMD had just shup up early and accepted to be part of the ""Streamline"" project that had Nvidia leadership and Intel participation, every single DLSS2+ game today would also support FSR4.",hardware,2026-02-07 15:30:45,32
AMD,o43jsaq,"consumers are partly to blame imo, all that hate for dlss came back to bite everyone in the ass. literally everyone in the past few years have been saying they don't need/want upscalers/frame gen until amd makes a subpar equivalent and then everyone praises it like it's the second coming of christ when it's not even close most of the time, it's like they don't want to admit that these features are actually really helpful just because it's made by nvidia.",hardware,2026-02-07 16:02:09,20
AMD,o43cw8z,It's Ok When AMD Does It^TM,hardware,2026-02-07 15:28:20,23
AMD,o45f6ad,"Amd was wrong from the beginning, Nvidia was right.",hardware,2026-02-07 21:46:03,9
AMD,o49sy4k,"Yep. When your *only* selling point it was available to everyone is gone, their isn't any point for devs to implement as lowest denominator. That will be Intel now.",hardware,2026-02-08 16:14:11,2
AMD,o43n4ay,IS what why Nvidia users of series prior to 4XXX love to use AMD frame scaling? Because it sucks? LOL,hardware,2026-02-07 16:18:35,-13
AMD,o44gur2,"What were they supposed to say? ""To get good results you need dedicated hardware, which we won't have for at least 4 years, and next/current gen consoles don't have""?  They played the hand they had. 100% normal.",hardware,2026-02-07 18:45:01,-7
AMD,o4356my,"This, people yelling at GN for putting pressure on nvidia please see the upper comment too",hardware,2026-02-07 14:47:47,9
AMD,o434n5s,"My annoyance is that their technology gets used in consoles, thus dictating how games and engines are developed. And because they're so widespread, console users will automatically come into the conclusion that Upscale, RT, and even Framegen are bad because FSR Upscale, Framegen  and AMD's ""at home"" RT implementation is terrible.",hardware,2026-02-07 14:44:46,51
AMD,o43ms1f,"Reminds me a bit of WAAAY back. When shaders first appeared and how 3dfx went under. Yes, novel tech is first slower or inferior than a mature legacy system.",hardware,2026-02-07 16:16:54,16
AMD,o434udg,Its a rope pull game of nvidia trying to justify early adoption selling of RT in Bf5 demo days which fell flat and needed more time in the oven and AMD justifiedly being late to the party on features people demand and mocked they dont care about RT. And a huge gray zone where people draw their own lines.  Because anyone with sense knew path traced rendering will be the future. But if its 5 years or 15 nobody knows for sure thanks to moores law,hardware,2026-02-07 14:45:55,14
AMD,o42xykd,They always lead from behind in everything they do.  AMD has a rich history in copying IP.  From their founding to modern implementation.. everything is unoriginal and 2nd rate.  They are the generic store brand cola of tech.,hardware,2026-02-07 14:07:13,3
AMD,o45fuoo,"AMD always had so bad marketing policy. AMD always says nonsense and then quietly backtracks on it. That's why I haven't trusted AMD for a long time and don't take seriously what they say, what they announce and what they claim.",hardware,2026-02-07 21:49:41,1
AMD,o436vtm,they are the wrong direction (RT and ML fuzz) imo but amd software was so fucking bad before this and now its even segmented so you have even more reasons to just go nvidia,hardware,2026-02-07 14:57:01,-5
AMD,o437796,"It was a fad. It was something that should have been sold to developers, not to consumers. There's a video out there, that people find non-raytraced water nicer to look at because that's what people got used to. Not only they were selling something that didn't look good to the consumer, they did so at a significant price step up and degrading performance. That is the sin of Nvidia. If Nvidia instead gave it for free and only started charging money once it was good, nobody would have complained.  AMD on the other hand, was caught with their pants down because they don't speak to developers as often as they should. They should be begging developers to tell them what tools they think could be good and copying Nvidia's homework on their spare time.",hardware,2026-02-07 14:58:44,-14
AMD,o47y7zx,"They are imitating the RTX 4000 series. RDNA 2 was AMD's Turing in terms of ML tasks. RDNA 3 then improved the AI hardware by adding First gen matrix cores, which currently remain unused even by the leaked Int8 FSR 4 shaders.  The 7800XT has ~3070 Ti level ML performance but with no software to use it.",hardware,2026-02-08 08:08:03,-3
AMD,o42woxy,"Well... it's a plus that Linux has workarounds that get FSR4 running on 7000 series to some degree.  Which shows that AMD's GPU marketing team is just fucking up, as usual. Why do they have to shoot themselves in the face so much? Doesn't it hurt?",hardware,2026-02-07 13:59:49,29
AMD,o44yql8,> Crap upscaling  This is very funny to me because HUB and AMD fans said FSR1-3 was so good.,hardware,2026-02-07 20:17:14,10
AMD,o42swsh,"Native is still there, they're not really screwed",hardware,2026-02-07 13:37:21,-17
AMD,o42xhph,"Most games I played lately (RE4 remake, Metaphor) run at 4k 60fps with no issues on my 6800XT.     I don't feel screwed over by my 4 years old GPU.",hardware,2026-02-07 14:04:32,-14
AMD,o42r5wo,"Well Tim (in the video) did a comparison between FPS scaling vs image quality and IIRC he prefered FSR4 INT8 Performance vs FSR 3.1 Quality, and the INT8 version performed better at like for like image quality.   Technically if the Game has FSR3, AMD Adrenaline can override similar to GeForce App .dll swapping. So optiscaler isn't as required if the game is relatively recent and already has FSR3.   I'd rather we keep talking about it, but if Nvidia is going to divert GPUs/Memory to AI. AMD can keep focusing on improving FSR Redstone and their 16GB RDNA4 will keep selling above MSRP.",hardware,2026-02-07 13:26:31,18
AMD,o42nc9s,People love to not realize the big 3 don't give a flying fuck about them and AMD is still a shit company with shit morals that just presents them better  Edit: being downvoted for saying that a big tech company has shit morals and doesn't give a fuck about you is peak Reddit,hardware,2026-02-07 13:01:36,69
AMD,o42sswc,Wtf are you talking about,hardware,2026-02-07 13:36:40,8
AMD,o42szrh,Can't tell if you think they're biased towards AMD or Nvidia. They've complained about both heavily over the years.,hardware,2026-02-07 13:37:51,3
AMD,o42v3rx,Can you give me examples such as direct quotes? Cos I don’t see it.,hardware,2026-02-07 13:50:26,-1
AMD,o42qean,"years ago Nvidia is angry about them for a negative review, and threaten then not having early review cards for testing. After that they change and become biased.  That's why I'm not watching them anymore, and go to other Youtubers like Gamer's Nexus or anybody else to confirm the news  Edit: here are some sources [https://www.youtube.com/watch?v=wdAMcQgR92k](https://www.youtube.com/watch?v=wdAMcQgR92k) this was 6 years ago  [https://www.reddit.com/r/hardware/comments/1kqrsw3/hardware\_unboxed\_nvidia\_accused\_of\_manipulating/](https://www.reddit.com/r/hardware/comments/1kqrsw3/hardware_unboxed_nvidia_accused_of_manipulating/) this was 9 months ago",hardware,2026-02-07 13:21:38,-8
AMD,o443hqd,"Nvidia 5000 series had melting power connectors and unstable drivers on release. 9000 series had the usual AMD issues, and not much more. Those are simply not the same, so obviously the reaction to them won't be the same either.  Also: AMD released after Nvidia and had the weaker hardware, so they adjusted their pricing accordingly, while Nvidia can just not give a shit about availability and pricing at all. And AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.  And Nvidia has been actively trying to restrict critical reviewers - considering that, I'd actually argue they are actually holding back their animosity.",hardware,2026-02-07 17:39:18,-5
AMD,o43agpu,Bias? Towards who? When both rdna4 and blackwell launched they were very critical of both when it came to prices being way above msrp,hardware,2026-02-07 15:15:59,-6
AMD,o42s23q,Comments which add little  to no value to a discussion are supposed to be downvoted. It's what the button is there for. Maybe if you'd addressed the content of the video your comment would have been valued higher.,hardware,2026-02-07 13:32:05,21
AMD,o42t67x,Downvoted for just being a stupid comment.,hardware,2026-02-07 13:38:56,13
AMD,o42w1ca,"> Here they are with the video to assure their fans they don't give unfair reviews by subtly hinting ""AMD bad""  HU have never taken sides, much like GN ~ AMD has just been simply less awful and anti-consumer than Nvidia and Intel, for whatever reasons, whether lack of monopoly or market power or otherwise, but HU still happily gives AMD plenty of shit whenever they fuck up. Especially when it comes to AMD's absolutely abysmal GPU marketing.",hardware,2026-02-07 13:55:56,11
AMD,o43zmdi,"YEAH, like I wanna upgrade next gen and especially here in Australia Radeon prices tend to be more competitive vs global MSRPs... but how much trust can I have in AMD's support of their GPUs? RDNA2 being the driver of the PS5 and Xbox and Steam Deck hasn't stopped AMD for trying to bury the RDNA2 Radeons to push upgrades or whatever their dumb plan is when it should be to gain marketshare, not annoy what they have.  Like we understand these GPUs aren't as advanced as the competition they had, but FSR4 INT8 is plenty of proof where they stand right now is not the end of the road, as long stuff works on the INT8 path it works.  Like FSR4 INT8 could literally be used on console too you know, not base PS5 which lacks it but Xbox? Yes, Series X and S on paper could use the INT8 model instead of being stuck with TAA and FSR3 and nothing better.",hardware,2026-02-07 17:20:03,24
AMD,o45htgt,They always try to pull this bs. Same with Ryzen 3000/5000 on initial motherboards.,hardware,2026-02-07 22:00:15,11
AMD,o448gc8,"But ""Nvidia bad"" according to reddit.  Meanwhile, 3080 owners enjoying dlss 4.5...",hardware,2026-02-07 18:03:46,22
AMD,o4684j0,"Well UDNA5 will have the FSD Redstone issues fixed, but not ported to RDNA4",hardware,2026-02-08 00:35:38,-1
AMD,o46nrzr,"Love that you still say this while I literally said I used FSR4 in Cyberpunk mate. It works, it's heavier sure but it's also not as bad as DLSS 4.5 on RTX 20/30 series and Nvidia still gave those GPUs the option.   Like 4ms or so isn't a killer of viability, for 60fps or lower it's actually still realistic to get going and beats trashing image quality with FSR3.  Like FSR3.1 vs 4 INT8 is night and day, and what I want out of FSR4 is pretty simple... keep my 4K monitor viable with my 6950XT without either integer scaling 1080p or having to resort to FSR3 which is not great, neither is native 1440p on a 4K panel a good idea.",hardware,2026-02-08 02:13:01,9
AMD,o431pyx,Nvidia always said AI and Hardware Acceleration was needed for the best quality.  Intel said the best quality was hardware acceleration but a weaker version will be available for others.  Intel and Nvidia both stuck to their words and never did virtue signaling for public points like AMD.,hardware,2026-02-07 14:28:37,109
AMD,o439wj3,And now their APUs are legitmate threats to AMD's again...,hardware,2026-02-07 15:13:04,57
AMD,o438e6r,"And a downgraded version is available for other gpus, amd cant even provide it for their own older gpus, which are still produced and sold mind you, and new products based on it are made",hardware,2026-02-07 15:05:07,16
AMD,o433wjx,"Although Intel did initially say XeSS would be open source, which it still isn't.",hardware,2026-02-07 14:40:43,14
AMD,o43bbv9,"It is not open source,  like they said it would be.",hardware,2026-02-07 15:20:25,8
AMD,o45pnhh,"Intel's frame generation is the best of all three, not going to lie. At least it is on Intel hardware, I can't speak for the fallback models. It has frame pacing that's better than AMD and latency handling that's better than Nvidia. Upscaling has fallen behind a bit, but it's still around the level of DLSS 3 and is better than FSR3.",hardware,2026-02-07 22:43:36,2
AMD,o434it5,They are ok on side by side basises to fsr4. But breaking preset parity buy downgrading sample resolution was a big no no,hardware,2026-02-07 14:44:06,1
AMD,o44kpja,"Don't worry, Intel will kill further development of XeSS once they have products with Nvidia GPU tiles on their products a few years from now.",hardware,2026-02-07 19:04:09,-1
AMD,o456x86,"Nah, let’s pay for FSR exclusivity instead",hardware,2026-02-07 21:01:50,8
AMD,o44ms12,"yeah, im sure it was ""everyone"" instead of 20 people online",hardware,2026-02-07 19:14:35,12
AMD,o445fon,"I wonder if it's that these two groups might just be separate?  Oh look, another limitation / issue with AI gaming I can completely disregard. Bliss.",hardware,2026-02-07 17:48:56,5
AMD,o48y5pk,"the hate was well deserved for DLSS 1 on 20 series, it was a shitshow of implementation that was also locked out of the 10 and 9 series with VERY limited game support (battlefield was the only big one, then more came in).  esp coupled with the fact that the 20 series was the first time in a long time when the prices was raised and you gotten down tiered at the same price (not the raw perf ofc, you still got more than before).  that first impression really hurt things, and it wasn't until DLSS 2, and esp with CP2077 where things start to turn around, and beyond that is when new games start to more or less always included some sort of upscaling.  but at that point, 30 series was out.",hardware,2026-02-08 13:23:36,-1
AMD,o46ve7e,AMD has been given a pass for so much shit it's unbelievable.,hardware,2026-02-08 03:01:11,10
AMD,o49z6ip,"I mean, they still somewhat fix this by overriding FSR 3 with 4, no?",hardware,2026-02-08 16:44:32,1
AMD,o43yedb,it's better than nothing i guess,hardware,2026-02-07 17:13:58,13
AMD,o45727q,Better than nothing doesn’t mean it doesn’t suck.,hardware,2026-02-07 21:02:35,0
AMD,o44v1ba,"AMD 100% knew Nvidia was working on dedicated ML hardware even before the 2000 series was announced. Employees go back and forth between the big 3 all the time, obviously bringing trade secrets with them. They just didn't realize how big of a deal it was until it was *way* too late.  Sure, the RX5000 series was already near-finished by this point so it was a lost cause, but there was all the opportunity in the world to make the clean break with RX6000, implement the ML hardware and then they'd only be one upscaling generation behind instead of 3.",hardware,2026-02-07 19:57:27,6
AMD,o436lgz,> people yelling at GN for putting pressure on nvidia please see the upper comment too  He's probably in the process of making similar video but first he needs to find a way to shift the blame towards Nvidia,hardware,2026-02-07 14:55:29,34
AMD,o4380t3,"Which pressure? the 4080 12gb? Prices have only been increasing, nvidia has only been getting richer and diversifying immensely. YouTube video isn't putting pressure on Nvidia when Nvidia is driven to maximize profits & current supply issues wont allow them to make as many gpus as they wanted.  The only pressure Nvidia will understand is the competition is even good alternative. For every 1 recommendation GN makes for an alternative 20  nvidia gpus are bought.",hardware,2026-02-07 15:03:08,5
AMD,o44we8h,">And because they're so widespread, console users will automatically come into the conclusion that texture filtering, z buffering, and even perspective correction are bad because Sony's ""at home"" implementation is terrible.    Time is a flat circle",hardware,2026-02-07 20:04:45,4
AMD,o46ncfh,"To be fair, AMD's strategy is because they're used in consoles. Every technical decision they've made has been dictated by the cost constraints of developing console hardware. What you get in the desktop was paid for by the console manufacturers and it is as powerful and feature-packed as the console manufacturers were willing to pay.   What I mean is, AMD's strategy with RT was meant to be cheap because it's the way Sony and MS got to have the tech while also not having spiraling hardware costs. Rdna2 is basically the tech those consoles got scaled up and rDNA 3 was rdna 2+.   Rdna4 was basically a rehash of what Sony paid for the ps5 pro. And rdna5 is likely going to be a beef up version of whatever the console manufacturers pay for the 2027 season.    AMD doesn't have anywhere near the r&d money nvidia has and it doesn't have the deep pockets Intel has to throw money to a hard problem until they crack it.   I'm going to be the first to complain about their lack of software strategy, but their hardware strategy makes sense when you consider they get a tenth of the market share nvidia gets and for the longest time had revenue much smaller than Intel's and Nvidia's. They make the best use of the tech MS and Sony pay for their consoles.    Maybe this time they will use that sweet sweet AI money to flesh out their hardware, but I'm not holding my breath. It's clear they have a hard time convincing buyers to give them a try.",hardware,2026-02-08 02:10:17,2
AMD,o45g4bn,Yes. The great tragedy is that both major consoles use AMD hardware.,hardware,2026-02-07 21:51:04,0
AMD,o44k899,Framegen is bad. At least the way smooth brains use it to pretend its the same as real frames.  One good thing about consoles being amd I guess,hardware,2026-02-07 19:01:43,-16
AMD,o44lb5t,"As a 5090 owner,  those are bad in nearly every game.",hardware,2026-02-07 19:07:11,-9
AMD,o496fo0,"Considering the bloating size of games, the terrible performance, and the ballooning costs I'm actually pretty sure the future of gaming is going to be stylization while a small few AAA developers chase realism where path tracing will be worth the performance hit.  So definetly closer to 15 than 5.  Hell, even bethesda said they're backing off total realism.",hardware,2026-02-08 14:14:46,1
AMD,o43bcap,The x64 arch and the first consumer dual core CPU came from AMD... On the graphic side they where leading in the Geforce FX era...,hardware,2026-02-07 15:20:29,15
AMD,o431xuj,not totally accurate they gave us zen architecture and great consumer CPUs,hardware,2026-02-07 14:29:49,18
AMD,o45jkg6,ATI/AMD was in the lead pushing for new memory tech on GPUs for like a decade. Nvidia ran the strategy of trailing behind both on memory and nodes up until Kepler when they reached parity by launching just months after AMD.  Theres whole memory standards that Nvidia never touched. GDDR4 and HBM1 were never implemented by Nvidia on any product iirc.,hardware,2026-02-07 22:09:45,3
AMD,o432wwr,"Well their Zen architecture was something actually novel and new, especially on the consumer market. They forced Intel out of their 4 core, 4% incremental upgrade cycle. People were hoping they'd use that momentum to innovate on the GPU side but they haven't really been able to do that.",hardware,2026-02-07 14:35:14,9
AMD,o43ipnz,"I'd like to see that water comparison video, especially in regards to the way those scenes are presented and interacted with. Also if they compared RT/non-RT within the same scenarios.  There can be many discussions about artistic choices, but RT reflections allow to retain the same art style without the drawbacks of SSR, which was the popular AAA dev choice before RT reflections came along.",hardware,2026-02-07 15:56:54,18
AMD,o43al1s,Amd IS asking devs what tools they think could be good. Why do people think amd is not some 300B corporation? Nvidia is simply able to execute the ecosystem better which led to amd running to catch up.,hardware,2026-02-07 15:16:36,6
AMD,o48tftc,"There are no matrix cores in RDNA 3, just added support for ML instructions.  Thus the core runs into contention when having to process graphics at the same time.     Everything up to the 9070xt has been brute forced. RDNA4 is the first feature complete AI radeon.  Making it Radeons Turing.",hardware,2026-02-08 12:51:04,3
AMD,o48ph1i,Why they don’t let rdna2 use FSR4 then?,hardware,2026-02-08 12:20:23,1
AMD,o42zczb,"> Which shows that AMD is just fucking up, as usual. Why do they have to shoot themselves in the face so much?   AMD's graphics division has historically never missed an opportunity to miss an opportunity.    Not only that, but even AMD's 2006 acquisition of ATI was financially fucked up to the degree in which they had to do [multiple write-downs](https://www.eweek.com/pc-hardware/amd-will-take-900m-q2-charge/) totaling over two billion dollars (out of a $5.4B initial purchase price) because they grossly overestimated how valuable the company and their technology was to begin with.",hardware,2026-02-07 14:15:13,47
AMD,o46rd1q,Anyone with functioning eyes has been calling FSR 1–3 unusable since day one. It’s essentially just aggressive resolution reduction in motion conveniently toggleable with a side of heavy artifacts.,hardware,2026-02-08 02:35:35,3
AMD,o45vowv,They are good. But transformer DLSS is significantly better.,hardware,2026-02-07 23:18:37,-4
AMD,o42zrxo,"The native argument used to work before DLSS transformer, but now after DLSS 4.5 it's really hard to argue against how fucking good it looks, it's basically just free performance with no downsides at this point, and any imperfections in the image are almost always present in native too.",hardware,2026-02-07 14:17:35,20
AMD,o42tli6,Native TAA looks like crap. Native FSR or XESS is okay but still worse than DLSS quality.,hardware,2026-02-07 13:41:29,30
AMD,o42tw48,Upscaling is being widely adopted as part of the optimization package and default anti-aliasing method.  So... Yeah you're screwed going forward more and more if you don't have access to good upscaling.,hardware,2026-02-07 13:43:14,32
AMD,o42ua54,"Turn on RT, see how well native runs",hardware,2026-02-07 13:45:34,9
AMD,o42ysqd,Try any of the ue5 titles.,hardware,2026-02-07 14:12:01,18
AMD,o42tf70,1440p quality DLSS is a slight downgrade for better FPS. It’s worth it every time.,hardware,2026-02-07 13:40:26,18
AMD,o437sd5,"Native is not a magic resolution where everything is correct. This is a statement people keep making that sounds correct if you don't think about it, but it's just flatly wrong.      Tons of geometry and textures have detail that can be smaller than a pixel in games today. The vast majority of effects and basically all the lighting (whether ray traced or not) is initially rendered at lower than native resolution.  TAA is already used in basically every game because of how common deferred rendering is. If TAA weren't used, the flickering and aliasing would be so intense it would make games unplayably bad.    Even if none of that were true, the idea of native being ""correct"" makes no sense. We would have had no need then for antialiasing techniques like SSAA or MSAA.    DLSS and all upscaling techniques today are a form of super sampling. It's just that they trade off spatial resolution and do the super sampling temporally. Using data from past frames is obviously worse than just rendering at a higher resolution in a vacuum. But you need cheap anti aliasing that covers everything anyway. And basically every game needs temporal accumulation (to run effects at sub-native resolution) in order to run fast enough with good enough visual quality anyway. And if you really can create an algorithm that effectively reuses data from past frames, then it can both look better and run faster than native.    Now could there be edge cases where something looks worse? Sure. All you need to do is look at FSR2/3 to see how worse algorithms make a ton more tradeoffs. But with good techniques, there will also be many situations where it can look better.    And if you truly care about quality above all else, then you can turn up graphical settings when introducing some upscaling. You'd have to compare something like ""Native"" with Low settings to DLSS with Medium or High settings (turning up the most impactful graphical settings until the framerate was the same).    Personally, at 4K output,  DLSS is always worth turning on. Your dumb PC gamer gatekeeping is hilarious because PC has by far the best upscaling and the most ways to tweak it.",hardware,2026-02-07 15:01:52,13
AMD,o42sn8c,The beauty of pc gaming is “choice.” So..nah keep buying them gpus fellow consumers totally not ceo,hardware,2026-02-07 13:35:43,21
AMD,o42woc1,"Hate to break it you my friend, but you're not actually playing at ""native"". 99% of games are using temporal filtering at ""native"" res (TAA), which to put it in a very simplified way, is essentially doing the same thing as DLSS/FSR...but **worse**.",hardware,2026-02-07 13:59:43,13
AMD,o42tbda,"omg, I need to trash my 4090 and get a Wii",hardware,2026-02-07 13:39:47,9
AMD,o42s1wi,What a pathetic attempt to gatekeep.,hardware,2026-02-07 13:32:04,18
AMD,o42t38e,"Agreed that it makes it look worse, strong disagree with everything else. There’s valid reasons to play at below native res.",hardware,2026-02-07 13:38:25,8
AMD,o42smys,"Bad take, I’m willing to sacrifice raw visuals for higher frame rates on my 4K 240FPS monitor and I’m not about to run out and buy a 5090",hardware,2026-02-07 13:35:40,7
AMD,o42rzs1,Naaa,hardware,2026-02-07 13:31:42,1
AMD,o42tbxk,"> optiscaler isn't as required if the game is relatively recent and already has FSR3.  only for fsr3*.1*, which a lot of software misses.",hardware,2026-02-07 13:39:53,9
AMD,o42t5y3,Not to mention that with RDNA2 you also have to mod the driver...,hardware,2026-02-07 13:38:53,8
AMD,o42pf7d,Reddit is so pro AMD it’s hilarious.,hardware,2026-02-07 13:15:23,45
AMD,o42p4sx,"AMD has done a great job pushing their narrative on reddit for years, making people treat these companies like sports teams (*I'm Team Red guys!!1!*). Now there are so many bootlickers defending their 'underdog' from all criticism, completely ignoring that AMD is just another soulless corporation who only cares about profits, exactly like Nvidia, Intel and all the others.",hardware,2026-02-07 13:13:29,33
AMD,o43gs9y,"In 2020, during the RTX 3000 series launch, Nvidia tried to control HU's videos because HU ""did not focus enough"" on DLSS. I believe it was either by refusing to send FE cards, or directly by telling them what to write? Not sure anymore, but after a public outcry and support from LTT, GN, and other outlets, Nvidia reversed their course.  Nonetheless: HU did not take this story well. They were previously indeed focusing on rasterized performance. After this Nvidia stunt, they continued to do that, and it took them until about ~2022/2023 when they finally started properly and broadly testing FSR/DLSS - after it became evident that a) there's enough games that support it and b) there is a demand for coverage from the pubic, because gamers are actually using it.  Even still, a lot of their FSR vs DLSS comparisons did acknowledge that DLSS was better, but the wording was always casting AMD in a better, or at least a good, light. Either FSR wasn't far behind DLSS, or the price differences on GPUs made AMD a better purchase, or whatever other reason they could find, but they had a very clear pro-AMD bias.  Only around the Radeon RX 9000 series release did HU balance their coverage. I believe this was because both companies, AMD and Nvidia, had done so much anti-consumer shit by that point, that the public hated them (almost) equally.  Before this launch, the Reddit majority opinion was ""AMD has shittier drivers, but GPUs are much better value, and we can handle worse FSR for the price difference, also they are much better for Linux and fuck Nvidia for many valid reasons"". After the launch, Nvidia invested into Linux drivers (but had Windows driver issues of their own), AMD did the same shit with pricing, and DLSS pulled away far enough from FSR that the difference became too difficult to ignore. Both companies now have done so much anti-consumer shit in one way or another that they are evenly disliked, imo.",hardware,2026-02-07 15:47:31,12
AMD,o42tka2,Yet it just so happens everything AMD does wrong can get fixed in their eyes by a PR statement and nothing more. Maintenence mode? Oh thats ok because they use vague language to say they might update drivers to support new games.,hardware,2026-02-07 13:41:17,4
AMD,o46kjcz,"If nvidia didn't let DLSS4 work on RTX 20-40 cards but accidentally let slip a working version of it, do you think HUB would describe it as a ""blunder"" or ""anti-consumer behaviour""? Would they be ""annoyed"" or ""outraged"" (and would they frame it as they're outraged or it's objectively outrageous)? Would the thumbnail say ""stop hiding it"" or ""nvidia F*CKS their customers""?",hardware,2026-02-08 01:52:44,-1
AMD,o42rdk3,"You don't confirm from a single source, you always go to multiple sources.",hardware,2026-02-07 13:27:51,14
AMD,o42ylzd,"GN is arguably even worse these days, starting from routinely calling stuff like GeForce Now a scam, and ending with blatant misinformation like calling Ray Tracing a proprietary NVidia tech.",hardware,2026-02-07 14:10:56,11
AMD,o449bmy,"Nice whataboutism  „Also: AMD released after Nvidia and had the weaker hardware“ so?  „And AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.„   Another point: this was only true if you only look at raster which HUBs loves to do because it makes amd look better",hardware,2026-02-07 18:08:01,2
AMD,o43bp2h,"they spend like every rtx 5000 video talking about how the MSRP was misleading and meaningless and will not be reached and they only set it so low to get the good reviews and how it was all fake simply because the cards were not at msrp at the day of the release.   2 months later they were  Meanwhile AMD actually had a fake msrp and it took them weeks to call them out in a single video.   Similar thing for DLSSa and RT, they did not consider those thing that in their value part of the video and only focussed on the Raster performance, why?",hardware,2026-02-07 15:22:15,9
AMD,o44iaj9,"3080 is going to age well, especially the 12gb version",hardware,2026-02-07 18:52:06,20
AMD,o44xsf4,"One being ""bad"" doesn't make the other not ""bad"".   I see this fallacy plastered all over Reddit constantly.",hardware,2026-02-07 20:12:11,8
AMD,o44ipr9,"Yes, Nvidia is bad? Its independent to whether AMD or Intel is bad or not.",hardware,2026-02-07 18:54:10,6
AMD,o44uv9c,"I am absolutely over whining about how unfairly poor dear innocent Nvidia are *slandered*, sir, absolutely slandered!  They absolutely deserve ever bit of criticism they get, and more.  AMD being half-assed and shitty doesn't excuse Nvidia one tiny bit.",hardware,2026-02-07 19:56:35,2
AMD,o44nu1z,"Yes, nvidia bad. nvidia very bad. They did the same shit or worse.",hardware,2026-02-07 19:19:56,-7
AMD,o43azsh,"Their CPUs might too giving they are trying to implement bigger cache like 3D v-cache. Add to that more cores for the i5 and below, AMD right now is what Intel was before Ryzen. Difference is Intel was market leader back then, AMD still isn't today.",hardware,2026-02-07 15:18:41,39
AMD,o4571h5,"to be honest, I doubt AMD cares much about consumer personal computing anymore. They are all about AI and enterprise customers.",hardware,2026-02-07 21:02:28,7
AMD,o43ccky,What good does open source really do?  Did nothing for FSR1-3.  Dlss is now pretty much everywhere and you can mod DLSS into games. No open source was needed.,hardware,2026-02-07 15:25:34,11
AMD,o49mn4g,"Well my point still stands, if group A is fine without upscaling then they should be happy either way, they can just choose not to use any of the features but having said features is still nice to have, the problem is that group A is so loud and proud about not giving a fuck about upscaling that if you mentioned one bit about how you would like for a better upscaling, they would just tell you to use native because upscaling is ""bad"" and that ""fake resolution"" is not acceptable. Which in turn affects group B who does want those features but can't quite afford or doesn't wanna spend extra money for the Nvidia counterpart.   If amd follow the hate blindly then they'll just be left behind at this point just like how they didn't believe at dlss at first and so did many people including reviewers. But I think that they did learn their lesson from that by quickly implementing frame generation despite the hate it's getting because who knows what frame gen might hold in the future, it might just turn into a fine wine like dlss.",hardware,2026-02-08 15:42:50,1
AMD,o4a52uc,"Only if AMD fold and add it, as standard without users having to jump through hoops. Otherwise FSR will have too little marketshare to justify adding it as an option.",hardware,2026-02-08 17:13:08,2
AMD,o44vyd8,Knowing what your adversary is doing doesn't mean you can counter quickly. Developing and bringing to market dedicated ML hardware takes more than 2 years.  Final hardware specs are decided up to a year before products hit the market.,hardware,2026-02-07 20:02:23,3
AMD,o44fnqw,"Give the tribalism a rest for once, I'm begging you.",hardware,2026-02-07 18:39:06,-7
AMD,o438rzr,With AI generating 10x profits. You are right consumer side lost all the leverage until this blows over. Even people bough more amd or intel. There isnt enough gpus to satisfy all prebuilt and side markets.,hardware,2026-02-07 15:07:11,-1
AMD,o495xnq,Reminds me of steam/everyone else where almost nothing they do can actually break into the market. Brand loyalty is pretty insane.,hardware,2026-02-08 14:11:45,1
AMD,o45o6l0,a good framegen implementation is the best motion blur available.,hardware,2026-02-07 22:35:19,5
AMD,o44z0kz,"You're 100% correct. Pretend frames aren't real frames. It's like gamers enjoy being lied to under the ruse of ""inferred frames""",hardware,2026-02-07 20:18:44,-15
AMD,o45qw8m,"That was ATI, AMD had nothing to do with the 9700 series back then.",hardware,2026-02-07 22:50:43,7
AMD,o44bhn2,"> On the graphic side they where leading in the Geforce FX era...  That was ATi though, before AMD acquired them. The first Radeons to be released after the acquisition were the HD 2000 series, which were a massive flop. Post-acquisition RTG never managed to match the Radeon domination of the 9700/9800, x850 and x1950. The closest they came were with the first two GCN generations, HD 7970 GHz and R9 290x. Everything after that has either been shit, too little too late, or at best a compromise.",hardware,2026-02-07 18:18:45,5
AMD,o43czjv,Intel shipped the first consumer dual-core x86 with Pentium D in early 2005. AMD came a months later with Athlon 64 X2.  AMD64 is a licensed architectural extension of intel’s x86 ISA,hardware,2026-02-07 15:28:47,5
AMD,o4bnwmt,"AMD often introduced new memory standards first, but NVIDIA’s architectures paired with older, mature memory still outperformed them. From NVIDIA’s perspective, there wasn’t much incentive to follow AMD down those memory experiments when skipping them didn’t cost performance leadership.  GDDR4 and HBM1 are good examples of why NVIDIA didn’t rush to follow AMD on memory transitions  GTX 8800 Ultra (gddr3) > HD 2900 XT (gddr4)  GTX 980Ti (gddr5) > Radeon Fury X (HBM1)",hardware,2026-02-08 21:38:24,2
AMD,o43d0v3,"> but they haven't really been able to do that.   In 2017 they weren't able to, that's fair enough.  But it isn't 2017, and they have plenty of resources to stop being so disappointing on the GPU side.  Especially as many of the disappointments aren't even due to resource constraints, just poor decision making.  Like the decision the video in the OP discusses.  They've *done* the work to let RDNA2/3 GPUs use FSR4!  All they have to do is add it to the drivers!    But no, they'd rather pull some bullshit notion of segmentation to 'force' people to buy RDNA4 cards, because they're in complete denial of the fact that AMD isn't many people's first choice to start with, and certainly not after they've alienated customers with a move like this.  They keep acting as if they dominate the GPU market, rather than being a very very distant second place in it.",hardware,2026-02-07 15:28:58,11
AMD,o435m6o,"Zen wasn’t some stroke of pure originality. AMD rebuilt the CPU team. They poached Intel and Apple architects, Jim Keller included, scrapped Bulldozer, and went back to IPC first design.",hardware,2026-02-07 14:50:08,7
AMD,o46peui,"Reminds me of a TPU forum post of an NV RT demo where dweebs who had clearly hadn't touched grass for a while were preferring super sharp sun shadows over RT soft shadows with variable penumbra, calling it blurry and low res. Those people really need to go outside.",hardware,2026-02-08 02:23:21,2
AMD,o44iob4,"I spent like 5 minutes looking for it on my history on youtube, and yeah, remembered how ass was search on youtube. The gist that I remember is that in a side by side blind comparison, people preferred the non-raytraced. I remember it well because someone commented like something like people probably wanted something that looked familiar, not realistic in the context of a video game, that I disagreed with because I think it was the jarringness of having realistic water with not very realistic, I think it was a log, of a pier. The basis of that is that humans are very good at finding inconsistencies.",hardware,2026-02-07 18:53:59,-3
AMD,o498j4s,"Do you think techpowerup is misleading? They clearly state that rdna3 has matrix cores but maybe theyre wrong?  Also amd themselves advertised rdna4 with second gen ai accelerators, so i wonder what happened to first gen ai accelerators.",hardware,2026-02-08 14:26:57,2
AMD,o48xup7,RDNA 4 is maybe 40% better than RDNA 3 in ML tasks. RDNA 3 AI accelerators exist and were advertised yet never used for anything official.     Don't let the marketing fool you.,hardware,2026-02-08 13:21:36,0
AMD,o48pmos,So people will have a reason to buy RDNA 4.,hardware,2026-02-08 12:21:39,1
AMD,o42zrz6,Pretty much...  AMD CPU marketing and choices? Amazingly good.  AMD GPU marketing and choices? Amazingly awful.  Just why is there even such a ridiculously dramatic difference between them...,hardware,2026-02-07 14:17:36,15
AMD,o46i4rg,"Nope people loved it, look at r/AMD threads and HUB videos about it",hardware,2026-02-08 01:37:31,-1
AMD,o465cre,They’re not good. FSR 1-2 are extremely basic and frankly bad. 3 is a little better but still far from good.,hardware,2026-02-08 00:19:25,4
AMD,o457s56,"To be honest, the Native argument was already beginning to fall apart after DLSS 2.5.      The Transformer models only just helped seal its grave.",hardware,2026-02-07 21:06:29,8
AMD,o42un8t,Wait native xess/fsr look worse than dlss q when dlss q isn't better than taa all the time?   [https://www.youtube.com/watch?v=O5B\_dqi\_Syc&t=1s](https://www.youtube.com/watch?v=O5B_dqi_Syc&t=1s)  [https://www.youtube.com/watch?v=T86IufvA4qg&t=1101s&pp=ygUfZGxzcyB2cyBuYXRpdmUgaGFyZHdhcmUgdW5ib3hlZA%3D%3D](https://www.youtube.com/watch?v=T86IufvA4qg&t=1101s&pp=ygUfZGxzcyB2cyBuYXRpdmUgaGFyZHdhcmUgdW5ib3hlZA%3D%3D),hardware,2026-02-07 13:47:43,-17
AMD,o42x0gd,"> Upscaling is being widely adopted as part of the optimization package and default anti-aliasing method.  Upscaling as ""optimization"" is a horrific thing ~ but anyone with a few brain cells knew that as soon as that was offered as a feature, it was going to be abused by publishers and studios to have an excuse to do less optimization. When I say ""optimization"" I mean the real thing, not crappy bandaids passed as such.",hardware,2026-02-07 14:01:42,-10
AMD,o42uqxt,Just lower settings & resolution & use fsr/xess native like dlaa,hardware,2026-02-07 13:48:20,-16
AMD,o42xivk,rasterization is good,hardware,2026-02-07 14:04:43,-9
AMD,o44s7b4,"not him but I get a semi-smooth 4k60fps in oblivion remaster with lumen on, which I hear is actually fantastic for that particular game.",hardware,2026-02-07 19:42:39,2
AMD,o43lgs0,"There was also that dark period in the 2010s when MSAA was pretty much phased out, and your choices were exclusively just horrific FXAA and other ineffective post-processing methods. I started playing Rising Storm 2 again recently where it's the only option, and it's cancer to the eyes. I'd take TAA over that any day.   Half-Life: Alyx is one of the very few ""recent"" games that used forward rendering, but the devs were very deliberate and meticulous in designing around that. As a result, it looks great with just x4 MSAA.",hardware,2026-02-07 16:10:26,10
AMD,o43bb7a,"And they did it again with ML frame gen, it has to be fsr 3.1.4, so a lot of games that only have fsr3.1 are cut off yet again from the upgrade",hardware,2026-02-07 15:20:19,7
AMD,o43bgc4,But amd said it wasnt in maintenance... right....,hardware,2026-02-07 15:21:02,5
AMD,o42wdrg,"This sub often has a weird hard-on for Nvidia ~ I've observed the comment history on here for long enough.  So, your comment is nonsensical.",hardware,2026-02-07 13:57:59,1
AMD,o42t0bs,Hardly.,hardware,2026-02-07 13:37:56,-9
AMD,o42rxk3,Intel is more fun to root for if you hate Nvidia these days,hardware,2026-02-07 13:31:18,-13
AMD,o42xkvw,AMD is one of the worst hardware companies ever. They dont know how to market their shit CPU products with the same 6core Zen5%. RDNA flat out sucks compared to Nvidia and their software stack sucks really. I dont know how can people defend such a company in 2026...,hardware,2026-02-07 14:05:03,6
AMD,o44mb2y,"> Nonetheless: HU did not take this story well. They were previously indeed focusing on rasterized performance. After this Nvidia stunt, they continued to do that, and it took them until about ~2022/2023 when they finally started properly and broadly testing FSR/DLSS - after it became evident that a) there's enough games that support it and b) there is a demand for coverage from the pubic, because gamers are actually using it.  I mean that's a fair attitude no? People might disagree with it, but that's not evidence of bias, it's just an opinion on the value of RT. RT has only really come into fruition in the last few years, before then it was a reflective puddle that halved the frame rate or something. Not focussing lots of benchmarking RT for the perhaps 5 games where it is worth it does make sense.  >Even still, a lot of their FSR vs DLSS comparisons did acknowledge that DLSS was better, but the wording was always casting AMD in a better, or at least a good, light. Either FSR wasn't far behind DLSS, or the price differences on GPUs made AMD a better purchase, or whatever other reason they could find, but they had a very clear pro-AMD bias.  Since DLSS 2 (1 was awful), they've always said DLSS was better than FSR. At times FSR wasn't that far behind DLSS, and when AMD cards were a fair bit cheaper it might be been worth it for some. Pointing that out isn't bias.",hardware,2026-02-07 19:12:11,1
AMD,o46p7fa,"You don't need to come up with hypothetical thought experiments, you can just look at some of their previous video titles to see they don't go easy on AMD.  1. [AMD Throws Loyal Radeon Customers Into The Trash](https://www.youtube.com/watch?v=KsjjFr9mB7w) 2. [AMD Keeps Screwing Up](https://www.youtube.com/watch?v=iLpAinbL8vA) 3. [AMD Fails Again: Radeon RX 7600 Review](https://www.youtube.com/watch?v=Yhoj2kfk-x0) 4. [AMD FSR Redstone Tested - There's Disappointing Issues](https://www.youtube.com/watch?v=LpAZF_-qsI8)",hardware,2026-02-08 02:22:04,2
AMD,o42xkez,yeah you are right,hardware,2026-02-07 14:04:58,2
AMD,o43kmiz,by far the worst video was about Frame Gen... recorded at 30 fps to amplify artifacts. It was hilariously bad and i've lost any respect i had for him after that - he is simply deranged at this point,hardware,2026-02-07 16:06:17,15
AMD,o44c355,">„Also: AMD released after Nvidia and had the weaker hardware“ so?  They actively release their GPUs at a value that makes it better than Nvidia. So obviously, they will look better in a direct comparison. When you flame Nvidia, your AMD flame has to be less because that's just how it is objectively.   >„And AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.„   Another point: this was only true if you only look at raster which HUBs loves to do because it makes amd look better  Since when is raster performance pricing? You good, mate?",hardware,2026-02-07 18:21:36,-4
AMD,o45af37,Nvidia did their best to shorten the lifespan of 30X0 serie. 3070 should have had at least 12GB and 3080 16GB for sure.,hardware,2026-02-07 21:20:42,17
AMD,o44jw6i,All 3 are bad. Different types of bad. You gotta lick which bad is least bad to you.,hardware,2026-02-07 19:00:02,15
AMD,o450vcz,With software support/features? No. They've done plenty bad and made some poor decisions but they've always done well on this front,hardware,2026-02-07 20:28:52,18
AMD,o44vv79,When,hardware,2026-02-07 20:01:55,4
AMD,o43dg6i,This is why it is important to have Intel around. We need there to be legit competition and Intel needs to rebuild itself which could be good for consumers.,hardware,2026-02-07 15:31:06,31
AMD,o44znn9,Top-end Nova Lake and Zen 6 X3D are rumored to have the exact same amount of cache. I'm interested to see what the independent benchmarks show in mid-2027 by which point both will hopefully be out.,hardware,2026-02-07 20:22:13,6
AMD,o448tcq,I remember AMD being about 10% to 15% slower per core most of the time when Intel was dominant. AMD is absolutely destroying Intel at gaming these days. It doesn't feel even remotely close.,hardware,2026-02-07 18:05:33,-4
AMD,o459ne2,"Even before the AI bullshit, the consumer dGPU was clearly low on the priority list.  5th place at best.",hardware,2026-02-07 21:16:35,7
AMD,o44js39,The AI XeSS frame gen would have been useful for Nvidia cards,hardware,2026-02-07 18:59:27,4
AMD,o43d5yl,"If it were open source then people could port the XMX model to non-Intel cards, which is probably why they changed their mind and kept it closed. Nvidia obviously doesn't need it but AMD cards could have benefitted.  The binary XeSS SDK also only supports Windows, not Linux. If it was open source as originally promised then the community could work on bringing it over.",hardware,2026-02-07 15:29:41,9
AMD,o45dlby,"Especially if you might think (or hope) that what your adversary is doing is ""digging a big R&D hole"". And the issues that DLSS1 and to a lesser extent DLSS2 had may have made AMD feel justified in not picking up their shovels.  But now it seems as though Nvidia's hole has long since struck gold so AMD needs to grow their mining operation quickly. The bet didn't pay off, so Nvidia's the one raking in the cash. Or, as much as they can in the *paltry consumer graphics landscape*.",hardware,2026-02-07 21:37:33,2
AMD,o44wzyi,"Never gonna happen when people make money off of it on YouTube. Anyone with eyes can see what video titles are assigned to what company  Case in point, we still get ""this company's MSRP was misleading"" this long after launch",hardware,2026-02-07 20:07:58,15
AMD,o447hmm,"> With AI generating 10x profits. You are right consumer side lost all the leverage until this blows over.  Willingly rolled over and parted their RND cheeks for ""AI Tech"". Everyone who was OK with this shite deserves their $800 RAM kit and 12GB GPU in 2026.",hardware,2026-02-07 17:59:00,1
AMD,o47xjhf,Motion blur is garbage though.,hardware,2026-02-08 08:01:45,-1
AMD,o45zfjg,"All frames are fake, the end result is what matters",hardware,2026-02-07 23:43:40,2
AMD,o43esde,"The difference between the releases isn't months, but days. The Pentium D was May 25th, 2005. The Athlon 64 X2 was May 31st, 2005.",hardware,2026-02-07 15:37:44,17
AMD,o441per,"Pentium D wasn't a dual core design.   It was a dual CPU design with two separate CPU's on one package.   This was implemented in the same way as dual socket systems of the time, where the CPU's interfaced with each other over the FSB.",hardware,2026-02-07 17:30:23,16
AMD,o449oje,"> Pentium D  I'd call this two-core, not dual-core. They didn't have intra-die comms in them pentiums if I remember correctly.",hardware,2026-02-07 18:09:48,3
AMD,o4c4l77,"And 4800 series trashed GTX 200 series on price/performance. Much thanks to needing less busswidth to feed the 4870/4890 thanks to GDDR5.   > GTX 8800 Ultra (gddr3) > HD 2900 XT (gddr4)  The 2900 XT was using GDDR3 actually. The first GDDR4 card was the 1950XTX.  And AMD soon after brought out the 3870. Which while still a bit slower than the 8800GT. Still fixed much of the 2900XT issues, and one big thing was taking the bus down to 256 bit thanks to faster GDDR4.  >GTX 980Ti (gddr5) > Radeon Fury X (HBM1)  The only reason they could even compete in that segment was thanks to HBM. Hawaii already had a 512 bit bus.",hardware,2026-02-08 23:07:23,1
AMD,o43j3f4,"Oh no I didn't mean to imply that they are trying and haven't been able to. I think they were trying to catch up with Nvidia with early RDNA, but yes the way they've marketed most of the RDNA tech and the anti-consumer bullshit they've been doing has definitely been disappointing. Bad leadership and bad business practices trying to be seen as slightly less shitty than Nvidia but not providing products that actually compete. I was rooting for AMD for a while just to provide some competition and alternatives to Nvidia's shitty business practices and bullshit but we are stuck with 2 shitty GPU Companies and even if Intel catches up they're also a shitty company.",hardware,2026-02-07 15:58:45,6
AMD,o46dlnk,"They had Jim Keller before either of those two companies, and brought him back on to help with the Zen uarch. All of these companies technically ""poach"" from each other in way. I find it weird to call amd, a cheap generic cola cpu, when Intel pumped out those broken raptor lake cpus not relatively long ago. The reality is, arm probably going to sneak up on both those companies.",hardware,2026-02-08 01:08:50,1
AMD,o4b5a3d,"The confusion is that AMD uses specialized matrix cores in CDNA. In RDNA3, the so called AI cores are just the graphics core with added flexibility (executing WMMA instructions.) That's why you aren't getting any AI RDNA3 tasks to run alongside raster, it's meant for non graphics application.  9070xt probably dedicates way more die space for AI instructions.",hardware,2026-02-08 20:05:27,1
AMD,o498sk5,"This article from amds official website literally states dedicated ai acceleration, so they do have something  [rdna3 revealed](https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html)",hardware,2026-02-08 14:28:29,2
AMD,o48q4jz,Looks like people are buying elsewhere instead.,hardware,2026-02-08 12:25:36,2
AMD,o4345tb,"> AMD CPU marketing and choices? Amazingly good.  That really depends on the timeframe in question.  They've only had two eras in which their desktop chips were on top:  * ""Athlon"" / ""Athlon 64"" vs Intel's ""Netburst"" (2000-2006)  * ""Zen 3"" and later vs Intel's ""Lakes"" (i.e. Rocket / Alder / Raptor / Arrow; 2021 to present).  Setting aside AMD's humble beginnings as a ""second source"" manufacturer of Intel's designs in the 1980s and their eternal game of catch-up in the 1990s after Intel discontinued second sourcing of their designs starting with the 486, they were completely noncompetitive between late 2006 and the advent of Zen 1 in 2017 due to their ""Phenom"" and ""Bulldozer"" architectures lagging far behind Intel's ""Core"" CPUs.    Note that even when Ryzen launched it still took three generations just to reach technological parity in 2019-20 versus Intel's then-current offerings (i.e. Ryzen 3000-series / ""Zen 2"" vs Core 10000-series / ""Comet Lake"") as well, so honestly it's *only* been 5+ years of AMD being in the lead so far this time around.",hardware,2026-02-07 14:42:07,18
AMD,o474ec0,"I was just trying to give them some credit, ""acceptable"" might have been the better word.",hardware,2026-02-08 04:00:32,1
AMD,o45kxab,"Yea dlss sr 2.5.1 dll was quite the improvement when it dropped, as it vastly reduced ghosting in a lot of games. And it keeps getting even better with the transformer dlss sr dll 310.x.x presets J/K and now M/L.   Though the overhead keeps increasing as well so from performance stand point it isn't completely free visual upgrades, especially at super high fps, but always the choice to use the older presets/whatever the game came with as well if you want to, instead overriding to the latest.",hardware,2026-02-07 22:17:11,4
AMD,o430bdf,"These are pre-DLSS 4 videos, although I wouldn't make that statement flat-out either, but it's kind of safe to say that just using DLSS 4/4.5 will get you very good results.",hardware,2026-02-07 14:20:39,18
AMD,o430ouh,"I used to be in the same boat when upscaling was kinda crap, it was only good if you really really needed more performance otherwise it's just better to stick to native.       However these days, it's actually so good that I can consider it an optimization, it basically increases performance without you noticing most of the time, isn't that what optimization is? Developers have been running certain aspects of games at lower resolution than the user's native resolution for decades at this point, that was optimization and it was good optimization cause you can't really tell that the cloud particles in the distance are actually only 1/4th the resolution you're playing at.",hardware,2026-02-07 14:22:49,21
AMD,o42y6yw,"Sure, that opinion is largely irrelevant to the reality we are facing though.  90% of people are happy to use it so it will continue to gain ground despite a minority opposition.  That means you can either keep up or fall behind, because this is the direction things are going.  It may be distasteful to you, but here we are.",hardware,2026-02-07 14:08:33,9
AMD,o42y0id,"optimization most of the time will just be devs lowering settings. Dont many will spend a lot of time with lods, unless they're given way more time",hardware,2026-02-07 14:07:31,10
AMD,o42zvsv,"Why would you lower resolution and then use whatever your preferred TAA solution at the lowered ""native"" render and not just upscale instead?",hardware,2026-02-07 14:18:11,10
AMD,o45xu0u,So we just can’t use any modern features then,hardware,2026-02-07 23:31:23,2
AMD,o43ksou,"This sub has certainly leaned towards nvidia and downplayed AMD. RDNA2 was barely being considered a 2080Ti competitor on the basis of the performance/power of RDNA1 5700XT.   And DLSS didn't magically end up being as good as it is today, despite heavily upvoted comments maintaining how it is 'as good or better' than native for years.    Now it'll be funny if RDNA5 or whatever AMD are calling their next-gen, ends up in consoles and optimizing for its raytracing pipeline is slower on nvidia cards.   https://www.reddit.com/r/hardware/comments/1e8sri7/leaked_rdna_4_features_suggest_amd_drive_to_catch/lea62vy/",hardware,2026-02-07 16:07:09,-9
AMD,o42tdq4,Bruh,hardware,2026-02-07 13:40:11,14
AMD,o43vila,"People on tech subreddits blamed me for driver issues on my 6750XT, driver issues AMD themselves admitted to.",hardware,2026-02-07 16:59:45,1
AMD,o42sqao,"I don’t hate any of them, it’s just hardware to me. I’ll purchase whichever product gives me the best performance within my budget",hardware,2026-02-07 13:36:13,19
AMD,o46rbod,"4 examples in 3 years is more outliers than a trend, and it doesn't diminish that they brought out the kid gloves here. You won't be able to find them going as easy on nvidia as they are here",hardware,2026-02-08 02:35:21,-4
AMD,o43rr62,And then doubled down in comments saying fps has no impact on image quality. It also made me completely shift my opinion on him.,hardware,2026-02-07 16:41:19,13
AMD,o45t7py,Who the fuck downvotes this,hardware,2026-02-07 23:04:07,6
AMD,o4c5cgo,I remember Nvidia justifying their 10GB card back in 2020 on Reddit,hardware,2026-02-08 23:11:48,1
AMD,o45fauk,Reverse is also true. Let’s not forget Intel was complacent for a very long time taking full advantage of their position for years until they started losing marketshare to competitors.,hardware,2026-02-07 21:46:44,6
AMD,o44ym0j,"During the Bulldozer era it was more like a 30 or 40% difference per core.  In productivity workloads, you saw 4 Intel cores generally beating 8 AMD cores, and in gaming it was a night-and-day difference.  Sandy bridge and its successors were so dominant over AMD that the company nearly went out of business.... and Intel decided to cease nearly all CPU development and improvement for 5-6 years, which eventually bit them in the ass.",hardware,2026-02-07 20:16:33,8
AMD,o449xjk,"maybe i'm crazy for this, but i feel like it's important to recognize workstation performance too when comparing the brands, especially when most people aren't chasing 1% differences. i'd much rather pick up a 14600k over a 9600x just for the fact that it will run the rest of the computer better, on top of being similar for gaming in experience",hardware,2026-02-07 18:11:01,10
AMD,o48frw1,The huge lead in gaming is only for X3D CPUs and intels next gen will also have a huge cache that could close the gap.  In normal single core and MT they are quite close already. Just a few gens back AMD had a huge MT lead of intel (80%+) but alder lake and raptor lake were able to catch up.,hardware,2026-02-08 10:54:15,2
AMD,o45hlx8,True,hardware,2026-02-07 21:59:07,2
AMD,o456or4,Doesn’t it run on any card now though?,hardware,2026-02-07 21:00:33,2
AMD,o43ernq,"With expensive xess dp4a is on amd cards, xmx will unquestionably be more expensive.  Fsr4 fp8 and int8 can be modded and ran on pre rdna4 but at high performance penalty than fsr3 or even xess dp4a. Significantly more on the fp8 version.",hardware,2026-02-07 15:37:38,12
AMD,o4554hi,"I see it more as pro-consumer thing to acknowledge these mega corporations' latest snafu, at least from gn and hub, none of these companies give a shit about  anything but quarterly profit.",hardware,2026-02-07 20:52:03,1
AMD,o45w6nk,"Fun fact, inter core communication on athalon x2 was done through main memory exactly like the pentium D cores. The pentium d would still experience higher inter core latency though because the memory controller was on the north bridge and not the same die as the athalon x2.",hardware,2026-02-07 23:21:30,3
AMD,o4798xr,"It was also not as important at the time. You had two cores mainly to dramatically offload activities off of the main core. Each core would run different and independent tasks, and they'd stick to the core they were assigned to.  It took long years for software to be (painstakingly slowly, one tool at a time) updated to effectively use two cores at the same time.   Nowadays, you've got multi-threaded software that benefits much more from comms and low latency between cores. We've also got insane schedulers having CPUs juggle tasks across threads within nanoseconds. But we took ages to get here, and a good chunk of software still isn't really there yet.",hardware,2026-02-08 04:34:47,2
AMD,o45wcqb,Neither did the athalon. Both cpus had to go out to main memory to talk to the other core.,hardware,2026-02-07 23:22:30,1
AMD,o4c59z4,"1GB GDDR4 2900 xt was also released, with memory clocked at 1000 MHz (2000 MHz effective), although performance differences between the two variants were negligible.",hardware,2026-02-08 23:11:24,1
AMD,o46m7hh,AMD has basically followed this pattern forever:  Intel 8080 → AMD Am9080  Intel 386 / 486 → AMD Am386 / Am486  Intel SSE → AMD adopts it after 3DNow! loses  Intel SSE3 → AMD Venice adds it later  Intel Hyper-Threading → AMD SMT  Intel Turbo Boost → AMD Turbo Core  Intel AVX / AVX2 → AMD adds support later   On the GPU side it’s the same story:  NVIDIA SLI → AMD CrossFire  NVIDIA G-Sync → AMD FreeSync  NVIDIA DLSS → AMD FSR  NVIDIA RTX ray tracing → AMD RDNA2 RT  NVIDIA Frame generation → AMD FSR 3   Generic rc cola,hardware,2026-02-08 02:03:10,3
AMD,o434x40,"I'm talking post-Ryzen AMD, not their prior struggles, where they were desperate to sell anything. Pretty sad marketing, but at least Sony and Microsoft could market the consoles for them...  We do need to recall that Intel's marketing and choices were pretty dogwater for a good decade prior to Ryzen, because they were just sitting on their monopoly, and no-one had any real choices. Intel could just coast on being the brand with the better performance, even if everything else suffered.",hardware,2026-02-07 14:46:20,5
AMD,o431b4a,Dlss 4/4.5 is better than xess/fsr native? 70%> of the time?,hardware,2026-02-07 14:26:19,-10
AMD,o4324hk,"It is still **not** an ""optimization"". This is exactly the crap logic publishers and studios want you to accept so they don't have to properly optimize and just tell you to use upscaling instead.  You don't seem to understand what optimization actually is ~ getting the same effective quality more efficiently. No, I don't mean using upscaling ~ but the same native resolution giving you more performance because you optimized your rendering technique. Upscaling in that regard is just a hack, not an optimization.  You can't use stuff like particle effects in your argument, as that is entirely distinct from upscaling from a lower final resolution to a higher one. That just says to me that you don't understand the difference.",hardware,2026-02-07 14:30:49,-24
AMD,o42ymhi,"> optimization most of the time will just be devs lowering settings.  That is not ""optimization"" ~ that's just lowering settings.  Optimization proper would getting the same effective quality more efficiently.  > Dont many will spend a lot of time with lods, unless they're given way more time  LODs should not be skimped on, because they offer massive boosts in performance. If devs skip them, they're simply lazy or just think so new hot crap like some UE5 Nanite equivalent will mean they can just skip optimizing. A waste of time and effort to implement compared to just make LODs with some program dedicated to producing them for you, which do exist. (Can't remember names...)",hardware,2026-02-07 14:11:02,-4
AMD,o478rhg,"Most games dont have ""modern features"" and you still enjoy them.",hardware,2026-02-08 04:31:12,1
AMD,o44a1rw,"> This sub has certainly leaned towards nvidia  Fuckin pull the other one pal, there's still some give left in it",hardware,2026-02-07 18:11:36,9
AMD,o436l1v,"The easiest way to farm karma is to complain about how biased Reddit is, while in a thread where the exact opposite is happening.",hardware,2026-02-07 14:55:25,-4
AMD,o43c0jv,"As you should. I buy AMD because it offered me the best prices for the performance I was looking for. Also, not having to deal with nvidia bullcrap on linux.",hardware,2026-02-07 15:23:51,6
AMD,o448gwr,"\>And then doubled down in comments saying fps has no impact on image quality.  Okay, I saw original video and cringed at testing with 30fps native, but haven't seen those comments. Steve REALLY wrote that?!",hardware,2026-02-07 18:03:51,5
AMD,o43v5if,> doubled down in comments saying fps has no impact on image quality  yea!,hardware,2026-02-07 16:57:58,5
AMD,o45ivqk,"Oh absolutely agree. We need more than one, was my main point. I wish we had more than the two on the CPU side like we used to.",hardware,2026-02-07 22:05:59,7
AMD,o48g1a6,"> Intel decided to cease nearly all CPU development and improvement for 5-6 years  They did not cease development, but failed for many years to get their new nodes online. It took many years of stagnation and failure from intel for AMD to catch up.  Hopefully Intel can deliver good yield in 18A and 14A, we need more highend foundrys besides TSMC.",hardware,2026-02-08 10:56:41,7
AMD,o44euls,"when people say amd is dominating perf on desktop, they're probably referring to gaming and x3d chips, specifically.  there's other comparisons to make, but that's the one that stands out",hardware,2026-02-07 18:35:08,7
AMD,o45mm8y,Yes,hardware,2026-02-07 22:26:35,3
AMD,o48xatb,"Just the downgraded dp4a version, not the XMX version",hardware,2026-02-08 13:17:56,1
AMD,o47zdd7,"No, that is not correct.  K8 was the first architecture with an integrated northbridge chipset, and memory controller, as well as two cores on a single die.  Inter-core communications were much faster, since they went over the brand new hyper transport link and stayed on-die   AMD64 x2 systems did not have a front side bus.",hardware,2026-02-08 08:18:46,1
AMD,o45z43x,Did the pentiums have something equivalent to hypertransport? you probably know more about this than I do,hardware,2026-02-07 23:41:42,1
AMD,o47zc5v,Somehow you missed x86-64 / AMD64,hardware,2026-02-08 08:18:27,1
AMD,o435u8s,"> Pretty sad marketing, but at least Sony and Microsoft could market the consoles for them...  They were lucky that MS/Sony wanted to stop cycling through exotic CPU architectures (e.g. IBM's PowerPC / Sony's ""Cell"") in favor of x86, combined with the fact that AMD was the only vendor at the time who could design semi-custom x86 chips with a suitable level of integrated graphics.",hardware,2026-02-07 14:51:22,12
AMD,o432erh,I'm not the one who said that btw. Just that you could probably use another comparison video.,hardware,2026-02-07 14:32:27,16
AMD,o43f2dm,"No, this is a bad take. Quality is what I can see. That's the ultimate baseline. If you can render me the same image faster by upscaling it from a lower fidelity image, then it's an optimization.",hardware,2026-02-07 15:39:08,15
AMD,o43fdn9,"Nah, that’s just you setting your own goal posts. “Same effective quality more efficiently” is basically the textbook definition of what upscaling is aiming to do.  All of graphics is full of these sorts of tricks, like are we going to say a game that has no LoDs and no visual culling is “optimized”? None of what you are looking at on screen is real, artificial purity tests are dumb, if it looks good and runs good it’s good.",hardware,2026-02-07 15:40:40,13
AMD,o431nji,"Depends on how much time the devs are given. All devs know what lods are, they're 30x more knowledgeable about game optimzation than me and you.   But if they're given the same timeframe most of them will rather reduce settings to make sure they hit consoles targets and whatever PCs they have in mind.",hardware,2026-02-07 14:28:15,9
AMD,o47a36c,Why not both?,hardware,2026-02-08 04:40:55,1
AMD,o436p5t,For awhile that comment I made up there was absolutely getting downvoted,hardware,2026-02-07 14:56:03,9
AMD,o44ojkj,"Yea, he did",hardware,2026-02-07 19:23:35,2
AMD,o4960xh,But everyone is on the AMD64 standard now and not the Intel one so I guess that would negate it being a cheap clone.,hardware,2026-02-08 14:12:18,2
AMD,o43ohwh,"> No, this is a bad take. Quality is what I can see. That's the ultimate baseline. If you can render me the same image faster by upscaling it from a lower fidelity image, then it's an optimization.  You are not rendering the same image faster! You are rendering a lower resolution image, and are relying on an upscaling model to fill in the blanks. That is not an ""optimization"" ~ it's a hack. I am talking actual render techniques, not workarounds.",hardware,2026-02-07 16:25:22,-13
AMD,o43o329,"> Nah, that’s just you setting your own goal posts. “Same effective quality more efficiently” is basically the textbook definition of what upscaling is aiming to do.  You are just changing the goalposts to sneak in upscaling as an ""optimization"" when it is not even in the same category as making more efficient code that does the same task, but better.  > All of graphics is full of these sorts of tricks, like are we going to say a game that has no LoDs and no visual culling is “optimized”? None of what you are looking at on screen is real, artificial purity tests are dumb, if it looks good and runs good it’s good.  You are deliberately conflating different kinds of things ~ upscaling is not even akin to LODs or visual culling! Those are actual rendering techniques to reduce the amount of work happening on both the CPU and the GPU. The GPU isn't wasting time rendering stuff that is never observed.  In contrast, upscaling doesn't do any of that ~ all it does is upscale a lower resolution final buffer to something that mimics a higher resolution output.",hardware,2026-02-07 16:23:20,0
AMD,o432jox,"> Depends on how much time the devs are given. All devs know what lods are, they're 30x more knowledgeable about game optimzation than me and you.   Not all devs are cut equal ~ these days, many devs just lazily lean on game engine defaults given to them by UE5 and Unity, and don't bother to actually optimize, because they have never learned how to. After all, they develop on high-end GPUs, and everyone else just gets screwed, because they were never taken into account.  > But if they're given the same timeframe most of them will rather reduce settings to make sure they hit consoles targets and whatever PCs they have in mind.  Reducing settings is still not ""optimization"" ~ lowering settings is just lower settings. You're not making anything more efficient ~ you're just cutting corners to force something to run not awfully. It's still not efficient or optimized.",hardware,2026-02-07 14:33:13,-5
AMD,o47dyi6,If it removes enjoyment from your games then the modern features are important. If not I rly dont care about rt. Only pathtracing looks nice,hardware,2026-02-08 05:10:06,1
AMD,o44lna4,Every AMD-criticizing post got like -8 in their first few minutes. Pretty sure someone's sitting here downvote botting a bit.,hardware,2026-02-07 19:08:53,3
AMD,o43p8vg,"To my eyes that's the same image. That's the only part that matters.   If my GPU could render a 1px by 1px black square and then transform that into the appropriate game image, *I don't care* that it's not native rendered. There is no such thing as a ""real"" frame. They're all fake.",hardware,2026-02-07 16:29:01,12
AMD,o43vns2,"LODs however can be readily observed, it’s not “the same task” it’s quite literally a different task, LESS things are rendered, just like less resolution is in the final buffer. Bad LODs are just like bad upscaling, and good LODs are just like good upscaling, a more efficient task for what ultimately brings you a better looking image with the same amount of hardware.   >	Those are actual rendering techniques to reduce the amount of work happening on both the CPU and the GPU.  >	In contrast, upscaling doesn’t do any of that  Are you even listening to yourself? A lower resolution final buffer = less work happening on the GPU.  Next you are going to say MSAA wasn’t an optimization and all games should have been super sampled.",hardware,2026-02-07 17:00:27,9
AMD,o43rta0,"> To my eyes that's the same image. That's the only part that matters.   And yet that doesn't magically make upscaling any more an ""optimization"".  > If my GPU could render a 1px by 1px black square and then transform that into the appropriate game image, I don't care that it's not native rendered. There is no such thing as a ""real"" frame. They're all fake.  There are such a thing ~ real frames are those directly rendered by the engine itself. Just saying ""they're all fake"" is a copout to justify your nonsensical logic.  None of it changes the fact that game developers are relying far too much on them to make up for extremely awful and basically non-existent optimization of their games and engines.",hardware,2026-02-07 16:41:36,-2
AMD,o47a1i2,"> LODs however can be readily observed, it’s not “the same task” it’s quite literally a different task, LESS things are rendered, just like less resolution is in the final buffer. Bad LODs are just like bad upscaling, and good LODs are just like good upscaling, a more efficient task for what ultimately brings you a better looking image with the same amount of hardware.  Bad LODs are not like ""bad upscaling"" ~ bad LODs are the fault of the developer. Bad upscaling is the result of AMD, Intel or Nvidia's driver-provided solution being awful.  You are just deliberately mixing up different things at this point.  > Are you even listening to yourself? A lower resolution final buffer = less work happening on the GPU.  That's just a hack. You would get the same result by just lowering the native resolution. That's not ""optimization"".  > Next you are going to say MSAA wasn’t an optimization and all games should have been super sampled.  MSAA is entirely different ~ edges are being sampled and scaled, which has a hit on performance.  None of these techniques are the same as upscaling the final buffer ~ they happen before the final buffer is composed.  All in the service of justifying the garbage that is upscaling as an excuse for not optimizing a game.  So many games need upscaling just to have not-garbage performance ~ they're rendering to a lower resolution, which the external upscaling solution then upscales. That's just a big hack, because they apparently don't know how to actually optimize code these days.",hardware,2026-02-08 04:40:35,0
AMD,o3f7swu,"Considering the price of DRAM has sky rocketed, an increase of 5-10% seems very reasonable to me. It's not like the AIB partners control the price of RAM.",hardware,2026-02-03 22:08:36,56
AMD,o3eb3x0,"Hello Hero_Sharma! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-03 19:36:11,3
AMD,o3eezmd,I thought AMD promised that they will not raise prices?,hardware,2026-02-03 19:54:19,6
AMD,o3mid55,Hey devs...uhhh optimize games plz or else RIP PC gaming.,hardware,2026-02-04 23:46:20,1
AMD,o3enjku,AMD board partners can kiss my gooch,hardware,2026-02-03 20:34:48,-2
AMD,o3hstkb,"AMD is irelevant as a GPU manufacturer for enthusiasts, so this will not impact the sales which are abysmal.",hardware,2026-02-04 07:46:48,1
AMD,o3f4r3i,"Deep inside I support prioritizing 8GB models. Lets force these lazy developers to optimize their code!!!! Yeah I know I know that's not developer issue but studio preference, but anyway MAKE THEM OPTIMIZE!!!!",hardware,2026-02-03 21:54:27,-4
AMD,o3fes26,Yet all the local AI models I want to use require 16gb,hardware,2026-02-03 22:43:15,-7
AMD,o3mihok,Open ai just cancelled prolly their largest order from nvidia. With the quarterly results coming out I would hope prices go back down. But MSFT or Alphabet will scoop up that order in a heartbeat,hardware,2026-02-04 23:47:02,5
AMD,o3enr1d,"hope it's not a repost, this price hike news is gonna hurt the wallet fr",hardware,2026-02-03 20:35:47,3
AMD,o3ef8pt,Where did they promise that?,hardware,2026-02-03 19:55:29,26
AMD,o3ek7uf,"They don't. AiB does it due to memory, not AMDs fault here.",hardware,2026-02-03 20:18:58,20
AMD,o3etuvo,They aren't though. It's their board partners.  Just like how everyone gave Nvidia a pass when the AIBs raised their prices for the 5070 ti.,hardware,2026-02-03 21:04:03,-3
AMD,o3fxdv6,"You expect the 3rd party vendors to eat the margins, instead of AMD?  What kind of sense does that make?  All things considered 5-10% is less than many have been forecasting.",hardware,2026-02-04 00:23:29,12
AMD,o3gg2ge,NVIDIA and Intel GPUs' prices are also going up from the memory price increases.  Unless your plan is to buy an APU or an old GPU off of the used market (which those have also gone up in prices).,hardware,2026-02-04 02:07:43,3
AMD,o3fx0pm,"8GB has been readily available and affordable on GPUs for almost a decade, if they were going to optimise it would have been done already.",hardware,2026-02-04 00:21:27,11
AMD,o3fxngg,"That's not how it works.  And as a rule, anybody who uses the term 'lazy devs' should be completely ignored as they dont know anything about what they're talking about.",hardware,2026-02-04 00:24:56,0
AMD,o3ggzu4,All this AI shit is why we're here in the first place,hardware,2026-02-04 02:12:58,5
AMD,o3egi5z,https://videocardz.com/newz/amd-says-its-working-with-board-partners-to-maintain-radeon-gpu-prices-close-to-msrp,hardware,2026-02-03 20:01:24,1
AMD,o3eksna,AIBs still get memory from AMD for each board unlike Nvidia,hardware,2026-02-03 20:21:44,-27
AMD,o3fqn1m,Nvidia has FE cards which still drop every now and then at msrp.,hardware,2026-02-03 23:46:26,5
AMD,o3hhd2f,"OVER a decade, R9 390 was mid 2015!",hardware,2026-02-04 06:08:14,6
AMD,o3vds1w,"The accurate term is incompetent devs. Its not lazyness, most of them simply arent high level coders. they dont have the competency needed. And the studios wont hire high level coders. They cost as much as 10 of current devs.",hardware,2026-02-06 08:54:14,2
AMD,o3h0nx0,You must be fun at the parties,hardware,2026-02-04 04:08:38,-7
AMD,o3eh63g,"He put it into perspective in the next sentence already. A promise to try doesn't mean anything.  >However, the Ryzen lead admitted that he can’t predict the future. He added that AMD was trying to work with the AIC (add-in card) manufacturers to maintain prices close to what AMD suggests.  [AMD Promises to Try and Keep GPU Prices Low Against the Ravages of the RAM Shortage](https://gizmodo.com/amd-promises-to-try-and-keep-gpu-prices-low-against-the-ravages-of-the-ram-shortage-2000710505)",hardware,2026-02-03 20:04:33,37
AMD,o3esgde,"This false rumour again? Nvidia still ships memory with their GPU to AIBs, confirmed by Gigabyte CEO",hardware,2026-02-03 20:57:34,24
AMD,o3es0tp,And memory price isn't something AMD can control these days.,hardware,2026-02-03 20:55:35,8
AMD,o3os21c,"And actually surprisingly easy to buy without bots, as long as you sign up for an alert service or discord.",hardware,2026-02-05 09:07:43,1
AMD,o3i97mb,But Der8auer said MSRP was fake,hardware,2026-02-04 10:20:36,-2
AMD,o3m8o1a,"I let clowns like you have that role, actually.",hardware,2026-02-04 22:53:10,2
AMD,o3gckn5,"Reading these articles, I struggle to find any promise from AND reps, just a title saying AMD made a promise but no direct quote I can use to push back on AMD with.  Anyone point me to the right direction in case I'm just blind?",hardware,2026-02-04 01:47:58,15
AMD,o3qcu33,I am literally a member of a team which recently completed 3 optimization phases of an AAA game which I cannot give name because of my active NDA. So you really do not have any idea about my 20+ years of development history in game industry. You are too naive.,hardware,2026-02-05 15:36:13,-1
AMD,o3gieue,It most probably doesn’t exist.,hardware,2026-02-04 02:21:01,15
AMD,o3tg7d2,"Anybody with real experience in the game industry would know that devs are not lazy and incompetent.  But hey, maybe tell us the name of this AAA game so when it has some issue I can blame you for being lazy and incompetent, since you seem to support these sorts of suggestions.  Seriously though, you're clearly a freaking liar.  No actual dev makes comments like this:  >Deep inside I support prioritizing 8GB models. Lets force these lazy developers to optimize their code!!!! Yeah I know I know that's not developer issue but studio preference, but anyway MAKE THEM OPTIMIZE!!!!  That is 100% ridiculous gamer commentary, not how a professional developer talks.",hardware,2026-02-06 00:38:29,0
AMD,o43a5qo,Can't wait for the 10% improvement and 50% extra cost gen over gen.,hardware,2026-02-07 15:14:25,35
AMD,o44108k,">The fact that Intel will use nVidia technology in the iGPU sector in the long term  You're takin the nv intel deal outta context. Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus. It's the premium dgx or halo type prosumer skus that are going with nv ip. There might be some gaming capable devices but just like strix halo they ain't primarily aimed at gamers. The price just ain't worth it compared to a standard mobile cpu + nvidia dgpu combo  Idk why this post is even a thing. All manufacturers are delaying dgpu products for gamers because they have shit margins compared to dc gpus. Nvidia's dc margins in the 70+%, why would they care about gamers? Gaming gpus have worse margins than diy cpus which have worse margins than dc gpus. It's the same for amd  Do ya know **how much money gamers are making for nvidia each quarter? $4.5-5b. How much do ai gpus make for nvidia? Over $50b.** How much for amd? Estimated at $4b+ for 2026. **Amd's making the equivalent of capturing 100% of the gaming gpu market from ai gpus and with even higher margins.** Why would they want to sell to gamers?  Fact that people just gotta face even if they hate it: The gamer money is nothing compared to the ai money. People are mad, but no amount of yelling at the sky and hoping for the ""bubble to pop"" will change that.",hardware,2026-02-07 17:26:53,18
AMD,o447pem,">The fact that Intel will use nVidia technology in the iGPU sector in the long term and that the development of HPC/AI accelerators no longer has much to do with consumer graphics cards from a technical standpoint also plays a role here.  If anything Nvidia just gave Intel another free year to catch up to the 50 series   By the time 2027 ends, Core Ultra 400 series iGPUs might actually be as powerful as a 5050 (but only use 1/3 of the power) lmao",hardware,2026-02-07 18:00:04,7
AMD,o4505hr,"Nvidia officially announced a vera rubin 128gb using gddr7 that's coming late 2026, the cutdown version of that should be the 6090, like the 5090 is to the rtx pro 6000 96gb. So I think there will be at least an announcement of the rtx 60 series in early 2027, even if its a paper launch and only for the 6090. I'd trust that more than rumors",hardware,2026-02-07 20:24:56,1
AMD,o43qon0,"If Intel seizes the opportunity and makes the follow on to Panther Lake and Nova Lake with twice the number of Xe tiles, then this may meet the needs of 80% of the gamers, and Nvidia and AMD may find themselves with no commercial market for discrete GPU cards.  The high end Pather Lake has 12 Xe cores, and it brings 1080p gaming to all day laptops. I wonder what 24 Xe would do, and of course the same approach applies to doubling whatever Nova Lake comes out with.  Intel could repeat the early days of the PC market when embedded VGA graphics eliminated the need for cards like the Diamond Speedstar and all the other discrete cards like it.",hardware,2026-02-07 16:36:03,1
AMD,o45ontc,"I bought a 5090 a few months ago as I saw the RAM situation starting to go crazy in the background in the industry, plus I game at 4K  Honestly this will be very tough years for PC gaming across DYI and gaming laptops",hardware,2026-02-07 22:38:00,1
AMD,o44er96,Outlook is good for those who bought a 5090 T MSRP,hardware,2026-02-07 18:34:40,-2
AMD,o43rnqt,"The sad thing is that I'll likely get it anyways, not for the performance, but because of the spicy power connectors I want to keep my card under warranty...",hardware,2026-02-07 16:40:51,3
AMD,o43xyqx,"Don’t worry, DLSS 5.5 / FSR 4.0 will be software locked to only work with the new ones",hardware,2026-02-07 17:11:50,-12
AMD,o45ny6i,"> Why would they want to sell to gamers?  Well.   > $4.5-5b.   4-5 billion dollars isn't nothing. Yes, it won't be the main focus anymore, but they would be fools to just ignore the billions",hardware,2026-02-07 22:34:01,5
AMD,o46frs3,"> Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus.  Indeed. But for this, their Xe3p will be probably good enough for years. So, why invest in Xe4 or more Xes - from Intel's perspective?",hardware,2026-02-08 01:22:32,1
AMD,o43bpnk,"Thank you for your submission! Unfortunately, your submission has been removed for the following reason:  * It is a submission that is largely speculative and/or lacks sufficient information to be discussed.  Rumours or other claims/information not directly from official sources **must have evidence to support them.** Any rumor or claim that is just a statement from an unknown source containing no supporting evidence will be removed.",hardware,2026-02-07 15:22:20,1
AMD,o46fik4,"Consumer/Gaming-Rubin is _not_ a cutdown version of HPC/AI-Rubin. Different architectures, different chips, completely different projects. nVidia just give the same codename, nothing more (same as with Ampere und Blackwell before).",hardware,2026-02-08 01:20:54,4
AMD,o43vj7c,We've already seen how this plays out with Strix Halo. Large iGPUs drive up costs to the point they're not price competitive with discrete cards.,hardware,2026-02-07 16:59:50,14
AMD,o48ep11,Don't big iGPUs like that just get bottlenecked hard by memory bandwidth? Which is why they're not really a thing outside of platforms that sacrifice general memory throughput for iGPU memory throughput?,hardware,2026-02-08 10:44:09,2
AMD,o43t2bg,Intel will release a CPU with a RTX GPU chiplet soon.  It was part of the Intel-Nvidia deal,hardware,2026-02-07 16:47:45,2
AMD,o445r5s,"DLSS works on all RTX cards from 20 series to 50 series, currently. Framegen is limited to some cards, and Multi Framegen is 50 series only. DLSS 4.5 on 20 series cards does seem to incur a bigger performance penalty, but end users still have the option of whether to use or not.  FSR on the other hand is more complicated, and AMD isn't helping. AMD has shown they can bring FSR4 to older cards with the INT8 implementation with very little loss of fidelity, but refuse to so far.",hardware,2026-02-07 17:50:29,12
AMD,o45oto5,everything has an opportunity cost  they don't have unlimited capacity,hardware,2026-02-07 22:38:55,10
AMD,o46ladu,"There are 2 types, the one that uses HBM (which is datacenter only) and the gddr versions, which are both workstation and become consumer cards. Same in current gens the workstation and consumer ones use the same dies when they both have gddr7. The 5090 is cut down rtx pro 6000, and its successor vera rubin cpx is confirmed as gddr7 and coming in late 2026: https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference",hardware,2026-02-08 01:57:26,4
AMD,o447asl,i think Intel going from 12 to 24 cores would be less of a size increase than AMD going from 16 core GPUs on Strix Point all the way to 40 on Strix Halo,hardware,2026-02-07 17:58:05,3
AMD,o449aml,"Rumoured to be Hammer Lake, 2027 or 2028. Probably 2028.",hardware,2026-02-07 18:07:53,1
AMD,o44ray0,"Nvidia refused to backport dlss for a while too. It's probably not a priority for them and I expect amd to work on framegen first for current gen gpus, since the 9070 actually CAN compete with the 5070ti even before you get into the crazy prices I see for the 5070 ti most of the time.",hardware,2026-02-07 19:37:58,-4
AMD,o47gl4v,But there is also value in diversification. Nvidia diversified away from gaming even before LLMs blew up which is what allowed them to be perfectly positioned with CUDA.,hardware,2026-02-08 05:30:49,5
AMD,o46ur4l,"I wouldn't call these “two types,” because that implies a closeness that simply doesn't exist technologically. HPC/AI and consumer/gaming are completely separate fields of development with completely separate chips. I mean different chips at the chip level, not just versions of chips. Take Blackwell, for example:  - HPC/AI Blackwell is based on GB100, which is the dual-chip name for GB102 (~800mm²) - HPC/AI Blackwell Ultra is based on GB110, which is the dual-chip name for GB112 (~800mm²) - Consumer/Gaming Blackwell is based on GB202 (750mm²), GB203 (378mm²), GB205 (263mm²), GB206 (181mm²), GB207 (113mm²)  Regarding Rubin CPX: This is also an HPC/AI chip that will not be used in the consumer/gaming segment, even though it looks technically similar. It is not a successor to RTX Pro 6000, but rather a separate development entirely for inference tasks.",hardware,2026-02-08 02:57:07,3
AMD,o44mvc2,"Maybe, hard to say without knowing more about Xe3P density. Either way it would still end up being a large chip that would in turn require to upgrade memory bandwidth to keep it fed.",hardware,2026-02-07 19:15:04,2
AMD,o46oom2,> Nvidia refused to backport dlss for a while too.  Where do people come up with these fantasies?,hardware,2026-02-08 02:18:45,6
AMD,o473kff,"blackwell has been a bit of an oddball. historically we got datacentre cards like the L40, T4, A40, P40, etc. that were fanless versions of the workstation cards. there's no B40 or B4 which is odd.",hardware,2026-02-08 03:54:55,1
AMD,o48rjeo,[https://steamcommunity.com/discussions/forum/11/3361397532252230605/?l=hungarian](https://steamcommunity.com/discussions/forum/11/3361397532252230605/?l=hungarian)  Now. APOLOGIZE.,hardware,2026-02-08 12:36:47,-3
AMD,o495bny,"This is purely frame generation. 20-series and above has always had day 1 access to the newest upscaling models, which is way more than can be said of AMD.",hardware,2026-02-08 14:08:05,3
AMD,o495jve,No. Locked features are locked features and you are shifting the goalposts on what is good or not.,hardware,2026-02-08 14:09:27,-2
AMD,o2ys0ji,"Its not perfect   ""The test shows visible artifacts, and 3x multi-frame generation can look worse than expected in motion. You can get similar trade-offs with other third-party tools""",hardware,2026-02-01 13:21:12,79
AMD,o2zbk7e,"Horrible artifacting at 4X, frame pacing issues and unnecessary high lattency at 3x, making only 2x viable, at this point why install it even?   No disrespect to the makers of the mod, I respect your work, time and effort, but for me it doesn't make sense to use.",hardware,2026-02-01 15:11:23,59
AMD,o31dpfh,man i hate when people mod in a Kinda working feature and instantly its claimed that they do what amd wont. Brother fsr dll swaps and multi frame gen are possible but if amd released them in the state that these mod are they would get dumbstered. They get shit on for way less.,hardware,2026-02-01 20:57:13,25
AMD,o2z8fk5,Did AMD fix the broken VRR FSR redstone FG release?,hardware,2026-02-01 14:55:26,14
AMD,o2z4ofk,"Is this the AI driven frame generation, in Redstone? Or the the FSR3",hardware,2026-02-01 14:35:32,10
AMD,o30uvpd,This is INT8 FSR4 presumably?,hardware,2026-02-01 19:25:59,3
AMD,o2z0yi0,"Hmm, will have to investigate, but hopefully this help with some of the frame pacing issues certain games were reportedly having using FSR FG.",hardware,2026-02-01 14:14:56,2
AMD,o38d1sh,lol There was no “beating” anyone here.   AMD did it and discovered it to be poopoo on 7000 series’s and didn’t release it.   Modders did it and also discovered its poopoo on 7000 series and released anyways because why not?   Now imagine if AMD released it in this state and called it a “release” product? It’s not even good enough yet to be added an optional feature by AMD.,hardware,2026-02-02 21:46:14,2
AMD,o2yrokf,"Hello NeroClaudius199907! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-01 13:19:06,1
AMD,o30czf9,"There may a limited point in doubling the frame rate, using whatever tech, but more than that is just a fucking stupid idea, and it's really bad, even on the RTX 50 series. Fake Frames are not the future.",hardware,2026-02-01 18:05:05,-5
AMD,o2yymno,"I always have a lot of respect for these kinds of third party projects. They have far fewer resources and less time to dedicate than AMD/Nvidia, but they often do make a lot of people happy during the interim before official releases",hardware,2026-02-01 14:01:26,51
AMD,o30c0fj,"Yeah but if it can be done by modders it can be done better by AMD """"soon""""",hardware,2026-02-01 18:00:50,5
AMD,o30bmhn,"Might be a good 2x FG solution for certain DX12 games that lack it (RDR2, KCD2)",hardware,2026-02-01 17:59:08,19
AMD,o3664bt,You are 100% right and unfortunately what you mentioned is 90% of the comments around. And the person liking the mod most likely is the same person that would complain (correctly) if AMD launched a broken feature,hardware,2026-02-02 15:40:41,7
AMD,o35ufvz,"AMD even got (kind of rightful) flak for their FSR once released, when it wasn't to everyone's liking anyway …",hardware,2026-02-02 14:42:38,6
AMD,o33xlfb,"But they do what AMD wont, because they are competing with official developer too lazy to actually release a feature at all.",hardware,2026-02-02 05:39:44,-9
AMD,o2zxwpk,"No, that's probably higher in priority than them bringing more forms of frame gen to other cards.",hardware,2026-02-01 16:56:33,17
AMD,o2zbbfv,Not AI driven FG. Just slightly better lossless scaling.,hardware,2026-02-01 15:10:11,26
AMD,o2z849q,Its try it for yourself. 1 min install,hardware,2026-02-01 14:53:47,-39
AMD,o33xra1,doubling framerate is great when i have a base 80 fps and framegen it for my 144 hz monitor. Its only bad if your base framerate is low.,hardware,2026-02-02 05:40:59,2
AMD,o30fl2v,Dont use it then,hardware,2026-02-01 18:16:45,1
AMD,o33xd0f,I think thats underselling it because quite often you end up with third party tools being the only option as the interim quickly becomes never official release.,hardware,2026-02-02 05:37:56,8
AMD,o32pc9k,> I always have a lot of respect for these kinds of third party projects.  ❤️,hardware,2026-02-02 01:06:28,7
AMD,o379vr3,"Yeah, about that … We all usually do — *Commercial thieves not so much*.  It's always fine, when guys doing such things out of fun and pure dedication in their spare time, yet \*never\* being rewarded for it, when it gets used commercially by big MegaCorpses afterwards …  There have been countless instances, where corporations blatantly stole open-source stuff, and always got away with it afterwards, reaping the very fruits of some guy's tedious months and years of spare-time labour …  ---- I mean, remember when Nvidia back then stole the code of open-source *SweetFX*/ReShade shader-techniques, integrating it literally 1:1 (with the open-source modders' verbatim comments included in official nVidia-drivers), then tout it as a graphics-revolution *of their own making* as their high-praised **Nvidia&nbsp;Ansel**, only to then blatantly lie about it being stolen to begin with afterwards when it came to light?",hardware,2026-02-02 18:42:52,2
AMD,o318fab,XeSS DP4A FG is already there for RDNA3 users.  If framepacing issue in Redstone is not fixed it might even be better.,hardware,2026-02-01 20:31:06,10
AMD,o38u45k,What’s the best option for 7900xtx and ue5 games?,hardware,2026-02-02 23:11:54,1
AMD,o3bicsi,"I understand that argument, but they could release, for example, the int8 model in preview or whatever since there are so many people asking for it. They did it before with unsupported FSR4 games through the GPU Profile Manager.   I really do not believe they would get backlash, because that other model (int8) was already extensively tested by users and reviewers on RDNA3 and RDNA2, including the Steam Deck. The feedback was positive despite the slightly worse image quality than the original model and the lower fps uplift when compared to FSR3, since it provides much better AA.",hardware,2026-02-03 10:27:39,-1
AMD,o34b1tn,"Dude, no, you can shit out a broken feature as a mod no problem, nobody expects anything from you. If you release the feature as AMD, Intel, Nvidia, Google, Apple and who ever else you need to have outreach teams, support teams, maintenance, marketing and implementation support teams at the ready. If you fuck up at any of these things people will hear about it and it will make you look like a fool.",hardware,2026-02-02 07:34:29,15
AMD,o30bg1n,"It is based on XeFG I believe (since it requires you to have XeSS dlls). Which is AI driven, but it can't use specialized hardware on NVIDIA cards and relies on the CUDA cores instead (with DP4a)  Even the non-AI FSR3 is leagues above lossless scaling, because it has access to temporal information.",hardware,2026-02-01 17:58:20,12
AMD,o2zvthx,Lol how unhelpful,hardware,2026-02-01 16:47:02,14
AMD,o32354a,"XeFG has the same frame pacing issues as fsr fg - that is, with either, you should enable vsync and / or set an fps limiter depending on the game. XeFG with XeLL works really well in cyberpunk, but it looked and felt awful until I forced vsync on to both smooth out frame pacing and keep it under 240 fps to prevent tearing. Basically identical to FSR fg ml.",hardware,2026-02-01 23:03:50,11
AMD,o33xgoe,DP4A FG is picking the worst option of the ones available.,hardware,2026-02-02 05:38:42,3
AMD,o3c31f3,">I understand that argument, but they could release, for example, the int8 model in preview or whatever since there are so many people asking for it.  They should, definitely and I believe they will. They just need to get things on track, they have very good challenges for that and how to handle old games. They cannot use the same approach as mods and they cannot inject DLLs around in any executables.  >I really do not believe they would get backlash, because that other model (int8) was already extensively tested by users and reviewers on RDNA3 and RDNA2, including the Steam Deck.  The problem is not the quality, is how to implement and they will have a hard time explaining the cost per chip, which is not a percentage. This can fire back very easily.  >The feedback was positive despite the slightly worse image quality than the original model and the lower fps uplift when compared to FSR3, since it provides much better AA.  Yeah, but you are comparing with a mod. They cannot use the same approach.",hardware,2026-02-03 13:07:30,2
AMD,o3g8ckf,"No prebuilt no laptop, no real life presence.   DIY is utterly irrelevant no matter how loud redditors roar.",hardware,2026-02-04 01:24:09,152
AMD,o3h64to,"Considering other AMD gens mostly appeared after a new Gen was released, this is legitimately fast for AMD",hardware,2026-02-04 04:45:06,15
AMD,o3hr8sj,It's less than 5090......a $2K (allegedly) GPU.,hardware,2026-02-04 07:32:34,5
AMD,o3fozhk,If you guys want to know everytime how small diy pc gaming share is and on top of that radeons is. Look no further. Guessing that 95% or more prebuilts just come nvidia. And this is considering that misreading hardware name bug doesnt exist on steam. Which it misread my friends rx6800 a year ago. Even with that. Amd is happy where its at making bank on AI sales and ryzen sales to shift fabs into more consumer radeon. And considering Q1 report and stock price Shareholders want even more. Leaving us to get duopolised by amd and nvidia. While nobody looks back on intel because gamers are dumb.,hardware,2026-02-03 23:37:21,32
AMD,o3hs7p9,no way the gt730 is such a big card lol. for real?,hardware,2026-02-04 07:41:18,3
AMD,o3fs2eu,"Great news. AMD is focussing on their 8gb sku's, just like Nvidia. Rather than buy some markershare on the gamer market team green is abandoning, they are following suit. AMD clutching defeat in the face of victory yet again? Time will tell....",hardware,2026-02-03 23:54:15,17
AMD,o3g6e9o,AMD had chance to comeback market share with MSRP prices instead they went ye old Nvidia -% or -100 strategy before the current AI market eating up all the DRAM,hardware,2026-02-04 01:13:06,15
AMD,o3hjt5k,So glad I dont rely on steam hardware survey as a source of happiness and validation,hardware,2026-02-04 06:28:22,10
AMD,o3ghuvj,"Just grabbed my 9070 XT today. Been team red for years, no regrets. Absolute breeze on Linux",hardware,2026-02-04 02:17:51,6
AMD,o3fbwgg,"Hello BarKnight! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-03 22:28:42,1
AMD,o3hm3zc,I am one of those owners lol,hardware,2026-02-04 06:47:57,1
AMD,o3khb7l,"Here in Brazil, AMD GPUs are getting a lot of advertising and sales. I receive promo offers and sale notifications every day. They are very well priced and competitive, users seem to be buying them. However, the Brazilian gaming market is probably small.",hardware,2026-02-04 17:53:22,0
AMD,o3ilw8g,"Remember kids, Steam Hardware Survey says nothing about market share. Market share is what is bought, SHS is what is owned. So old hardware dominates it. If you look at actual sales data you'll see Nvidia still dominating, but nothing close to what SHS suggests, more like 40-60 in favor of Nvidia in the enthusiast/DIY market, like on Amazon.  The 9070XT is actually selling very well, I think it's fair to call it a hit, especially now it's so cheap.",hardware,2026-02-04 12:06:38,-6
AMD,o3fkzpb,"""Paltry""  Biased reporting, why is TH allowed here again? Anyone forget the ""just buy it"" propaganda?",hardware,2026-02-03 23:15:42,-30
AMD,o3izc8g,I've seen 9070 / XT in a fair few pre-builts.,hardware,2026-02-04 13:32:24,12
AMD,o3i5gcz,"They have like 10%~ market share nowadays. Released only 3 GPUs (one more for OEM) and completely ignored several price points including $250~ which even Nvidia went for. They are making themselves irrelevant and will so as long as they get the Playstation and Xbox contracts.   If Intel can get factories to prioritize putting Intel GPU dies on boards (right now this is the main obstacle for them), they will overtake AMD in a couple more gens.",hardware,2026-02-04 09:45:38,17
AMD,o3i4uoo,more like entire Radeon alone is already irrelevant these days.,hardware,2026-02-04 09:39:52,21
AMD,o3nxb2n,DIY Nvidia still runs circles around Radeon regardless.,hardware,2026-02-05 04:45:18,1
AMD,o3kdiq2,Go on Twitter and see how Tech Jesus is acting.  That's their arbiter of truth,hardware,2026-02-04 17:36:03,-1
AMD,o3im8fl,"DIY is what most of us care about, since that's what most of us are. Also the enthusiast/DIY market tends to lead the mass markets, since enthusiasts/DIY'ers tend to know what is the best value in the market right now.",hardware,2026-02-04 12:09:07,-3
AMD,o3gyyp0,"And why ""gamers are dumb""? Intel had a ton of issues with drivers and no support for dx9 and lower, meaning old games barely working.   If anything, gamers are waiting for arc to mature.",hardware,2026-02-04 03:57:48,33
AMD,o3hbfbi,"It's not just AMD not doing their part on breaking into prebuilt/laptop space (though it is huge for market share). In my country basically nobody buys US-style prebuilts, 99.99% of complete PCs sold, anything from ultra-cheap ""typewriter"" tier of machines and to top end gaming rigs, are assembled by small companies that use off the shelf parts (a few fancier ones have their own case designs, and that's that). They aren't bound by contracts with nvidia or amd or their connections, and they don't exactly have much with AIBs either, not since we are sanctioned to hell and back. So, in this climate of being generally poor, with prebuilts coming with parts that people want to have because system integrators have their hands untied, there are currently about 1600 various offerings with 5070ti. There are 161 with radeon 90 series, *2* of which are 9070XT.      I think there are areas in which reddit isn't wildly misrepresenting the general public attitude, but the love towards AMD gpus is massively inflated for sure lmao.",hardware,2026-02-04 05:22:26,15
AMD,o3izmgc,Think you're also forgetting that the majority of people just simply can't/won't spend big bucks on a gaming system.  They want cheap machines with iGPUs (<1k) or a budget GPU (e.g. RTX '60 class).,hardware,2026-02-04 13:33:59,3
AMD,o3gdeu4,Even in DIY I see so many people sticking to the 1000-series GTX cards reluctant to move over to the RX 9000-series in spite of many many glowing recommendations from people that took the plunge.,hardware,2026-02-04 01:52:42,8
AMD,o3i9as4,"It's been in circulation for an eternity, and is still ""good enough"" for old pixel-art indie games and the like.  As such, it's not *too* surprising that a certain percentage of them survived all these years and are hanging on in the ultra low-end / non-techie user niche.",hardware,2026-02-04 10:21:25,14
AMD,o3kdapp,"They only stopped selling it new with 30 or 40 series, when they replaced it with GTX 1630 and GT 1010",hardware,2026-02-04 17:34:59,5
AMD,o3tpjds,Its in every single university lab pc ever. That isnt cad heavy. Built brand new with a i7 12700,hardware,2026-02-06 01:33:31,3
AMD,o3hk9h9,"It's because the gaming market share isn't worth buying, particularly when Nvidia can steal it back whenever they do choose.",hardware,2026-02-04 06:32:12,8
AMD,o3i56vd,"8GB vram GPU will age like fine wine for many years to come in gaming, just like Quad core CPU did by intel.  If a game couldnt run well with 8GB Vram GPU, it is the game developer's problem now.",hardware,2026-02-04 09:43:08,-4
AMD,o3hsihn,More like Nvidia +100 in reality without their fake launch prices in many places.,hardware,2026-02-04 07:44:01,23
AMD,o3i2dz9,I don't think that chance ever existed. I just don't think AMD ever booked (or could book) the necesssary production capacity to compete for market volume.,hardware,2026-02-04 09:16:13,4
AMD,o3hzg5t,"Pricing is consequence, not the cause. If AMD made 3-4 times more RDNA4 cards those naturally would have been at MSRP and AMD could have sold them all, especially while Nvidia also had not enough supply before summer. But AMD strategy is CPU and data center focused, consumer GPUs literally don't get enough wafers to take away meaningful market share from Nvidia.  And that is not going to change while AMD stock is doing good. Despite plenty of questionable decisions from top brass about how Radeon division is run as long as line goes up there won't be any scrutiny from shareholders.",hardware,2026-02-04 08:48:27,-2
AMD,o3hr11k,"This dumb argument again. So what do you think they should value them at? It's got 5070Ti performance. So when the 5070Ti is 750, should a 9070XT be... 400? Seriously?  Oh no that would be ye old Nvidia -350 because everything relates to nvidia in your head",hardware,2026-02-04 07:30:39,-12
AMD,o3puxd5,"Got mine a few weeks ago, €400 cheaper than 5070ti and comparable performance, I'm more than happy.",hardware,2026-02-05 14:04:11,2
AMD,o3luek3,"50 series and 90 series launched at basically the same time, this makes zero sense. 9070xt is not selling ""very well,"" it doesn't even register on the charts.",hardware,2026-02-04 21:42:18,6
AMD,o3o3jsg,> more like 40-60 in favor of Nvidia in the enthusiast/DIY market  Which is a tiny segment in the whole market.,hardware,2026-02-05 05:29:53,1
AMD,o3fl9mn,How do you define 0.16 market share then? Please enlighten us,hardware,2026-02-03 23:17:12,39
AMD,o3mz7cq,"You have seen it advertised, no one bought them.",hardware,2026-02-05 01:20:03,-6
AMD,o3i6en7,"Agreed, AMD has fumbled 3 generations now by having weaker raytracing and upscaling.  edit: 3 is an understatement - really AMD has been fumbling since about 2012. The GTX 680 was gimped at launch because AMD did not have the expected generational uplift. My personal problems started with having a Radeon HD 6950 where Catalyst Control Center would crash half the time.",hardware,2026-02-04 09:54:43,-6
AMD,o3o2jph,"Of course, but I wanted to avoid any potential discourse about that one lone German retailer that shifts like 4 figure gpu per month.",hardware,2026-02-05 05:22:25,4
AMD,o3sn83m,"Any examples you could provide, I don't have Twitter.",hardware,2026-02-05 22:01:53,3
AMD,o3invea,">Also the enthusiast/DIY market tends to lead the mass markets, since enthusiasts/DIY'ers tend to know what is the best value in the market right now.  Reality has shown time and time again mass market is almost always going to opposite direction of the enthusiast echo chamber.",hardware,2026-02-04 12:20:50,16
AMD,o3hmr1a,Gamers who don’t have $500 to drop on a gpu are buying the B580 as fast as Intel can make them,hardware,2026-02-04 06:53:22,5
AMD,o3hw0d6,"cause they are dumb, most of them are buying shitiest prebuilts that exists out there with the most silicone waste gpu's. I mean even if you buy a prebuilt you can pick a decent one, but those rarely get sold. As for troubleshooting i really pray for people who works in customer support of these prebuilts, even the shit ones they don't deserve a torture like that.",hardware,2026-02-04 08:15:51,-10
AMD,o3heeox,People in your country don’t buy laptops? Those began to outsell desktop worldwide two whole decades ago.,hardware,2026-02-04 05:44:58,4
AMD,o3gvu8f,"To get people to switch, they need to go above and beyond while being cheaper. AMD is satisfied with being behind nvidia.",hardware,2026-02-04 03:38:30,19
AMD,o3hb5qy,"I mean, the thing is, the 9060 xt is still ""only"" 150% or so above the 1080 ti which can be had for 150 or so used.  most people who are holding on to their GPU for that long are going to buy something cheap, and for just a 50% increase, not even doubling, you are paying 350 for a 8 GB card and 450 for the 16 GB card.  while a 5060 ti 8 G is also 350 and 550 for 16 GB  even before the vram shift, the gap between the 60ti of both variants are not that huge, if you are paying that much anyways for a new GPU what is another 50 or so bucks when you are talking 300-500 dollar gap...",hardware,2026-02-04 05:20:30,3
AMD,o3kihn0,Interesting thx,hardware,2026-02-04 17:58:41,1
AMD,o3ijy0c,Anything can run fine on anything if you lower the resolution and settings enough. Such a weird talking point,hardware,2026-02-04 11:52:09,4
AMD,o3spnya,Yeah it's like most people ignore the countless people posting their local shops AMD cost more.   If AMD made more than 1 per 9 to Nvidia they might be able to compete on price.   But then AMD would have to divert from the more lucrative enterprise market. But Nvidia is ruining gaming.,hardware,2026-02-05 22:13:54,4
AMD,o3i2xsz,"If the end-goal is for consumers to ditch nVidia for AMD, then yeah, severely undercutting team green's pricing is the way to go. If you want a new GPU, but 750$ is a bit high, you'll probably go for a 600$ one from the other guys, if it runs about as well.   That being said, it's pretty clear AMD don't care that much about this side of the market and they're content with the situation as it is.",hardware,2026-02-04 09:21:31,6
AMD,o3q6j8f,Hell yeah. I'm installing mine this weekend. Going from a 5800x + RX 580 to a 9850x3d and 9070 xt. Should be a massive difference,hardware,2026-02-05 15:05:28,2
AMD,o3fxuc2,"Particularly when their top selling card from the series has lower usage rates than a GPU that is sold to business customers who have no clue what they're doing.  There is literally no use-case for the GT 730, particularly in 2026, yet it's still more popular than the RX 9070 a year after it was introduced.  Hell, it's still being beaten by the Intel HD 4600, an iGPU from TWELVE YEARS AGO.",hardware,2026-02-04 00:25:57,20
AMD,o3fnrdv,Amd doesnt make enough gpus when AI and higher margin ryzen exists?  Edit: doesnt need to make more enough to cut price even more aggressively and make less money.,hardware,2026-02-03 23:30:39,-9
AMD,o3fltz8,"You report the statistics as-is without injecting your own opinion into it.   If you're going to make some witty comment, you'd better have a point to make. Seems like they're trying to copy Steve from Gamers Nexus without any of the industry analysis, or for a lack of better words, relevance to consumers.",hardware,2026-02-03 23:20:15,-34
AMD,o3q0u0t,I bought one with a 9800X3D from cyberpowerpc with 64GB of ram! Just before ram prices started skyrocketing so things worked out really well for me.,hardware,2026-02-05 14:36:03,4
AMD,o3ib7ag,"Even if they had them way better than nvidia, people still wouldn't buy them. They already had better gpus and still couldn't get more than 50% of the market.   Those $$$ put into ""marketing"" were not wasted. And AMD didn't help with its reputation too.",hardware,2026-02-04 10:38:40,9
AMD,o3j6j4p,"Ray tracing was effin pointless until rtx 4000 because the performance penalty is huge, cards are weak and games are terribly optimized",hardware,2026-02-04 14:11:21,-10
AMD,o3snhs2,"Don't forget the whole continent if Australia. I got a good source stating 9000 series outsold all of RTX 50.   Oh right at launch, between 3-5PM.",hardware,2026-02-05 22:03:13,3
AMD,o3j19r5,"Put ""so called enthusiasts"" because time and again, we see prevailiing ""enthusiast"" takes that don't update with time and changes in technology or market like people flaming Intel efficiency in Lunarlake threads",hardware,2026-02-04 13:43:06,1
AMD,o3ip6f6,Except without fail the mainstream market follows the enthusiast market literally every single time.,hardware,2026-02-04 12:29:53,-8
AMD,o3hqtvk,Those are buying laptops or a  5060 and 5050.,hardware,2026-02-04 07:28:52,19
AMD,o3mzbu6,source?,hardware,2026-02-05 01:20:46,2
AMD,o3i9ot0,Redditors overestimate the games average buyer is playing lol. All these games run well on 8gb still. [https://store.steampowered.com/charts/bestofyear/2025](https://store.steampowered.com/charts/bestofyear/2025),hardware,2026-02-04 10:25:01,8
AMD,o3hhgf6,"I didn't count them since AMD doesn't want rnda4 in them on the level of not even offering a single SKU, and because I don't have any sales data, only what I can see in retail, in online store aggregators, and what I know people have; what we have in our company and in our (massive) parent company and in companies we deal with. Subjectively, if laptops are outselling desktops here, there's also a matter of laptops getting retired way earlier, after a few years, ending up in closets and landfills while 10yo desktops keep suffering through daily usage.",hardware,2026-02-04 06:08:59,2
AMD,o3j2zne,I dont think I will buying AMD Radeon until they hit at least 30% total market share & maintain upthere.,hardware,2026-02-04 13:52:24,1
AMD,o3h40uc,">To get people to switch, they need to go above and beyond while being cheaper  AMDs gaming revenue was up 51% YOY in 2025, which they attributed to strong Radeon demand. Their products right now are what they are, and people are free to pay more for Nvidia's products instead if they think they're worth it. I personally benefited a lot from the ""AMD bad"" mentality last year when I upgraded my GPU (got a 7800XT for 350USD), and I'll be happy to keep riding the last-gen wave for as long as it continues.",hardware,2026-02-04 04:30:45,-11
AMD,o3igl3h,"I think if you have a 1080ti today, you probably didn't buy one on release. That or you life circumstances change dramatically. I just find it exceedingly unlikely that a significant portion of those owners, which were discerning enough to go for the top of the line, are still running such an old GPU.    People with 10 series GPUs today are probably second-hand owners. That is to say that that I agree with you on the fact that they're probably looking for cheap GPUs. Sadly, even a 30 series that is an improvement is too expensive to upgrade to and would probably get into a new 60 series GPU territory.",hardware,2026-02-04 11:25:27,5
AMD,o3tpnme,Some higher some lower (my country),hardware,2026-02-06 01:34:15,1
AMD,o3i38rp,If AMD needs to recoup production and R&D cost that simply isn't feasible and you're just rooting for 4000$ nvidia card monopoly.,hardware,2026-02-04 09:24:28,0
AMD,o3spzcg,That's not an upgrade - that's a new way of life!   Crank the settings and go ham!,hardware,2026-02-05 22:15:27,2
AMD,o3i45j3,"No one is **buying** GT 730s anymore.   They've simply been installed into millions of machines over the past decade.    Also, this doesn't make the 9070 a bad product - simply an unpopular one.",hardware,2026-02-04 09:33:10,4
AMD,o3g84q3,The conclusion remains they don’t exist in consumer dGPU.,hardware,2026-02-04 01:22:55,15
AMD,o3fm8hm,"Okay so paltry it is then, got it",hardware,2026-02-03 23:22:27,32
AMD,o3goqco,"Paltry is an apt descriptor. The author also says they literally own a 9070 themselves, so I’ve no idea where this victim complex is coming from.",hardware,2026-02-04 02:56:46,13
AMD,o3ir321,"People used to buy Radeon.  Theh have been behind for a while but its not set in stone, if they had the competitive products they wouldn't have found themselves in this situation.  5000/6000/7000 series did a lot of damage to the brand.",hardware,2026-02-04 12:42:45,14
AMD,o3ij8tb,"Real life = prebuilt + laptops dominated     No OEM contracts, no sales. Simple as.",hardware,2026-02-04 11:46:52,4
AMD,o3jvllj,"This is 100% true as a former 4080 Super owner who cashed out and went with a Radeon 9070xt when I caught the ramp up window in valuation on the Nvidia side with the Physx debacle in the 5k products and the Radeon 9070xt's settled into sub $600 territory.  As much as the marketing engine wants us all to believe otherwise, FSR and DLSS are upscaling technologies that were designed to trade performance/framerates for reduced image quality.    The heaviest weight in video card selection should still be basic rasterized rendering performance at your choice of resolution in the games you play most often.   This is what you are going to lean on most often for the life of the card while you are gaming... the rest is situation nice-to haves or concessions of image quality or latency in trade for framerates and perceived ""smoothness.""  Ray Tracing is still inconsistently supported and implemented and comes with a huge performance penalty even in Nvidia's product stack that often requires leaning on reduced image quality or greater latency with upscaling/framegen tech.   It's a tech demo feature that gets a ton of it's value from marketing hype vs widespread support and optimization in games.  I still don't consider RT a mainstream tech, cards that can support it natively while maintaining consistently better than 60 FPS 1% lows are going to land in the 4080 / 5070ti performance class or higher .. which is back in $1000+ territory with the effective discontinuation of the non -80 series 16gb cards in the Nvidia lineup recently.  Cyberpunk 2077, a 5 year old and highly Nvidia optimized game, still struggles to stay above 60 FPS on the 1% lows with RT/medium presets enabled on a 7800x3D machine at 1440p while making the whole experience look like weirdly ""shiny"" like someone poured epoxy over everything.",hardware,2026-02-04 16:13:33,-12
AMD,o3ixd44,?   Enthusiasts want modularity and upgradability. Mass market moves towards soldered things and mobility.  Can you actually come up with an example of the _opposite_?,hardware,2026-02-04 13:21:13,15
AMD,o3i2u0o,I agree otherwise but 'budget gamer' and 'laptop' don't really go together.,hardware,2026-02-04 09:20:31,-4
AMD,o3o35ho,"There isn’t, because barely anybody buys the B580 at all and barely any OEM carries it.",hardware,2026-02-05 05:26:53,1
AMD,o3mzkum,You are just making things up.,hardware,2026-02-05 01:22:12,1
AMD,o3o9g7d,"i mean with the vram thing yeah, nv has stopped making 5070tis because the same vram can make a 5080 that they get more margin on, but before this that wasnt true  and right now 9060xt 8gb vs 5060ti 8gb here are very close, and only the 16 gb gap widens because nvidia rather allocate the ram to 5070 for higher margins",hardware,2026-02-05 06:16:53,1
AMD,o3vfg1a,That should be the norm.,hardware,2026-02-06 09:10:19,1
AMD,o3i3n3p,"Don't get me wrong, I'm rooting for them to actually start hitting nvidia hard (got a 9070xt myself) but I don't see a way for them to actually get market share unless they really drop prices and offer a better deal to the consumer, instead of just doing the bare minimum to be slightly more advantageous than nvidia.",hardware,2026-02-04 09:28:18,2
AMD,o3is0lv,"It was stagnation of Radeon 300, 400 and 500(literally rebranded and overclocked 400), then Vega 56/64 that killed it",hardware,2026-02-04 12:48:43,13
AMD,o3pp1gy,"I think there is a middle ground, of a casual psuedo enthusiast, who doesn't follow hardware and just wants something to game with, but does somewhat know what a GPU/CPU is and has heard of Nvidia and possibly AMD. They're they're the ones more influenced by enthusiasts. You see them more on /r/pcgaming and subs like that, tech adjacent.",hardware,2026-02-05 13:31:03,1
AMD,o3iuzw3,They do. It comes with keyboard and screen and its mobile. Mobile sector outweighs tower pcs for a decade now Edit: to the point laptop 3060s 4060s and 5060s destroy the charts,hardware,2026-02-04 13:07:19,10
AMD,o3ijgvy,It very much does because the laptop can be their only device they also use for everything else. You can’t take a desktop to a library or coffee shop.,hardware,2026-02-04 11:48:34,6
AMD,o3i7wgt,Nvidia spends 2x on R&D what AMD does and AMD is also leading consumer/server CPUs. AMD has competitive DC GPUs hardware-wise.  If it was that simple - as people on reddit like to make it look - there would be plenty of competitors outpacing nvidia. I could see Apple do it but they would keep the hardware for their own :),hardware,2026-02-04 10:08:34,0
AMD,o3slixn,"Check back, those mostly had feature parity and sold at decent market value and more important had decent production numbers (at least Polaris).   RDNA1 was a huge shift which essentially cost AMD any good will Polaris and Vega had, which wasn't much.   Almost double price from Polaris10/20/30 to Navi10 and no feature parity to RTX 20.   From their it was all downhill. The price shift made anything below 5600 pointless as Polaris products were faster and cheaper.   Rinse repeat with RDNA2 except now you had Crypto pushing prices even higher with still weak feature parity.   Redditors got conditioned to pay a premium for something ATi gave away - VRAM and Raster.   RDNA3 was just a pathetic attempt for revelance carried on the back of Youtubers misleading their audience.   RDNA4 is made in small batches and sold at a premium and only good will was false advertising yet again carried out by Youtubers.    Edit: typos",hardware,2026-02-05 21:53:42,2
AMD,o3soged,Can you honestly call anyone fitting that description an enthusiast.   And I'm of opinion an enthusiast would rock an i3 OCed to the moon (is it isn't about cost but time investment/knowledge).   My time at r/pcgaming is majority are out of touch due to only following streamers and YouTubers.   Edit: typos reddit on mobile is cancer...,hardware,2026-02-05 22:07:57,1
AMD,o3iv5pz,"I understand that, but I don't think a 'budget gamer' can afford a $1000 dollar device under any definition. If that's budget then a desktop 5060 for $300 is the deal of the century.",hardware,2026-02-04 13:08:19,-1
AMD,o3ix3ys,It is a lot easier to justify than a separate $800 desktop + $500 laptop (which they may _need_). How low can a budget pc get anyways nowadays with brand new parts?,hardware,2026-02-04 13:19:46,5
AMD,o27urvh,"Margin of error improvements...seems like a pointless release, the $30 price premium presumably reflects this.",hardware,2026-01-28 14:19:08,39
AMD,o27tna3,tl;dr  +1-2% stock   +2-4% PBO  [worse than expected](https://old.reddit.com/r/hardware/comments/1qi35je/machines_more_9850x3d_vs_9800x3d_whats_in_400_mhz/o0oe2co/),hardware,2026-01-28 14:13:20,44
AMD,o2803tu,"Finally, I will be able to play Stalker 2 at 103.6 fps instead of 103.1.",hardware,2026-01-28 14:45:46,10
AMD,o27v0ff,"Lmao. So hertz. Much improvements. Wow  Still, I'm not against these special editions, like the 9900KS, 8086K. It's nice for collectors and see how far the tech can go, even if it doesn't make much sense.",hardware,2026-01-28 14:20:20,8
AMD,o2jltcv,AMD in its 14nm+++ era,hardware,2026-01-30 04:17:15,2
AMD,o27yzdd,The heck is wrong with AMD lately??? This....rdna++++++.... gorgon point....,hardware,2026-01-28 14:40:16,9
AMD,o280bz9,Doesn't hurt. Personally if I was buying a new top end PC I'd want more than 8c/16t though. The 7950X3D and 9950X3D are the best of both worlds.,hardware,2026-01-28 14:46:54,4
AMD,o280z2c,"AMD Ryzen R7 9850X3DMILK, because AMD wanted to reset their 14 months old CPU MSRP. So now you pay 499$ for 1-2% more than 9800X3D and 9800X3D will probably stop production and slowly vanishes from stores.",hardware,2026-01-28 14:50:00,5
AMD,o291jgi,"More power draw, hotter temps, 4 percent increase in performance. Welcome back 14900ks.",hardware,2026-01-28 17:31:57,2
AMD,o29a9lm,"I will be interested to see how these dies undervolt, mind.",hardware,2026-01-28 18:09:19,1
AMD,o2fdjjn,"id be much more interested in a 12/24 x3d chip vs a better binned 8/16, which is all i really see this as",hardware,2026-01-29 15:36:49,1
AMD,o282n74,Seems like a pointless product with such a small uplift in performance.,hardware,2026-01-28 14:58:02,1
AMD,o29ia3j,"That's not a new CPU, that's an overclock.  My r7 7700 also gets a 3-5% boost in certain games while overclocked (more in synthetic benchmarks or multi threaded workloads).",hardware,2026-01-28 18:43:33,1
AMD,o27y4p9,"9850x3d?  It’s a pass for me, Bob.",hardware,2026-01-28 14:36:02,-1
AMD,o283otp,"This got to be AMD's Skylake - Kaby Lake moment, no wonder why even AMD themselves was so bored and not excited at all when they revealed it on their AI focused CES 2026 lol. I really hope that Zen 6 is much better than Zen 5, and offers new higher per tier core counts and provides decent ipc uplift over Zen 4 - Zen 5,   if it doesn't then AMD truly has lost it, and i can clearly see Intel will retake their throne soon, especially with how promising panther lake is looking on the laptop side.",hardware,2026-01-28 15:03:00,-4
AMD,o28rro6,"When comparing average gaming performance to gaming power consumption: \~2% more FPS for \~28% more power, so about \~20% worse efficiency.   Keep in mind, this is an average, actual gains and power use vary by game so some titles may show bigger or smaller differences, but you get the idea of how minimal the gain is relative to the efficiency penalty.",hardware,2026-01-28 16:49:11,0
AMD,o2886w0,I do wonder how many people would purchase this over 9800X3D simply due to latter’s failure concern on Asrock/Asus mobos,hardware,2026-01-28 15:23:56,-1
AMD,o28kbs1,Draws way more power and with increased thermals so you can barely see a difference playing at 1080p medium settings on a freaking 5090 ???? must be an AMD joke.,hardware,2026-01-28 16:17:01,-3
AMD,o2a0f5s,"Seems like they did this just to have something, so people cant say that they did nothing",hardware,2026-01-28 20:03:44,6
AMD,o28z07z,"yeah fr, not seeing the value for such a small bump. marketing move maybe...",hardware,2026-01-28 17:20:56,2
AMD,o283kd3,"Yeah, Zen 6 needs that new effing IO die.",hardware,2026-01-28 15:02:25,31
AMD,o28deqw,kinda disappointing tbh was expecting more from the x3d tech hope next gen's better,hardware,2026-01-28 15:47:15,4
AMD,o2cx4bp,">11% extra $$$ for ONE % extra performance on average (2.5% best case) ?\ F*#k that.   If a PC with 9800x3d cost $2000 and a PC with 9850x3d cost $2030, it would be 1.5% extra $$$ for the 1-4% performance increase",hardware,2026-01-29 05:07:10,2
AMD,o285ybt,"This is going to sell like hot buns.  No wait, they need some software with this, ideally one that adds barely no difference with the previous version but change the letters for tuning presets.",hardware,2026-01-28 15:13:40,4
AMD,o32eumn,You don't have to buy it.,hardware,2026-02-02 00:08:29,1
AMD,o286gu3,"I wish they actually made the i3 7360X. Stupid? Yes, but Kaby Lake X was stupid and it would have been cool for overclocking or benchmark records.",hardware,2026-01-28 15:16:03,3
AMD,o28ibpk,"Nothing is wrong with AMD. It's just this sub that is extremely anti-AMD.   If it was Intel releasing another KS product, it would be greeted more positively.",hardware,2026-01-28 16:08:24,-5
AMD,o2ai3bf,"Intel's going to do their down desktop refresh in a few week/months as well, and not just one new SKU. And everyone knows Zen 6 is coming some time this year.",hardware,2026-01-28 21:21:38,1
AMD,o28w7z5,"You do take a hit on gaming performance since those each only have 3D Vcache on 1 CCD. If gaming is your top prio, then 9800x3D is still king (or at least a much better deal)  But with 9950x3D2 coming, that will probably be the new GOAT CPU",hardware,2026-01-28 17:08:45,3
AMD,o28fvme,"More point to it than Intel's KS series, but yeah.",hardware,2026-01-28 15:57:53,-8
AMD,o28ryuf,not to mention it draws a lot more power and it runs hotter compared to the 9800X3D just to gain a few percent increase in performance at 1080p,hardware,2026-01-28 16:50:02,-1
AMD,o28davj,I doubt the 9850X3D is different enough to avoid the problems unless AMD has been selling 9800X3Ds so poorly binned they fail,hardware,2026-01-28 15:46:47,8
AMD,o29aktc,"There is a point there, since the rate is strongly biased toward certain batches. It may well be free of a manufacturing failure, possibly even unintentionally.",hardware,2026-01-28 18:10:37,1
AMD,o2bd9it,To be fair 9800x3d dies also on MSI and Gigabyte boards.,hardware,2026-01-28 23:49:11,1
AMD,o2cywyl,"Yup, both this and the mobile lineup which is at most +100mhz. Just to say there is something at all at ces.",hardware,2026-01-29 05:19:45,-1
AMD,o2dszfy,AMD needs to deliver at faster pace... They're too damn slow.,hardware,2026-01-29 09:37:54,1
AMD,o28e8d4,"It's a +400mhz, I would've been surprised if it's actually more. Heck, you can try overclocking your own CPU, any CPU, by 7-8% higher clocks and you'd probably see similar uplift.",hardware,2026-01-28 15:50:49,7
AMD,o28n4zc,"It's always interesting to see how Intel tried to wipe KBL-X from history (I remember the Threadripper panic, the Linus roast video, the updates removing support...) but overclockers didn't let it happen. Harvested desktop die on a HEDT platform was certainly something.",hardware,2026-01-28 16:29:13,3
AMD,o29tgqf,"KS SKUs are routinely clowned on, what are you on about?",hardware,2026-01-28 19:32:31,7
AMD,o29gruo,"Pretty small. My 7950X3D was cheaper than a 9800X3D and I'd take it any day for the way better multithreaded performance. Even if your use case is just 'gaming', the extra cores help with any background tasks there as well as making a massive difference to things like installing/compressing/decompressing/compiling shaders/etc that are part of running/modding games and supporting applications.",hardware,2026-01-28 18:37:10,-1
AMD,o2czlug,It’s the exact same thing.,hardware,2026-01-29 05:24:41,1
AMD,o2ebx2w,They’re very fast and delivering yearly upgrades on leading nodes…………in data centre gpus,hardware,2026-01-29 12:14:22,2
AMD,o2d327z,"Yes, I agree that I used a conservative number, and the value gets better the more other costs you add. I am glad that you agree with me",hardware,2026-01-29 05:50:19,4
AMD,o2bcvmd,"Not really, 7950x3d is still way worse than 9800x3d in gaming even if you count background tasks. You've made a bad choice and now just try to justify it.",hardware,2026-01-28 23:47:10,0
AMD,o2gpgok,KS series have ridiculous prices,hardware,2026-01-29 19:11:42,3
AMD,o2c6b6s,"The original point I was going for was just that if you're getting a brand new high end CPU, it's probably overkill anyway unless you have a 5090, and to me being overkill in multi threaded stuff is more interesting than being overkill in games.  I was talking generically in that situation, arguing even for people that do primarily just play games being overkill in multithread is more fun than being overkill in single thread.  For me *personally* the choice was very easy because I don't play competitive games at all and have a 4K/120hz monitor and DO do a lot of workstation-y stuff like hosting servers in the background, seeding torrents, running VMs, etc.",hardware,2026-01-29 02:25:10,3
AMD,o2c02j8,"I play at 4K/120hz even my 5900x was fine, I am happy with my choice.",hardware,2026-01-29 01:51:12,-2
AMD,o2cff5k,To be fair 5900x was also weak compared to 5800x3d.,hardware,2026-01-29 03:15:14,0
AMD,o2cx1z5,"Haha, true.",hardware,2026-01-29 05:06:44,0
AMD,o2lj35o,The ram cost more than the high end CPU,hardware,2026-01-30 13:18:36,77
AMD,o2llz71,"Interesting, I was incidentally telling my brother, whose rig I just built using DDR5-4800 spares to avoid the current RAMageddon, that in gaming they performed just the same. These tests confirm that it's just about the extra 0.X%, I don't remember it being this clear-cut for previous generations.",hardware,2026-01-30 13:34:30,85
AMD,o2me2nl,Love how they used all the irrelevant presets on Cyberpunk except the bandwidth intensive ones 🤣,hardware,2026-01-30 15:51:39,35
AMD,o2lm43f,The RawTherapee benchmarks are nice to see.,hardware,2026-01-30 13:35:14,14
AMD,o2nwtw2,"The conclusion that is mostly the same, is not true for all the games  tarkov for example changes a lot, like a 10%  https://www.youtube.com/watch?v=gbfjfuE1ZRM  and hardwareunboxed had a video with the 7800x3d which works just the same as 9800, and there's huge difference, over 10% in some games between 6000cl30 and 5200cl40  https://www.youtube.com/watch?v=aD-4ScpDSo8  Memories with low latency have better 0.1 and framepacing in a lot of games. There's a reason why before the rampocalipse, people started using on top tier gaming pcs the new cl26 kits. It outperformed everything on am5.",hardware,2026-01-30 19:55:22,12
AMD,o2nk4je,I tested 9800X3D( vs 14900KS ) in CPU heavy games with RT on a 5090 4k Ultra Performance and got 10% improvement with RAM tuning from 6000C30 XMP to 8000C36 manual.   You can check my profile for the overclocking sub threads.,hardware,2026-01-30 18:57:37,7
AMD,o2nrfly,"I would be curious what the 1% lows are between the two. They didn't test that or at least didn't report on it.   Probably not much difference since it's an X3D chip, but my understanding is worse 1% lows, the worse the framerate will feel. More hitches/tears.",hardware,2026-01-30 19:30:32,2
AMD,o2o8fov,"What I hate about all the gazillions benchmarks done by reviewers and tech influencers about how important RAM speed is that they never test manually tweaked RAM, they only test EXPO/XMP, which is considerably slower than even loose but manually tweaked timings. So people get the impression that RAM speed doesn’t matter because reviewers are comparing slow RAM with slow RAM. It seems like people also ignore that fast RAM smoothes out the frame pacing and can be the difference between a stutter-less experience and a stuttery mess even if the average frame rate stays the same. The linked test didn’t even attempt to show the 0.1% lows, so it’s worthless.   Buildzoid said a million times that gamers are unable to understand how RAM works and it always checks out.",hardware,2026-01-30 20:50:25,3
AMD,o2mkqbr,"totally agree, unless you're doing something super demanding, an older pc can still handle most stuff just fine",hardware,2026-01-30 16:21:28,1
AMD,o2mrn6z,TL;DR: Doesn't matter. Take the 4800.,hardware,2026-01-30 16:52:04,1
AMD,o2uozjs,"Ryzen AM5 CPUs are way cooler using DDR5 4800, even using DDR5 5200 at JEDEC speed and voltage is hotter than DDR5 4800.   This first generation of AM5 CPUs are better from a power and heat production point of view using DDR5 4800 at JEDEC speed and voltage.",hardware,2026-01-31 20:39:42,0
AMD,o2ls9y0,If I sell my 64GB kit I could but 2 or 3 9800X3D.,hardware,2026-01-30 14:07:37,28
AMD,o2lr3a0,"The Linux advantage is that the system uses less than 1gb of ram on idle, unlike windows 5gb. So 16gb ram is enough for most people.",hardware,2026-01-30 14:01:26,-12
AMD,o2lpgs0,It's just the 9000X3D series has so much L3 cache that what's needed NOW for a frame to be computed can be accessed in L3 like that.,hardware,2026-01-30 13:53:01,105
AMD,o2lqoh1,Pretty sure it was the same for the 7800X3D,hardware,2026-01-30 13:59:19,14
AMD,o2lqrhr,For non x3d chips difference is slightly bigger,hardware,2026-01-30 13:59:46,19
AMD,o2me1g3,"From personal experience I've only ever seen a major uplift upgrading from DDR3 to DDR4 (observed in OW 1), which seemed to give me about a 10% uplift. I've upgraded to RAM with a higher clock speed afterward and didn't really notice much- it doesn't seem like the juice is worth the squeeze at a certain point.",hardware,2026-01-30 15:51:30,6
AMD,o2mf4fb,"Idk. I have seen serious in depth tests by overclockers, and it seems, 6400 1:1 is the sweet spot. The caveat is, it only makes a serious difference when it's running FPS unlocked and the CPU is near 100%, then the fps gains are very much worthy, like from 120 to 160 and low 1% get better as well. The thing about ram overclocking is that you only really see real gains when the CPU is near max, since the 3XD cache gets full and needs to access memory more (like the other comments clarified).     I like tinkering a little (not an overclocker) and since ram was cheap back then (lol) I got a 6400 kit and run at 1:1. Took time and testing, but I had fun, I guess :P     I think for the most part, the fun part was learning. Now I know a lot more about ram and their settings. However, if I was time constrained and had less patience, I would just run 6000mhz and the lowest CAS I could find at an affordable range.    I play on 4k, doing 6400 1:1 probably didn't even get any fps gains, was just the fun of tinkering and getting my PC to be as good as possible    I avoided Infinity Fabric overclocking and going too low at the secondaries and tertiaries. Plus I ran the SOC at 1.3. The amount of extra time testing everything would take too much time.",hardware,2026-01-30 15:56:21,5
AMD,o2mrtlc,"IIRC back in the DDR3 days, as long as you are on at least 1600, you were good, anything above like 1866 and 2133 were deemed waste of money. 1600 was the vast vast majority of ddr3 around the time of intel 3rd/4th gen, they were cheap and very available. I think it was only after the release of Zen1, people started to put more focus on ram, because Zen really benefits from it due to fclk, on 6th gen intel, which was also ddr4, people just bought whatever is the cheapest.",hardware,2026-01-30 16:52:52,4
AMD,o2n5k9t,This is more related to the X3D more than anything.,hardware,2026-01-30 17:54:23,3
AMD,o2skedd,There are heavy multiplayer scenarios that overload the 3D VCACHE and have to fallback on memory. Marvel rivals time square is a perfect example of this. Or bf6 with a really big fight.,hardware,2026-01-31 14:25:32,1
AMD,o2om759,">https://www.youtube.com/watch?v=aD-4ScpDSo8&t=731s  > You really shouldn't be building a zen4 based system without using ddr5 6000 CL30 memory, as a 32GB kit costs as little as $85   https://imgur.com/a/eRlwlkr",hardware,2026-01-30 21:55:28,14
AMD,o2s10tz,It's not so simple though. Most people set a memory profile because it's easy and just works most the time. Manually tweaking RAM is a time consuming process and can have consequences such as corrupt files. The time for that and the risk involved isn't on most peoples table.,hardware,2026-01-31 12:19:34,10
AMD,o3eqymd,Manually tweaked ram is highly variable depending on which unit you have.,hardware,2026-02-03 20:50:43,1
AMD,o2xrud7,"> they never test manually tweaked RAM  because diminishingly small number of people manually tweak RAM. Like, so small they are not worth mentioning as an option.",hardware,2026-02-01 08:11:44,1
AMD,o38t63h,Ain't nobody got time for that and the amount of people that will unknowingly insert instabilities because they don't know enough is just going to be too high.    Reviewers aren't going to review something that an exceedingly small portion of the audience will do. Especially one that is so unrepeatable.,hardware,2026-02-02 23:06:52,1
AMD,o2lxki0,"And then all the software is using some kind of web wrapper (forgot the name, but it's hated) and ends up using the same as Windows.  edit: Electron! thanks for the answers :)",hardware,2026-01-30 14:34:17,20
AMD,o2m9jst,"Windows can and will free memory in case other programs need it. Okay, Linux uses less than 1 gb on idle, and? How exactly is it an advantage? What matters is how responsive the system is during actual workload that uses lots of RAM...",hardware,2026-01-30 15:31:12,7
AMD,o2ly6o9,"Maybe if you have a bloated Windows setup with loads of apps set to 'start on Windows launch' or something, but mine is only about 3GB for a normal idle state.  Used to be more like 2.4GB, but I guess over a dozen years on the same OS will cause some bloat to come in.  EDIT: I guess it's important to mention I'm still on W10.  I guess W11 might be different.",hardware,2026-01-30 14:37:21,8
AMD,o2xqmu1,Windows kernel uses 600 MB. everything else is optional.,hardware,2026-02-01 08:00:37,1
AMD,o2lx15t,"Yeah no. 16GB is definitely not enough for most people, even with Linux being less of a memory hog.   Most distros definitely don't use less than 1GB when idle unless you remove a bunch of stuff and use very lightweight DEs like xfce or lxqt, and by default Linux is set to use swap whenever memory usage is greater than 60% - which needs a single line of config edit added to the grub bootloader to have it apply persistently.   This is not something your noob user will know about if they switched to Linux recently.",hardware,2026-01-30 14:31:37,1
AMD,o2lxdue,"It can actually be quite big with non-Vcache chips.  Depends on the specific kit of course, there's more to it than just the MT/s.  Generally the bigger problem with low MT/s kits is that they're some of the lowest quality DDR5 out there and will usually have poor latency as well for overall poor performance.  Like straight up, you can nerf your performance by a whole generation of CPU performance(10-15%) with poor RAM in certain cases.",hardware,2026-01-30 14:33:22,19
AMD,o2n45uf,HUB did some videos proving pretty definitely that DDR5 is a worthwhile upgrade on DDR4 at this point.  It always takes a bit of time for the memory standard to mature and then performance gains become quite real.,hardware,2026-01-30 17:48:15,6
AMD,o2xrphb,DDR3 to DDR4 is twice the bandwidth (assuming same timings).,hardware,2026-02-01 08:10:29,1
AMD,o2n4x2s,"Even with Zen 2/3, it was still plenty to simply be on 3200Mhz, and anything more was for overclockers/tinkerers.  So not really that much different.    Just like with DDR5, usually 6000Mhz is considered a good sweetspot for affordability and performance.",hardware,2026-01-30 17:51:35,4
AMD,o2s1au4,"None of what you said addresses anything I said, but okay.",hardware,2026-01-31 12:21:49,-3
AMD,o3f52fb,Read my comment.,hardware,2026-02-03 21:55:55,1
AMD,o2xs5wm,Wrong.,hardware,2026-02-01 08:14:40,-2
AMD,o2mdlpa,Let's hope the L2 stack will give a hefty boost too.,hardware,2026-01-30 15:49:31,5
AMD,o3ba8xp,We should make one and call it... X3D2!,hardware,2026-02-03 09:09:08,1
AMD,o2lzr9x,"The word you're looking for is ""Electron"" but yes.",hardware,2026-01-30 14:45:06,5
AMD,o2m0jue,electron?,hardware,2026-01-30 14:48:56,3
AMD,o2mptlx,"I didn't do any official benchmarks, but my sister's old laptop with 4GB of RAM was extremely slow in Win10. After putting Lubuntu it was way faster using Chrome, and was able to open many tabs without slowing down. And that was even before I enabled zRam.",hardware,2026-01-30 16:44:01,2
AMD,o2msnop,"W11 is the same, it's just that some RAM is allocated and people not in the know see ""hey, 8GB of 32GB is used while idling Windows bad reeee"". It's like they don't want to use any of the RAM they bought.",hardware,2026-01-30 16:56:36,11
AMD,o2m99jf,"16gb is fine for most people.  I don't currently build or buy anything with less than 32, but 16 is still fine for gaming.",hardware,2026-01-30 15:29:54,7
AMD,o2n7hfc,"> Yeah no. 16GB is definitely not enough for most people, even with Linux being less of a memory hog.  16GB is fine for 99%+ of people",hardware,2026-01-30 18:02:42,5
AMD,o2mfcp6,The X3D cache operates at 2.5 TB/s and DDR5-8400 memory at 67 GB/s per channel.  So you would need DDR5-20000 in an 8 channel setup to match it. It simply is impossible and not even takes into account that much of the X3D performance comes from 10x reduced latency not bandwidth.,hardware,2026-01-30 15:57:22,21
AMD,o2m8bce,"I can back you up on there being a group of people who made that argument. They weren’t completely wrong, but ram tuning is pretty hardcore it’s not like enabling pbo.",hardware,2026-01-30 15:25:33,19
AMD,o2lz497,Friend you're missing a huge asterisks as that uplift is only precent in a very small select number of games.  That reddit is still clueless on RAM scaling baffles me when it's been a discussed topic for as long as I can remember.  I'm of the generation when you bought slower RAM because it was cheaper and OC'd it. Today people just drop hundreds extra for minimal gains on both their CPU and RAM while ignoring their GPUs and worst their PSUs.,hardware,2026-01-30 14:41:59,21
AMD,o2mbt0c,I experimented on this myself with Overwatch and Rivals:  https://www.reddit.com/r/buildapc/comments/1lluich/14900kks_vs_99509800x3d_4090_vs_5090_some/,hardware,2026-01-30 15:41:22,2
AMD,o2nml26,"I tested 9800X3D vs 14900KS in 5 high CPU usage games with RT, and 14900KS w8400 RAM was faster overall. Started a firestorm in the overclocking sub where I posted it November.  RAM tuning on 9800X3D 8000C36 made it faster overall in averages, but 14900KS still leads in 1%.  14900KS is on Asrock Z790I Lightning, arguably the best memory OC motherboard for Intel 13/14 series and 9800X3D is on the 2DIMM Apex X870E",hardware,2026-01-30 19:08:34,1
AMD,o2m43e8,"you can get most of the performance by just messing with timings a bit as long as you're not donig bandwidth constrained workloads, though like oyu said there's a higher chance of running into a chip that you can't push that much further even manually",hardware,2026-01-30 15:05:54,3
AMD,o2n76t3,I managed to buy DDR3 right after DDR4 came out and upgraded to DDR4 right when DDR5 was rolling around. Hopefully I can get on the right side of the curve next time I upgrade,hardware,2026-01-30 18:01:24,3
AMD,o2n8l2x,"Thanks, certainly sounds like something to trust over my anecdotal experience.",hardware,2026-01-30 18:07:28,1
AMD,o2osadp,"I remember Zen2 and especially Zen3 people were pushing more for 3600MT, I don't remember witch gen, but it's the gen they started to let fclk to decouple with ram mhz by letting it run 1:2 or 1:1, and people find out the sweet spot was 3600 and stay 1:1.",hardware,2026-01-30 22:25:28,1
AMD,o2trn2q,I mean my point is why do such a test and use it as an argument when it's not something most people can or even want to attempt.    I'd still like to see it of course don't get me wrong. Like OCing a CPU on custom loop or liquid nitrogen. Fun to push the limits but lets not pretend it is achievable by most users.,hardware,2026-01-31 17:59:57,4
AMD,o38yh6w,"The amount of people that buy 5090 is significantly higher than the amount of people that tune memory. You could ask instead why they review Intel GPUs. My guess is because Intel sends them hardware for free to review and because even that market is larger.     If there's really an untapped audience for it, why not capitalize on it?   The fact that your argument is calling names, shows a lot about who you are.",hardware,2026-02-02 23:35:44,2
AMD,o3fycf6,"Your comment has been removed for the following reason:  Be respectful of others: Remember, there's a human being behind the other keyboard. If you have nothing of value to add to a discussion then don't add anything at all.",hardware,2026-02-04 00:28:42,1
AMD,o2m4avn,"You can use earlyoom to keep them in check, no ?",hardware,2026-01-30 15:06:52,-5
AMD,o2n5k42,"4GB will definitely be very easy to run into problems on Windows, for sure.   But regardless, 1-2GB either way isn't making much of any difference on a bigger scale here.  Wont make 16GB 'ok' on Linux when it's not on Windows outside of some fringe cases.",hardware,2026-01-30 17:54:22,3
AMD,o2nazjb,"100% of the time when people throw around comments like 'something is true for 99% people', then it is not a statement of fact, but an opinion.",hardware,2026-01-30 18:17:54,-2
AMD,o2qugif,"X3D is still limited by Infinity Fabric speed ~70GB/s, so when cache is exhausted (RT, LLMs) the X3D chips are limited by their fabric speed pretty heavily. 14900K will be over 100GB/s with fast ram.  X3D with stuff that can fit in its L3 will dominate, but there are some niches where the old 14900k can out perform due to better available memory bandwidth.",hardware,2026-01-31 05:54:03,3
AMD,o2mjkqy,"Honestly i've always overprovisioned my CPU because it's too much of a pain in the ass to upgrade/replace, and by the time i actually upgrade (5+ year cycles) chances are we're on a different socket already so there's no point in upgrading it, so time to get another board+CPU.  That being said, yeah, IMO the PSU is something you should *never* cheap out on. Shitty PSUs take more than just themselves with them to the grave more often than not, absolutely not worth the pittance it costs to just get a good one. Plus you can carry it across multiple builds too.",hardware,2026-01-30 16:16:17,10
AMD,o2mscg2,"Yeah, I recommended a 5600MT/s kit to my brother because it was half the price of a 6000MT/s kit 3 years ago. The FPS for a Ryzen 7600X / RX 7800 XT between those RAM kits were really close, at most 10% but in average 3% iirc.",hardware,2026-01-30 16:55:12,2
AMD,o2ogetx,That actually means you're very much on the same ideal curve.  You're just not as far along on the time axis as some others.  Not a bad place to be really.,hardware,2026-01-30 21:28:00,2
AMD,o2p9sc5,"Well that's the same as 6400Mhz with DDR5.  Ideally if you could overclock both memory and IF to the same factor, there's a small reward, but 3200Mhz got you most of the way to what DDR4 was gonna deliver for you.",hardware,2026-01-30 23:58:52,1
AMD,o2xs8tj,Because quality discussion should not be limited by what the average Joe wants or can do. I thought this was obvious. Building a computer is also not something most people want or can do yet here we are.,hardware,2026-02-01 08:15:23,-2
AMD,o2m5g27,"Eh, just use zram/zswap/swap. No reason to sit there running task killers for what are web browsers and videos games. We've long since solved OOM issues on normal desktops with normal applications. With 16GB and a browser, most times you'll only push a few hundred MB of RAM into any sort of swap unless something is reaalllly choking you out.",hardware,2026-01-30 15:12:15,5
AMD,o2ncp4z,"I would be very interested to see benchmarks, and I think with zRam there are probably a few games where memory usage could make a real difference for 16GB computers",hardware,2026-01-30 18:25:15,1
AMD,o2xqs3t,yeah id say its about bellow 6 GB where it becomes hard to use windows.,hardware,2026-02-01 08:01:57,1
AMD,o2ra7c7,Sure that is true it depends on the workload. If the workload is load from ram use once and never use again like LLMs (but who runs them on a CPU?) then benefit from X3D cache is none. But most stuff like game engines run a loop thousands of times on the similar data. It does not have to fit in the cache - even if half of it fits this half is already there. So this also benefits the non-fitting half by not competing for memory transfer with it.,hardware,2026-01-31 08:12:55,1
AMD,o2n2q2t,Make sure you do. DDR5 tuning makes DDR4 tuning look like a walk in the park. Had to update bios on two machines just to get rated expo to boot.,hardware,2026-01-30 17:41:49,10
AMD,o34jhav,Ram OC is a young mans game and a hobby for the unemployed (its a joke),hardware,2026-02-02 08:54:23,3
AMD,o2o25ns,"In my case a 4x3733 XMP DDR4 kept crashing until I pushed in the RAM sticks a bit harder. Literally all it took to fix the crashing was to just push them in a bit more, even though they did click in before.",hardware,2026-01-30 20:20:27,2
AMD,o2nqjo9,"I hear you, and I raise you my home server running jedec 4800 because I don't wanna mess with it",hardware,2026-01-30 19:26:28,1
AMD,o2n6v2f,"> That being said, yeah, IMO the PSU is something you should never cheap out on.   I did this back in 2015 and now I'm wondering if a PSU ever ages out...",hardware,2026-01-30 17:59:59,3
AMD,o2xrdrd,When it comes to gaming i play genres that are CPU bottlenecked so i spend more on CPU than on GPU most of the time.,hardware,2026-02-01 08:07:29,1
AMD,o2xr7dx,"Its very interesting to see game engines with x3D. when the model fits entirely inside the 3D cache it runs great, once it exeeds the size and you need to start swapping from memory performance drops off a cliff.",hardware,2026-02-01 08:05:51,3
AMD,o2sztn1,"For LLMs or image gen running locally on consumer hardware, you often have to blend the workload and split layers between GPU and CPU (ex: Flux dev FP16 or non quantized models / bigger LLMs) even on a 4090. This uses your CPU and GPU, and then becomes reliant on system RAM bandwidth, while also still giving pretty high performance. It's why people can run huge models on a 4090 and it still be pretty fast. 14900k is better in this case.   Same with RT - in RT games, they seem to miss L3 cache and hit system memory (I don't know why). Granted, most people are GPU bound for RT, so they'll never notice, but technically here the 14900k can also perform better.   Niche stuff that most people won't care about or notice, but still a thing.",hardware,2026-01-31 15:46:20,2
AMD,o2npuia,"If it isn't sufficient for new capacity needs or if it fails...that's pretty much it.  I've had maybe one PSU failure in 20+ years of building my own PCs, usually I am replacing when I need more powaaa",hardware,2026-01-30 19:23:16,3
AMD,o2nbhj3,Bad PSUs are definitely still a thing.,hardware,2026-01-30 18:20:03,3
AMD,o2nuxbc,"They certainly can, capacitor degradation is a real thing, especially if you are having many transient loads. Also the RTX 30 series and newer are more prone to transient spikes of 2x their rated wattage (e.g. 350W to 600W+ for a microsecond) that can cause Over Current Protection to engage. Not *quite* the same as aging out, but PSUs are subject to the same time decay as everything else and feature enhancement. If you have a newer GPU, it's worht getting a ATX 3.0 or newer PSU, for that plus the native cable to deliver 600W in a single cable for high power GPUs.",hardware,2026-01-30 19:46:38,1
AMD,o2xri3b,the only PSU failures i experienced was noncritical. Mostly just the fan stops spinning and you replace it because you dont want to risk it overheating. Never an actual cease to work failure. Ive been building since 1995.,hardware,2026-02-01 08:08:35,1
AMD,o3vmgwt,CPU consumption is double now for every servers.  The next great supply shortage and price increase for CPU is coming,hardware,2026-02-06 10:17:12,58
AMD,o3vwwvi,This little maneuver's gonna cost us 5 Years - Sam Altman,hardware,2026-02-06 11:46:48,49
AMD,o3w99n1,Whats next? The PSUs getting 5x more expensive?,hardware,2026-02-06 13:10:40,12
AMD,o3x0r9r,Same in the US.  I have customers waiting 4 or 5 months on server shipments due to CPU constraints.,hardware,2026-02-06 15:34:45,5
AMD,o3xidra,"so more server CPUs being made, fewer gaming CPUs being made in future",hardware,2026-02-06 16:57:30,2
AMD,o3wbzba,These lead times are tiny. Everything else has lead times much higher than this,hardware,2026-02-06 13:26:15,2
AMD,o40ou2q,"I feel good about picking up a refurbished Lunar Lake laptop for relatively cheap back in December. I figured the RAM and NAND shortage would price me out of the Panther Lake models with a large iGPU, but now it sounds like the availability of the chips themselves is going to be problem.  If you are in the market for a thin and light laptop, definitely consider looking at some of the Lunar Lake models. Prices have actually fallen.",hardware,2026-02-07 03:04:27,1
AMD,o3wgrwb,Is it too late to buy $TSM?,hardware,2026-02-06 13:52:29,-2
AMD,o3vobnl,"Ye, we went from Intel selling 7nm CPUs like RPL with large discounts well below MSRP in early 2025. To them straight up starting to raise prices of 7nm chips above previous MSRP instead.",hardware,2026-02-06 10:34:14,34
AMD,o3wxym8,I’m tired boss…,hardware,2026-02-06 15:21:07,7
AMD,o3w8lb1,"Fortunately those CPUs aren't required for critical day to day operations in non AI businesses upon which our economy depends, so nothing to worry about.",hardware,2026-02-06 13:06:37,9
AMD,o3vosn6,Until Nova lake hopefully,hardware,2026-02-06 10:38:31,3
AMD,o3zedqa,One of the many reasons Nvidia entered into the partnership with Intel. Nvidia still needs a healthy supply of CPUs to sell into data centers. Being bottlenecked on CPU supply will negatively impact Nvidia too.,hardware,2026-02-06 22:30:15,1
AMD,o3xiq48,-Scam Faultman,hardware,2026-02-06 16:59:07,14
AMD,o40mftt,"""I'm going to make affordable computers disappear with this pencil trick."" - ~~Joker~~ Altman",hardware,2026-02-07 02:49:26,1
AMD,o3x4ddy,[https://www.techpowerup.com/345062/power-supplies-and-cpu-coolers-are-next-for-price-hikes-6-10-increase-expected](https://www.techpowerup.com/345062/power-supplies-and-cpu-coolers-are-next-for-price-hikes-6-10-increase-expected),hardware,2026-02-06 15:52:00,6
AMD,o3yoi6p,Theres always room for family- Vin Disel do eeet buy in and it has a dividend!,hardware,2026-02-06 20:20:20,2
AMD,o3vqix8,Yes it's absolutely crazy.   There are small cores CPU out there for mini PCs have went up 400% in price vs 6  months ago.  Xeon cpu out in market which has huge amount of inventory 6 months ago now sold out everything with huge backlog or orders.,hardware,2026-02-06 10:53:50,26
AMD,o3xfs54,It's so nutty that Intel 7 is capacity constrained. It's like 3 nodes behind the leading edge at this point.,hardware,2026-02-06 16:45:20,13
AMD,o3xh9pp,I mean for Intel's bottom line they kinda need it.,hardware,2026-02-06 16:52:18,3
AMD,o40m5jl,"A few months ago, the old Raptor Lake CPUs' prices were also hiked up by Intel.",hardware,2026-02-07 02:47:39,3
AMD,o3xgp5p,This is wrong. A shortage in CPUs will absolutely be coming.  The reason is because AI agents use tools and tools run on CPUs. Most of the software that AI agents build will be deployed on traditional CPU servers.,hardware,2026-02-06 16:49:38,1
AMD,o3wh7oh,The reason why CPUs are experiencing a surge in demand is because writing software has become quicker. It's a knock on effect of coding agents.,hardware,2026-02-06 13:54:50,-2
AMD,o3xn9jy,Nova Lake uses N2 which us fully booked as well.,hardware,2026-02-06 17:20:48,5
AMD,o3y8dib,"Aah crap...  At this rate I won't be surprised if service providers (from ISPs to Spotify/Netflix) start jacking up *their* prices, just passing on their own increased costs to customers...",hardware,2026-02-06 19:01:02,6
AMD,o400hlg,And that's just citing raw material costs.  Gonna be another bump from analog semi suppliers increasing prices as they also enter an upcycle.,hardware,2026-02-07 00:35:11,1
AMD,o3z5hmi,"Ai pcs dont need a good cpu right? Cuz the llm is ran inside the gpu? Like, its not like games where u need both a good cpu and gpu even tho the gpu does most of the work, right? Or am i wrong? I dont know honestly",hardware,2026-02-06 21:45:04,2
AMD,o3zdiep,"Intel constantly pushes their nodes forward, so as time progresses the older nodes have less and less capacity as the newer ones take over. they aren't like TSMC where they have lines dedicated for older nodes, that make up the vast majority of their volume.",hardware,2026-02-06 22:25:42,1
AMD,o3yj4u9,"It's 1000% coming.  They'll use the excuse for sure, just like every company used covid supply chain as an excuse, and prices never went back down.",hardware,2026-02-06 19:53:39,5
AMD,o40mam8,Electric utility companies have already passed the AI datacenter costs onto residential users. My electric bill is about 30% higher per kW than last year.,hardware,2026-02-07 02:48:32,3
AMD,o3zew87,"The R&D fabs in Oregon turn over a lot but not the production fabs. Intel isn't building any new Intel 7 capacity, but Intel 3 and Intel 18A are both produced in brand new fabs not converted Intel 7 fabs.",hardware,2026-02-06 22:32:58,8
AMD,o44etk9,"But there's also been [quite a lot of inflation in the last five years](https://www.bls.gov/data/inflation_calculator.htm), accounting for prices not going down.",hardware,2026-02-07 18:34:59,1
AMD,o3xqg8a,"[https://llvm.org/docs/AMDGPUUsage.html#processors](https://llvm.org/docs/AMDGPUUsage.html#processors)  So far adds to the list of gfx targets yet to be released.   \-RDNA3.5-   gfx1153 - 'APU'   \-""RDNA4""\*-   gfx1250 - 'APU'   gfx1251 - 'APU'   \*(IIRC 125x is DCAI, CDNA6 possibly, from Kepler)   \-RDNA5-   gfx1310 - 'dGPU'",hardware,2026-02-06 17:36:05,13
AMD,o3x7nj7,"Interesting that they explicitly call it RDNA 4m instead of some 3.x.   Could be marketing, or it could be that while being RDNA3-derived it has some more substantial changes compared to 3.5. At least FP8 support seems to be there.",hardware,2026-02-06 16:07:27,12
AMD,o3zny96,"Generational ragebait if they launch new RDNA3 stuff with hardware to run fsr4, while the rest of RDNA3 cant",hardware,2026-02-06 23:22:43,4
AMD,o3x96of,Could be just rdna3 iGPUs on newer nodes,hardware,2026-02-06 16:14:44,14
AMD,o3xhk0v,Exynos 2600?  m for mobile?,hardware,2026-02-06 16:53:38,5
AMD,o3xbup7,Probably Medusa Point.,hardware,2026-02-06 16:27:11,12
AMD,o3xck91,Ehh doesn't sound likely since they are adding new instructions,hardware,2026-02-06 16:30:26,4
AMD,o3xqurt,Was there a rdna3m for exynos 2500 and 2400? If no then this being e2600 is unlikely,hardware,2026-02-06 17:38:04,5
AMD,o3xjqhs,>There is also now a merge request adding new FP8/BF8 conversion instructions for the GFX1170 target.  Perhaps they're taking steps to make sure that Medusa Point supports FSR4?,hardware,2026-02-06 17:03:54,4
AMD,o3xnup1,Rdna 3.75,hardware,2026-02-06 17:23:37,7
AMD,o40pols,"Ever since Exynos 2400 Samsung been using mRDNA 3. Exynos 2500, 1580 and 1680 use mRDNA 3.5",hardware,2026-02-07 03:09:52,3
AMD,o3xnhwd,Hopefully it will apply to all rdna3.5 iGPs.,hardware,2026-02-06 17:21:54,1
AMD,o3xnzur,"Well amd is calling it rdna 4m, wonder what's that about",hardware,2026-02-06 17:24:19,2
AMD,o3yk63e,"Unfortunately if they wanted to support those they would have already released it. And I mean officially, not through a leak.",hardware,2026-02-06 19:58:43,6
AMD,o40pizu,No. It's supposed to be Its own branch from 3.5. It was known before as RDNA 3.5+,hardware,2026-02-07 03:08:53,4
AMD,o3xpgy3,An in idealistic world it would be rdna4 iGPs and mobile dGPUs.   We know it ain’t.,hardware,2026-02-06 17:31:20,2
AMD,o408od5,Yup. Not even supporting strix halo is hilarious,hardware,2026-02-07 01:24:10,7
AMD,o3xpx0q,Normally but it would be gfx12 and not gfx11,hardware,2026-02-06 17:33:30,3
AMD,o3xrgzj,Because it clearly ain’t a new arch.   It’s most definitely just rdna3.5+++.,hardware,2026-02-06 17:41:01,10
AMD,o3xryep,Anything but using rdna4😅,hardware,2026-02-06 17:43:21,4
AMD,o48bm40,"The big feature people want is FSR4, and this seems to add the minimal support for features used for it.  Basically RDNA3 + FSR4 I think.",hardware,2026-02-08 10:15:04,2
AMD,o27u1dp,TLDW:     14 game average:    9850X3D was:    - 5% faster than the 9800X3D     - 15% faster than the 7800X3D when CPU limited    - 28% faster than the 14900K    - 34% faster than the 9700X   - 43% faster than the 5800X3D,hardware,2026-01-28 14:15:19,83
AMD,o27vjbp,5% faster for 20+% more powerusage - yay,hardware,2026-01-28 14:22:59,64
AMD,o27vuhp,Hardware Canucks ripped AMD a new one lol.,hardware,2026-01-28 14:24:32,25
AMD,o29urll,"Not related to the 9850X3D, but I am extremely curious what has happened with the Battlefield 6 results because there are massive differences compared to the ""33 CPU benchmark"" video released a couple of months ago.  The 14900K now sits just below the 7800X3D, with higher 1% lows even, while in the 33 CPU test it was reported to be worse than the 9700X. 14900K's average FPS is 26% higher compared to in the 33 CPU test video, and 1% lows are 10% higher. The 14600K and 285K show similar improvements.  The 9700X also shows good improvement in average FPS, but 1% lows actually decreased.   The 5800X3D shows huge improvements too - from 124 avg FPS to 158; and 1% lows from 89 to 106.   The 7800X3D now achieved \~10fps higher compared to 9800X3D's result in the 33 CPU test video.   The 9800X3D has also improved, but to a much lesser extent, which is probably partially due to GPU bottleneck at this point.  **Timestamed graphs:**   Battlefield 6: Multiplayer CPU Test, 33 CPU Benchmark: [https://youtu.be/nA72xZmUSzc?si=6NgiSgpCH5l9p4eU&t=313](https://youtu.be/nA72xZmUSzc?si=6NgiSgpCH5l9p4eU&t=313)  AMD Ryzen 7 9850X3D Review & Benchmarks vs. 9800X3D, 7800X3D, 285K, 14900K: [https://youtu.be/d2hGLaQQpUk?si=RxV13wBgD0Dg609j&t=436](https://youtu.be/d2hGLaQQpUk?si=RxV13wBgD0Dg609j&t=436)",hardware,2026-01-28 19:38:23,3
AMD,o27voob,Power consumption not looking good in pcgh review.,hardware,2026-01-28 14:23:44,12
AMD,o28k5bt,"Draws way more power and with increased thermals so you can barely see a difference playing at 1080p medium settings on a freaking 5090 ???? SUPER PASS, this is just an overclock, lmao.",hardware,2026-01-28 16:16:15,8
AMD,o27vxx9,"So basically the fastest ZEN 5 gaming CPU is now 15% faster than the fastest ZEN 4 gaming CPU. That's not terrible.  EDIT: Guys I'm just talking about the improvement in performance gen over gen. I didn't once mentioned the price or if it is good or bad value for the money. I'm just comparing tech, not deals and 15% per gen seems solid to me.",hardware,2026-01-28 14:25:00,3
AMD,o2l3t1c,7800x3d needs to drop in price,hardware,2026-01-30 11:36:43,1
AMD,o325a8f,"I bought an AMD 7900X3D on a dip a long time ago, it seems",hardware,2026-02-01 23:15:33,1
AMD,o2a6k9b,Apple when they have the fastest chip = 20%+ year on year improvements   Amd when they have the fastest chip = sit around scarching their ballsacks and then smelling their hand,hardware,2026-01-28 20:31:01,0
AMD,o2afid5,Man 7800x3d was such a good GPU to launch. Been 3 years and still identical 1% lows and average FPS at 1440p and above.,hardware,2026-01-28 21:10:19,-1
AMD,o27sqtp,What's the tl;dw?  A 27 minute video is pointless.  *edit* nvm TPU has a nice text review.,hardware,2026-01-28 14:08:42,-27
AMD,o27x242,"Watching this in my recent upgrade $280 R7 7800X3D feels kinda pretty good tbh, especially comparing to how a lot more these newer 9800X3D - 9850X3D costs compared to what i got my 7800X3D for.  We are reaching the point of AMD's Skylake era stagnation where they only release CPUs that offers very little performance uplift over last generation for a lot more power consumption and costs, which is also another reason led me to choose the 7800X3D as my upgrade path from my previous R5 7600.  I think i will be staying on this beast of a gaming cpu until at the least Zen 6 3D or tbh even until AM6 at this point, and i think everyone else with similar gaming performance should as well.",hardware,2026-01-28 14:30:36,-7
AMD,o28cooc,"I think the real TL:DR is that you get about 4-5% higher clocks for 25% more power consumption and 10 degrees hotter temperatures than the 9800X3D.  At least when being fully pushed.   We already knew what the baseline was gonna be here, just a matter of what small incremental improvements you got for it, and what it costs you to do so.",hardware,2026-01-28 15:44:04,61
AMD,o283usa,"Honestly, I'm pretty happy (as a 5800X3D owner) that what appears to be the best gaming CPU on the market is only 40% faster than my 5800X3D. A 10% gain annually for four years isn't *insanely* huge and reassures me I can be quite happy with my CPU for another few years or more.",hardware,2026-01-28 15:03:47,18
AMD,o3qoty2,"If someone already owns a 9800x3d, I would think it would be foolish to purchase a 9850x3d. However, I'm upgrading from a 9700x that I purchased back when 9800x3d was hard to find, and I am definitely dropping the extra $20 for the extra performance of the 9850x3d. A bit more power and heat? Easily solved with a bit better cooling paste and a bit better air cooler. Nothing a Thermalright Royal Pretor air cooler and Thermal Grizzly Duronaut paste can't handle.",hardware,2026-02-05 16:31:30,2
AMD,o281h70,i kinda wish they would show off vs older cpus with these tests. like yeah knowing how well a 9850x3d performs vs a 5800x3d is cool but its only a ~4 year old cpu. show us how well it would run vs something like a 9900k or a 3800xt,hardware,2026-01-28 14:52:27,1
AMD,o2d6xu0,">43% faster than the 5800X3D  Well, that also means it is also 40%+ faster than my laptop CPU 8745HX which is just a Ryzen 7700 non-X that is as fast as 5800X3D.",hardware,2026-01-29 06:20:45,0
AMD,o27w5jk,"Death, taxes, and the V/F curve",hardware,2026-01-28 14:26:04,51
AMD,o27w72w,Interesting to see if they still randomly die. Could be more often with that power usage.,hardware,2026-01-28 14:26:17,16
AMD,o27wiam,"TPU has it at equal power usage in synthetic tests, and 7% more in gaming. Haven't seen HUB's yet, but that's a massive difference between reviews.",hardware,2026-01-28 14:27:52,10
AMD,o27yzlv,Ain't called Zen 5% for nothing.,hardware,2026-01-28 14:40:18,13
AMD,o27yqp1,"Truly an Intel Kaby Lake moment right there lol. I also don't like how the Ryzen 5's core counts has been the same for nearly a decade at this point, I really hope that Zen 6 will be different this time around.",hardware,2026-01-28 14:39:06,4
AMD,o28oo9j,That's how it always is for high end hardware. I'd say AMD won this one,hardware,2026-01-28 16:35:51,-1
AMD,o287xz4,"This isnt Intel, i think its at most 10-20W more",hardware,2026-01-28 15:22:48,-7
AMD,o2agmwm,They made a video recently about how finally win11 is consistently faster than win10 at gaming. Maybe that´s it.,hardware,2026-01-28 21:15:19,2
AMD,o2a5zrn,"Yeah, it's an overclock with a warranty. The decision tree is pretty simple:  - If you're upgrading from a previous generation to this one, you might as well go for this part since it's a marginal cost increase for a marginal performance increase over the other parts in the generation. - If you're already on on a current generation, there's no reason to get it.",hardware,2026-01-28 20:28:29,3
AMD,o27zt61,"It's the same situation as Lovelace / Blackwell, which everyone said was shit  More performance at the cost of more power with a higher price",hardware,2026-01-28 14:44:20,7
AMD,o27xz6w,"It's downright terrible value if you compare the price between them though, you can get 7800X3D at around $300 on sale for a while now, just recently got mine brand new at $280 compared to this which likely will retail at $500+ it's a terrible value gaming cpu aimed to squeezing more money from gamers who only wants the best gaming cpu but doesn't care about money / value.",hardware,2026-01-28 14:35:15,1
AMD,o28g8xk,It's still frustrating that Zen 5 seems to \*need\* a lot more than 32MB of L3 to see bigger gains over Zen 4.  At least for gaming.  Basically makes all the Zen 5 lineup except the Vcache parts kind of pointless unless you can get them for roughly the same price as a Zen 4 equivalent.  Hoping Zen 6 ups the L3 as standard and/or does other optimizations to help bring out the potential of Zen 5's wider architecture without needing to buy the later and more expensive Vcache variants.,hardware,2026-01-28 15:59:28,1
AMD,o27x9d4,"Yeah, but it's a huge premium. Like it's 80% more for 15% gains. Not good.",hardware,2026-01-28 14:31:37,-6
AMD,o27wwcj,The 7800X3D is to these what the 5700X3D was to the 5800X3D. Zen 5 is a pretty flawed architecture because it's gains are limited by some bottlenecks. Zen 6 should alleviate it and addore cores.,hardware,2026-01-28 14:29:48,2
AMD,o27wzed,"I’m unironically shopping for a 14700kf because I have DDR4 and an old LGA1700 mobo laying around  Massive improvement in ST, MT, and about 10% faster in games over my 5800x3d… which doesn’t matter anyway as I play at 4k",hardware,2026-01-28 14:30:13,4
AMD,o27u02y,Do commenters ever not whine?  There are bar graphs you can pause and it's a picture no longer a 27 min video.  You can also just watch the conclusion and that's like a 3 minute video.,hardware,2026-01-28 14:15:08,26
AMD,o2djrhp,Such nonsense. It has been 1 release and despite the uplift being relatively low in games compared to other zen releases the 9800X3D is still double digits faster than the 7800X3D. Something that did not happen in the Intel Stagnation Era which went from Sandy Bridge through to Rocket Lake. 11 gens of upgrades.  This is 1 release and a mid cycle refresh because they are getting enough top bins to make it worth selling a marginally higher tier part for a marginally higher price. It is effectively a KS variant but without the huge cost increase.  So saying 1 gen and a KS bin equates to the stagnation era is just absurd.,hardware,2026-01-29 08:10:54,5
AMD,o27zrs6,"Right, but 7800X3D was  450$ on release, every new CPU is going to be terrible value compared to older generations, thankfully AMD isn't Intel so both are on AM5. Prediction time: 10800X3D on release is also going to be bad value compared to 9800X3D",hardware,2026-01-28 14:44:09,12
AMD,o280aim,"So, AMD releasing a binned CPU for a bit more.... is stagnation.   Meanwhile Intel with the KS series or now the Core 300S aka 200k OC edition, charging 150-300 bucks more per CPU....is innovation in your eyes? Tells me enough lol",hardware,2026-01-28 14:46:42,-3
AMD,o28rlzz,"25% more power for 5% more performance looks like a mediocre overclock. Hardware Canucks' review shows how much hotter the 9850X3D is at gaming for barely any performance increase, including the aforementioned power draw increase.  I guess a lot of early 9800X3D buyers might have a sample that can be overclocked to achieve the same or better performance than the 9850X3D, during the launch window they were not binning any CPU whatsoever so there should be a lot of silicon lottery winners out there.",hardware,2026-01-28 16:48:30,24
AMD,o28on7y,"I bought a 5700X3D from Aliexpress for $131 mid-2024. Its a shame, the lowest i can do now is $325",hardware,2026-01-28 16:35:44,7
AMD,o28daal,"CPUs have historically been a ""buy one and pair with 2-3 GPUs"", with how small the improvements right now it still is true I think. Like if you bought one of the legendary CPUs of its time 4770k,  you could've paired it with a GTX 780, 980, then an RTX 2080 without much problem. That's also what I plan with my 5700X3D, first with a 6700 XT, now with an RX 9070, probably next GPU because of the prices is either 8000-series Nvidia or three gens from now AMD.",hardware,2026-01-28 15:46:42,17
AMD,o29bfpd,"Yeah I’m hoping to hold onto my 5800X3D until AM6. I know that’s still quite a few years away, but getting in on the start of AM6 and having so many more years of potential upgrades is a lot more appealing than upgrading to one of the last gens on AM5.",hardware,2026-01-28 18:14:16,4
AMD,o28eibq,"I get it, but you should also be able to extrapolate that information from seeing comparisons of say a 5800X3D vs a 9900k in 5800X3D reviews, which is probably easier information to find.",hardware,2026-01-28 15:52:00,6
AMD,o28hel9,"After a quick look, the 5800x3d was (is) about 20% faster than the 3800xt for single threaded benchmarks, so you could do 20% times 43% (1.2 * 1.43) and reasonably expect the 9850x3d to be 71.6% faster than the 3800xt.  Sure, measuring things directly would be better, but you get a general idea this way.",hardware,2026-01-28 16:04:26,3
AMD,o285cet,"It's a \~0.65% failure rate based on the biggest retailers with RMA data, hardly novel, and only a bit above the 7800X3D.",hardware,2026-01-28 15:10:49,1
AMD,o285pnx,"TPU also has just 1-2% more performance on the 9850X3D, could HUB be using a mobo that has more aggressive PBO ?",hardware,2026-01-28 15:12:33,4
AMD,o28fecg,"Derbauer's results are in line with HUB's generally.    TPU can be great for GPU testing, but they are not great at CPU testing.  If the CPU is not being fully utilized and you're regularly GPU limited, you will probably not see much if any difference in power consumption on average.  They \*are\* the same CPU at the end of the day, after all.",hardware,2026-01-28 15:55:49,1
AMD,o285t5f,"Well, it's Zen 15% now.",hardware,2026-01-28 15:13:00,8
AMD,o2830se,"Even though IDGAF about the V/F curve, the only thing keeping me from putting my 9800x3d in a box to put the 9850x3d into service are the rumors that Zen 6 will be > 8 cores per ccd.   Best part of holding off for that is that if that doesn't pan out I won't be bothered to make a change until the next platform.  If it does I'll make the switch the (x?)x950x3d variant for general compute purposes more than anything (folding @ home mostly).  Either way I'll feel like a winner.   I am a winner, right?",hardware,2026-01-28 14:59:49,4
AMD,o288owz,atleast watch the videos,hardware,2026-01-28 15:26:13,7
AMD,o282juf,Simply not true.  HUB: 4% increase  LTT: 4% increase  Level1Tech: 7% increase  DerBauer: 4% increase  GamersNexus: 4% increase   I've seen your name on here so many times. Do you ever get tired of spreading misinformation?,hardware,2026-01-28 14:57:36,46
AMD,o2815gy,"LTT was also ""pleased"" with the results",hardware,2026-01-28 14:50:52,3
AMD,o2bcqkn,"What about the decision for me, who’s building their first pc $ has not bought their cpu yet? I just bought an ASUS 5080 16gb TUF Gaming with Corsair 6000mhz 32gb ddr5 ram. Stuck between the 9800X3D or the 9850X3D",hardware,2026-01-28 23:46:26,1
AMD,o2a6jff,"I would prefer a cheaper, cooler and more efficient 9800X3D with the same performance at 1440p ultra as the 9850X3D.",hardware,2026-01-28 20:30:55,1
AMD,o27ywy4,I got a 7800X3D in AliExpress 11.11 sales for €232.78,hardware,2026-01-28 14:39:57,4
AMD,o281rzh,">it's a terrible value gaming cpu aimed to squeezing more money from gamers who only wants the best gaming cpu but doesn't care about money / value.  That's true of almost every high-end product. Value is subjective. I don't particularly care about the difference between $300 and $500 on a CPU for a build I'll use for 2-3 years. There's a pretty broad spectrum of price/performance options right now, though, so no one should feel squeezed into paying up for the best, since it's a pretty marginal improvement.  That said, I just bought a 9800X3D and don't regret not waiting a few weeks for the 9850X3D.",hardware,2026-01-28 14:53:52,4
AMD,o2a6khh,"Zen has been pretty heavily limited by the performance of the memory controller on the consumer side for a while now, which is why having a large cache really helps.  If Zen 6 brings a new and improved one, I'd expect the gap between the X3D and the regular parts to drop as the masking effect of the large cache becomes less critical.",hardware,2026-01-28 20:31:02,3
AMD,o27ytpg,>80% more   Wtf are you talking about?,hardware,2026-01-28 14:39:30,12
AMD,o27yxde,"At least in Amazon, I'm seeing the 7800X3D at around $380 to $400. Nowhere near 80% higher price.",hardware,2026-01-28 14:40:00,9
AMD,o27uxcm,"It does suck that so much authoritative content is moving to youtube. Even with an adblocker, I ain’t gonna watch a half hour video when I can skim an article in a minute or less for the info I want.  I miss Anandtech, but I understand why that business model has (mostly) died",hardware,2026-01-28 14:19:54,1
AMD,o2d9dep,I hope they always whine when its a video. Video is the worst format for this.,hardware,2026-01-29 06:40:40,1
AMD,o27uxay,Not WHINE? ON REDDIT? GTFO,hardware,2026-01-28 14:19:54,1
AMD,o27v7pk,"I just feel sad for the reviewers.  If they had access to the (China-exclusive) [DDR5+Hyper 612 bundle](https://www.techpowerup.com/345648/unusual-collaborative-ryzen-7-9850x3d-ddr5-kit-cooler-bundle-turns-up-in-china), that would've made for interesting content.  This is just an OC'd 9800X3D.  At least the 9950X3D2 is something cool (3D V-cache on both CCDs).",hardware,2026-01-28 14:21:21,-7
AMD,o28abos,"10800x3d is going to be way faster, at least 25%",hardware,2026-01-28 15:33:33,1
AMD,o280o83,">Meanwhile Intel with the KS series or now the Core 300S aka 200k OC edition, charging 150-300 bucks more per CPU....is innovation in your eyes? Tells me enough lol  I think you missed the point of my comment, in no way i was praising Intel with their history of stagnation in my comment lol, i was criticizing AMD for doing the same thing as Intel did nearly a decade ago back with Skylake - Kaby Lake, because Zen 5 certainly feels like it over Zen 4 which i felt was a decent uplift over Zen 3 and same can be said with Zen 3 over Zen 2 and then prior.  The point is AMD felt innovative with respective performance jump prior Zen 1 - Zen 2 - Zen 3 up until Zen 4 and then Zen 5% comes out and suddenly all of that track record of theirs is broken even the X3D version which everyone liked was not a worthy uplift enough to upgrade from the likes of 7800X3D, and same can be said with 9850X3D compared to 9800X3D - 7800X3D.",hardware,2026-01-28 14:48:33,1
AMD,o2d7orj,Its just the better bined died being sold as a different SKU with a pre-overclock. No wonder it looks exactly like what it is.,hardware,2026-01-29 06:26:47,6
AMD,o3ft21l,"i just bought a 9800x3d for 330 at MC, before prices jumped a month or so ago..  multi core: 23,800 single core:2170 .. -30 offset, +200 mhz, 5.45 ghz, 5.3 all core  for 9850x it is basically the same   multi core: 23,850 single core: 2250 .. runs slightly higher and at 5.6-5.65ghz  but it cost 500, so i saved 170 with about 2-3% less power but same performance real world  my brother got a 7800x3d for 250 the same day, only downside of that chip is that its locked with no OC option, even though he has great cooling, 5.05ghz max. BUT we undervolted -20 offset with 80c max temp, so its more than enough for his needs, 2k 240 + fps in apex, marvel, overwatch, etc  9850x is only worth it at about $400, 9800x3d at $350, 7800x3d at $300.",hardware,2026-02-03 23:59:40,1
AMD,o2905hj,"I was so mad i missed this boat. They were $200 in February on Ali when i was shopping. Then suddenly $250 a few days later. Settled on a 5800xt for $125, and even thats price hiked now.",hardware,2026-01-28 17:25:56,3
AMD,o2d7ti0,It depends on what you use it for. My 4560k was obsolete long before i got my 1070.,hardware,2026-01-29 06:27:52,2
AMD,o29k2o5,"That was true in the past, but lately GPUs have improved significantly faster than CPUs, the improvement from 4770k to 9850X3D is much smaller than from GTX 780 to RTX 5090. I don't think you will upgrade to new GPU that late, by that point CPU will be slow, 9070 with 5700X3D is pretty balanced setup anyway.",hardware,2026-01-28 18:51:04,2
AMD,o2f1rua,"My idea exactly, going from AM4 to 6 will hopefully be a massive jump.",hardware,2026-01-29 14:41:38,1
AMD,o29k350,"For synthetic benchmarks, but I'm pretty sure it's much more in games.    5000 non-3D vs 3000 was already 15% in games, no? So I'd imagine the X3D pushes that beyond 30% uplift.    Edit: quick check, Computerbase's 5800X3D review puts it at geomean 53% ahead of 3800XT in games...",hardware,2026-01-28 18:51:07,3
AMD,o28tbh2,"That's about the same failure rate as the 12VHPWR connector, funny how the response is so different.",hardware,2026-01-28 16:55:56,18
AMD,o28cfaf,"Overall it looks just like really an overclocked 9800X3D, with some tests I see that limit to 5.2Ghz being actually identical to 9800X3Ds lmao. AMD just overclocked 9800X3Ds for $30, presumably being more stable than a manual overclock. It's up to buyers if they consider that worth the $30 more.",hardware,2026-01-28 15:42:56,7
AMD,o2d8a5l,Zen 5% x3(D).,hardware,2026-01-29 06:31:38,3
AMD,o2896mu,"Just did, HU and GN reported like 20W, as i thought",hardware,2026-01-28 15:28:27,-5
AMD,o28808b,anyone did comparison with OCed 9800?,hardware,2026-01-28 15:23:06,1
AMD,o28j044,"Hey DwarfPaladin84, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-28 16:11:20,1
AMD,o2budrt,"Look at your total system cost, the relative cost of the CPU, and the net impact on performance.  You're looking at a total system cost of... mmm... call it about $3000 for the whole rig. Let's say you're paying $20 more for the 9850X3D than the 9800X3D. That's a 0.7% increase in cost for a ~4% increase in performance, _and_ with that GPU you'll actually be CPU bound enough of the time to make it worth considering.  I would say the 9850X3D is worth it for your rig.  However, it's not a universal truth - if you're running a much older GPU, or have a lower system budget, then something like the 7800X3D makes more sense. On the flip side, if you're playing simulation games like Stellaris or Factorio, then it might be worth the higher end CPU even if you're running older GPUs or lower cost hardware.",hardware,2026-01-29 01:19:33,2
AMD,o2bq4nh,"I'm holding out till tomorrow when it releases. Figure why not, but I'm either going this or 9800x3d",hardware,2026-01-29 00:56:12,1
AMD,o2d8ql1,"undervolting is always an option. wrangling modern boost not to actually boost above what you want is a bit harder. Modern chips think ""its running low temperature? lets boost something insane to make sure its running high temperature""",hardware,2026-01-29 06:35:24,2
AMD,o27zrj7,"Yep, i have noticed the 7800X3D is getting a lot cheaper these days, especially on sale, that even i can't ignore. it's an absolute no brainer choice over the likes of 9800X3D - 9850X3D which offers very little performance uplift for significantly more cost.",hardware,2026-01-28 14:44:07,1
AMD,o2d7fh6,AMD really need to put a larger L4 cache on the I/O die.,hardware,2026-01-29 06:24:42,1
AMD,o2ifkpu,There's also the potential that Zen 6 may be like the Strix Halo APUs and the higher end RDNA3 parts in that they use Infinity Link (Fanout with 1:1 connections between the chiplets) vs Infinity Fabric (serializes/deserializers which narrow down the communication pathways between chiplets to allow communication over fewer traces).  That would help the communication bottleneck.,hardware,2026-01-30 00:19:43,1
AMD,o28gqso,Prices can vary a lot by region.,hardware,2026-01-28 16:01:35,0
AMD,o2800zj,"At least on asian market, you can get a tray type version of 7800X3D at less than $300 even cheaper than that if you have coupons.",hardware,2026-01-28 14:45:23,0
AMD,o27w77v,The same way you skim through text you can also skip 27 minutes of the video to just check the final graphs.,hardware,2026-01-28 14:26:18,7
AMD,o28f89n,"Sure if the rumours about them increasing the cache even further are true, but its not going to be a good deal, because by that point 9800x3d (and 9850x3d) is going to be heavily discounted and 10800x3d is going to be atleast 600$ if recent price increases in HW are anything to go by.",hardware,2026-01-28 15:55:06,0
AMD,o297kvo,"Zen 5 isnt a stagnation per se, nor is the Core stagnant itself.   If it truly were Intels Sky/Kabylake era... A Ryzen 5 wouldnt have been around 10-20% faster in MT/ST every fucking Gen except Zen 5 since that was only a major core and arch overhaul that should have been delayed a year for the overhauled IO Die that comes with Zen 6. Would even say it makes sense its a stagnation in performance in gaming and workloads that DONT use AVX512 instructions.   Stuff that uses AVX512 instructions is a major uplift Zen 4 vs 5 since they broadened the pipelines, as well as doing a true AVX512 implementation.    You could also say, why didnt they use Zen 4/5 Dense instead of the normal Zen Core structure, since it iant missing anything the Big Zen has...  I can tell you, because the performance uplift came from a combination of frequency and architecture itself.  Zen Dense clocks alot lower in some cases, which sure, gives you more cores, but in turn ruins the gains the Arch ITSELF has to offer, which scales with frequency to a certain point.   Then there is also the issue with Zen 5 lacking internal bandwidth and external (DDR5) in 1:1 mode. Which shows up, since a 9950X3D despite lower max boost clock, is faster than the non 3D. Same will be with the 9950X3D2, will be faster, since both CCDs will have access to a big chunk of extremely fast and low latency brick of storage.  Zen 6 will definitely be a massive change, since it comes with a 12 Core CCX, the reworked IO die for alot higher memory speed aka more bandwidth from the get go and a refreshed Arch which takes the groundwork of Zen 5 and iterates on it.   Zen 5 is a stopgap and rework of the old Zen 2/3 Arch.  Its basically what Zen/Zen+ to Zen 2 was. A major groundwork rework, but nothing spectacular for performance.   Imo, calling AMD stagnant like Kaby Lake-Skylake Era is not fair. Especially looking at all the facts, Kaby Lake to Skylake you were lucky to get single digit improvements evey Gen. Which is only true for Zen 5 and only when forgetting that Zen 5 is also used in stuff more than just gaming.",hardware,2026-01-28 17:57:51,1
AMD,o2ykp6x,"Ali has lowkey become a decent place to get cheap parts. There are still “fake” parts on there, but if you get a reputable seller who ships from a local warehouse (for easy returns) you avoid all of the issues.",hardware,2026-02-01 12:29:39,1
AMD,o2b7adq,"If the 58003d was 53% faster then that would make the 9850x3d over 118% faster (2.18x) than the 3800xt, ""in theory"".",hardware,2026-01-28 23:18:04,2
AMD,o2dn06i,i'll give you two guesses on that one,hardware,2026-01-29 08:41:13,2
AMD,o28ucbn,We got lulled into a good decade of basically zero failure rates with CPUs that probably make them having a low failure rate more prominent than components that do fail regularly.,hardware,2026-01-28 17:00:20,3
AMD,o2d83ej,"I agree with the premise of your comment. However, devils advocate. We dont really know why AMD CPUs are failing, but we know that the connector is failing because its just a bad design.",hardware,2026-01-29 06:30:06,-1
AMD,o295p94,"Does limiting the 9850X3D to 5.2 GHz improve power efficiency compared to a stock 9800X3D? If not, then it’s basically just a factory overclocked 9800X3D.",hardware,2026-01-28 17:49:53,5
AMD,o28l867,The fact that it's only 30 bucks for that is surprising/nice at least,hardware,2026-01-28 16:20:54,3
AMD,o28jtk9,"yea, thats 20% more for 4-5%",hardware,2026-01-28 16:14:52,9
AMD,o289mpw,Not in any of the ones i listed.   But GN noted that you could quite easily OC the 9800X3D reliably to 5.4ghz so it'd be even less of a performance increase between the two.,hardware,2026-01-28 15:30:25,9
AMD,o2853sz,>Everyone but HuB had poor results  I've just given you 4 extra reviewers that had the exact same or higher.. so this comment was bullshit.,hardware,2026-01-28 15:09:42,31
AMD,o28e2sz,"More honest assessment: HUB was exactly in line with every single other one, except one whose results were actually higher.    You are proving you are out to bash HUB with completely dishonest intentions.  It could not be more clear at this point.    Basically, you are not here in this sub in good faith in general.",hardware,2026-01-28 15:50:08,13
AMD,o2a2b3u,User benchmark fan confirmed,hardware,2026-01-28 20:12:07,4
AMD,o28jzv5,"Obviously, but the implication is that a 7800X3D is $277 when that isn't remotely true even on the used market.",hardware,2026-01-28 16:15:37,1
AMD,o2bg44e,"Just curious about these tray CPUs, is there any issue with warranty due to being grey market?",hardware,2026-01-29 00:04:04,2
AMD,o293u10,So buy that,hardware,2026-01-28 17:41:54,1
AMD,o27y7om,"There’s a huge difference between skimming an article or paper vs randomly seeking thru a video, hoping you find something useful, while still having to listen to people talk at 100 WPM when you can read at 600 WPM",hardware,2026-01-28 14:36:27,2
AMD,o28fpaq,"It’s not the cache, they are rumoured to support way faster memory and have crazy high clock speeds. If the 9800X3D could do that then the gains would be massive, these CPU’s are extremely gimped by the IO die.",hardware,2026-01-28 15:57:08,3
AMD,o2ehaon,You missed the point and are showing your bias. Is it bad design if it’s within typical hardware failure rate?,hardware,2026-01-29 12:50:21,2
AMD,o28jzz1,"gotcha, thanks. Weird that noone did test with OC'd 9800, but considering abysmal level of Tech Bloggers on youtube - not surprising",hardware,2026-01-28 16:15:38,-5
AMD,o286w3v,The account is anti-AMD to the extreme. Look at their history.,hardware,2026-01-28 15:17:59,17
AMD,o298mzk,As reported by another user that very much can be the case. In my country there's typically a 50% or more gap between a 78 and 9800X3D. A 20 USD gap at that point would be fairly big and it will actually be a bigger gap due to import taxes on products and the fact that I don't believe there's official AMD sellers here.,hardware,2026-01-28 18:02:20,-1
AMD,o2cdsu9,"They still come with warranty but shorter than the retail version only 1 year instead of 2 - 3 years that comes with retail, but with my history of buying tray type cpus i never had any problems with them whatsoever, they feel like retail version and even comes with their own coolers depending on certain model of cpus.",hardware,2026-01-29 03:05:58,1
AMD,o2d97vh,if its a legitimate retailer you get warranty through retailer as required by law (lenght varies by country). If its some ali express shell company then no warranty.,hardware,2026-01-29 06:39:24,1
AMD,o296o9x,Already did at $280 way better value than 9800X3D at $480,hardware,2026-01-28 17:54:02,0
AMD,o28um2y,"I usually just click on the ""X game average"" shortcut in the description, since HUB is good about putting those in their videos.",hardware,2026-01-28 17:01:33,6
AMD,o2kgsfx,"Yes? Failure rate is not a necessary requirement for a design to be bad. Merging all lines at the card end would be bad design even if failure rate was 0, for example.",hardware,2026-01-30 08:13:16,0
AMD,o2d8fh3,"everyone was comparing it to OCed 9800 before the release, and then turns out noone actually tested what they talked about. Typical techfluencers.",hardware,2026-01-29 06:32:52,0
AMD,o2878gn,"I know, that's why i said i'd seen him on here so many times. Often in HWUB threads getting mad about their 7800XT review from 2 and a half years ago.",hardware,2026-01-28 15:19:32,13
AMD,o2d94ei,"I’ve lost all respect when they all forgot about 7600/7700 non X variants, which they tested, when they fell for AMD marketing trick about 9600/9700x efficiency. It was shocking, honestly",hardware,2026-01-29 06:38:36,0
AMD,o3qs2ma,"120,000 wafers per month is massive. Wonder if the AI boom will still be going on by the time this comes on line though.",hardware,2026-02-05 16:46:33,49
AMD,o3vts94,"ELI5, does this do anything to alleviate the DRAM crunch? I figure not but, why?",hardware,2026-02-06 11:21:46,1
AMD,o3ychiu,They are worried about China taking and keeping market share. Love competition.,hardware,2026-02-06 19:21:00,1
AMD,o3smkyz,They can turn HBM lines to LPDDR so it's not an issue,hardware,2026-02-05 21:58:47,28
AMD,o3t0vtw,"Or they can just give those juicy HBMs to us customers, it would do wonders for high-end GPUs",hardware,2026-02-05 23:11:55,20
AMD,o3sptrv,It's most definitely an issue if it leads to overcapacity.,hardware,2026-02-05 22:14:42,1
AMD,o3t6wec,"Would also do wonders for laptop GPUs. AMD and Apple reported something like a 50% efficiency improvement between the 5500M and the 5600M, with the 5600M having HBM memory. Granted, it also had a few more CU, I forget how much.   Also makes it that much easier to get more than a 256Bit bus on a dGPU, which is exceptionally rare if not even never seen at all in laptops (outside of Apple’s SoCs). 256Bit bus of GDDR7 gets something like ~800GB/s of bandwidth, maybe 900 in the “5090 Mobile”, which is almost exactly half of the real 5090 in both bus width and bandwidth. Were it not for them using 3GB chips it would also be half the VRAM capacity. Whereas HBM is up to 1 to 2 TB/s a stack now, and you can easily do 2 or 4 or even 6 stacks of HBM. (I know B200 and AMD equivalent use 8 stacks, but those also have at least 2 compute dies which not even RDNA3 has done yet on consumer GPUs.)  There was also those Intel CPUs with AMD HBM “”iGPU””s as another example, but I know less about them. Would _very much_ love to see HBM more in laptops if the price ever comes down.",hardware,2026-02-05 23:45:35,17
AMD,o3t44x7,the issue of HBM is not the cost of it per GB but the cost of the interposer. Plus high end GPUs are very very niche market,hardware,2026-02-05 23:29:54,5
AMD,o3v7dn9,"You first would need a chip designed for HBM. Then you need consumers to accept that HBM4 is way more expensive than HBM1 used way back in AMD cards, a tough call when they cant accept that TSMC charges way more for 3nm and 2nm than they did 16nm. They can't even accept how surging memory prices affect components that use memory like GPUs",hardware,2026-02-06 07:53:30,2
AMD,o3suq86,If we get overcapacity it will be in 2028,hardware,2026-02-05 22:39:28,4
AMD,o3wi34r,Not an issue for us,hardware,2026-02-06 13:59:31,0
AMD,o3u2bqf,I completely forgot there was a random last hurrah HBM2 AMD GPU in a Mac before they went to Apple silicon.  They couldn’t have sold many of those.,hardware,2026-02-06 02:49:32,8
AMD,o3t634k,AMD used to put HBM in Vega GPUs so surely it's not that infesiable.   And it's better to have niche GPU with HBM than niche GPU with GDDR at least for us customers anyway.,hardware,2026-02-05 23:40:59,16
AMD,o42q564,"HBM is better for a high end laptop GPU, but we don't need HBM for desktop GPUs. GDDR7 is not satured in any way",hardware,2026-02-07 13:20:02,2
AMD,o3htxp2,Q3 vs Q4  |Segment|Q3 2025 Revenue|Q4 2025 Revenue|Seq Change ($)|Seq Change (%)| |:-|:-|:-|:-|:-| |**Data Center**|$4.3B|$5.4B|**+$1.1B**|**+25.6%**| |**Client & Gaming (Total)**|$4.0B|$3.9B|**-$0.1B**|**-2.5%**| |**Client**|$2.8B|$3.1B|**+$0.3B**|**+10.7%**| |**Gaming**|$1.3B|$843M|**-$457M**|**-35.2%**| |**Embedded**|$857M|$950M|**+$93M**|**+10.9%**|,hardware,2026-02-04 07:56:50,19
AMD,o3gt19s,"Lol the usual suspect tryin with the dishonest comment again   1. Revenue is down q1 because client and gaming are down 2. Dc is guided up, both cpu and gpu for q1 3. There's an extra $360m this quarter because they finally dumped the mi308 which was stuck in inventory to china, this accounts for the roughly $300m lower revenue for q1 4. Amd said that dc growth is expected to be >60% in 2026 and 2027 5. Amd not including any potential mi325x figures to china in guidance just to be safe  tldr q1 revenue excluding china ai revenue is about the same or potentially higher even with seasonality decline in client, some users didn't read. **Q4 was guided with $9.6b and q1 at $9.8b**",hardware,2026-02-04 03:21:41,53
AMD,o3fy9bu,"Only embedded segment didn't grow compared to the other segments.  Was that Xilinx that didn't grow, so was that a bad purchase on AMD's part?",hardware,2026-02-04 00:28:13,8
AMD,o3i86co,"Huh, and Radeon just recently had their sales record appearing on Steam",hardware,2026-02-04 10:11:07,4
AMD,o3fuw1k,How on earth do you get revenue to drop next quarter in this AI boom?,hardware,2026-02-04 00:09:47,45
AMD,o3g405c,%50 gaming revenue up at the lowest point of console sales? Rx9000 was good enough even with limited sales.,hardware,2026-02-04 00:59:38,8
AMD,o3mtgmg,AMD will never catch up to Nvidia.,hardware,2026-02-05 00:47:17,0
AMD,o3gc30f,"yeah man, if it's not subsidized it's gonna be a tough sell. hope they don't go too crazy with the price",hardware,2026-02-04 01:45:11,-1
AMD,o3icvgh,Showing their priorities well here. They lost 450 M on gaming and gained 1.1 B on data center.,hardware,2026-02-04 10:53:35,16
AMD,o3h1kzc,Lol he lives on this sub,hardware,2026-02-04 04:14:33,11
AMD,o3g1l7v,No. Not a bad segment. Just in a down cycle. Xilinx is going to provide consistent revenue regardless of the rest of the industry.,hardware,2026-02-04 00:46:25,25
AMD,o3gtn3j,"Embedded products are mostly slow to correct inventory, customers overbought during covid and were left with years of supplies, besides that the npu ip came from xilinx and it's expected to take >70% fpga share in a few yrs.",hardware,2026-02-04 03:25:15,10
AMD,o3howka,I don't think they're making much (if any) money from the Xillinix products themselves but the IP was probably their real target with that purchase.,hardware,2026-02-04 07:11:52,3
AMD,o3gdasl,....Q4 vs Q1?,hardware,2026-02-04 01:52:04,34
AMD,o3fwbgv,Probably why they are down 8% after hours.,hardware,2026-02-04 00:17:38,36
AMD,o3h227c,Gaming and client CPUs are more seasonal. DC was guided up,hardware,2026-02-04 04:17:39,19
AMD,o3ihura,"Seasonal cycle, most likely.",hardware,2026-02-04 11:35:55,5
AMD,o3g5bpt,"AMD is most definitely producing like an order of magnitude or more less GPUs than Nvidia.  hell their gaming GPUs were out of stock for ages and selling out and they just cracked the bare minimums in steam hardware surveys  they literally dont obtain much chip orders from TSMC to begin with. at least for Radeon division that is,",hardware,2026-02-04 01:07:05,13
AMD,o3g3bf5,Probably due to those laptop “AI” CPUs not selling that well,hardware,2026-02-04 00:55:51,10
AMD,o3ggu7s,"Seasonality is big when client is such a big part of their revenue. If they ever manage to really ramp DC, that should be less noticeable.",hardware,2026-02-04 02:12:06,10
AMD,o3iclm5,"Wait, where are you seeing revenue drop?",hardware,2026-02-04 10:51:12,2
AMD,o3gof3c,maybe TSMC's capacity is all bought out and there is no real way to get more allocation from it?,hardware,2026-02-04 02:54:59,2
AMD,o3o3eai,Because they're probably limited by capacity. All companies compete with each other on fab space and memory purchases.,hardware,2026-02-05 05:28:45,1
AMD,o3iocln,"Massive price increases and shortages in RAM and Storage -> Very high prices for products that use them (Laptops, desktops, GPUs, Consoles etc) -> lower sales volumes -> revenue drop.",hardware,2026-02-04 12:24:11,0
AMD,o3g3t9o,"I think thats AMD's way of saying. ""We are earning good enough and still are evaluated at 200 dollars per share. And we dont want to go under when AI bubble pops so lets not get too crazy before openai implodes"" ?",hardware,2026-02-04 00:58:33,-11
AMD,o3gy5ev,"2025 was the PS5's best year for unit sales. It is estimated to have sold 22 million units throughout the year.  Xbox is basically a dodo at this point, but the PS5 is still selling very well.",hardware,2026-02-04 03:52:44,15
AMD,o3gdvy7,9000 series are good.,hardware,2026-02-04 01:55:22,10
AMD,o3ied4a,Not necessarily their own doing. Maybe gaming just had bad quarter. Nvidia has several quarters of growth in gaming & teletubers & most of reddit still believes they dont care about gaming anymore & they're just about to quit it.   Amd's next quarter is probably going to be minimal change due to ram & supply.,hardware,2026-02-04 11:06:38,17
AMD,o3ira7x,Only player who could fumble embedded was intel with altera,hardware,2026-02-04 12:44:03,6
AMD,o3hbk43,lol yeah makes sense. those segments go up and odwn but xilinx is solid long term fr,hardware,2026-02-04 05:23:26,1
AMD,o3go72q,"yeah, kinda crazy considering everyone thought they’d ride the AI wave big time",hardware,2026-02-04 02:53:43,9
AMD,o3gn6yl,lol mayeb they missed some AI opportunities or something? market's wild rn,hardware,2026-02-04 02:48:05,-1
AMD,o3hjsst,"In Norwa, Germany and Poland when 9070XT launched both it and 9070 launched with only 2 models at msrp. They were sold out in matters of seconds and had no stock for 2-3 months. They continued to hold prices at almost 5070Ti (50€ more) retail price for about 3 months after release.   You guessed it, I got a 5070Ti and got my sister a 5070Ti on release of 9070XT.  Thereafter 5070Ti fell in price by 50-100€ and only about 4-5 months after release, 9070XT fell in price and then kept falling and falling until it was 200€ less than 5070Ti and you could get one for 615€ at its best pricing (multiple times in Europe) while cheapest 5070Ti were 770€ for a long time.   Personally I would still pay 150€ more for 5070Ti over 9070XT, it’s just a better card with better software stack and better support.   So AMD again snatched defeat from the jaws of victory. I’m 30 years old, been a PC hobbyist since I was 7-8 (when I first build my PC). I seen AMD fumble it so many times, that it feels like it it’s just their personality trait. I could use both hands to count the times  they fumbled or over hyped stuff in the last decade.",hardware,2026-02-04 06:28:17,2
AMD,o3g7jze,These “AI laptops” from AMD will have dramatically lower ASPs I suspect this year.,hardware,2026-02-04 01:19:39,6
AMD,o3o3h8z,"It's under ""current outlook.""",hardware,2026-02-05 05:29:21,2
AMD,o3jk9n8,Watch Nvidia's number next quarter go up massively despite this.,hardware,2026-02-04 15:20:52,3
AMD,o3g5t09,“Not get too crazy” would be going faster in client instead of just doing rebrands after 1.5 years.   Meanwhile their data centre products are on a yearly cadence. They are absolutely all in on AI. They just aren’t able to sell as much as Nvidia.,hardware,2026-02-04 01:09:47,13
AMD,o3gbcmu,"> And we dont want to go under when AI bubble pops so lets not get too crazy before openai implodes""  The chip companies aren't going to go under. Nvidia's shares literally 10x-ed in value since the start of the AI boom - they can lose half their value to the bubble popping and still be 5 times better off than they started. Plus, Nvidia's cards are still largely general-purpose, they're not AI training ASICs, and can thus be easily repurposed for other types of compute. Trust me, humanity will always find use for spare computing power.  I think the only big *hardware* company that was truly killed by the dotcom crash was Nortel, and even then the crash itself only wounded it, it was the internal scandals and resulting mismanagement that killed them off.",hardware,2026-02-04 01:41:03,9
AMD,o3h4nhn,"SoCs for consoles are probably sold a long time before the consoles are manufactured. the revenue for that stuff should have been accounted for many quarters ago. if theyre still selling more console SoCs now, it would be for future hardware.",hardware,2026-02-04 04:34:59,0
AMD,o3gef0w,But obviously extreme low volume given the paltry figures.,hardware,2026-02-04 01:58:20,13
AMD,o3j7209,"right, they said high sales of Radeon GPUs, the decrease in gaming sales is likely Xbox & PS5 volume",hardware,2026-02-04 14:14:05,7
AMD,o3kz15n,> Not necessarily their own doing. Maybe gaming just had bad quarter.  It is their own doing. An inferior RX 9060XT (8GB) costs the same as an RTX 5060. They have nothing good below $500.,hardware,2026-02-04 19:13:09,-5
AMD,o3iasiv,Amd never misses an opportunity to miss an opportunity,hardware,2026-02-04 10:35:00,1
AMD,o3jvzw4,"Of course revenue will go up for Nvidia! According to their financial statement, 95% of their revenue is generated by AI datacenter hardware and that's what causing the shortages.  45% of AMD's revenue is generated by client and 55% by data center.",hardware,2026-02-04 16:15:21,0
AMD,o3hjmql,It could be because of the imminent (hopefully lol) release of Valve's Steam Machine featuring their 'semi-custom' AMD chip.,hardware,2026-02-04 06:26:52,1
AMD,o3ptrzg,That's not going to be a huge volume product like a console.,hardware,2026-02-05 13:57:48,1
AMD,o285w6u,"This one stands out from the other 9850X3D reviews for testing it with slow DDR5-4800 to see if it makes any difference. It barely does, so you might want to save some money there.",hardware,2026-01-28 15:13:24,45
AMD,o2cd8ew,All those people that complained about GN's other content and said he should just review hardware... Where are you now? Barely any engagement on this video.,hardware,2026-01-29 03:02:45,13
AMD,o28epvp,"HUB also tested with 4800 memory (well, 6000 memory with EXPO disabled), but also threw intel cpus with both memory profiles for comparison. HUB also did RT tests showing Intel getting ahead (but comically couldn't explain it).",hardware,2026-01-28 15:52:54,27
AMD,o2ed71n,"Same outcome as it was found reviewing 5800x3d, bigger cache reduces frequency of  data fetches from RAM and increasing effective bandwidth.",hardware,2026-01-29 12:23:14,4
AMD,o2jfavi,"The HUB review was posted first on this subreddit. This post came 20 minutes later. The first review post gets all the discussion. Nothing overly suspicious here.  Then again, the der8auer post came after this post and got much discussion. Probably because der8auer used a clickbait title, so people discussed the actual efficiency claims.",hardware,2026-01-30 03:37:40,6
AMD,o2cgcqm,Haven’t been reading this sub for a while. Do people actually complain about GN’s other content? It’s been great to see someone exposing the corrupt side of tech. Raising awareness through well researched content can’t possibly be a bad thing.,hardware,2026-01-29 03:20:41,8
AMD,o2cwot9,I guess they had no obligation to give him another chance.,hardware,2026-01-29 05:04:16,5
AMD,o28jizw,Because BVH cache in rt fills the cache easily and the 9800x3d has to do fetches from memory and slow infinity fabric chokes it again,hardware,2026-01-28 16:13:37,20
AMD,o29bij4,"Don’t forget that EXPO does a lot more than XMP does on Intel systems. It doesn’t just overclock the RAM but also the IF which has a huge impact.   You can get like 80-90% of the uplift from EXPO by overclocking the IF manually.   In most reviews that use stock settings for both, AMD almost always underperformed.  It’s just normal that ""reviewers"" Test with overclocked CPUs.",hardware,2026-01-28 18:14:36,3
AMD,o2cjp70,">Do people actually complain about GN’s other content?  yeah, sadly. especially when GN is attacking their favourite brand.",hardware,2026-01-29 03:40:34,13
AMD,o2ea4iw,"The unfortunate reality is that this sub is increasingly plagued with tribal mindsets who refuse to value a piece of content on its own and hold grudges indefinitely.  I love how all aspects of the hardware industry are covered here, but would leave in a heartbeat to get away from the endless price complainer posts and personal gamer vendettas.  Turns out, the other tech subs are either too niche or even more tribal.",hardware,2026-01-29 12:01:39,7
AMD,o2de8zl,"Oof, so it actually does matter, amd just conveniently leaves out RT benchmarks in their launch showcase",hardware,2026-01-29 07:21:55,5
AMD,o28zx6x,"Yeah I know. I mean, I'm nowhere near knowledgeable about the rendering pipelines, but on a high level this stuff can be explained and understood easily enough I think.",hardware,2026-01-28 17:24:56,1
AMD,o29esfc,"i don't think that is true, at least on zen4 or zen5. the EXPO profile doesn't contain FCLK, nor is FCLK linked to MCLK.",hardware,2026-01-28 18:28:35,7
AMD,o2dq0sm,EXPO and XMP have the same behaviour for FCLK. Regardless FCLK is always 2000mhz on zen5 unless manually set.,hardware,2026-01-29 09:09:44,3
AMD,o2i4bnm,"We're quiet because this is finally a review, why would we bash?  Just look at his last 2 months of content, 2 reviews by my count, the others are some sort of hype building or some sort of tours. Look at the view count. Any video where he's bashing a company has 2-3x the views of all the other videos. Do you really think he isn't leaning into that especially when there's so little to review?  Instead of being creative and coming up with cool things to review, he's just leaning into drama.  Compare the last 2 months of videos with 8 months ago. Review, review, review, review, lots of interesting product videos, interesting collab with L1, and a bunch of news. 21 reviews or interesting product videos in just 1 month. No bashing, no ragebait, no bullshit. We're talking shit because I used to watch the videos within hours of release, and are tired of more ragebait in our feed.",hardware,2026-01-29 23:18:47,1
AMD,o2q7zvm,"Question is, why isnt this more widely talked about? Honestly first time ive heard about it",hardware,2026-01-31 03:17:20,1
AMD,o29f949,"The expo profile doesn't but mainboards usually use an ""auto"" setting that automatically overclocks the if.",hardware,2026-01-28 18:30:34,3
AMD,o2lm2gd,"> We're quiet because this is finally a review, why would we bash?  In nearly every GN post there's a sizeable group calling for more reviews, neutral tones and technical content and less drama. Yet when one finally arrives, it sits at +20 with a measly <60% upvote ratio.  That's not being quiet, that's quiet disapproval from those who dislike anything this channel does.  Ironically, the derBauer review with the more dramatic, accusatory tone got the most engagement among all 9850X3D reviews.",hardware,2026-01-30 13:35:00,0
AMD,o2gjl1p,"Then its not EXPO doing it  XMP 3.0 also makes the intel MBs do some weird voodoo with vccsa, tx vddq, vdd2, etc. But its not the memory profile, it is the motherboard itself.",hardware,2026-01-29 18:44:52,3
AMD,o2n84pe,Correlation is not causation. Nobody is disapproving anything out of spite. Nobody is here making comments.,hardware,2026-01-30 18:05:30,0
AMD,o1talp3,"Note the perf / W chart for Cinebench nT does not start at 0-0 and the two axes are not commensurate, so a bit of a doozy.",hardware,2026-01-26 13:20:54,10
AMD,o1ttex5,The funniest thing is that based on announced laptops AMD seems to refuse to lower the price on this rebrandeon thing compared to strix point despite being on an ancient node NOT used by their upcoming Epyc/Instinct. OEMs simply switched to offering more of the lower end skus like 465 over 470 and 445 over 450 and no lower prices.  So now you have a upper mid range chassis like the Lenovo Yoga Slim 7a......topping out at the 445 with 2+4 core config and 4CU (comparable to circa 2020 Tiger Lake Xe performance)......,hardware,2026-01-26 14:58:29,17
AMD,o1xrq55,AMD is a shit company.,hardware,2026-01-27 01:50:11,7
AMD,o1ta96o,"Hello -protonsandneutrons-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-26 13:18:54,1
AMD,o1uxrz3,"It's important to remember that AMD’s SoC engineering resources are finite. While Strix Point was an impressive milestone, a massive portion of their design man-hours is currently dedicated to the 2027 console cycle. Between the PS6, its rumored portable counterpart, and the next-gen Xbox ecosystem (including their own handheld), the semi-custom division is likely consuming the bulk of their top-tier APU integration and development talent.",hardware,2026-01-26 17:55:06,-1
AMD,o1yjh29,"I wonder if 4+0+4+4 Panther Lake can target lower end Gorgon Point. that said, 226V/228V could easily target that level of perf if what intel need is to shift inventory and the OEM require soldered memory.",hardware,2026-01-27 04:28:53,1
AMD,o1yq5c4,"it's no longer a small company if it can acquire lots of startup and most importantly, Xilinx. It just shows that mobile client has been AMD's least priority, something that has been proven for years.   At least now they have a reason to push RDNA5-based APU, or maybe prioritize RDNA4 APU if they still haven't cancelled that yet",hardware,2026-01-27 05:13:25,2
AMD,o1yetq2,32 billion revenue and it’s still a smol bean indie company incapable of multi tasking and all the physical limitations in the world exclusively affect amd and not everybody else.,hardware,2026-01-27 03:59:53,5
AMD,o1z1al0,"Who said AMD is a small company? If they acquired other companies for other purposes, that doesn't mean AMD can now design and launch **each year 3-4 new chips**.",hardware,2026-01-27 06:38:13,1
AMD,o1z0yse,"By your silly logic, NVIDIA with a 200 B yearly revenue should design and lunch 20 chips each year. It took you a long time to come up with that? The limitation is man hours, number of engineers!!!",hardware,2026-01-27 06:35:32,2
AMD,o253ltj,"Whether they can design so many chips is not the concern, it's just mobile client is being treated as third class now rather than third (Rack scale / Epyc, Ryzen, Ryzen Mobile) while it used to be the preview for scaled up solutions like intel still does today.",hardware,2026-01-28 02:34:49,1
AMD,o1z1pcl,What stops them from hiring me? They only have themselves to blame,hardware,2026-01-27 06:41:34,7
AMD,o26jfus,"Nothing had changed in the last 6 years in AMD's APU launch strategy. Client is not a third class. Since Strix Halo AMD has actually accelerated the development of client products.  Picasso is a rebrand of Raven Ridge  Hawk Point is a refresh of Phoenix  Gorgon Point is a refresh of Strix Point.  CES 2021 - **new** Cezanne die + **new** RTX 30   CES 2022 - **rebranded** Cezanne (Rembrandt) + RTX 30 **Refresh**   CES 2023 - **new** Phoenix die + **new** RTX 40   CES 2024 - **rebranded** Phoenix (Hawk Point) + RTX 40 **Refresh**   CES 2025 - **new** Strix Point die + **new** RTX 50 (Strix Point was found in just a few laptop models in 2024, 2025 was it's major release in 100+ design wins)   CES 2026 - **rebranded** Strix Point die (Gorgon Point) + RTX 50 **Refresh** (canceled because of VRAM shortage)   CES 2027 - **new** Medusa Point die + **new** RTX 60  CES 2028 AMD will launch a **rebranded** Medusa Point to go with a **refreshed** RTX 60.. There won't be a new APU design.",hardware,2026-01-28 08:33:51,2
AMD,o1z9h8e,">What stops them from hiring me?  If you ask genuinely, you will get the answer as to why high tech companies in semi find it hard to hire good talent and then scale design/test/delivery products. You have no idea",hardware,2026-01-27 07:47:40,2
AMD,o1zczdp,Its sarcasm bro,hardware,2026-01-27 08:19:28,2
AMD,o3k2yow,"Cool, don't cry when AI crashes and burns and a huge percent of the business PC sector got used to stretching refresh timelines and home users found new hobbies and stopped caring about PCs.",hardware,2026-02-04 16:47:22,173
AMD,o3k3b2b,AMD's Q4 results currently tanking the market and triggering the AI bubble pop could fix the memory crunch real quick.,hardware,2026-02-04 16:48:56,67
AMD,o3kclqm,"This is not really a shock. Nvidia became the most valuable company in the world, worth over $4.5 trillion. They did not do that because of consumer level products. AMD is a publicly traded company; they are going to chase AI, even though the bust is coming. We are not their friends, only profit is.",hardware,2026-02-04 17:31:46,35
AMD,o3l8r2e,">company wants to focus on growing 'higher-end of the market'  Exactly opposite of what they said when launching RDNA4, of course. lol   Fucking corporations man.  They'll literally say whatever's most convenient at the time.",hardware,2026-02-04 19:58:42,16
AMD,o3re9lu,"She also down played the not as good stock even though it was up not what people expected far from it.   She didn't pivoted to saying oh the new Xbox is coming out with our chip.   Xbox has always used their chip and so has Sony for the past consoles.   This woman is just like the rest they're all greedy, Jensen leading the pack.   AMD picking up the scraps that Nvidia leaves behind.   Can't wait for this whole AI scam to blow over.   They will come crawling back to the consumers but hopefully consumers won't forget.",hardware,2026-02-05 18:29:16,5
AMD,o3kbvvn,"Our focus areas are AI, AI, AI, AI...",hardware,2026-02-04 17:28:23,9
AMD,o3ll4yn,"car companies did the same BS during covid. They don't realize the downstream effect of this. Which is loosing your customers forever. As you lose the home market, then when at work - they still don't buy your stuff.",hardware,2026-02-04 20:58:09,5
AMD,o3krs5c,"Buuut... I thought AMD was gamer's best friend, acting solely for the love of gaming, gamers and gaming techtubers  !!! We cannot not be the most important demographic for CPU/GPU manufacturers ! Won't AMD think of HUB and tech Jesus Steves ?",hardware,2026-02-04 18:40:40,16
AMD,o3kxd7r,The AI bubble will sure crash and burn just like Blockchain and metaverse did. It is just a tool to increase productivity not the messiah that can do all the work on its own without issues. Most of the AI startups are running out of money and operating on losses. Lets see how much they can feed these companies before going bankrupt.,hardware,2026-02-04 19:05:33,10
AMD,o3mfmzf,I can't wait for China to take over the market,hardware,2026-02-04 23:31:02,2
AMD,o3v31ym,If the Chinese markets start supplying the market with competitive memory and chips while the others still have stock issues because they’re supplying AI I will never again for the rest of my life buy from anyone but Chinese manufacturers and even if the bubble bursts and they come crying for my money even to the point they try to sell their stuff at a loss to get back into the market I’ll just laugh and ignore them,hardware,2026-02-06 07:13:55,2
AMD,o3kg7o1,"Dont blame ceo for that. Ceo reports to the board of directors and shareholders, and these mfs are not here because of pc market. Profit.",hardware,2026-02-04 17:48:22,3
AMD,o3n75s1,Nothing changed from this announcement. Su was just reiterating what was already known (apparently not to everybody),hardware,2026-02-05 02:05:56,1
AMD,o3ocaau,"To all die hard AMD or Intel or Nvidia or whatever Inc. fans: see, Corporations will throw you  under the bus for more money for shareholders.",hardware,2026-02-05 06:41:15,1
AMD,o3nj5yh,Maybe getting out of memory and the fab business was bad? Oh well.,hardware,2026-02-05 03:14:28,1
AMD,o3nw2ee,"At this point, Apple is the only company which remotely gives a fuck about their customer lmfao 😂🤣😂😭",hardware,2026-02-05 04:36:52,-1
AMD,o3jzvww,"Hello BarKnight! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-04 16:33:18,0
AMD,o3lld68,"Dear Chip designers, manufacturers, board partners.  The high end market is the HEDT, and top end consumer place.  Enterprise is its own segment. Thats why its called enterprise.  Its not a high-end market. Its a high-volume market. The high-end market exists within the consumer market only.  Edit to the downvoters: Its like calling the white ford, dodge, and GM duty truck commercial segment of their portfolios the ""higher-end of the market"" instead of commercial. It has high end parts, but its not a high-end market, its a high volume market. Its not exclusive or an indicator, but it is definitively a wrong label.  Its enterprise or datacenter market, not high-end.",hardware,2026-02-04 20:59:13,-3
AMD,o3koyda,She's just saying the stuff shareholders want to hear. Hopefully this all trickles back down to us gamers.,hardware,2026-02-04 18:27:53,-18
AMD,o3kacsx,Gotta love how we were all a stepping stone and once they got enough money out of us they just ditched us all.,hardware,2026-02-04 17:21:15,74
AMD,o3lj08w,Glad Crucial is not going to cry and come back. They burned that bridge and salted it.,hardware,2026-02-04 20:48:01,5
AMD,o3k95jf,In capitalism you either you're either doing ok for now or you're crying,hardware,2026-02-04 17:15:43,5
AMD,o3lp9kq,"bruh i hate to break it to you but you (average consumer) just aren't that important nowadays, just look at the dc revenue vs consumer. also considering the entire economy is held up by this AI right now, when AI crashes so will the whole market, and the impact will be greater than 2008, by that time most people would be lucky to even have jobs that feed themselves instead of money to blow on gaming pcs",hardware,2026-02-04 21:17:48,4
AMD,o3l0898,This is AMD would we expect anything less from them or there marketing team? I think not it be very un AMD to not fumble the ball on marketing.,hardware,2026-02-04 19:18:41,3
AMD,o3lipjr,The PC sector has been used to it for a VERY long time. Intel did hardly a thing from 2012-2017. 2017-2023 was a decent uptick though.,hardware,2026-02-04 20:46:35,1
AMD,o3p6lvq,"buy Intel lol.  In terms of % of profit, Consumer actually have more impact on intel profit than AMD.",hardware,2026-02-05 11:23:27,1
AMD,o3ls7i5,All they will be left with is commodity business PCs bought in bulk at that point and I can't imagine how razor thin those margins are and will be.,hardware,2026-02-04 21:31:51,-1
AMD,o3k7v76,This gives me the vibe that the next economic crash will be caused by the AI bubble bursting.,hardware,2026-02-04 17:09:50,18
AMD,o3z288o,And you got NVIDIA saying no new GPU's in 2026 due to the crunch...   [https://www.theinformation.com/articles/nvidia-delay-new-gaming-chip-due-memory-chip-shortage?email\_hash=e50ef4703dd585546390b89a0248a20d](https://www.theinformation.com/articles/nvidia-delay-new-gaming-chip-due-memory-chip-shortage?email_hash=e50ef4703dd585546390b89a0248a20d),hardware,2026-02-06 21:28:53,1
AMD,o3m6quk,"They (along with everyone else in this space) took a look at NVIDIA's margins for AI/HPC systems and decided ""I want a piece of that."" This is nothing more than that.",hardware,2026-02-04 22:43:11,1
AMD,o3ocd4h,"Hey, hey, that is four Another Indians !",hardware,2026-02-05 06:41:56,3
AMD,o3uzut5,Looking at you Volkswagen,hardware,2026-02-06 06:46:13,1
AMD,o3od954,"It's ok, everybody knew this when the prices for their 9xxx series GPUs were announced. They clearly don't care about anything other then data center, which is mainly server CPU from their side, as their ai accelerators sales will drop fast when everybody is building their own.",hardware,2026-02-05 06:49:36,2
AMD,o3meb6r,Did typing that out make you feel better?,hardware,2026-02-04 23:23:41,-13
AMD,o3lrkma,"""Dont hate the player, hate the game""  Nah, I'm gonna hate the players, too.  I've always hated this line.  It's true a CEO's are gonna do what CEO's do, but I dont have to give them a pass for it.    Also, share price is not everything.  It's not even most things.  Higher share price doesn't necessarily give a company more money or anything.  It's not the end all, be all of a company's financial situation by any means.  Nvidia's share price could go down to $50 tomorrow and pretty much nothing would materially change for them.",hardware,2026-02-04 21:28:48,13
AMD,o3l49u1,"This just corporate talk telling the shareholder what they want as  directors (their bosses) do, this is like trusting HR or something dumb like that we all know not to",hardware,2026-02-04 19:37:42,2
AMD,o3lj5zi,Better than Intel’s board for sure,hardware,2026-02-04 20:48:47,-2
AMD,o3krstj,"Totally, trickle down economics always benefits the ones at the very bottom",hardware,2026-02-04 18:40:45,10
AMD,o3kax7o,I mean what did you really expect? End of the day we are just customers and these companies have never been our friends. Just chasing whatever profit they can get,hardware,2026-02-04 17:23:53,89
AMD,o3klntc,"DC and business/government have been always their biggest income, AI or not. Historically, innovations in computing are often driven by enterprise demand: personal computer came decades after first programmable digital computer, then multicore CPU, 64-bit architecture, virtualization, high-speed networking ...",hardware,2026-02-04 18:13:05,23
AMD,o3m0guq,"They really don't owe us any long term loyalty.    It sucks currently but moore's law has been steadily slowing down for a while now and when this bubble eventually pops, their competition will be old server shit with gobs of VRAM (Like the 16gb V100s right now) that's getting liquidated.  If you've got a halfway decent rig, the improvement over the past 4 gens (8 years!!!!) hasnt even been that big.",hardware,2026-02-04 22:11:27,3
AMD,o3kv5px,"Unfortunately that's the way most businesses operate. Ethics is an absolute joke in corporate America. It's okay to be a shit bag if it's for the ""share holder."" Which apparently most consumers don't seem to know is that the corporate executives and senior leaders in these companies are in fact shareholders. So when they shit on their core market to pivot to the hot new thing Wallstreet has a hard on for and the stock goes up, they make even more money.  Even if their product/services are weaker, because all they seem to care about is $$$.  This is why Costco is one of my favorite companies. They're one of the few companies out there that refuse to do this kind of bull shit.",hardware,2026-02-04 18:55:37,-2
AMD,o3kpbrm,Short term quarterly outlook investment speed running endhitification of everything.   Give me a good 5 year plan instead.  What's the long game for these fools when there's no companies left to ruin and no one has money to buy their products,hardware,2026-02-04 18:29:32,6
AMD,o3lr6jx,Did you even read what I said?   I'm not just talking about me.   Enterprise PC purchases are getting put on much longer refresh cycles now and I will laugh when AMD cries about it after their AI customers go broke.,hardware,2026-02-04 21:26:55,3
AMD,o3yud5r,"In that case, the AI companies and chipmakers should not be considered ""too big to fail"" and given bailouts. Companies like Micron have literally already received massive subsidies to build fabs under ""national security"" pretenses. Let all of them fail or get nationalized with all previous management thrown out if they are saved.      Also ""entire economy being held up by AI"" is such bullshit. Plenty of industries and sectors which have nothing to do with AI are doing fine. The US economy without AI is likely in a slight recession but to claim we should be grateful for our AI overlords for keeping the economy afloat is ridiculous.     Most of those benefits are going back to other companies in the AI circlejerk anyway, with only a few going to power and infrastructure sectors",hardware,2026-02-06 20:49:48,1
AMD,o3llrst,"This is like saying NBD the East Coast is used to hurricanes right before Sandy hits, ignoring all the exacerbating factors.",hardware,2026-02-04 21:01:08,1
AMD,o3oatyr,Wishful thinking,hardware,2026-02-05 06:28:33,3
AMD,o3kcn5w,"With around a trillion and a half U.S. dollars on the line, that wouldn’t surprise me. If there’s indeed an AI bubble burst on the horizon (and I see no reason to think otherwise), it’ll be an economy buster.",hardware,2026-02-04 17:31:58,23
AMD,o3krdxn,"AI will provide huge productivity boost for economy in long term, but the pace of spending at the moment is unsustainable. AI companies move too aggressively, capital market is already showing signs of not catching up.  OpenAI, Anthropic, Databrick, SpaceX/xAI are all racing to IPO this year and it could mean that the founders feel the top and are trying to capitalize on hype.",hardware,2026-02-04 18:38:53,2
AMD,o3kpuzy,"there is no AI bubble though, so you're out of luck",hardware,2026-02-04 18:31:55,-16
AMD,o3m9h2q,I get it.  And I'm still gonna criticize it.  I dont have to be ok with the policies of a corporation simply because I can see it'll help them make more money in the short term.,hardware,2026-02-04 22:57:23,4
AMD,o3oq34p,Yeah it does,hardware,2026-02-05 08:48:35,3
AMD,o3m6csy,"Senior leadership compensation at these large tech companies consist of mostly stock. It would be a material change for the leadership's personal financial situations if AMD share prices drop by 75% tomorrow, so it shouldn't be a surprise that they want the line to go up.",hardware,2026-02-04 22:41:13,-2
AMD,o3lh8cr,"i hope so dearly that we have like.. state funded or community-backed computing R&D at some point. computers are sick and it sucks that whatever the latest business fad is entirely changes the direction big companies move in, as well as their only intrinsic motive being competition. nvidia just... slows down entirely in the consumer section the second they hit a high enough market share, and releases XX90 cards for $2.5k for the hell of it",hardware,2026-02-04 20:39:30,1
AMD,o3kovuu,This is somewhat unprecedented though in they are completely neglecting the PC segment.  Including business PC.  Not just focusing more on higher end.,hardware,2026-02-04 18:27:33,2
AMD,o3m7ejr,"> They really don't owe us any long term loyalty.  And conversely, we (as consumers) don't owe any company our loyalty either.   When it comes time to upgrade my PC, I will look at what's the best bang-for-the-buck components on the market at that time, whether it be from NVIDIA, AMD, Intel, or someone else.",hardware,2026-02-04 22:46:34,7
AMD,o3m4xjw,"welcome to costco, i love you",hardware,2026-02-04 22:33:59,4
AMD,o3lwf9b,"> Unfortunately that's the way most businesses operate. Ethics is an absolute joke in corporate America. It's okay to be a shit bag if it's for the ""share holder.""  Not catering to oppressed gamers = unethical and being a shit bag lmao peak reddit",hardware,2026-02-04 21:51:54,18
AMD,o439plr,Not sure why you got downvoted for speaking truth.  I've personally run into so many scummy things in search of short term gains.  From people committing fraud to simply alienating the best customers.  Gotta hit that quarterly number.,hardware,2026-02-07 15:12:03,2
AMD,o3l8xlx,"Their plan is that you'll all coming running back to them no matter what, cuz consumerism DOES NOT STOP.",hardware,2026-02-04 19:59:32,9
AMD,o3luwv6,"by enterprise PCs do you mean laptops or servers? laptops are chump changes and amd was never big in those anyways, that's intel's market to lose. the bigger point is, when their AI customers go broke, so will the economy, and all of us, including you, will be crying harder",hardware,2026-02-04 21:44:44,2
AMD,o3kp40t,"And as always, whenever the US economy sneezes, the rest of the world catches the flu",hardware,2026-02-04 18:28:34,9
AMD,o3phfu3,> AI will provide huge productivity boost for economy in long term      Computer industry failed to do that in the last 50 years. Whatever benefits it provided were intangible.,hardware,2026-02-05 12:44:26,2
AMD,o488gnr,And voting in the upcoming election feels better than commenting ;),hardware,2026-02-08 09:45:17,1
AMD,o3m9s5u,Only proving my point more - that we can absolutely be critical of 'the players' and not just 'the game'.,hardware,2026-02-04 22:59:00,8
AMD,o3maicp,>  if AMD share prices drop by 75% tomorrow  It's down 17% today,hardware,2026-02-04 23:02:52,1
AMD,o3luokk,You think the population for your country is going to be cool with  taxpayer-funded gaming rigs?,hardware,2026-02-04 21:43:38,17
AMD,o3l5pzj,"They're not selling small batch artisanal CPUs. We always have been, and always will be, just a number at best rounded into increments of 100k.",hardware,2026-02-04 19:44:31,11
AMD,o3l58wy,"> totally, it's all about those profits... feeling more like a number than a customer nowadays lol  You always were a number. Companies can't care about you when they don't know you even exist. You're chump change in the bottom line, the coin that got lost in the couch cushion that nobody noticed.",hardware,2026-02-04 19:42:17,8
AMD,o3kx9dd,"They already abandoned designing CPUs for PCs years ago. Zen is their server architecture, they just repurposed some of the dies to make a small amount of desktop and laptop CPUs, but it is fundamentally designed for servers.",hardware,2026-02-04 19:05:05,6
AMD,o3mxivf,Entitled redditors are the worse kind,hardware,2026-02-05 01:10:22,8
AMD,o3mec9o,"You have zero concept of what is actually going on right now. Open corruption, price fixing, and that is just a few of the things going on.",hardware,2026-02-04 23:23:52,-5
AMD,o43o95q,"It's viscous out there. I remember my job searches over the past several years.  As someone that watches scammer payback all the time, there are many predators out there running ""legal"" scam companies. Just wait until these scammers realize there are ways to do this shit legally by taking advantage of ignorance.   The majority of the SaaS industry is one big shakedown to take advantage of customers so the SaaS vendor gets their's and whoever the partner who is doing the implementation gets paid big.  You basically have to hire an outside consulting firm to validate all the shit or have a very skilled team that understands deploying software. Part is ignorance but I've owned a consulting firm. These big companies are very calculated. I call whats best for billing not for the customer. And guess what, thats ""okay"" because like you said. Gotta hit them numbers.",hardware,2026-02-07 16:24:10,3
AMD,o3laj4w,It will if unemployment keeps going up indefinitely if their grand plan of replacing as many jobs as they can with AI and limp governments do nothing about it.,hardware,2026-02-04 20:07:32,1
AMD,o3ln03i,Canada was pretty resilient against the 2008 crisis caused by shitty mortgage trading practices in the USA.,hardware,2026-02-04 21:07:01,4
AMD,o3mczah,That only brought them back to where they were on January 9th this year. A 75% drop from here will bring their stock back to April 2020.,hardware,2026-02-04 23:16:21,1
AMD,o3mpfnv,"If it's a choice between the pork project being more advanced ways to kill people and the pork project being more advanced virtual reality headsets, I know which one will get _my_ vote.",hardware,2026-02-05 00:25:11,8
AMD,o3mww4v,"computers get used for other things, my god 😭  imagine you had funding that went into computer research to further protein folding discoveries or something. that could be cool",hardware,2026-02-05 01:06:44,8
AMD,o3l1q01,That is not neglecting the PC market.  They literally make the best desktop CPUs now because of that design philosophy.,hardware,2026-02-04 19:25:40,-4
AMD,o3lpvq8,"I really dont know.  It genuinely seems like even with the cost of living crisis, consumerism has only grown *more* rampant and irresponsible.",hardware,2026-02-04 21:20:45,2
AMD,o3lqft4,"Well, good for Canada, but for example, my home country, a relatively small EU country, needed 15 years to recover the GDP per capita to the same level as 2008. US wasn't the only reason for this but still...",hardware,2026-02-04 21:23:25,3
AMD,o3mds3r,I knew a lot of people that worked at Cisco during the dot com boom.  Their stock went from $80 in 2000 to $10 in 2002.  https://finance.yahoo.com/quote/CSCO/  I've been in the chip industry for 30 years.  I don't know when this AI bubble will pop or how bad it will be but AMD may be happy that they aren't as invested in AI as others when it pops.,hardware,2026-02-04 23:20:45,2
AMD,o3opwh4,It's so reddit-brained to always fixate on gaming as the first use-case for high end compute,hardware,2026-02-05 08:46:49,11
AMD,o3oimlc,"You dont need gpus for other stuff for example, nor 16+GB RAM.   Regarding the protein stuff that research runs on those entreprise products at same datacenter as AI.",hardware,2026-02-05 07:38:09,-1
AMD,o3le1bk,"Lol, straight to attacks rather then even trying to understand what I was saying.   Zen is designed for mainly multi-threaded workloads rather than single threaded workloads. The same cores are used on their 128 core Epyc CPUs as in their 16 core Ryzen CPUs.  If you wanted to design a CPU for single threaded applications you would have fewer but much larger cores with a wider decoder and dispatch and more l2.  Only with Zen 5 have they started making each core wider moving for a 6 wide decode to an 8 wide decode. But the inside of the core is still much narrower that say Apple's M4 P cores (and yes it is fine to compare x86 and ark core internals since once you get past the decode stage the instruction set doesn't matter).",hardware,2026-02-04 20:24:17,4
AMD,o3lerxg,>> says someone else is trying to sound smart without fundamentally understanding computer architecture  >> says there is no difference needed between server and consumer CPUs because they are both x86  bruh,hardware,2026-02-04 20:27:48,1
AMD,o3lvtkp,>US wasn't the only reason for this but still...  Was it related to similar levels of lending and trading regulations styled after the American banking system within your own country?,hardware,2026-02-04 21:49:02,-2
AMD,o3rahqp,high end compute intended for every taxpayers home pc? I'm sorry but the average person's needs are so far detached from a state-funded high end compute home desktop initiative.   and before you shit your pants. re-read the context of this thread.,hardware,2026-02-05 18:12:00,1
AMD,o3tmtre,Iceland isn't in the EU.,hardware,2026-02-06 01:17:12,0
AMD,o3tymqb,I didn't say it was.   You can base a lending system of another country's without being in that country.,hardware,2026-02-06 02:27:51,1
AMD,o4bfzq9,It was fine 5 years ago and it's still reasonably fine today.  You just have to turn down settings for newer games.,buildapc,2026-02-08 20:59:17,9
AMD,o4bdwuw,DDR4 Ram ( yes! ) lol!  I'd say so...  Maybe 3060?  Don't know your budget,buildapc,2026-02-08 20:48:53,3
AMD,o4bemt3,"Yes, this is still a good enough PC even in 2026. Try to get the RTX 2060 with at least 8GB of memory (not 6GB).",buildapc,2026-02-08 20:52:31,1
AMD,o4bgemj,"Depends what you use it for. I just upgraded from a 5 3600 and a gtx 1660 super (more or less the same gpu as a 2060 but without raytracing and slightly less vram) to a ryzen 7 5700x and an rtx 5060. I don't play many intensive games so i havent noticed a diff there but in professional creative softwares there's been a huge leap in performance (premier, photoshop, illustrate, and blender)  If you just want to game that's a good setup, as long as you're fine with medium-low graphics (also some new singleplayer titles like indiana jones might not work since they have raytracing you can't turn off, and the 2060 struggles with that feature)",buildapc,2026-02-08 21:01:20,1
AMD,o4bmbm5,It is but.  I would advise you to save more and spend a better time.,buildapc,2026-02-08 21:30:31,1
AMD,o4bmpsq,Yeah but I would spring for the RTX 2060 Super instead since it has 8GB VRAM and a wider memory bus (The VRAM gets 448GB/s instead of 336GB/s).  I had a similar setup and it was really good. There's also a good upgrade path from there.,buildapc,2026-02-08 21:32:29,1
AMD,o4c3pex,It's fine with 60Hz monitors,buildapc,2026-02-08 23:02:09,1
AMD,o4cyhld,"Haha, that was my exact build I upgraded from. It can run Cyberpunk with some aggressive DLSS settings @ 60 FPS. You will see a fair amount of tearing with that, but it will run stably.   Might be able to do better, though. 3060 is a much better card if you can find one for a good deal.",buildapc,2026-02-09 01:57:35,1
AMD,o4bjeod,"It's decent, but the GPU will be lagging behind, I'd get a 3060.",buildapc,2026-02-08 21:16:07,0
AMD,o4ben54,Hey man thanks so my budget is like 500 to 600,buildapc,2026-02-08 20:52:34,1
AMD,o4bf5yi,"If OP can splurge a bit and get a 5600 instead of the 3600 id recommend it.  I have the 3600 and it is definitely starting to slow things down.  Still meet the minimum on most games I want to play, but just the minimum.  I have a feeling in 1 years time the 3600 won't cut it anymore.  The 5600 should help extend the life of OPs computer by a couple of years.",buildapc,2026-02-08 20:55:11,5
AMD,o4bf30i,Thanks man :),buildapc,2026-02-08 20:54:46,1
AMD,o4brlxn,"I don't know what ""budget"" means to you guys but the 3060 is about twice the price of a 2060. And the often recommended 5600 is about twice the price of a 3600.  OP can just get what they asked for in the title and they'll be happy. Not everyone wants/expects the latest and best graphics.",buildapc,2026-02-08 21:56:52,2
AMD,o4bho07,"You can probably go up to a 3060 or 4060 and a 5000 series cpu on this budget if you're savvy. make sure your ram is DDR4 since thats still relatively cheap. get your motherboard and case second hand (this'll be a big money saver). Find a power supply on sale, I picked up a 750w corsair one on amazon for 50 (usually 80). Could also get a HDD rather than a SSD since it'll be cheaper, will be a lot slower to boot though. but again, if you're here for lower-performance gaming it might not make too much of a difference. I have a 1tb HDD I store my games on and a 300gb SSD I use as my boot drive/easy access storage.",buildapc,2026-02-08 21:07:33,5
AMD,o4bg61w,A used 3060 Ti would be better and still within budget if careful. It'd give you some more VRam and a few more raw power for the price: [RTX 3060 Ti 8GB ](https://ebay.us/m/L3dORi),buildapc,2026-02-08 21:00:08,1
AMD,o4bffp7,That’s a good point. It’s not much more money and will last much longer.,buildapc,2026-02-08 20:56:31,1
AMD,o4bfj4j,Oh okay ty i didnt knew that thank you for that,buildapc,2026-02-08 20:56:59,1
AMD,o4bm0li,The 5600 is about double the price in a lot of places. That's money that can be spent on the RAM.,buildapc,2026-02-08 21:28:58,1
AMD,o4bid3p,Yeah i think the 3060 is better because i got a budget of MAX 600 so i think the 4060 is al ittle hard to fit it with the other components you know ?,buildapc,2026-02-08 21:10:57,1
AMD,o4bjy55,"I recently got a low end budget PC deal for a friend with a used Ryzen 5 5500, RTX 3050 8 GB, 16 GB of DDR4 3200 MHz and a 1 TB NVMe SSD for 330 €. Complete PC including the Windows 11 license.",buildapc,2026-02-08 21:18:47,1
AMD,o4bgngd,Oh yeah u are right i looked it up so i will try a 3060 with a 5 5600 thats should be much better?,buildapc,2026-02-08 21:02:34,2
AMD,o4bfsrq,"The 3600 is a great chip, I used it since it came out a long ass time ago.  That said it is definitely old.  The price difference between Zen 2 and Zen 3 on Amazon is close to $40 and that would be worth it.  Otherwise you might be upgrading again in a year.",buildapc,2026-02-08 20:58:18,1
AMD,o4bsf0u,That i was not aware of.,buildapc,2026-02-08 22:00:57,1
AMD,o4c2oqg,if you can find it on sale it might be a shout. i got my 5060 for £250 on sale. you be frugal with the other components and it'll be fine,buildapc,2026-02-08 22:56:15,1
AMD,o4bhfwu,"Also consider:  A bit pricier but with 16gb of vram, AMD technologies are catching up with Nvidia...  [RX 7600 XT](https://ebay.us/m/gs1sW2)",buildapc,2026-02-08 21:06:26,1
AMD,o4bgv32,Yep,buildapc,2026-02-08 21:03:35,-1
AMD,o4bhy07,I only just stopped using mine. it served me well. but holy fuck the 5700x is leagues ahead of it,buildapc,2026-02-08 21:08:54,2
AMD,o4bhq5u,Oh okay it looked nice i will think about it thank u for that idea,buildapc,2026-02-08 21:07:50,2
AMD,o4bgyca,Okay man thank u,buildapc,2026-02-08 21:04:02,2
AMD,o4bi72r,"Yeah I debate with staying on AM4 or doing a microcenter bundles for AM5.  I dont know how much longer my motherboard has and it is starting to show signs of failure.  That said, fuck these RAM prices.  At least with microcenter my wallet won't be completely ass raped for an upgrade.",buildapc,2026-02-08 21:10:09,2
AMD,o4c2j3o,"i just swapped out my cpu and gpu. kept the ram since i've got 32gb and it works perfectly fine. we'll probably be on DDR6 by the time i do a full upgrade, barring any accidents or sudden failures",buildapc,2026-02-08 22:55:21,1
AMD,o45w0l7,if you wanted to save some cash you could most definitely get away with using something like a phantom spirit 120 instead of a 360mm aio.,buildapc,2026-02-07 23:20:30,2
AMD,o46cqw1,You really don't need an expensive aio for these Ryzen chips. I'm using a 10 year old hyper 212 evo with my Ryzen 5700x and the temps are good. Just get a phantom spirit or peerless assassin which seem to be the most recommended fans these days.,buildapc,2026-02-08 01:03:29,2
AMD,o45pstz,Great cpu choice. Take a look at this mobo  Asus TUF GAMING B550-PLUS WIFI II ATX AM4 Motherboard,buildapc,2026-02-07 22:44:26,1
AMD,o45rz98,"Thanks, you are a legend my friend :)",buildapc,2026-02-07 22:56:57,1
AMD,o43kvte,"UserBenchmark is the subject of concerns over the accuracy and integrity of their benchmark and review process.  Their findings do not typically match those of known reputable and trustworthy sources.  As always, please ensure you verify the information you read online before drawing conclusions or making purchases.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2026-02-07 16:07:35,3
AMD,o43mlbe,"CPU - R5 5600/x or 5700x    GPU - Best you can afford, likely a 5070/9070   Everything else is fine, no need to upgrade",buildapc,2026-02-07 16:15:59,2
AMD,o43mqys,"I’d upgrade the CPU 5600/5600X/5700X/5800XT depending your price and budget. Make sure you update to a compatible bios before swapping them.   Then I’d buy the best GPU you’re comfortable spending on, the 9060XT 16GB is a good starting point.   What power supply do you have?",buildapc,2026-02-07 16:16:45,2
AMD,o43qifm,"eyo I am on an ryzen 5 5600 and an RX 6700XT. I play on 1440p, with most stuff on ultra/high and i get about 80fps on like BF6.  tbh, I would invest into an GPU and try to overclock the CPU. If thats not enough for you, you might want to try to get a used 5600, that wont be very expensive and a good upgrade.  If performance works fine for you without a new CPU, I would suggest to then just wait till AM6 and by then just upgrade completely to a new platform.",buildapc,2026-02-07 16:35:12,2
AMD,o43l22m,"I am aware, thanks bot. I used it mostly to post my specs and get a general idea of my computers performance :)",buildapc,2026-02-07 16:08:25,1
AMD,o43te8w,"Thank you, I'll give those CPUs a look, since I didn't really consider upgrading it before.",buildapc,2026-02-07 16:49:22,1
AMD,o43t6s2,"Thank you!  My PSU is the ""550 Watt Seasonic Focus GX""",buildapc,2026-02-07 16:48:22,1
AMD,o43th9e,"Overclocking is an idea. i do have a AIO watercooler on it, so i think i could get a bit more out of it!",buildapc,2026-02-07 16:49:47,1
AMD,o43zfit,"I'm a bit late to the party, but I highly recommend you visit [passmark.com](http://passmark.com), at the top there's a ""Benchmark"" button of which allows you to see different component benchmarks. In your case you'd only need the cpu and video card (gpu) options, then on the left row there's a way to compare up to 5 parts between each other. Hope this helps!",buildapc,2026-02-07 17:19:07,2
AMD,o44fyky,The 3600 is showing its age   The 5600 is a cheap upgrade that'll more consistently get you 60 FPS in modern AAA,buildapc,2026-02-07 18:40:37,2
AMD,o43vprn,I’d look at getting a 750W power supply.,buildapc,2026-02-07 17:00:43,2
AMD,o43zzn1,"Appreciate the input, I'll have a look! Thank you :)",buildapc,2026-02-07 17:21:52,2
AMD,o44nqqn,cheap upgrade sounds good. I should probably grab the 5600 then and then see whats left for my GPU,buildapc,2026-02-07 19:19:28,1
AMD,o43w17c,"This might be a stupid question but here goes:  Only if I get another GPU, or even with a new CPU?",buildapc,2026-02-07 17:02:16,2
AMD,o43wjc5,If you get another GPU.,buildapc,2026-02-07 17:04:44,2
AMD,o4401nf,"Thanks, I'll have a look at my options.",buildapc,2026-02-07 17:22:09,2
AMD,o442nh7,"You bet, good luck with the upgrade.",buildapc,2026-02-07 17:35:05,1
AMD,o44a94b,Thank you! :),buildapc,2026-02-07 18:12:38,2
AMD,o3cfhnr,9950x3d,buildapc,2026-02-03 14:17:41,2
AMD,o3crf7b,"If you plan to use it for work and you don't mind higher price, pick 9950X3D. 9850X3D/9800X3D is the best for gaming (it doesn't mean that it can't handle work related tasks) but 9950X3D is more useful for work.",buildapc,2026-02-03 15:18:57,1
AMD,o3ctqxn,"If you are using your PC for professional and compute workloads, then get the 9950X3D.  If mostly gaming with lighter workloads, then get the 9800X3D.",buildapc,2026-02-03 15:30:06,1
AMD,o3d0t05,"If you're not actually doing any meaningful work yet, 9850x3D would be better. With that said for $50 more (plus the gift card) to get to the 9950x3D, personally that's what I would go with. Regardless of what CPU you go with, get a reasonable cooler. Dual tower like a Peerless Assassin or Phantom Evo, NH-D15 G2, etc. Water cooling I have no clue, I personally prefer air.",buildapc,2026-02-03 16:03:38,1
AMD,o3ocnxl,"of course 9950x3d lol, cuts the compilation time by almost half",buildapc,2026-02-05 06:44:31,1
AMD,o3cgsp7,"From what I've read, they're both very close. It sounds like you already got RAM, but Microcenter has good bundles on the cpu, motherboard, and RAM for almost the same price as just the cpu.",buildapc,2026-02-03 14:24:42,0
AMD,o3f9zjf,"9800x3d for gaming (9850 has bad power efficiency)  9950x3d for productivity  Honestly, if you have to ask whether you need a 9950X3D, you probably don't.",buildapc,2026-02-03 22:19:08,0
AMD,o3ciosd,Do I really have to be concerned about the higher temperatures and power usage of the 9950X3D?,buildapc,2026-02-03 14:34:45,1
AMD,o3choc1,"I’m in Europe, so no MicroCenter, NewEgg or other deals available for me(",buildapc,2026-02-03 14:29:21,2
AMD,o3coylc,"the 9850x3d and the 9950x3d are two completely different chips, I wouldn't call them close.",buildapc,2026-02-03 15:06:51,0
AMD,o3cjh10,Not as long as you have a sufficient cooler.,buildapc,2026-02-03 14:38:52,1
AMD,o3cp498,only you can answer if you care about the power usage. The higher temperatures mean that you would benefit from better cooling/airflow but the wouldn't release a chip that you couldn't cool.,buildapc,2026-02-03 15:07:37,1
AMD,o3ctv9w,You can also undervolt and overclock the CPU for more performance with less power and heat.,buildapc,2026-02-03 15:30:40,1
AMD,o3ci7cx,That's too bad. Should have been obvious from the currency signs!,buildapc,2026-02-03 14:32:10,1
AMD,o1ujth2,a 5600x or a 5800xt are really the only options you can reliably get new at the moment at about $160/$200 respectively. Even though they aren't X3D chips they would still be a MASSIVE upgrade over your 2700x.  Short of going to a AM5 DDR5 system this is probably the most impact you could do without spending $400+ on a 3 generation old 5800x3d/5700x3d CPU.,buildapc,2026-01-26 16:54:17,23
AMD,o1ujx4j,"9800X3D will not work on your current mobo, it's AM5, your mobo can only use AM4 socket CPUs.  5700X3D could work, but it's also hard to find and overpriced. Definitely not worth the same cost as a 9800X3D, imo.   Depending on your budget, a 5800XT or similar could work and would still  be a big upgrade from your current CPU.",buildapc,2026-01-26 16:54:43,5
AMD,o1ukppl,"5800XT is the obvious choice at this point as others have said. Either that, or buy a used and overpriced 5800X3D/5700X3D *or* a full rebuild with an AM5 motherboard and something like a 9800X3D.",buildapc,2026-01-26 16:58:03,3
AMD,o1uyp3o,You could also wait to see what AM4 cpus have revived production. Nothing concrete yet but prices could drop in the very near future  https://www.tomshardware.com/pc-components/cpus/amd-ryzen-chief-teases-return-of-older-zen-3-chips-to-fight-soaring-ram-prices-thats-something-were-actively-working-on-right-now,buildapc,2026-01-26 17:59:00,5
AMD,o1uumyi,"If that's what you feel is wisest then go for it, personally I'd advise against passing up on a good used deal if you see it. Especially on eBay since it has damn near bullet proof buyer protections these days. My PC is mostly used and parts bin and is pretty much bulletproof so far. Built and booted first time without issue and cost me not much over a third what I would pay now for it in this economy.",buildapc,2026-01-26 17:41:04,3
AMD,o1ulk9m,"for 9800x3d you will need a new motherboard and ddr5 ram which at the moment is vary expensive so it is hard to recommend.   Buying used 5800x3d is not a bad idea if you can get it for fairly cheap. The are the parts that are among the hardest to destroy, so if you can get it from a ""trusted"" seller, you are good to go.  You said that it is similar price to 9800x3d... for 5800x3d I wouldnt pay more than 250$.   Good alternative is 5700x3d which is a bit slower than 5800x3d but are selling for quite cheaper. Both 5800x3d and 5700x3d are the best gaming cpus you can get without changing board or ram.       The third option is 5700x. It is an amazing value cpu since they are selling for around 90-130$. It is quite a bit slower than both 5700x3d and 5800x3d chips but it is miles ahead of what you currently have.       Any of these three options would be a good choice for reducing or eliminating the bottleneck you are seeing in some games...",buildapc,2026-01-26 17:01:41,1
AMD,o1uph29,Make sure you update bios before switching as old boards might not work with new cpus using old bios.,buildapc,2026-01-26 17:18:38,1
AMD,o1uqrzp,"Went from 5600x to 5700X3D last year, good upgrade for gaming. Anything from 3-5 series is better than your old CPU and is a good upgrade. Make sure your mobo can support it via bios update before switching.",buildapc,2026-01-26 17:24:12,1
AMD,o1uuf9o,"I know cost is a factor, but there is a good chance a used 5800x3d wont be terrible for you if you can overcome it. If it lasts 3 to 5 more years, is it really that concerning?  But otherwise refer to the other cpu recommendations.",buildapc,2026-01-26 17:40:04,1
AMD,o1vjdyo,5700x3D or else the usual 5700x/5800xt. Don't the 5700/G.  Any non x3D is still a significant upgrade vs the 2700x,buildapc,2026-01-26 19:25:55,1
AMD,o1vquo7,"I would look for an R7 5700x. Slightly cooler, should be cheaper and still available easily enough. The 5800x/xt are also options. You could get an R5 for similar performance for a bit cheaper, but if it were me I would look to a full 8-core (single CCD) Zen3 which will basically max-out your AM4 system (until your next full-build) for an average consumer/productivity on a budget.",buildapc,2026-01-26 19:58:14,1
AMD,o1we34p,If you could sell your platform and keep the ddr4 you could go lga1700 like the 14th gen 14600k and it's faster than ryzen 5000 and still affordable compared to moving to am5,buildapc,2026-01-26 21:41:15,1
AMD,o1x4zck,"5700X3D’s on ebay have dipped under £300, and you have buyer protection. Even if you don’t want to buy used, i think that’s how you maximise your money.",buildapc,2026-01-26 23:50:46,1
AMD,o1y02yy,5800xt is what i’d get no point of paying for  a  used and overpriced am4 x3d chips people are mental wanting these chips at the same or more of the cost of a 9800x3d….,buildapc,2026-01-27 02:36:03,1
AMD,o1ukina,"If 5700x3d and 9800x3d have the same price. It’s no brainer to jump to 9800x3d , but make sure you have the budget for ddr5 ram , and new motherboard.",buildapc,2026-01-26 16:57:14,0
AMD,o1uksz0,"Thank you, I'll look into them and do some benchmark comparisons and whatnot.",buildapc,2026-01-26 16:58:27,2
AMD,o1uknzb,"Okay, thanks. I'm sticking with the AM4 at the moment because I just don't have the money to a full overhaul like that, especially with RAM prices being what they are at the moment. Until that gets fixed, I'm just going to try to squeeze all the juice I can out of this. I found a 5800XT for about 200 quid which is within my budget so I'll probably just pull the trigger on that then. Cheers.",buildapc,2026-01-26 16:57:52,2
AMD,o1w2wut,"Oh that would be huge, and a massive relief, probably cant wait that long but if they drop enough in price might be worth upgrading to a 5800x3d if they do restart prod. Thanks for sharing!",buildapc,2026-01-26 20:51:34,3
AMD,o1v3lrg,"Even with the RAM apocalypse, there's not much reason to consider the 5800X3D at >$400, or the 5700X3D at >$350. Just not a smart purchase imo.  I would rather pay $180-200 for a 5800XT and save the rest for a platform upgrade in the future.",buildapc,2026-01-26 18:19:39,1
AMD,o1ukztj,"I do not unfortunately, and I don't think I have enough spare organs to sell either to fund it.",buildapc,2026-01-26 16:59:15,1
AMD,o1uq8yj,"Moving to 4K will put more load on the GPU, but will also result in lower frames per second.  Did you use something to determine that your 2700x is indeed holding you back?  (Do NOT rely on internet bottleneck calculators).  Did you use HWMonitor or AMD Adrenaline to determine that CPU was at full utilization during gaming?  According to [PCPartPicker.com](http://PCPartPicker.com), the 5600XT is available for $155 and the 5800XT is available for $200.  Single thread performance of the 5600XT is only 2-3% less than 5800XT.  Not enough to justify the $45 price increase for gaming.  If you were using Davinci Resolve, Adobe Premiere, or Blender, it would be worth it.  Then again, only you can decide how much it is worth it to you.  (A 5700G is in the middle price-wise, but is actually worse for gaming than either, since it sacrifices half it's cache to include the iGPU.)",buildapc,2026-01-26 17:21:58,3
AMD,o1vvsg9,I went from 2700 to 5700x and it is a very noticeable upgrade. In hindsight absolutely kicking myself for not spending the 100 extra for the x3d but it is still holding up fine.,buildapc,2026-01-26 20:20:00,2
AMD,o1um2et,"Go get 5700x, its should be work just fine.  On average 1080p, it has 10-20% performance loss compared to its x3d counter part.  On higher resolution, the gap become closer.",buildapc,2026-01-26 17:03:51,1
AMD,o1w1l85,"I run OBS from time to time to record sometimes but thats about it. My budget is anything under £300, but obviously better value is better value. The games I play have ingame graphs showing various usage and my CPU sometimes spikes to triple digit numbers which has more or less confirmed to me its the CPU (ive also had the System Monitor open and it goes to the moon).  I have seen sources say that moving to a higher graphic output lile 4k or even 1440p would lessen the load by shifting it to my gpu but even then i feel like its probably worth it to upgrade the cpu anyway before they go the way of RAM (even more than they are).",buildapc,2026-01-26 20:45:42,1
AMD,o1wbzwr,"Lessen the load, yes, but then the GPU becomes the bottleneck and you get lower framerates.  The reason the CPU utilization is lower is because you are generating fewer frames because the GPU can't output the same number of 4K frames/sec as 1080p frames/sec.  I mean if you have 100% CPU utilization and 15% GPU utilization, maybe you might still see the same number of frames., but if you have 100% CPU utilization and 50% GPU utilization, switch from 1080p to 4K results in 4x the work for the GPU.  While most people don't recommend bottleneck calculators, they can can show you RELATIVE performance.  While I wouldn't necessarily trust the actual frame rates and bottleneck percentages, they do show you want kind of shift you might experience going to a higher resolution.  The table below assumes a Ryzen 7 2700 and RX 6800 16GB playing Cyberpunk 2077 at medium settings.  As you can see, CPU becomes less of an issue, but FPS goes down as the GPU workload increases due to increased resolution.  |Resolution|Avg FPS|Min|Max|Playability|In-game Bottleneck| |:-|:-|:-|:-|:-|:-| |1920 × 1080|**63**|54|72|Good|CPU Bottleneck (47%)| |2560 × 1440|**57**|49|66|Okay|CPU Bottleneck (36%)| |3840 × 2160|**43**|37|50|Average|CPU Bottleneck (16%)|",buildapc,2026-01-26 21:31:59,0
AMD,o434xxh,At the right price. Maybe 200. Your monitor with 75 Hz in my opinion doesn't justify overspending on a CPU,buildapc,2026-02-07 14:46:27,29
AMD,o43eics,Honestly i'd do it and then try and hold on to it untill am6  / ddr6.,buildapc,2026-02-07 15:36:22,6
AMD,o43ju7m,"Not worth it.  Go AM5 or wait it out.  If you get a great deal on X3D, sure.  It's unlikely.  I will add that a good monitor is a great upgrade.  Recommend OLED, but they obviously cost more.  You can get decent models for $200 or so depending on what you want.  OLED starts about $400 and goes up.",buildapc,2026-02-07 16:02:24,5
AMD,o448a21,"Honestly if you're trying to skip AM5 all together, the 5700X3D is so worth it and along with 5800X3D the pinnacle of what you can get on an AM4 board. I did the same (when prices were still acceptable, 150€ I believe) and would probably also spend 300 for it today.  I paired the 5700X3D with a 4070 ti super and I can tell you that it's a nice fit.",buildapc,2026-02-07 18:02:55,3
AMD,o44i2bl,"This chip is a legitimate upgrade if you're playing ultra-modified Skyrim at 1080p, and trust me, you can feel the difference. Cache is more important than cores for this particular game. Just ensure that your A320 board's BIOS update supports this chip. If this chip is a little out of your price range, consider a used one, which may be a little cheaper. CPU bottlenecks cannot be fixed by updating your monitor.",buildapc,2026-02-07 18:50:59,3
AMD,o4346pm,"Damn bro that upgrade would be night and day, especially on 1080p.  I’d recommend upgrading your monitor to a 144hz.",buildapc,2026-02-07 14:42:15,6
AMD,o43hxr7,The biggest upgrade is spending 200€ on a lg 27gs85 (not the 75!),buildapc,2026-02-07 15:53:08,2
AMD,o43iy1x,"The first question I’d ask - do you live near a microcenter?  Second question, are you willing to try and sell your old kit.  I suspect you’ve got about $200 in sellable base parts of cpu/ram/mb.  Combine that with the $300 you were going to spend and see what AM5 combos you’d be able to afford.  At $500 + tax you’re looking at the 7700x combo with 32GB RAM or at $430 the 7600x3D with 16GB RAM.  I know it’s dependent on your access to microcenter and your willingness to hustle your old parts.",buildapc,2026-02-07 15:58:02,2
AMD,o44b99w,Use the money on a better monitor instead.,buildapc,2026-02-07 18:17:36,2
AMD,o43dnfp,"Last year 5700x3d was as low as 110$ on aliexpress (got myself 2 of them, also upgraded from 5600), it was best time to upgrade, but now it goes for over 300$ which seems way too much.  If its fine for you, then why not, 5700x3d is good cpu, not that you have other options with how fucked up market is.",buildapc,2026-02-07 15:32:06,1
AMD,o43j7kt,"Monitors are pretty stable, getting a 5700x3d is not as easy so I’d prioritize the CPU since if the AI bubble doesn’t burst soon parts are likely going to get harder to find at reasonable prices.  $200 is a little high, but not insane price and I’d be fine paying that with the current market.  It will be a big upgrade immediately and then you can save and take the time to find a good monitor.  OLEDs are getting cheaper every day, IPS high hz monitors are now getting really cheap because OLEDs keep coming down.",buildapc,2026-02-07 15:59:18,1
AMD,o43mqde,just buy a 1440p monitor with at least 120hz refresh rate and keep your cpu,buildapc,2026-02-07 16:16:40,1
AMD,o43nezl,I did from 5600x to 5700x3d a year and a few months ago. Planned to skip am5 with that,buildapc,2026-02-07 16:20:03,1
AMD,o43t5xs,"Out of curiosity, aren't the A320 motherboards not recommended for the X3D CPUs? Or can you upgrade without many problems?",buildapc,2026-02-07 16:48:15,1
AMD,o44615w,ugrade monitor,buildapc,2026-02-07 17:51:51,1
AMD,o447l7l,"Tbh I think it's more worth it if you upgrade your monitor even to a 1080p 180z monitor but I recommend 1440p 180z if you can find one for $200 to get most of your 4070 super. You won't notice the difference of the 5700x3d if your limited by your monitor anyway. That will be more noticeable and your 5600 is still fine in 1440p too since I don't have any issues with it on my 1440p monitor with a 5060ti 16GB playing cyberpunk with path tracing, DLSS balanced and 2x framegen or even space marine 2 which is more CPU intensive especially if there's hordes of enemies for example.",buildapc,2026-02-07 17:59:30,1
AMD,o4496x5,spend the money on a new monitor,buildapc,2026-02-07 18:07:24,1
AMD,o44omd0,"I’ll be honest, those CPU’s are hard to find at this point. It’s probably not worth it, unless your CPU dies and you need a new one.",buildapc,2026-02-07 19:23:59,1
AMD,o44rqqg,I would just get a 1440p 144hz monitor,buildapc,2026-02-07 19:40:15,1
AMD,o457trz,Are you really CPU bound? Your monitor limits you to 75 FPS. Certainly your 5600 can hit that framerates on a 15 year old game. I run skyrim on the 5700g and it performs fine. Not sure what my FPS is though.,buildapc,2026-02-07 21:06:44,1
AMD,o45ur7z,"Not sure how that poor board will handle it, I'd be careful. Monitor temps if you decide to upgrade, especially on the VRMs.",buildapc,2026-02-07 23:13:06,1
AMD,o435hwk,Yes its very worth upgrade and your going with x3d which it will boost your performance in games more,buildapc,2026-02-07 14:49:30,1
AMD,o438029,"Yes, upgrade and sell the 5600 once you are satisfied with the x3d. That will be a very nice system.  After that you can consider upgrading your monitor.",buildapc,2026-02-07 15:03:01,1
AMD,o439o7b,These CPUs are not available at 200,buildapc,2026-02-07 15:11:51,13
AMD,o48fsj1,"In my area, its impossible to find this CPU in that price range, its like a bigfoot sighting even if there's one. A used  5700x3d normally goes for 340+. I really wish I upgraded my cpu earlier when the price was only 170 last year",buildapc,2026-02-08 10:54:25,1
AMD,o48h7cw,Thank you!,buildapc,2026-02-08 11:07:37,1
AMD,o48h0bh,Thank you!,buildapc,2026-02-08 11:05:46,1
AMD,o48grwn,Thank you!,buildapc,2026-02-08 11:03:35,1
AMD,o48gzf1,Thank you! Do you think that this A320 board is enough for this CPU?,buildapc,2026-02-08 11:05:31,1
AMD,o48gty7,Thank you!,buildapc,2026-02-08 11:04:07,1
AMD,o48ic82,Thank you!,buildapc,2026-02-08 11:18:05,1
AMD,o48gnqd,"Unfortunately we don't have a microcenter in my area, I live in SEA. This is actually my plan earlier this year, sell my current AM4 platform then buy a new AM5 setup, but due to the circumstances, I didn't do it, Also, DDR5 prices here have skyrocketed overnight from 1st week of January til now, a 32gb of DDR5 kit normally goes for 450+ bucks now",buildapc,2026-02-08 11:02:30,1
AMD,o48iahx,Thanks!,buildapc,2026-02-08 11:17:40,1
AMD,o48hf9a,"Thanks, yup, I remember when this CPU was only 170 bucks last year before AMD announces it's end-of-life. I really wish I bought it then",buildapc,2026-02-08 11:09:38,1
AMD,o48hmjr,"Thank you, would you pay 310 bucks for this CPU in this situation?",buildapc,2026-02-08 11:11:29,1
AMD,o48i3c0,Thank you!,buildapc,2026-02-08 11:15:47,1
AMD,o43xlec,Did you see much difference?  What you did you have?  I have a 5600x and 6650xt. Not sure if going for a 9070 or 5700x3d would give more bang for the buck,buildapc,2026-02-07 17:09:59,1
AMD,o48hpkw,Thank you!,buildapc,2026-02-08 11:12:16,1
AMD,o48iczq,Thanks,buildapc,2026-02-08 11:18:17,1
AMD,o48idir,Thank you,buildapc,2026-02-08 11:18:25,1
AMD,o48id90,Thanks,buildapc,2026-02-08 11:18:21,1
AMD,o48i0cb,"Yup unfortunately, I'm using a heavy Wabbajack modlist (it's sometimes heavier than a typical modern games release these days) and its really obvious that the CPU is the limiting factor",buildapc,2026-02-08 11:15:02,2
AMD,o48h66i,"Thank you. Yeah, I'm kinda uncertain too if my current motherboard will suffice for this cpu",buildapc,2026-02-08 11:07:18,2
AMD,o48h8fr,Thanks you!,buildapc,2026-02-08 11:07:54,1
AMD,o441ewz,"They were, and at current prices they are not worth it, which is my point. I have seen them for 260 yesterday, but hard to justify. At 300 you need a strong reason to grossly overpay it",buildapc,2026-02-07 17:28:55,2
AMD,o48g5h7,I guess now it makes zero sense to.buy it.,buildapc,2026-02-08 10:57:47,1
AMD,o4bcaep,Yes,buildapc,2026-02-08 20:40:45,1
AMD,o4a0grx,"Smooth 75hz > stuttery 180hz, so likely yea because CPU bottlenecks often present themselves worse and really take me out of the experience.  It depends though, it's kind of a personal preference, but I find most people appreciate smooth frametimes over raw performance.    Edit:  Let me put the caveat that if your current CPU isn't giving you any issues, you might be better off with the monitor.  A higher refresh rate is also tougher to hit with CPU limits, so you could still argue get the CPU first, monitor second.",buildapc,2026-02-08 16:50:43,1
AMD,o45jhdz,"i mostly noticed a difference when pushing on high res for a handful of games.  similar to what many report as a benefit from the cache size difference.  its not to say that its necessary, but i also got mine when they were very cheap doing one of those aliexpress deals.  if i had to pay lets say double now, i probably wouldnt want it unless i was really trying to push my system to last for as long as possible, or if you were really aiming for high res on a really strong gpu.  fyi, i have a radeon 9700xt now, but at the time of the change i was on a 2080ti.  cyberpunk for instance was an improvement as it has more dependency on cpu.  moving to the 9070xt however the 5700x3d certainly offers a little more headroom, and the 5600x would have caused some bottleneck at lower resolutions (if i played at lower than 1440p on anything)",buildapc,2026-02-07 22:09:17,1
AMD,o45sv31,"The more expensive spec is ideal compared to having 8gb vram, you probably would need the performance for rdr2.",buildapc,2026-02-07 23:02:05,2
AMD,o465od3,"About three years ago I had a r5-5600, an A520 motherboard, 32gb of ddr4-3200 CL16 memory, and a RX6600.  With that I played games such as RDR2 using optimized high settings and Cyberpunk 2077 with optimized high settings with quality upscaling on a 1440p monitor averaging about 75fps.  Currently I have replaced the RX6600 with a RX9060XT 16gb and things just play better because of the improved GPU strength.  I played Star Wars Outlaws using optimized high settings with medium RT and quality upscaling and for me it looked great.  Keep in mind the R5-5600, 5600T, 5600X, and 5600XT are all the same CPU with a minor difference in boost clock.  Performance wise the difference from top to bottom is only about a 3% difference.  If you are thinking about a R5-5500 I would skip it if you are able.  Overall for gaming it has about 20% less headroom than the 5600 series as it's a different CPU altogether.",buildapc,2026-02-08 00:21:18,1
AMD,o47jziv,"Id go for the 16gb card. Vram limitations suck and just having more is better, plus it means the card will last longer and when the time comes 8gb isnt enough who knows if youll find a larger card thats not astronomical in price.",buildapc,2026-02-08 05:59:01,1
AMD,o4ba6vn,"Get the first one, but with the GPU from the second one. That setup will be your best bang-for-buck",buildapc,2026-02-08 20:30:08,1
AMD,o47k5ax,Hell a 1080 could get you by on high/medium mix with good framerates. My i7-4790 rig i put together 2 weeks ago could push in the 50s and that was with a bunch of ultra settings too. I still would say the 16gb card for some future proofing would be better.,buildapc,2026-02-08 06:00:24,1
AMD,o4937bg,"and how long do you think it could last me compared to 8gb vram? if I want to upgrade I'd have to build a completely new pc because of the am4 motherboard which is dead apparently  in such a case, I am thinking about getting the cheaper one since I'll have to upgrade it some day anyway. so why pay so much money for my first build that's not so great in the long run and can't really upgrade it??",buildapc,2026-02-08 13:55:16,1
AMD,o3xdkn4,"RM850x is slightly more but is made with better components, would recommend that over the 850e.",buildapc,2026-02-06 16:35:05,1
AMD,o3xe4wi,"I created some room in your budget, now you can get the OLED if you want.  Main thing is that you don't need CL30 RAM, it makes like a 1% (or less) difference compared to CL36 when you have a X3D CPU. Save the 100 euro.  https://de.pcpartpicker.com/list/Tcddn2",buildapc,2026-02-06 16:37:40,1
AMD,o3xh2p6,"Your picks honestly look more like you've chosen more about hitting the 2500EUR mark rather than things making sense.   You don't have BAD choices here, but you've got some needless spending. A 9800X3D does NOT need a 360mm AIO (I've left it, but chosen a cheaper one), for instance. Or like, you can get a faster SSD for less money. You don't need a five pack of fans if the chassis already comes with four. And like, I cannot stress this enough - CL30 RAM is optimal, sure. Back when CL30 could actually be had for less than higher CL values, there wasn't a reason to NOT get CL30. But in this market, it is **NOT** worth paying additional money for it. Especially not when you can save 100EUR by using CL36.   Here's what I would do:   [PCPartPicker Part List](https://de.pcpartpicker.com/list/gwM48Z)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor](https://de.pcpartpicker.com/product/fPyH99/amd-ryzen-7-9800x3d-47-ghz-8-core-processor-100-1000001084wof) | €444.89 @ Proshop  **CPU Cooler** | [Thermalright Frozen Prism 70.4 CFM Liquid CPU Cooler](https://de.pcpartpicker.com/product/99XV3C/thermalright-frozen-prism-704-cfm-liquid-cpu-cooler-frozen-prism-360-black) | €51.39 @ Amazon Deutschland  **Motherboard** | [MSI MAG B850 TOMAHAWK WIFI ATX AM5 Motherboard](https://de.pcpartpicker.com/product/w6dMnQ/msi-mag-b850-tomahawk-wifi-atx-am5-motherboard-mag-b850-tomahawk-wifi) | €199.00 @ Amazon Deutschland  **Memory** | [Crucial Pro Overclocking 32 GB (2 x 16 GB) DDR5-6000 CL36 Memory](https://de.pcpartpicker.com/product/QF9wrH/crucial-pro-overclocking-32-gb-2-x-16-gb-ddr5-6000-cl36-memory-cp2k16g60c36u5b) | €406.99 @ notebooksbilliger.de  **Storage** | [Lexar NM990 2 TB M.2-2280 PCIe 5.0 X4 NVME Solid State Drive](https://de.pcpartpicker.com/product/HJpD4D/lexar-nm990-2-tb-m2-2280-pcie-50-x4-nvme-solid-state-drive-lnm990x002t-rnnng) | €234.90 @ notebooksbilliger.de  **Video Card** | [ASRock Challenger Radeon RX 9070 XT 16 GB Video Card](https://de.pcpartpicker.com/product/Q8sMnQ/asrock-challenger-radeon-rx-9070-xt-16-gb-video-card-rx9070xt-cl-16g) | €697.90 @ Mindfactory  **Case** | [Lian Li LANCOOL III RGB ATX Mid Tower Case](https://de.pcpartpicker.com/product/WggFf7/lian-li-lancool-iii-rgb-atx-mid-tower-case-lancool-3r-x) | €150.36 @ PC Componentes  **Power Supply** | [ADATA XPG Core Reactor II VE 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://de.pcpartpicker.com/product/PChv6h/adata-xpg-core-reactor-ii-ve-850-w-80-gold-certified-fully-modular-atx-power-supply-corereactoriive850g-bkcus) | €89.90 @ Alza   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **€2275.33**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2026-02-06 17:51 CET+0100 |",buildapc,2026-02-06 16:51:22,1
AMD,o3xaziq,"I'd get the cheaper XFX Mercury over Gigashyte. I've only had two Gigabyte GPUs and both had problems in the first month of owning them, one completely failed from 110 degree memory. Gigabyte also love using sleeve bearing fans that aren't designed for horizontal mounting and don't last long.  AMD partners tend to make much better AMD cards than the other GPU-makers. ASUS can be hit and miss.  Otherwise just look at cheaper RAM, cheaper PSU, etc.... to get a QD-OLED.  Maybe even downgrade from a 9800x3d to a 7500f. FPS at 1440p native will probably be the same between the two CPUs with a 9070 XT, but the display will look much, much better.",buildapc,2026-02-06 16:23:12,1
AMD,o3xiqj4,"Would it be worth it to get a CL36 DDR5-6400 instead?  There's other issues I have with getting an OLED (that I won't go into detail), so it's not really something that I'm fixated on",buildapc,2026-02-06 16:59:10,1
AMD,o3xmbqi,"I'm not hellbent on an AIO to be honest, this is my first time building a PC from scratch and last time I had a desktop tower was early 2010s, so I'm not very familiar with the current ""meta"".  As per another person's comment, I dropped the RAM to G.Skill DDR5-6000 CL36 instead of 30  Would you say an air cooler for a 9800x3D is more than enough? If so, any recommendations?  Also, a lot of these components were chosen due to their availability at the merchants I feel comfortable buying from",buildapc,2026-02-06 17:16:18,1
AMD,o3xmmg3,"No it hardly makes a difference when you have the X3D. I mean if it doesn't cost anymore sure, but it doesn't make much difference.",buildapc,2026-02-06 17:17:45,2
AMD,o44u14h,If you just gonna game upgrade to 5600x and have fun. 5600x=5900x for game performance.,buildapc,2026-02-07 19:52:11,2
AMD,o449yky,"CPU upgrade is a good idea, your motherboard will support a ryzen 5000 series after a bios update.  Just about any 5000 series will be a good option, expect for the 5500, 5700, or any one that contains the letter 'G'. If you want to keep the same number of cores, you will want a 5700X or above.",buildapc,2026-02-07 18:11:10,1
AMD,o44899z,"Absolutely upgrade your CPU to something like a 5700X / 5800X or 5800XT, depending on prices in your region (performance of those 3 is pretty similar, so you could just go with what's cheapest, but the 5800XT is the best AM4 gaming CPU without considering X3D CPUs).",buildapc,2026-02-07 18:02:48,0
AMD,o4doxjw,"I'd go AM4 especially since your planning on a 14th gen Intel and the 12th gens are not great.. trust me but if you find a really good deal on the I7 14700 go for it.  Your gpu bottlenecks your CPU at 1080 and 1440p, I'd suggest at least a 7700xt or 7800xt with your CPU even for editing, and what's your monitor? I'd suggest scouting for a good budget 1440, ktc makes one for 145USD 180hz 1440p 27"" IPS.  If you can spend more on a monitor I suggest getting a miniled especially for editing the ktc m27t6 is the one I have and it's great 1152 so barely any blooming and not noticeable in editing or gaming also hdr1400",buildapc,2026-02-09 04:26:33,1
AMD,o4dqpg5,yeah i though about AM4 but in mongolia its hard to find a cpu for AM4 i could find only AM5 cpus and its not that common in mongolia to sold out fast and i7 14700 is 297 usd in market and i thought about shiping from USA or other countries but shipping fee like 200 usd,buildapc,2026-02-09 04:38:42,1
AMD,o4dqyj8,and one question 7600 will bottleneck you said but what about 9060xt is it enough,buildapc,2026-02-09 04:40:27,1
AMD,o4dsx7r,Oh that's fair I'm in AUS so it's a lot easy for me but the Intel's a fine chip still but you might want to read some of the 14th gen horror stories on Reddit.  Also the Arc B580 is also a good graphics card if you can't get a 9060xt 16g or a 7800xt,buildapc,2026-02-09 04:54:16,1
AMD,o4ds9ko,Oh yeah a 9060xt would be more than enough as long as you keep in mind your video editing and gaming,buildapc,2026-02-09 04:49:37,1
AMD,o3zhooy,That's a nice upgrade 😁 i felt the same way when going from an am3 to an am5. Felt like I stepped into the future lol,buildapc,2026-02-06 22:47:50,3
AMD,o3zj9vj,"At least 2x better, it’s a good jump 👍",buildapc,2026-02-06 22:56:26,2
AMD,o3zx34x,>how big is this jump?   You should figure that out *before* you spend the money.,buildapc,2026-02-07 00:15:33,1
AMD,o3zjlrg,"Yeah I got a 32in 1440p VA Panel. I wasn't impressed that the 1080ti & 16GB RAM were big limitors as I had to set a few to 900p to get high/ultra, I was more shocked at how well the 1600x was holding up.   Felt amazing 100%(Story wise) DMC 5 at 1440p/Ultra without having to tweak shit like I did with my past rig.",buildapc,2026-02-06 22:58:15,3
AMD,o3znc1k,"I only had a fx8350 with an rx560, and I spent more time tweaking things instead of playing games lol. After building my am5 system, I saved up and bought an rx6800 and I felt spoilt 😂",buildapc,2026-02-06 23:19:07,2
AMD,o40t6bp,The FX8350 lasted my bud until earlier this year.  14 goddamn years.  Only with UE5 which is a gigantic piece of shit did it ever actually get unplayable.  Everything else was carried on the shoulders of a 1060 6gb.,buildapc,2026-02-07 03:32:30,2
AMD,o3vr9fu,If you want ai and an amd card the 7900xtx might be worth considering due to the extra vram.,buildapc,2026-02-06 11:00:13,3
AMD,o3vj7ky,"I'd def avoid Asrock (and probably Asus?) due to those motherboards killing 9000 series cpus.  What do you mean by ""two cards""? You'll need to look at potential motherboards to see how they allocate pcie lanes to slots to see what happens when you add more than one pcie card.",buildapc,2026-02-06 09:46:44,2
AMD,o3vk1z0,Are you sure the ai models you want to run will work well on the AMD GPU?,buildapc,2026-02-06 09:54:45,2
AMD,o3vl5jp,"You could ditch noctua for thermalright, cools better for fraction of the price  AI will work, but slow i assume",buildapc,2026-02-06 10:05:03,1
AMD,o3vnnc3,"Would recommend alternate case with more airflow.   ASUS or MSI motherboard with one of the following chipsets: B650E, X670E, X870, X870E",buildapc,2026-02-06 10:28:05,1
AMD,o3wf6fh,"That Noctua cooler is meant for Threadripper / EPYC chips, not Ryzen. Get a Thermalright Aqua Elite V3 360 for $50.  Gigabyte B850 Aorus Elite Wifi7 is the overall best motherboard available. x870 chipsets are purely for specialized connectivity",buildapc,2026-02-06 13:43:55,1
AMD,o3wopsw,which gigabyte board do you recommend?,buildapc,2026-02-06 14:34:23,1
AMD,o3vjv98,"Your PSU is too large, unless you want to keep room for a much more power hungry gpu in the future.",buildapc,2026-02-06 09:53:00,1
AMD,o3vknq7,"16GB VRAM seems very limiting. If you are dead set on AMD card, get the pro line with 32GB VRam. You might not need it now, but you will in the future. Models will only want more and more vram.  If you're dead set on 64GB DDR5, you must have a deep wallet so go all the way with the AMD pro card.",buildapc,2026-02-06 10:00:26,1
AMD,o3vkemg,Thank you. Are there sources for this? my google fu seems weak today,buildapc,2026-02-06 09:58:02,1
AMD,o3vkd6y,Yes,buildapc,2026-02-06 09:57:39,3
AMD,o3wftxl,"All b650 chipsets are outdated -> bad availability, higher prices.  Gigabyte has easily the best customer satisfaction this generation. X chipsets are useless if you don't need special things.  B850 is what most people should buy, as long as it's a good board otherwise (like Gigabyte's lineup, having PCIe 5.0 GPU on pretty much every board)",buildapc,2026-02-06 13:47:26,1
AMD,o48luf9,I am using that exact case. Airflow is awesome.,buildapc,2026-02-08 11:49:50,1
AMD,o3vkbfw,I might use 2,buildapc,2026-02-06 09:57:11,1
AMD,o3vmold,"My wallet isn't too slim, but I am no millionaire",buildapc,2026-02-06 10:19:12,1
AMD,o3voh4w,"https://videocardz.com/newz/asrock-confirms-internal-reviews-after-multiple-reports-of-dead-ryzen-9000-cpus  For determining pci-e lane setup, in motherboard manuals there's always a spec summary telling the layout, and sometimes a block diagram. For example the X870E AORUS ELITE WIFI7 motherboard:  From Product Specifications table:  * https://i.imgur.com/fAxhaxj.png  We can see under Expansion Slots that the main gpu slot PCIEX16 is attached directly to the cpu (doesn't share pcie lanes with anything else), and it runs at pcie 5.0 x16 lanes unless there is an M.2 ssd in M2B_CPU or M2C_CPU connectors, in which case it runs at x8.  The other two pcie x16 slots are pcie 4.0 and not full x16 lanes - they are x16 size, but only have x4 lanes hooked up to each.  So this would be a terrible board to use two GPUs on, since it only has a single gpu x16 slot. The 9070XT 16gb is pcie 5.0 (twice speed of pcie 4.0) and uses all the x16 lanes, and would be running at 1/2 speed and 1/4 bandwidth on either of the other 2 pcie x16 sized slots.  So you'll need to look at various motherboards to find one that support 2 cards at full speed.  Edit: same info in diagram format: https://i.imgur.com/fpHvVYM.png  On the other hand, iirc gpu bandwidth mostly affects model loading, so running at x8 or even x4 might be acceptable if the model fits in gpu vram memory and you're not switching models often.  I don't know how gpu bandwidth affects model training, although I assume it's running data throught the gpu as fast as it can and thus you need the full x16 lanes speed.",buildapc,2026-02-06 10:35:37,1
AMD,o3vmon6,"For running multiple models at same time on different gpus, or parallelizing one model across 2 gpus - i saw a video that tested parallelizing a big model across 2 gpu cards took a big performance hit.",buildapc,2026-02-06 10:19:13,0
AMD,o3vnf7v,"16 VRAM is basically entry level for modern AI, but if it's a big upgrade from the current rig you will still be able to utilise AI with it.",buildapc,2026-02-06 10:26:00,1
AMD,o3w1rsa,A NVIDIA card with 32GB starts at 2500$ here (Switzerland)   An AMD pro Card is 1300 USD,buildapc,2026-02-06 12:22:20,1
AMD,o489reb,"The 9070 is the better card on paper. It has better raster performance and has access to FSR4 (which is quite acceptable) if you end up changing your mind on upscaling. If you play at 1080p you will be able to max just about everything and enjoy a high framerate. Even at 1440p you will be able to max all but the most demanding of settings and still get 60fps.     All that being said, if for some reason you dislike AMD, or prefer NVIDIAs feature set and support, the 5070 is not a bad card either.",buildapc,2026-02-08 09:57:34,76
AMD,o489dc4,If the DLSS stuff isn't a winning factory for you then there is a real argument for 9070 with its extra performance which can be tuned to get even closer to the XT and the additional v-ram that'll make it last longer without having to compromise on texture quality.  The only game I know where the 5070 beats the 9070 outright is Tarkov.,buildapc,2026-02-08 09:53:51,33
AMD,o48g03t,the finals is heavily cpu-bound. whats are your current specs?  Im running a 9070xt/9800x3d and its a great combo.  im at 1440pUW tho..,buildapc,2026-02-08 10:56:23,5
AMD,o4apgfz,5070.,buildapc,2026-02-08 18:48:37,3
AMD,o49kfue,"My vote would be the 9070 mainly for the VRAM. I went from a 3080 to a 9070 XT and it's been a very nice upgrade.  That said, holy hell the prices of these cards now. I got my 9070 XT Reaper two months ago for $580. That same card is now $800.",buildapc,2026-02-08 15:31:37,10
AMD,o49bkp4,I was in the same exact boat as you with the same prices and decided on the 9070 for $570 from micro center. I didn’t care about ray tracing that much and the 9070 has better raw performance and the 16gb of vram. The guy at microcenter said you can’t really go wrong with either but he would pick 9070 just bc of higher vram,buildapc,2026-02-08 14:44:23,6
AMD,o49m6ja,"Since you have a 2070 Super, the 9070 would be a bigger jump in performance, especially for future 1440p gaming, and the extra VRAM will probably come in handy.",buildapc,2026-02-08 15:40:30,6
AMD,o49iblp,"I have a 5070 and its great but I regret not getting the 9070 as most of the games I'm playing are older now apart from arc raiders so not much use of DLSS features. I did play through cyberpunk and if you want to experience path tracing I think the 5070 makes a better choice as Nvidia has a huge advantage with Path Tracing and RT, less so with RT that AMD does fairly well. Not like I'd say the PT experience is amazing, 50-45FPS native is rough although with 3x frame gen its playable but everything is smeared on fast camera movement and needs a lot of tweaks and mods to get PT to look good(base PT in cyberpunk is pretty underwhelming, I use a mod called ultra+ best performance and run PTNextV3 at medium preset and a couple more mods to remove PT rendering bugs). Both the 9070 and 5070 overclock and undervolt fairly well, I have my 5070 running at 3.05ghz 1.01v 220W so extremely efficient card (important for me as electricity pricing here is absolutely cooked).",buildapc,2026-02-08 15:20:42,8
AMD,o49g5g4,If you use gpu mainly for gaming rx 9070. If you have some productivity work beside gaming then rtx 5070.,buildapc,2026-02-08 15:09:19,3
AMD,o4a95c1,"if you’re at 1080p it won’t be much of a change but DLSS is genuinely approaching levels of magic for me.  I have a 5070 and it takes Half Sword (an Early Access buggy mess that runs as low as 20 FPS on some maps and turns it into a 100+ fos seamless experience (more with frame gen too)  I also have had poor experience with AMD gpu software, so team green for me.",buildapc,2026-02-08 17:32:48,3
AMD,o4b8c4t,For gaming: the 9070  For gaming *and* productivity: probably the 5070.,buildapc,2026-02-08 20:20:45,3
AMD,o4bkcim,1 those cards are also very close and they are mostly decided by technologies like dlss 4.5 MF6 PT and RT I know nVidia and amd have more VRam but honestly. I play at 3440x1440p on a 5070ti and I haven't seen a game that would it took me over 12.5gb Helldriver2 takes me 9gb of vram on ultra and supersampling better and more demanding than native,buildapc,2026-02-08 21:20:45,3
AMD,o489egz,Honestly RTX 5070. It is the most popular card of this generation for a reason. 12gb will be enough in all of the games in 1440p with the exception of ultra settings in some of the most unoptimized unreal engine 5 titles,buildapc,2026-02-08 09:54:08,22
AMD,o489rmr,"Just curious, why don't you care about dlss or fsr? I think it's a huge part in this gen cards.",buildapc,2026-02-08 09:57:38,9
AMD,o49wofq,Are this prices without taxes?,buildapc,2026-02-08 16:32:25,2
AMD,o4aieb0,I would say without any context the 9070 but with the info that you play the finals I would go with the 5070 because of reflex 2,buildapc,2026-02-08 18:16:39,2
AMD,o4b0hyh,If you're into AI at all 16GB will be bteer than 12GB always,buildapc,2026-02-08 19:41:54,2
AMD,o4buvs7,"Just like almost everyone else has said, based on your description and needs, the 9070 is your GPU. More VRAM and more raw power outweigh the benefits the 5070 offers to your specific needs. But the 5070 is also an excellent card if you decide to go that route. You won't have buyers remorse with either.  Although not mentioned, if you ever want to dabble in a Linux distro in the future, the AMD's GPU drivers are much better supported than Nvidia's in the land of penguins (Linux). Just figured I'd mention that since more people are getting fed up with Windows 11 by the day.",buildapc,2026-02-08 22:13:48,2
AMD,o4dj8so,"5070 imo. I love AMD, and it has better value for money in terms of raw power, but DLSS is lightyears away from FSR. I've read in your comments that you dislike relying on DLSS, but it's really hard not to when it's convenient and actually good now. Having an RTX 5000 series card will unlock DLSS4, which imo is so good, I rarely turn off DLSS nowadays. Look up DLSS 4.5 as well, while you're at it.",buildapc,2026-02-09 03:49:28,2
AMD,o49mn5e,5070 the plug and play champ,buildapc,2026-02-08 15:42:50,4
AMD,o49jwt7,5070 all day,buildapc,2026-02-08 15:28:53,6
AMD,o48g41d,"9070 if you plan to play 1440p ultra/max settings on AAA games in the future   Otherwise, the 5070 will still be good for a while",buildapc,2026-02-08 10:57:24,2
AMD,o49ygrt,"I was anti Upscaling until I got a new card. It’s gotten significantly better overtime. I think my mindset is that it’s a tuning feature just like texture or AA. Older game that I can max out my screen in native, skip it, newer title that I want to run higher fps, adjust it till I see diminishing returns, just like anything else. 9070 will have higher raster performance, and potentially longer lifespan with 16gb.",buildapc,2026-02-08 16:41:03,2
AMD,o4bfjwy,"5070, better features, way longer support, DLSS. Don't buy AMD unless it's waaaaaaaaaaaaaaaaaaaaaaay cheaper than Nvidia.",buildapc,2026-02-08 20:57:05,2
AMD,o4dtqgs,"9070 is 15-25% faster now,i made the jump from 9070 to 5070,games look way better have more stability drivers donr crash and dlss4.5 is a quality beast",buildapc,2026-02-09 05:00:04,1
AMD,o4bkr7z,"9070.  13% faster, uses 20W less power and has 4GB more VRAM.  (and isnt using the problematic 12V High Failure)",buildapc,2026-02-08 21:22:47,1
AMD,o4a5oxz,"Anyone even REMOTELY suggesting a 5070 is fucking wrong.  Less VRAM will always lose to similarly performing cards in the long run, NO question.  This isn't even a fucking contest, look at how well RDNA2 is holding up to the 30 series which is dying on 8gb of VRAM right now which has an even LARGER disparity of RT performance.  It's a matter of FPS over a curve.  The 9070 has more raw performance for one, so it's already a wash, but when VRAM minimum demands go up, the 5070 will fall on its fucking face the second it's exceeded.",buildapc,2026-02-08 17:16:04,0
AMD,o48f4fc,I hate microcenter,buildapc,2026-02-08 10:48:10,-8
AMD,o48i36p,thanks for the reply!  i'm slightly leaning towards amd honestly but it seems that i cant go wrong with either card which is making it hard haha,buildapc,2026-02-08 11:15:45,12
AMD,o4ak7r1,"Tbh haven't encountered a game yet that I can't max out with my 9070 on 1440p, though I've only played  Clair Obscur and Marvel Spiderman Remastered as ""heavy"" games. Not really interested with Cyberpunk, maybe I'll check Portal with RTX if it can go past 30 fps lol.  Edit: probably not, a 9070 XT barely clears 30 fps in 1440p in Portal with RTX.",buildapc,2026-02-08 18:24:56,5
AMD,o48hw1c,thanks for the reply :)  the tarkov part is interesting but i dont think i play enough for it to be a factor,buildapc,2026-02-08 11:13:54,5
AMD,o48h3m1,"yeaah my build is a bit older with a 5800x3d + 2070 super, i plan to upgrade later but that means getting a new mobo + ram later too, i get around 100-120 fps on 1080p for the finals on medium settings",buildapc,2026-02-08 11:06:37,3
AMD,o4cocqo,I'm regularly at 14 gigs+ VRAM usage on my 9070XT at 4K. I am playing Nioh 3 these days and it uses 11 gigs just by itself.,buildapc,2026-02-09 01:00:31,2
AMD,o4b9o74,"thanks for the story, itll probably end up looking similar for me haha",buildapc,2026-02-08 20:27:30,1
AMD,o4c48zl,thanks for the input :),buildapc,2026-02-08 23:05:21,1
AMD,o4b9ue9,"thanks for the detailed response, i think i was a little bit interested in raytracing, but yeah i dont wanna rely on frame gen",buildapc,2026-02-08 20:28:23,2
AMD,o4acjll,"In the handful of games where the 12gb wouldn't be enough at native, it will be plenty with DLSS. And as someone that can very much enjoy games on my AMD card that only has access to FSR3, which I'm told is trash by the Nvidia fans, I have to think DLSS is a no-brainer anyways.",buildapc,2026-02-08 17:49:12,6
AMD,o4c13yo,">12gb will be enough in all of the games in 1440p with the exception of ultra settings in some of the most unoptimized unreal engine 5 titles  yeah so a 9070 is better and can handle it regardless.  Like do you people fucking hear yourselves.  What's the better card.  The 9070.  ""But the 5070 is more popular"" because why?  Because Nvidia's propoganda.",buildapc,2026-02-08 22:47:20,4
AMD,o4cudbr,"This is what I was told when I was getting 3070. Nope, Vram is the crucial part. I can play with low shadows or tinker with the most effective AA but I won't budge on textures. 8gb killed 3070 for me. Textures are performance penalty free and can make the game look so much better, why would you do this to yourself.The number of affected games will only go up.",buildapc,2026-02-09 01:35:57,1
AMD,o48aohy,"For me.  How can I care about features that are either...  not in the games I play, (older games)  not necessary to reach my desired performance (Doom Eternal as an example)  or will negatively impact how ****I**** performe in a game (esports games)",buildapc,2026-02-08 10:06:16,10
AMD,o48hbl0,"i was never a huge fan of how upscaling looked, even with the newer transformer models for dlss, i just like how my games look motion wise without it, but i do see the appeal for games like Cyberpunk tho. but yeah pretty much personal thing honestly",buildapc,2026-02-08 11:08:42,1
AMD,o4b9zo7,"yeah without fees, just what the store has them listed at",buildapc,2026-02-08 20:29:07,1
AMD,o4c4n9f,honestly me being able to run linux in the future might be the deciding factor here so i appreciate you telling me that haha  my only issue with linux is not being able to play games with kernel anti cheat which is what i've heard is a big issue for others. me personally i might be down to dualboot linux and windows but i'll have to look into it more  thanks for the input :),buildapc,2026-02-08 23:07:43,1
AMD,o4dq6f7,"hey thanks for your reply :) i updated my post with more info but the DLSS vs FSR discussion only really influences my choice by a little bit because realistically i'd only be using upscalers like 5% of the time if i'm being generous, but that's pretty much just bc of my personal preference. i do see how others benefit from it though !",buildapc,2026-02-09 04:35:02,1
AMD,o4b9ma7,thanks :),buildapc,2026-02-08 20:27:13,1
AMD,o48hgh7,"thanks, ill keep that in mind for the 9070, it is def something im thinking about",buildapc,2026-02-08 11:09:57,1
AMD,o4ba9wh,"thanks for sharing your story, i think in some games i can definitely use it like Cyberpunk but for my main FPS based games, my main issue with upscaling is the lack of motion clarity, which for me personally i can definitely notice it when playing games like The Finals, they've definitely improved on responsiveness but the visual blur is still noticable imo",buildapc,2026-02-08 20:30:33,1
AMD,o4b8ohl,"Except some people prefer DLSS 4 or 4.5 over FSR4, prefer long-term NVIDIA support over potentially questionable AMD support. Also, some games still perform better on the 5070 over the 9070. Admittedly, these examples are rare, but one is Alan Wake 2. Also, depending on models available, the 5070 is usually still cheaper than the 9070, and for some people every dollar saved matters.",buildapc,2026-02-08 20:22:29,2
AMD,o48hqfw,damn :(,buildapc,2026-02-08 11:12:29,4
AMD,o4afzy3,Wheel decide it. Bo3,buildapc,2026-02-08 18:05:25,5
AMD,o4brekq,Spiderman 2 is why I added the caveat. I just finished that game and some of the RT stuff is heavy lol,buildapc,2026-02-08 21:55:52,2
AMD,o4as86b,"keep the 5800x3d, go for the 9070/5070",buildapc,2026-02-08 19:01:32,2
AMD,o4acwz4,And you will still be limited with rastor performance in these games to fully utilize vram. That is why rtx 5060ti 16gb vs 5070 is not a debate and you should always take rtx 5070 at the same price,buildapc,2026-02-08 17:50:57,3
AMD,o48fxfg,"DLSS 4 years ago had already shown to decrease input latency on games… if that’s not a good thing for esports games idk what is. DLSS has only improved since then. It’s literally just free FPS at this point.   Frame gen / MFG on the other hand, yeah “fake frames” add latency.",buildapc,2026-02-08 10:55:42,16
AMD,o48fcsw,This guy is brainwashed,buildapc,2026-02-08 10:50:20,-4
AMD,o4bb2g2,"Every, single one of your exceptions is basically just smoke.  Questionable AMD support has never manifested in a way that has fucked people over.  Also Nvidia drivers are dogshit right now it's not even a joke, even 12+ months after the fact.    Sure, the 5070 is better at RT (and only marginally so), not that it matters when its VRAM dies sooner than later.  Also neglecting that Alan Wake 2 is a PT/RT heavy game.  RT is still a gimmick and can be turned down significantly, and the 90 series is basically on par with the 40 series in terms of RT performance while having VRAM advantages.    Sorry but a 5070 being cheaper than a 9070 is not the W you think it is, because the virtue of spending more now to spend less later is the difference between a poor person who gets the most out of their dollar and someone perpetually fucking poor and suffering for it.  For the record, I bought a 9070xt for 580 just before the GPU prices kicked in, which in absolutely no way can ever get beaten by a 5070 in practical circumstances for gaming.    VRAM is king.  It is the #1 determinator of functional performance for every GPU since the fucking late 90s.  Take it from someone who grew up then and has known people who were actually alive since well before then to know.    This shit isn't rocket science.  The 5070 is a loser's purchase.  For gaming, the 9070 trounces it in every conceivable manner aside from MFG (which ALSO costs VRAM, so as soon as its core native FPS is reduced and or VRAM is exceeded, it cannot benefit from it whatsoever).  For productivity, the 12gb of VRAM is a total wash, in both machine learning and regular productivity.  A 5060 ti 16gb would likely be a better purchase in those situations.",buildapc,2026-02-08 20:34:34,0
AMD,o49lea3,"They're probably just one of the folks who get salty when MC pricing is posted here and on the sales subreddit. Lots of folks think MC links shouldn't be allowed in the sales sub but I think someone found that roughly half the country lives within an hour of a Micro Center, so it sucks to be the other half that doesn't I guess.",buildapc,2026-02-08 15:36:31,2
AMD,o4afezz,"I had someone trying to argue 5060ti 16gb over the 5070 12gb a while back, and they just couldn't understand why the 5070 is still a much better card. For one thing, the memory timings and bandwidth mean the 5070 can actually hold more stuff in VRAM over time than the 5060ti. Like, it can churn through the VRAM and replace what needs to be replaced faster than the 5060ti can simply use what's being stored in the VRAM.",buildapc,2026-02-08 18:02:48,4
AMD,o4b2r34,It's extremely obvious to me when DLSS/FSR is on (I'll admit I haven't tried DLSS 4.5). They have their uses but a long way from 'free fps' imo.,buildapc,2026-02-08 19:53:02,-1
AMD,o48fuof,?,buildapc,2026-02-08 10:54:59,9
AMD,o4bd37n,"The entire reason I responded was that you said there was NO REASON to pick the 5070 over the 9070, which was hyperbole.     You failed to address the DLSS vs FSR point.    The questionable AMD support is literally fucking RDNA 2 & 3 owners over right now since AMD won't and refuses to release FSR4 on those cards, even though it's proven that it can work (see H.U.B. latest video for TLDR).    Alan Wake 2 performs better on NVIDIA. That was not an opinion, it was a fact. I used it as a point that a handful of games will still be better on NVIDIA GPUs. The inverse is true for AMD, too, where some games will just run better there, but it's worth considering depending on what someone specifically wants to play.   Something being cheaper is always worth considering. People who recommend PC builds always do this, where they suggest ""oh for 50 more bucks you could get...."". Budgets are budgets. If the 5070 is cheaper in by $50 or more, then it is worth considering. Likewise, I would suggest a 9060XT if someone had a $450ish budget and couldn't afford the next tier up. In this case, it appears OP found the two within $20 of each other, and hence, the price isn't much of a consideration.",buildapc,2026-02-08 20:44:46,3
AMD,o4c5p0q,"thanks for the input :)  i always ran games on lower settings so i never appreciated the vram difference, but i was curious if gddr7 (nvidia) vs gddr6 (amd) is something i should worry about? regardless i appreciate reading your takes on each card ^^",buildapc,2026-02-08 23:13:47,2
AMD,o49nxsa,Or you're just not from the USA like me. The prices you guys have are absolutely insane.,buildapc,2026-02-08 15:49:19,1
AMD,o4b0x47,"For AI higher VRAn always allows for the possibility of bigger models, but yes for a model that can be ran with both 16gb vram and 12GB vram, the 5070 is faster",buildapc,2026-02-08 19:43:58,1
AMD,o4baeov,what quality level were you using? dlss 4 dlaa and sometimes quality and dlss 4.5 balanced and up is the best AA other than super sampling now. supersampling is an incredible performance hog and as such isnt really viable,buildapc,2026-02-08 20:31:13,3
AMD,o48wn7w,"Upscalers decrease input latency, not increase it. It will not make you performance worse in e-sports, but better.",buildapc,2026-02-08 13:13:34,4
AMD,o4c5a56,thanks for the input :)  i'm not a big upscaler person but it was interesting to read what's happening on both sides with it regardless. also thats really stupid they won't release fsr 4.0 on older cards   and yeah the two options are so similarly priced for me its nice to read all the takes everybody has on the respective card,buildapc,2026-02-08 23:11:26,1
AMD,o4ci69k,"Nope, while there are performance differences between the GDDR chips, it has very little bearing on the overall effect of what makes one chip better or worse than the other.  It's basically like comparing an audi sport sedan to a honda sport sedan, even if one component ""spins faster"" it doesn't necessarily mean that car will make more power in general, or overall.",buildapc,2026-02-09 00:26:36,2
AMD,o49oxaj,Well prices in general have gone up in just the last few months. The price OP is looking to pay for a 9070 is just $10 less than I paid for a 9070 XT back in early December. This same card was only $500 back then if memory serves. The card I bought for $580 on sale is now $800.,buildapc,2026-02-08 15:54:14,3
AMD,o4blc9j,"I've tried more or less every setting across several games. Upscaling never looks as good to me as native and not worth the frame rate boost - I'd just rather turn settings down elsewhere.  This is just my opinion, not sure why I was down voted before.",buildapc,2026-02-08 21:25:39,2
AMD,o48yw4x,"Not performance, how I perform. Fps doesn't mean anything if I can't see a far away enemy because is getting distorted by fsr3 or whatever CS2 is using.",buildapc,2026-02-08 13:28:22,9
AMD,o4bsgpj,Whats your opinion on frame gen if you find even upscaling very noticable?,buildapc,2026-02-08 22:01:12,1
AMD,o49uskr,CS2 only has FSR1. It's nothing at all like the latest models.,buildapc,2026-02-08 16:23:15,1
AMD,o494wxz,It's probably just that game. though I use an Nvidia GPU and don't play CS2. DLSS looks really good in the games I play.,buildapc,2026-02-08 14:05:36,0
AMD,o4a2zns,Which I don't use either.,buildapc,2026-02-08 17:02:58,4
AMD,o4a6je7,"If you haven't used them then you can't know what you're talking about. Yeah, don't use upscalers in games that are using outdated models. Anything that's using the latest tech only benefits. The new models often surpass native image quality when you use super sampling or the highest quality setting.",buildapc,2026-02-08 17:20:10,4
AMD,o3xm7wu,"If I had 3 grand to put out, I’d probably be trying to find a prebuilt with a 5080 or catch one on buildapcsales",buildapc,2026-02-06 17:15:48,3
AMD,o3y0sss,"The parts list is good, but I feel like it's too expensive, even given the price increases. You can find prebuilts for cheaper.  You can probably find ram for closer to 350. Perk of 3d chips is they depend less on fast ram, so you can use that to save money a bit. Can most likely find cheaper PSU.   Can try looking for a CPU bundle to cut some costs. If budget isn't a concern, you can pull trig. It's a good build.",buildapc,2026-02-06 18:25:15,3
AMD,o3xn07a,"If you can afford it this is a very solid build, basically the same as mine and I've been loving it",buildapc,2026-02-06 17:19:35,2
AMD,o3znfll,This is almost exactly the build I have. Like others mentioned definitely go for the Microcenter bundle if there's one you can reach- you'll save one or two hundred there.  For aio the sub $100 ones are great now and look nicer IMO. With how much you're already spending I'd probably send it. Peerless assassin is totally fine for a 9800x3d though if you like air cooling.   I think the 9070xt is right for long term high/epic 1440p for a min 60fps especially with how demanding new games are. I also thought about the 5080 but the price premium was seemed absurd for the performance gains. Especially since Linux works out of the box with amd.   Enjoy!,buildapc,2026-02-06 23:19:42,2
AMD,o3zqsau,Thanks everyone - I did it (with a few tweaks).  Treat Yo Self!,buildapc,2026-02-06 23:39:09,2
AMD,o3xn6x6,Thanks for the tip. I am trying to avoid nVidia this time around though.,buildapc,2026-02-06 17:20:28,1
AMD,o3zrncz,"Thanks for the tip. I did check the Microcenter bundles but my closest one is Boston and with the drive time and gas it didn't seem worth the ~$200ish discount. Plus the bundle the mobos on the offer weren't really what I wanted. I'm sure they would have been fine, but ultimately I decided not to go that way.",buildapc,2026-02-06 23:44:09,1
AMD,o400i86,"Badass build. Building a new system coming from a 3600x myself too, that thing just kept on kickin ass",buildapc,2026-02-07 00:35:17,2
AMD,o3zsqav,Sounds good. Also checkout r/buildapcsales they recently posted some x3d bundles you might like,buildapc,2026-02-06 23:50:24,2
AMD,o42s7r5,Yeah it just kept going. But my list of games that I want to play that I can't (or at least the performance wouldn't be acceptable enough) finally got long enough that I decided to just bite the bullet. I've been waiting too long for the market to get better and its always just something else.,buildapc,2026-02-07 13:33:04,1
AMD,o3zu2qb,Sweet! I did not check that out but I definitely will while all this stuff is still in the return window. Thanks!,buildapc,2026-02-06 23:58:10,2
AMD,o3e2xip,"1. Seems like a good deal!  2. Yes but that will always be the case with an older CPU and a much newer GPU. It's not a problem in any way, I run a 5900X with a 5080.  3. Yes, great for 1440. It can run anything at high to ultra settings.",buildapc,2026-02-03 18:57:53,8
AMD,o3e0vml,"That CPU won't be a significant bottleneck with that GPU, especially if you're playing at 1440p. It should work great at that resolution.",buildapc,2026-02-03 18:48:44,17
AMD,o3e1jnw,"1: sounds like a great deal. Only way to test is plug it all in and look at benchmarks.   2: definitely will have a cpu bottleneck but with a higher end card it’s hard not to.  3: it will handle 1440p with ease.   Have fun with the new pc, I’m totally not jealous of you /s",buildapc,2026-02-03 18:51:43,6
AMD,o3e278c,Perfect for 1440p,buildapc,2026-02-03 18:54:37,3
AMD,o3e3apw,"I was gaming 1440p with a 5600X and 3080 TI just fine, so you should have no problems whatsoever. Although I did upgrade to a 5700 X3D and have been really happy with the boost to performance, so something to consider down the road.",buildapc,2026-02-03 18:59:34,2
AMD,o3e4a6m,That’s my exact build and it works wonderfully. 1440p with that combo is perfectly fine.,buildapc,2026-02-03 19:04:09,2
AMD,o3e4hny,"I've been running ryzen 5600 with 5070ti. It bottlenecks somewhat in some very specific scenarios of some very specific games, but overall works really well   5070 will be more than fine with this CPU   As for the price, looks fine, considering current RAM and SSD prices. I don't know the EU market well enough, you could probably get individual parts cheaper if actively looking. But as far as prebuilt goes, this seems okay",buildapc,2026-02-03 19:05:07,2
AMD,o3e4xiy,"If you play at higher resolutions and settings, you gonna be fine",buildapc,2026-02-03 19:07:11,2
AMD,o3e59af,I think price is kinda high.   It is great for 1440p,buildapc,2026-02-03 19:08:41,2
AMD,o3e8ktf,Price is fair. You didn't get a great deal but you also didn't overpay,buildapc,2026-02-03 19:24:13,2
AMD,o3ec7m8,"With current prices I'd say fair. I would have like to see 32gb of ram but ram is at such a premium right now its hard to complain. I paid more for less during the GPU-pocalypse.  When it came to gaming, my 5600x and 6700xt combo did great for 1440p. This should do better.  CPU heavy games you would notice a decent difference from a 5600x vs 5800x vs 5800x3d if you had them side to side. But coming from that laptop, this will be an exponential upgrade; you won't miss out    Every PC has a bottleneck in an application, a 9800x3d bottlenecks the Nvidia6000. The SSD could be a bigger bottleneck than the processor. As long as its not a handicap (let's say a 3800x with 4gb of ram, a 7200rpm HDD and a 5080) don't worry too much about bottlenecks.  Enjoy your new rig!",buildapc,2026-02-03 19:41:25,2
AMD,o3f5dgb,"If you’re playing graphically intense games or higher settings, probably not much of a bottleneck at 1440p  I was using a similar performing OC’d 10700k and a weaker 3080 GPU, and it was a bottleneck for me at 1440p only because I lower all my settings and mostly play easy to run cpu bound games.",buildapc,2026-02-03 21:57:19,2
AMD,o3f8cwo,"A 5600X amd cpu overclocked with a decent cooler is enough to 1440p game, i'm doing it, but i have a 6800xt oc gpu.   The gpu is the bottleneck in most games, only really big games with lots of calculations/moving units in total war, have cpu bottle necks.   16gb ram gpu should be your min for 1440p+ gaming.",buildapc,2026-02-03 22:11:14,2
AMD,o3f8v1b,"I had a Ryzen 5600 and upgraded to a Ryzen 5800XT by waiting for sales.  The Ryzen 5600 was a great CPU.  Efficient and fast for gaming.  Would not think twice about a Ryzen 5600X.  You can upgrade to 32GB RAM eventually, just keep checking for deals.  You might have to manually enter memory timings.  I technically have 4x8GB sitting around that I know does DDR4 3200 CL16-18-18-38, they were ADATA RGB sticks.  Never could get them to work in 4 slots at DDR4 3600 CL18 as advertized.  In general, on Zen3 the ones with a 12nm Global Foundries IMC, I found anything above DDR4 3200 unreliable.  I think the interleaving bonus is nice from 4x1rank sticks though.",buildapc,2026-02-03 22:13:40,2
AMD,o3fehzz,Should be perfectly fine for 1440p. You can always drop in an X3D CPU later on if you find one for a good price.,buildapc,2026-02-03 22:41:50,2
AMD,o3ff0nv,"It's a solid pairing, especially with today's RAM prices.  I've been running a 5600X and 6750XT with 32 gigs of DDR-3200 for two and half years. It's solid in 1440p gaming.  The only games that really push the CPU utilization is UE5 games like Borderlands 4.",buildapc,2026-02-03 22:44:27,2
AMD,o3fmv9r,"It depends on the game. Some games like Baldur's Gate 3, Cyberpunk 2077, Halo Infinite, and other titles are more heavy on the CPU. Others will use your GPU more and there will be no bottleneck.",buildapc,2026-02-03 23:25:51,2
AMD,o3fnt0p,"I can't speak to price, as I'm in the US.    I would expect some bottlenecks on cpu intensive games. My 5800xt is similar to a 5600, and games like cyberpunk I still max out the gpu first, which maxes out my monitor at 160fps.  (3440x1440p)  But overall an am5 cpu would provide better performance  Good for 1440p Edit typo",buildapc,2026-02-03 23:30:53,2
AMD,o3fpti7,1. Seems fair. 2. Yes and no.  (This is a bad question.) 3. Yes.  Bottleneck is an overused term.  You can fix a lot of balance issues with the graphical settings and resolution.,buildapc,2026-02-03 23:41:54,2
AMD,o3e79on,Will you even notice the bottle neck?,buildapc,2026-02-03 19:18:04,1
AMD,o3f3j0p,Bottleneck is the boogeyman around here. That word is mentioned every 5 seconds and I would venture 99% of people will never even notice or care.,buildapc,2026-02-03 21:48:44,1
AMD,o3f8y27,don't forget to check the psu,buildapc,2026-02-03 22:14:04,1
AMD,o3i6q2n,CPU bottleneck exists but you gonna be fine   I have 13400F+5060 and enjoyed myself in 1440p gaming   Recently I upgraded to 14600KF and got about extra 30fps,buildapc,2026-02-04 09:57:40,1
AMD,o3t52mq,"For 1400 eur, you can actually squeeze in a good am5 cpu.  https://de.pcpartpicker.com/list/2H9ttC  I'd say 16gb of ram will hold you back more than 5600x. Ideally, you'd get 32gb of ram and an am5 cpu. Which would mean downgrading the gpu. Or squeezing really hard like this:  https://de.pcpartpicker.com/list/9dCFfp  7500f is still much better than 5600x, and you get your 32gb of ram. Even if the ssd is small, it's worthwhile imo.",buildapc,2026-02-05 23:35:11,1
AMD,o3wcrn4,"1. Depends on how your regional pricing is. At that price I would have wanted 32gb of ddr4 tbh as it's not as expensive as ddr5.  2. At 1080p - definely yes. For 1440p maybe a bit depending on the game,if the game is gpu heavy then there should be no bottleneck at all.  3. 5070 is a 1440p gpu so you should be fine for many years to come. I have one as well and it does the job very well on every game I throw at it.",buildapc,2026-02-06 13:30:39,1
AMD,o3e9q71,Yes it will. I had a 5600 with a 5070. It was not great. Now I have a 7800x3d with a 5070. It's overkill (:,buildapc,2026-02-03 19:29:40,-1
AMD,o3xnz8b,No system is going to be the same so it’s impossible to give accurate information,pcmasterrace,2026-02-06 17:24:14,2
AMD,o3xunoi,"A Ryzen 5 5600X, Gigabyte B550 Aorus Elite AX V2, and RTX 2070 pulls 94 watts from the wall through a Corsair CX650M PSU.  Because it is doing, right now.",pcmasterrace,2026-02-06 17:56:07,1
AMD,o48ccmw,If you care about idle power consumption a lot you should be looking for an intel cpu or monolithic am4 one ig. Chiplets aren't good in that department.,pcmasterrace,2026-02-08 10:21:53,1
AMD,o48vnmo,But perhaps some info from HWmonitor if you can?,pcmasterrace,2026-02-08 13:06:48,1
AMD,o48xa8j,You recon GTX would eat a lot?,pcmasterrace,2026-02-08 13:17:50,1
AMD,o48w94a,"Seen some YT and where at some point some measurements were below 10-12Watts on CPU package and my current is about 15Watts, GPU is another another 15-18Watts rest is the motherboard and rest of bits.",pcmasterrace,2026-02-08 13:10:54,1
AMD,o48y1v2,The only valid measurements are those obtained by power plug meters and not by hwinfo or other polling software.,pcmasterrace,2026-02-08 13:22:54,2
AMD,o40aelt,Holy moly thats clean.,pcmasterrace,2026-02-07 01:34:53,40
AMD,o40hz7y,This is art,pcmasterrace,2026-02-07 02:21:28,24
AMD,o41y68o,£1000 on a mobo is a bit on the extreme side,pcmasterrace,2026-02-07 09:15:13,16
AMD,o40b65s,![gif](giphy|13A5D3NN7UjEeQ|downsized),pcmasterrace,2026-02-07 01:39:39,21
AMD,o40c8sc,What is the slot that looks like a RAM slot to the right of the 4 RAM slots?,pcmasterrace,2026-02-07 01:46:19,6
AMD,o41q41g,Amazing looking build. Unfortunate about the CPU's dying.,pcmasterrace,2026-02-07 07:57:20,8
AMD,o41dt5g,This looks fucking incredible,pcmasterrace,2026-02-07 06:05:23,5
AMD,o41kc6f,Gorgeous,pcmasterrace,2026-02-07 07:03:18,6
AMD,o41zele,Hopefully you were able to build another PC with the parts you replaced if you couldn't return them at this point.,pcmasterrace,2026-02-07 09:27:40,3
AMD,o422c99,https://preview.redd.it/2xd45p1ap1ig1.jpeg?width=1080&format=pjpg&auto=webp&s=76516105e9cfae936bba5b9853a5016cb8486be3,pcmasterrace,2026-02-07 09:57:05,3
AMD,o423fs4,sick,pcmasterrace,2026-02-07 10:07:58,3
AMD,o423gxu,God damn 🤤,pcmasterrace,2026-02-07 10:08:17,3
AMD,o425o9g,Super clean!,pcmasterrace,2026-02-07 10:29:50,3
AMD,o426l92,"It looks good, but I am glad I am not the one to set it up.",pcmasterrace,2026-02-07 10:38:46,3
AMD,o42rj8o,"I don´t wanna upgrade my AM4 platform but i see this, and HW drugs comes to me,  ahaha  What a beatiful motherboard !",pcmasterrace,2026-02-07 13:28:50,3
AMD,o421l4v,That is peak PCMR. Beautiful.,pcmasterrace,2026-02-07 09:49:37,4
AMD,o42pe2a,"I think it's luck of the draw, to be honest. I had been intel forever, and I decided to switch to the 9850x3d, and i had 3 bad processors - I ended up returning them all and switching to an Ultra9",pcmasterrace,2026-02-07 13:15:11,2
AMD,o41l3io,Have you done any tuning or just let it run stock? Properly overclocked 14900ks rarely degraded due to vmin shift.,pcmasterrace,2026-02-07 07:10:11,3
AMD,o42ev8m,Look at those pretty 8pin connectors not burning and shiz.. pay attention Nsuckia,pcmasterrace,2026-02-07 11:55:15,2
AMD,o41p91j,You grilled the Intel CPUs 🥲,pcmasterrace,2026-02-07 07:49:12,1
AMD,o42dudu,Those pipes are porn.,pcmasterrace,2026-02-07 11:46:26,1
AMD,o432dl1,what are the specs on the new pc?,pcmasterrace,2026-02-07 14:32:16,1
AMD,o43wtam,You better update bios to the latest since you have 9800x3D or else… 💣💥,pcmasterrace,2026-02-07 17:06:05,1
AMD,o44kp2k,Best of luck to you brother. I hope it doesn't fry itself 🙏,pcmasterrace,2026-02-07 19:04:05,1
AMD,o45a5kk,"People who buy ASUS or Corsair products in 2026 baffle me. Looks clean af though.  Edit: Don't trust XMP or any built-in one-click MOBO ""Optimization"" for this generation.  Edit2: I just clicked through to the second image. GOT DAYUM. That's pretty. GFJ.",pcmasterrace,2026-02-07 21:19:18,1
AMD,o4djdft,"Have you checked, if your build is stable before doing a custom water cooling? If there are issues better to know at the begining if the build, then after doing all of this…",pcmasterrace,2026-02-09 03:50:15,1
AMD,o417ge7,wait until you Ryzen 9000 suddenly died hahaha,pcmasterrace,2026-02-07 05:14:10,-2
AMD,o43lfnb,Did you update the BIOS in the intel one?? Or just fried 2 cpus because ignorance??,pcmasterrace,2026-02-07 16:10:16,-1
AMD,o43nxf6,I have a z790 and 14900 what were you running that was making it unstable? I have windows 11 pro mostly playing steam games 1080p.,pcmasterrace,2026-02-07 16:22:33,-1
AMD,o42a7gy,100% how I dream to have a pc one day,pcmasterrace,2026-02-07 11:13:15,6
AMD,o41zcjx,I already feel bad for spending 320€ on aorus elite x870e x3d,pcmasterrace,2026-02-07 09:27:06,9
AMD,o40edtg,thats the dimm.2 slot that asus has on their high end motherboards   you can put a card in there that has 2 extra m.2 slots,pcmasterrace,2026-02-07 01:59:25,15
AMD,o423yzg,Had my 10900k build with 6900xt so I've been chilling.,pcmasterrace,2026-02-07 10:13:10,3
AMD,o4223in,peak braindead,pcmasterrace,2026-02-07 09:54:41,-2
AMD,o41m1dz,Bought this in January 2025 ran stock with updated bios. First cpu wouldn't even let me install windows without blue screening. 2nd cpu is crashing every game after 5 seconds. Intel refunded both cpus through RMA. And throughout the year I replaced every component including psu and even MB to isolate the issue to the cpu. Also did a bunch of testing in bios to try and fix the issue and nothing worked. My temps were almost ambient at idle and 60 under full load. Occt always showed logical core errors on both cpus.  Z690 and 790 are plagued chipsets I've come to the conclusion of.,pcmasterrace,2026-02-07 07:18:52,4
AMD,o4djvbm,"Have been building for years now. Everything is always bought brand new. There should be no reason anything is unstable at stock speeds... but to answer your question, no I don't. That defeats the first boot up experience after all the hard work. This is my first time dealing with these unstable issues on a brand new system.",pcmasterrace,2026-02-09 03:53:20,1
AMD,o43088d,Thats  because Asus and Asrock  not Amd,pcmasterrace,2026-02-07 14:20:10,0
AMD,o43uwbm,"Even if he didn't update it, it wouldn't be ignorance on his part, the CPUs should work out the box at stock with no stability or degradation issues.  Quick to blame consumers for Intel's shitty products.",pcmasterrace,2026-02-07 16:56:44,3
AMD,o43wa29,https://youtu.be/OVdmK1UGzGs,pcmasterrace,2026-02-07 17:03:28,2
AMD,o420417,"That is definitely more reasonable. Mobos have really started to creep up. When I got my MSI carbon z790 it was £300, now for the x870e they are £440.   I’ll just buy another brand.",pcmasterrace,2026-02-07 09:34:48,5
AMD,o42xd0q,$450 on Trx50 aero d,pcmasterrace,2026-02-07 14:03:46,2
AMD,o443l5d,"I paid 160EUR for an MSI X670E GAMING PLUS WIFI, got pretty lucky ig finding it on sale at amazon.",pcmasterrace,2026-02-07 17:39:46,1
AMD,o44bi43,Bruh my Strix B850-F was priced the same so I'd say you're on cheaper side here.,pcmasterrace,2026-02-07 18:18:48,1
AMD,o41ne1g,Your first cpus 100% got the fab oxidization issue and second one degraded super fast due to mobo makers pumping way to much voltage. Chipsets are not plagues but manufacturers doing dick measuring contest with voltage.,pcmasterrace,2026-02-07 07:31:37,4
AMD,o432zz3,"https://preview.redd.it/jco14zay23ig1.png?width=1313&format=png&auto=webp&s=ec746afec9e12f088594be426f7fa6edddb504a1  Gigabyte   MSI  Yeah, Keep Denying kid",pcmasterrace,2026-02-07 14:35:43,1
AMD,o44pexf,Interesting I have a KF chip hopefully it’s lasts,pcmasterrace,2026-02-07 19:28:07,2
AMD,o42ytld,Are you happy with gigabyte? I'm switching from a faulty msi tomahawk,pcmasterrace,2026-02-07 14:12:09,2
AMD,o44capy,I was actually considering going for the Strix B850-F. It's just so much for a B850 and you don't even get 90° frontpanel headers...  You happy with it? If Gigabyte turns out to be shit aswell I might shoot me the F,pcmasterrace,2026-02-07 18:22:38,1
AMD,o41r4gy,Puget reporting 5% failure rate. Bro just got super unlucky.,pcmasterrace,2026-02-07 08:06:54,6
AMD,o43styj,Threadripper and Gigabyte💪,pcmasterrace,2026-02-07 16:46:36,0
AMD,o43uh3v,"Less than 10 reports on MSI and gigabyte boards in the last year combined, well within normal failure rates.   9000 series deaths are down to Asrock and Asus, and them alone.",pcmasterrace,2026-02-07 16:54:41,0
AMD,o430ysu,Yes  its pretty good  and supports several good OSes,pcmasterrace,2026-02-07 14:24:23,1
AMD,o44u19j,I just liked the look and it was the cheapest current-gen Strix mobo. Feature-wise it's more than enough for me,pcmasterrace,2026-02-07 19:52:13,3
AMD,o41rnip,There are first batch 13900Ks that was properly overclocked from the start showing zero signs of degradation. Crazy what proper voltages can do when you ignore mobo manufacturers stock settings.,pcmasterrace,2026-02-07 08:11:57,5
AMD,o41uo57,I bought day one. Ran it at 1.3V at 5.5Ghz. Zero problems.,pcmasterrace,2026-02-07 08:41:06,2
AMD,o40rurz,EDIT: MY CONCERN IS WILL THE INCREASE IN MULTI CORE AFTER TWEAKING AFFECTS THE PERFORMANCE OF MY GAMING?,pcmasterrace,2026-02-07 03:23:48,2
AMD,o40r5nx,I mean... you can... but don't overdo it. Everything can emit light....   Once.,pcmasterrace,2026-02-07 03:19:19,1
AMD,o411v6e,[https://www.intensewebs.com/index.php/amd/amd-cpu/key-points-ryzen-oc](https://www.intensewebs.com/index.php/amd/amd-cpu/key-points-ryzen-oc),pcmasterrace,2026-02-07 04:32:45,1
AMD,o4122ge,had a 5700X that I ran 4000MT CL16 RAM kit ... CPU was a monster ... hit almost 16k on R23  had a -30 offset & +200Mhz but I had scalar at 7x,pcmasterrace,2026-02-07 04:34:11,1
AMD,o412l3u,when you benchmark it what's the GHz speed topping out at?,pcmasterrace,2026-02-07 04:37:59,1
AMD,o40rdsc,but in terms of gaming? will the 14k multi core have less performance compare to the almost 15k despite having a good single core benchmark?,pcmasterrace,2026-02-07 03:20:47,1
AMD,o414y9n,"https://preview.redd.it/cnc9kmzy60ig1.jpeg?width=539&format=pjpg&auto=webp&s=e073cb7ca4125af80a18c122c678f1b66b4369ca  this it it but I change the PPT, TDC, and EDC. Was able to achieve  Multi core- 15,799 Single core - 1,584  Maximum temp was 76 degrees but it is a fishtank case and only 240mm AIO, the room is quite hot too since I’m from PH",pcmasterrace,2026-02-07 04:55:17,1
AMD,o40ryff,"Tbh, I'm not a tech support pro nor do I play one on TV, but you can, but in moderation yknow? Too much too fast can brick your pc.",pcmasterrace,2026-02-07 03:24:28,1
AMD,o44o2kp,Air flow seems wrong,pcmasterrace,2026-02-07 19:21:09,3
AMD,o47roet,Why is that?,pcmasterrace,2026-02-08 07:07:33,1
AMD,o48idyo,"The CPU cooler is mounted with fans pushing toward the back, cant see the direction of the back case fan.",pcmasterrace,2026-02-08 11:18:31,2
AMD,o48xdwx,"That’s correct, wouldn’t fit the other way around because the RAM is to high. Rear fan is exhaust, all 3 move air in the same direction, temps are good.",pcmasterrace,2026-02-08 13:18:30,2
AMD,o48yprh,OK! ;) enjoy,pcmasterrace,2026-02-08 13:27:12,2
AMD,o3zotpv,"I believe 3200MT/s is safe for AM4 socket, 3600MT/s is EXPO overclocked.  For AM5 on AMD 4800MT/s is safe and 6000MT/s is EXPO overclocked.  For Intel its the same thing with XMP profiles they are all basically an RAM overclock.  As for memclock/infinity fabric you don't need to worry as that's something for X3D chips, And it's really not an issue to run 1:32 on the 9000 chips in most real user cases.     1:1:1 just means all the timing are set to the same MHz so 2000/2000/2000 but it really really don't matter unless you going for some LN2 overclocking.",pcmasterrace,2026-02-06 23:27:43,2
AMD,o3zmpak,3600 MT with Ryzen 5600 is like nitro boost man syncs better keeps performance smooth you'll feel the difference,pcmasterrace,2026-02-06 23:15:31,1
AMD,o3zproh,"AMD's a mess 😂  Okay, thanks. Actually, the equation seems simple given RAM prices; I don't want to take risks and get stuck, so I'm leaning towards the 3200; at least it's safer...",pcmasterrace,2026-02-06 23:33:12,1
AMD,o3zn38m,"The thing is, with RAM costs skyrocketing, if I buy the 3600 and realize it doesn't work by the time I upgrade to the 3200, I'll have lost a ton of money. So I'm worried.  So. That's nicely put, but I don't know why you're saying it. I'm waiting for other replies 😗 But thank you 😊",pcmasterrace,2026-02-06 23:17:44,1
AMD,o3zrn1q,"AMD is simple, Get Ryzen 5600x 3200MT/s DDR4 32GB, GPU, Play games.  Getting 3600+ MT/s isn't worth the extra price mostly due to it being AM4, It's like buying a 2025 Ferrari taking out the engine and putting it into a 2000's Escort, It's past its best and within a few years you gonna be looking at upgrading again.",pcmasterrace,2026-02-06 23:44:06,2
AMD,o3ztjki,"Thanks, yes, I understand your logic, but strangely enough, there's only a €5-10 difference between the two kits. 😅 What threw me off was that a website that lists motherboard QVLs ranked Patriot's 3600 as the most compatible and recommended RAM by Gibabyte for my M550. 🤦‍♂️ That threw me off.  Well, I went and watched some videos on YouTube, and the price increase is negligible, and sometimes even the 0.1% and 1% increases are worse, even if the average is better. 🤷‍♂️",pcmasterrace,2026-02-06 23:55:05,1
AMD,o3zzeyf,"Yeah the 5600 isn't that ram sensitive unless it was the X3D version but it's something they  fixed in the 9000series's X3D, if you can get 3200MT/s CL18 or as low as possible you will be golden, On my 5700x I used 3600MT's CL18 and it was good.  https://preview.redd.it/s2n2t1swvyhg1.png?width=670&format=png&auto=webp&s=649b948aadf57cf52c4007c939419340e7f3e251",pcmasterrace,2026-02-07 00:29:00,1
AMD,o401mrl,3200 cl16 / 3600 cl18 is what I can get for the same price 😐   😂,pcmasterrace,2026-02-07 00:41:46,1
AMD,o402uob,"if the price is the same 3600 cl18 but you not gonna see/feel huge differences, It's 400MT/s you not gonna feel it in reality.  You will see more saving on the RAM say getting 3200MT/s and putting the extra saving into getting at 5700x, You will see more gains in gaming over the 5600x and 3600MT/s",pcmasterrace,2026-02-07 00:48:53,1
AMD,o4047tj,"The same price, give or take $7. Yes, the difference is minimal, I agree. And as I said, given the current climate, it's perhaps not the time to gamble with our RAM. Even if it's supposed to work...  (When I read on the website that they recommended the 3600, I was worried, in the sense that I thought it meant it was more stable. But that's not the case; it's more powerful, but not necessarily more stable.)",pcmasterrace,2026-02-07 00:56:58,1
AMD,o4049bo,"Anyway, I think I'm not going to play with forces I don't understand and will go with the 3200.",pcmasterrace,2026-02-07 00:57:13,1
AMD,o4055fk,"Yeah, 3200MT/s is fine you will not be missing out.",pcmasterrace,2026-02-07 01:02:38,2
AMD,o407e04,"🙇‍♂️ Okay, I can rest now  ![gif](giphy|kVtORskkiMC1a)",pcmasterrace,2026-02-07 01:16:15,1
AMD,o43l1ky,this looks like a monitor failure,pcmasterrace,2026-02-07 16:08:21,1
AMD,o42jnvo,Happy gaming my friend!,pcmasterrace,2026-02-07 12:34:44,2
AMD,o42ljvo,"I'd open up the back of that cabinet, unless you want to starve it of fresh air.",pcmasterrace,2026-02-07 12:49:02,4
AMD,o42k11c,Congrats,pcmasterrace,2026-02-07 12:37:33,1
AMD,o42usot,"Very awesome! 180hz really is a game changer, i should maybe add it to my flair..",pcmasterrace,2026-02-07 13:48:37,1
AMD,o459ouo,"Welcome. Now begins the constant battle of ""Should I upgrade this component now, or wait?""",pcmasterrace,2026-02-07 21:16:48,1
AMD,o44oqgx,thank you!,pcmasterrace,2026-02-07 19:24:34,1
AMD,o44os50,it does have a small opening for the cables but i believe theres enough room for it to breathe,pcmasterrace,2026-02-07 19:24:49,-2
AMD,o44oswj,thank you!,pcmasterrace,2026-02-07 19:24:56,1
AMD,o44ovt8,"indeed, when i went back to my laptop to transfer some things that alone made it feel insanely slow",pcmasterrace,2026-02-07 19:25:21,1
AMD,o469dy4,ahahah we’ll see how that goes in the future,pcmasterrace,2026-02-08 00:43:07,1
AMD,o44pe6i,Guess you'll find out after it's under load for a while.,pcmasterrace,2026-02-07 19:28:00,3
AMD,o44qlxi,"I remember before i got a 180hz monitor and had a 75hz, i had a 144hz laptop. Switching between the 2 was painful, especially since my desktop is more powerful than my laptop",pcmasterrace,2026-02-07 19:34:19,1
AMD,o44y6nl,free heating,pcmasterrace,2026-02-07 20:14:17,0
AMD,o44y5kc,144hz laptop and 75hz monitor is diabolical,pcmasterrace,2026-02-07 20:14:07,1
AMD,o4512q6,real,pcmasterrace,2026-02-07 20:29:59,1
AMD,o450vg0,"real😭  ive got a 180hz main monitor now though so its okay, the 75hz is now secondary",pcmasterrace,2026-02-07 20:28:52,1
AMD,o49dgwq,"Hi, I'm having the SAME problem with my HP Victus for the past week too. I tried reinstalling all the drivers, alternating between using the two extra monitors I have, but the CPU still freezes. I'm going crazy, really",pcmasterrace,2026-02-08 14:54:47,1
AMD,o3z41a1,"The combo is not ideal, but you should be able to play every single game. You will absolutely be CPU bottlenecked sometimes,  but I’d imagine you’ll get at least 60FPS in most games. My recommendation is to crank up the graphics settings and enjoy every frame you’ll get from the 2700. Lowering CPU intensive settings like shadow quality, render distance and character density will help reduce the demand on the CPU.",pcmasterrace,2026-02-06 21:37:52,3
AMD,o3z49rq,"Are you unhappy with the performance you're getting as it is? I wouldn't be surprised if you're CPU limited, but it doesn't *really* matter that you are if you're happy with the frame rate that you're getting. Feel free to turn up your graphics settings to make as much use of the 4070 as you can.  Outside of doing things in software to help your CPU out, such as closing all background tasks while you're gaming or even reinstalling Windows if you have an old bloated install, you'd have to get into overclocking to make your CPU and RAM faster.",pcmasterrace,2026-02-06 21:39:01,3
AMD,o3z6yq5,I mean if you can upgrade your CPU that will be your cheapest route if you're looking to stay on AM4 and say with your graphics card.,pcmasterrace,2026-02-06 21:52:24,2
AMD,o40ttl1,An AMD GPU would be much better due to MUCH lower CPU overhead.,pcmasterrace,2026-02-07 03:36:49,1
AMD,o41jjeq,"The CPU hands down. I upgraded mine from a ryzen 5 3600 to a 5600 and even a small upgrade like that, I'm getting like 40 more fps average. Sometimes more.",pcmasterrace,2026-02-07 06:56:03,1
AMD,o3z580l,I didn't even think about cpu intensive settings. Will try lowering these settings. Thanks a lot!,pcmasterrace,2026-02-06 21:43:45,1
AMD,o3z7z51,"I'm okay with what I have now, I'll change settings and closing unnecessary things while gaming as you suggested. Thank you for your help!",pcmasterrace,2026-02-06 21:57:22,1
AMD,o3z9era,"Alright, I'll save up for 5700X3D then. Thanks.",pcmasterrace,2026-02-06 22:04:34,3
AMD,o3z9l0d,You’re welcome!,pcmasterrace,2026-02-06 22:05:27,2
AMD,o40zoeu,Don't forget to update your bios before you put a newer cpu in.,pcmasterrace,2026-02-07 04:16:57,1
AMD,o4171ka,"Honestly might be a bit harder to find a 5800x3d but tht would be the best. But a 5600, 5700x or 5700xt are also great options.",pcmasterrace,2026-02-07 05:10:59,1
AMD,o48zoyz,It's very obvious that they've tried to push users towards the new cards by putting the rdna 2 cards on maintenance drivers and not offering far 4 for rdna 2/3.  The problem is that only works when you have a monopoly in the market and people have no other choice.  My last three cards have been AMD card mostly because they offered decent value and I wasn't interested in Nvidia's features and didn't want to support a monopoly by Nvidia.  However if AMD won't support their products long term then I'll just have to buy Nvidia.  It's not my preference but AMD can fuck right off with this nonsense. They're not offering enough to be pulling this bullshit.,pcmasterrace,2026-02-08 13:33:29,256
AMD,o497a9t,By not doing so they gotta be losing some customers for future products. It basically tells me the 9070xt when the next generation will immediately be made obsolete when their next feature set releases. Whether thats true or not will shape some people's decisions while the competition is continually supporting 2018 cards with upscaling features.,pcmasterrace,2026-02-08 14:19:44,182
AMD,o4964q3,"Exactly not sure what AMD has to lose right here. If Nvidia can bring DLSS4.5 to older series cards why can't AMD do this.   Time and time again they have shown not to deserve our money. RDNA2/3 in maintainance mode soon RDNA4 will be in maintainance after RDNA5 launches. Who's to say future FSR updates won't come to RDNA4. At this point AMD is off the ball for GPU. RDNA3.5+++ and consumer graphics card being priced close to Nvidia - 50 MSRP with so much less features, rebate for the initial launch of RDNA4 they removing them. The list goes on with stuff like RedStone.  At this point Intel has a better chance of competing against Nvidia than AMD. Its really appalling for a company to make that many mistakes at this scale as if these decisions are made by some vindictive person internally.  Look im all for competition but if the competition actually cannot make it and makes their competitors seem stronger then what's the point then. I really hope Intel comes in stronger and provide better value to us consumers with Celestialo or Druid than this RDNA crap.",pcmasterrace,2026-02-08 14:12:56,59
AMD,o49av46,"I have a 7900xt so pretty much 0 reasons to upgrade to a 9070xt. If they actually never make FSR4 accessible this is the last AMD card I will ever buy, out of principle.",pcmasterrace,2026-02-08 14:40:23,19
AMD,o495rik,"Maybe because they can get away with it?    AMD has been hurting their customers for years now whenever it benefited them.   But they have a loyal fan base that buys their products regardless, they know they get away with it and have conditioned their fans to a degree that they'll defend AMD over that.    Just look anywhere on reddit, it's a huge AMD circle jerk that had been going on for years.   This is the first time AMDs hostility is taken serious in any way it still doesn't really get attention.    Tech media knows that their audience are mostly AMD fans that prefer negative headlines over other companies.",pcmasterrace,2026-02-08 14:10:43,21
AMD,o497evf,Making Nvidia look like the pro-user company by allowing DLSS 4.5 all the way back to 2018 RTX cards is a bad look from AMD.,pcmasterrace,2026-02-08 14:20:29,42
AMD,o49i97q,"AMD constantly shoots themselves in the foot with Radeon and makes their competition constantly look better and better. The only 2 reasons I’m not switching over is because of the lack of proper Linux support and the 12V2x6 connector.  Radeon has little to no goodwill and no good reason to continue existing if they’re just choosing not to compete. AMD is constantly losing to NVIDIA, and Intel is catching up quickly in the GPU market. Their only good gaming sales performance outside of Ryzen is in the console and handheld markets (which is also being challenged by NVIDIA and Intel). It’d just be better to roll Radeon into their Ryzen division and have them entirely focus on integrated graphics as their resources are being constantly wasted on a GPU division undermined by their marketing division.",pcmasterrace,2026-02-08 15:20:21,6
AMD,o498zn4,"AMD is really dropping the ball here. I went from a 6950XT to a 5070Ti primarily due to this decision. If you go to any of the AMD subs, you'll see many more people who have made similar decisions.  Even if you didn't care about customer loyalty, this is just bad business. The people who are annoyed that they aren't getting FSR 4 aren't going to upgrade to RDNA 4; they'll just move over to Nvidia. It also wouldn't cost AMD anything to implement INT8 FSR 4. It is clearly already in a functional state. If they released it, nobody would expect them to continually update it. They literally could just attach the already compiled DLL to the FSR SDK and forget about it from there on.",pcmasterrace,2026-02-08 14:29:37,21
AMD,o49flvt,"They make my 7900XT feel like a redundant product despite FSR4 already working on the cards without issues really. It's super annoying because Optiscaler makes anti-cheat games throw a fit but FSR3 is kinda visual torture.  https://preview.redd.it/zx29c4oddaig1.jpeg?width=576&format=pjpg&auto=webp&s=ef8b2015467ffe00c4755ff6f536caa4839907be  I don't suggest AMD GPUs at all anymore because of this lol. Same thing will happen from RDNA4 to UDNA. I say this as someone whose owned like 3 AMD cards (and I did have a 3080 too) since Nvidia supports their products from 2018 or 2019 while AMD can't even be bothered to do one generation ago. Their cards are good from the bargain bin (got my 7900 XT at 480 lol) but little else. Not sure where they get the idea that they can charge premium prices for non-existent features and ultimately, slower cards.",pcmasterrace,2026-02-08 15:06:24,11
AMD,o49wbl7,"Radeon has always been slower to release certain technology, and there’s likely a reason for that. One plausible explanation is that AMD may have used copyrighted or non‑redistributable training data for early versions of FSR 4, which would prevent them from open‑sourcing it. If that’s the case, they would need to train a new model from scratch using clean, legally safe data. Achieving the same or better performance and making it open source across all platforms would naturally take time.  Training a high‑quality machine‑learning model can take many months, sometimes close to a year, depending on scale and resources. That’s not unusual in this field.  AMD philosophy has historically leaned toward open standards and cross‑vendor support. They are not Nvidia, and they don’t operate like Nvidia. FSR 4 being closed is likely a temporary exception, not a permanent shift. Once the clean model is ready, even Nvidia users will be able to use it—just like with previous FSR versions.  I’ve been on Radeon/ATI hardware for 19 years. None of this is surprising. Long‑time Radeon users generally don’t buy AMD GPUs for cutting‑edge proprietary tech; that’s never been AMD’s identity.  What *has* changed in recent years is the influx of Nvidia users who switched to RX 6000‑series cards and brought their impatience with them. Many of these people would never have touched Radeon before, and now they expect Nvidia‑style rapid proprietary feature drops.  Tech influencers make this worse. Many of them promote products they barely understand and compare things that shouldn’t be compared in the first place. It’s like comparing Android and Apple devices: the ecosystems are so fundamentally different that raw number‑to‑number comparisons often make no sense.    It’s like comparing an Alcatel phone to the latest iPhone—technically possible, but practically useless.  Most Apple users will never leave the ecosystem, and the same is true for most Android users. GPU ecosystems aren’t identical, but the mindset is similar: different philosophies, different expectations, different user bases.",pcmasterrace,2026-02-08 16:30:42,7
AMD,o4ar32f,"I can't believe AMD would stumble into such an unforced own-goal. This has never, *ever* happened before.",pcmasterrace,2026-02-08 18:56:07,3
AMD,o49n9y2,"The thing is, even with this I still don't see how AMD doesn't offer better price to performance. In EU for the past decade they've had the better deal on GPUs. Even now for only a bit more you can get the 9060 XT which has double the VRAM of the 5060 for similar performance.  Like I'm drooling at the Nvidia guys using DLSS 4.5, now that devs do not optimize the games anymore and rely on upscaling, but at the same time coming from an RX 6700 XT there's just nothing good to buy value wise from Nvidia. This is just awful for consumers, you have a company which offers terrible deals for unreasonable prices and another which gives better value but finds a way to shoot itself in the foot at any opportunity.",pcmasterrace,2026-02-08 15:45:59,4
AMD,o49br15,"And that's the consequence for AMD deciding to finally give a shit about RT, machine learning and upscaling so late. To think the only reason RDNA 4 has an ML based upscaler and good RT performance is Sony insisting on it for the PS5 Pro.",pcmasterrace,2026-02-08 14:45:22,5
AMD,o498mf8,"AMD just lives a W on the table.   AMD thinks it's better to force people to get new GPUs than to keep and increase their customer trust.   Are they gaining more than they are losing? Because that's the logic here, this is what it boils down to, nothing more.",pcmasterrace,2026-02-08 14:27:29,2
AMD,o499bee,"Everyone on the internet has a love/hate relationship with Nvidia but they still go and buy their entry level cards well above MSRP.  I did actually buy an RDNA 3 GPU and honestly I don't know what AMD is losing by not giving us FSR4. FSR3 is not great and that's a valid criticism people have besides Ray Tracing not being great either, and with newer AA and AAA games becoming more reliant on upscaling and Ray Tracing to even run it's a very bad strategy not giving us actual consumers a way to improve our performance against the very well established competition.  New buyers won't exactly consider buying AMD over Nvidia for the sub standard tech. They will buy AMD over Nvidia for the extra raw power and the price/performance. Gatekeeping FSR4 is dumb.",pcmasterrace,2026-02-08 14:31:30,2
AMD,o49cwdt,"As a 7900XT owner I'd really like to not be left in the dust in the coming few years as upscaling becomes more and more standard.   I guess we have FSR 3 but since we know FSR 4 works they really ought to just give it to customers who bought their high end cards. Only 7900XT/XTX owners and higher though, since we proved our loyalty with our wallets.   Jk jk don't shoot me is joke",pcmasterrace,2026-02-08 14:51:39,2
AMD,o49g9m1,atp just use xess which I believe is better than fsr3. kinda crazy how intel caught up with upscaling.,pcmasterrace,2026-02-08 15:09:56,1
AMD,o4a2hjv,"Too many company making breakthroughs and then engineering ways to control the release, use, lifecycle of products instead of just releasing. If the company is just real, maybe they sell less GPUs but they will grab way more market share.  Yeah I know that's how business works, I am not a moron, but that is exactly why we are in this situation.",pcmasterrace,2026-02-08 17:00:30,1
AMD,o4a5kip,I’m not buying amd again,pcmasterrace,2026-02-08 17:15:28,1
AMD,o4aozyh,"AMD can do whatever they want. I can also do whatever I want, buying an Nvidia gpu is one of those things I did. Seems like AMD wants to lose more of their already dwindling customers.",pcmasterrace,2026-02-08 18:46:32,1
AMD,o4atdlj,"Learned my lesson with my Vega 7, deprecated as soon as rdna launched. Next cards were 3080 and 5090.",pcmasterrace,2026-02-08 19:07:05,1
AMD,o4ax6wp,I’d be very surprised if AMD didn’t eventually release a version of INT8 for older GPU’s.,pcmasterrace,2026-02-08 19:25:42,1
AMD,o4b10rx,They need to spend all their money on AI garbage generators okay.,pcmasterrace,2026-02-08 19:44:28,1
AMD,o4b1hpr,"Such consumer friendliness, much open source, very fine wine, wow",pcmasterrace,2026-02-08 19:46:47,1
AMD,o4b662d,With the amount of money AMD has their treatment of the RDNA cards with drivers is disconcerting. They are Scrooging out in a manner which will hurt the mindshare of the public when it comes to deciding whether to risk buying an AMD graphics card. As at this point it's a gamble because of what they are doing & how they're disregarding the RX6xxx & RX7xxxx generation.,pcmasterrace,2026-02-08 20:09:52,1
AMD,o4b75bm,Idk why people still wank AMD's pizzle.,pcmasterrace,2026-02-08 20:14:43,1
AMD,o4bz743,"That's what I was thinking about at some point. Like Nvidia with their dlss update added things for the 50 series but also 40 and 30... And for AMD, FSR4 is only available for the 90 series. I have a 7800 xt so not an old card yet I can't get features update because it's apparently outdated or whatever the reason they're saying. Really make me want to buy a Nvidia GPU as my next.",pcmasterrace,2026-02-08 22:36:53,1
AMD,o4c07yi,"I used to be an amd fanboy over multiple generations and many cards but then I watched them REMOVE features from the newer drivers as my cards aged and as I looked for new cards, it was clear that amd can’t or won’t keep on the software side into the future. I begrudgingly went team green and… it’s been great. Now as things go more AI, it only makes a stronger case for team green.",pcmasterrace,2026-02-08 22:42:25,1
AMD,o4c08ag,"Weren't they talking about making FSR 4 open source? What happened to that discussion?   It's not like the competition is gonna ""steal"" their upscaler, DLSS is at least on par with FSR4 (with DLSS 4-4.5 being much better)  Like... I don't think that not releasing it (officially) for older GPUs is gonna influence their future sales. If anything it makes the customers change brand   Also, we all know that if someone wants to have FSR 4 on older GPUs he's gonna have FSR4 on older GPUs, just not in the official way  The only thing that makes sense (imo) is that they don't have enough AI engineers to create a stable version (since they're probably 99% allocated to the mi450) and they prefer to wait indefinitely rather than releasing/promising something half a$$ed",pcmasterrace,2026-02-08 22:42:28,1
AMD,o4c9q7c,"Meanwhile AMD's CEO announced the company's focus will stay on “Enterprise”  AKA ""fuck consumers, we will slurp on AI companies dongs""",pcmasterrace,2026-02-08 23:37:28,1
AMD,o4do4x2,"Don't worry guys, just gotta age that wine a little a more before it become 'fine wine'.",pcmasterrace,2026-02-09 04:21:14,1
AMD,o4bghkd,"I’m confused. You can literally go and enable FSR4 on RDNA3 GPUs, they open-sourced it.",pcmasterrace,2026-02-08 21:01:45,1
AMD,o492x8c,Better value doesn't work out if the other card gets better over time and yours is abandoned off the shelf,pcmasterrace,2026-02-08 13:53:35,91
AMD,o491esc,Amd/ATI just being themselves. I don't get the Radeon devision. They took ages to actually create a good driver. Now they actually need to figure out to support their GPUs. They behave like they think they're Nvidia.,pcmasterrace,2026-02-08 13:44:15,26
AMD,o495e3x,"If they’d published FSR int8, we’d have been seeing AMD Fine Wine posts every where. Now it’s AMD WTF",pcmasterrace,2026-02-08 14:08:29,20
AMD,o494szg,Yeah for all the shit people give Nvidia they usually make their features backwards compatible if possible. The 3060ti in my old rig can use DLSS 4.5,pcmasterrace,2026-02-08 14:04:58,13
AMD,o495d09,I used to work for them for this reason. I have seen FSR4 working on older cards and they main reason they dont release it on older cards is probably they want to force people to buy newer cards. With UDNA as well they might even sunset RDNA4.,pcmasterrace,2026-02-08 14:08:18,5
AMD,o49dhwf,Are 7000 cards even able to run MFG / FSR4? That's what I'm trying to understand,pcmasterrace,2026-02-08 14:54:56,4
AMD,o4910db,I am sure lisa sue with all her AI money and family ties to jensen hung will not care about us gamers in that regard if they loose more market share in the gpu space she would not care.      Sadly we are in a situation were nvidia could buy intel or amd.     I think a bigger sign of amd walking away from gamer's was the lack of high end gpus and there recent refresh of cpu's.,pcmasterrace,2026-02-08 13:41:45,3
AMD,o497fhx,"Yeah, like Nvidia for all of their faults have brought the most important technology which is upscaling back to the 20 and 30 series. Albeit with 4.5 in a performance degrading way and the 40 series doesn’t really have any performance degradation compared to the 50 series; if anything AMD feel like the people who really should be trying to bring FSR 4 to older hardware because they are already relatively struggling to get games to support the technology and I think there would be a lot more support if the number of potential users was massively increased by supporting last generation AMD cards.  I told someone to buy the 9070 XT this generation because it was much better value especially at the time than the 5070 TI, but if they don’t get FSR five next generation because AMD feel like they can push people to upgrade by keeping better upscaling exclusive I will be recommending Nvidia next generation unless of course Intel happens to have something compelling.  People expect to be able to keep their card for basically 5 to 7 years and I think they should be able to expect to be getting upgrades of features they have on their cards.",pcmasterrace,2026-02-08 14:20:35,3
AMD,o49k0qx,"Unfortunately, i was one of the people who upgraded from RDNA 2 to RDNA 4 but only bc I didn't want to pay stupid high prices for a 70-class card later this summer. I checked again a month after purchase and the same GPU i bought is now sold out.",pcmasterrace,2026-02-08 15:29:27,1
AMD,o49sqa7,They don't even sell any iGPU with the latest tech. Their absolute top mobile chip can't officially run FSR 4,pcmasterrace,2026-02-08 16:13:05,1
AMD,o4a4550,"Bruh I knew something was up with these new driver updates, after October my benchmarks were about 3-5fps less!! In games where I barely hit 60fps on high/ultra settings this matters a crap ton, I tried rolling back and it would not let me! Anytime I roll back the driver forces me to update.",pcmasterrace,2026-02-08 17:08:34,1
AMD,o4agox7,Just go.intel,pcmasterrace,2026-02-08 18:08:40,1
AMD,o4bwzr2,"> It's very obvious that they've tried to push users towards the new cards   The worst part is even THIS is a stupid reason for them to do what they're doing.  * There are no RDNA 4 laptop chips * Their primary APU is going to be RDNA 3.5 for at least another year and change * All of their handhelds are RDNA2 or RDNA3  DIY desktop graphics cards alone can't possibly be a big enough market for them to do this.  Why not just label it an ""experimental"" feature?",pcmasterrace,2026-02-08 22:25:01,1
AMD,o4a0cfl,"if you are open to go Nvidia you should never been going radeon to begin with.  Radeon have a backtrack of this and that dont surprise anybody that are on radeon since the ATI days.  Radeon don't do proprietary tech. FSR4 will be abandoned for a solution that will work on all manufacturer.  FSR 4 is an anomaly that will be rectified.  Creating a new AI model take months and years, specialy one made with legally own data.    And they cannot open source it the current one for probably legal reason.    As it's very likely, like for Nvidia one, being train on steal data.    As nvidia itself as been catch doing a lot of Data Scraping.",pcmasterrace,2026-02-08 16:50:08,-2
AMD,o498owt,Upvoting all these comments because I want AMD to hear this that we consumers aren't pushovers. We want FSR on our older GPUs and drive updates as long as Nvidia.,pcmasterrace,2026-02-08 14:27:54,69
AMD,o4b4ndn,"as a 9070 XT owner this has def left a bad taste in my mouth. i went from 6700 XT to a 3080 12 GB for better upscaling and good RT performance (i love CBP2077 w all my heart lol). over the years switching to Linux has seemed like a canon event, i do not want a 12VHPWR anywhere in my house, and i personally prefer Radeon’s software experience to Nvidia. being on AMD has seemed like the move, but i wanted a card that could upscale well and handle RT.   when the 9070 XT was announced i knew i was getting one, and i’m glad i did! however, if this card doesn’t have FSR 5+ support i will be fuckin pissed. i’m pissed for the ppl on RDNA 3, but if RDNA 4 doesn’t receive future upscaling support i will likely not buy an AMD card again. esp when Nvidia has still supported the 20 series w DLSS 4.5.  Nvidia is willing to let the 20 and 30 series cards get the 2nd gen transformer model even if it runs like ass, so there’s no excuse for AMD when we’ve seen the open source FSR 4 can work on RX 6/7000 with a performance hit. even Xbox/PS, PC handhelds, and all their laptop APUs would benefit",pcmasterrace,2026-02-08 20:02:21,21
AMD,o4avcgp,I don’t think they care. The execs got the case of AI mind virus.. they publicly said they don’t care about consumer products and they want to go after ai sales. Even before this ai craze the Radeon team just followed what nvidia is doing even to the point of price matching them.,pcmasterrace,2026-02-08 19:16:40,3
AMD,o4d33x9,Yeah. I have a 7900XTX and they lost me on my next graphics card upgrade.,pcmasterrace,2026-02-09 02:21:50,1
AMD,o4deyy3,"Yeah honestly. I love my 9070xt but don't feel the love by amd. I'll be most likely*** prices pending, go for the 6080 or equivalent card from nvidia in a few years.",pcmasterrace,2026-02-09 03:24:19,1
AMD,o4dkh62,Upgraded from a 7900xt to a 5090 partly from seeing how AMD felt like they were abandoning the 7000 series buyers with updates when Nvidia dropped updates for DLSS for MULTIPLE generations.,pcmasterrace,2026-02-09 03:57:12,1
AMD,o4axshn,"Never been happier that i returned the 9070xt and got the 5070 ti  Might have been 200$ (Canadian) more expensive but im paying for better upscaling, frame gen and long term support",pcmasterrace,2026-02-08 19:28:37,1
AMD,o4a8sqd,nvidia can support DLSS on turing because they future proofed it by including tensor cores. AMD was selling cards that were obsolete on release date until 2025 and now they are paying the price for it,pcmasterrace,2026-02-08 17:31:07,-1
AMD,o4b97s0,"This is a misunderstanding of the problem. RDNA4 has FP8 support at the hardware level, so there's no inherent blockers for future products. Older RDNA GPUs don't have FP8 support at the hardware level and never will because...well, the hardware is kinda carved in stone.",pcmasterrace,2026-02-08 20:25:11,-1
AMD,o49kmje,"Either shareholders said no, they're afraid DLSS 5 could drop a day later that's even better than FSR 4 and DLSS 4/4.5, or the ""if it works on steam machine and PS5 pro, we can safely port it over to PC officially"" and stakeholders are happy",pcmasterrace,2026-02-08 15:32:35,0
AMD,o49bnoq,Probably because it's a different model that will need quite a bit of dev work and maintaining to release as an official product.    It definitely makes sense from their perspective not wanting to maintain multiple versions at the same time (and opens up a more complicated situation in communication).,pcmasterrace,2026-02-08 14:44:51,-3
AMD,o498zsw,> If Nvidia can bring DLSS4.5 to older series cards why can't AMD do this.  old(er) cards may be able to 'use' DLSS 4.5 - but not it's full feature set..,pcmasterrace,2026-02-08 14:29:38,-12
AMD,o4a8g8k,they can't bring FSR4 to older cards because they don't have the hardware to do it properly. it's embarassing for AMD that they have been selling basically obsolete hardware for the past 7 years until RDNA4,pcmasterrace,2026-02-08 17:29:26,-9
AMD,o49drfh,"7900XT owner here, 5700XT before that, and I do believe I'll go with Nvidia next time if this stays this way. I don't even use fsr, it's the principle. RDNA3 is literally 1 generation ago.   And upscaling will become more and more adopted in the coming years it looks like. Would be a bitter pill if they don't bring support to older generations",pcmasterrace,2026-02-08 14:56:24,10
AMD,o4c0mnx,Tbf there's no reason to upgrade because they have the same performance most of the time. No sane person would upgrade like that  I'll be riding this gpu to its end.,pcmasterrace,2026-02-08 22:44:39,1
AMD,o496mn7,"Yeah exactly most people forgot Zen5%, 9800X3D being dead, RDNA4 being a very bad launch, Redstone actually very bad. Heck the list goes on. Like I like competition but AMD isnt competing at all with Nvidia. They bought ATI but they can't even properly compete with Nvidia after so long but Intel is up and coming only after a few years...",pcmasterrace,2026-02-08 14:15:54,4
AMD,o4977dj,"Seems like people are finally waking up, the radeon sub is riddled with posts everyday with people being sick and tired of amd",pcmasterrace,2026-02-08 14:19:16,1
AMD,o49kuyp,"Which is important for people wanting longevity, high resell value, or re-usability to pass down to a family member",pcmasterrace,2026-02-08 15:33:47,8
AMD,o4990pu,old(er) cards may be able to 'use' DLSS 4.5 - but not it's full feature set..,pcmasterrace,2026-02-08 14:29:47,-19
AMD,o4bj3f5,"You’d be surprised, Nvidia Linux support is getting better and better each day.",pcmasterrace,2026-02-08 21:14:35,1
AMD,o49bqht,I went from an R9 380 to an RX 6600 to a 5060Ti 16GB. Genuinely so much happier. I hope AMD fixes their Radeon division.,pcmasterrace,2026-02-08 14:45:17,6
AMD,o49l5po,"Where I'm at, the 5070Ti is at least $300 more expensive than the 9070 non-xt and I have a small ITX build. For me I don't think the extra price hike for the same FPS isn't justified if I could use that money elsewhere",pcmasterrace,2026-02-08 15:35:19,3
AMD,o4a2lwe,"As an Nvidia user, I agree with all of this. I wish everyone did, but sadly we don’t live in that kind of world 😔",pcmasterrace,2026-02-08 17:01:06,4
AMD,o49ql1v,">now that devs do not optimize the games anymore and rely on upscaling  I’m just passing on those games for now, it’s the upside to not being in the same toxic relationship with nvidia, AMD, the games industry etc as most of the rest of you.",pcmasterrace,2026-02-08 16:02:28,1
AMD,o4aivnt,"Just depends on your market. In the US, for a long time 5070s were common $80 under MSRP and 5070 Tis were at MSRP. Meanwhile, the 9070/XT has only sniffed MSRP a few times. I think the opposite has been true in some markets like AUS.  Edit: I’m also not confident that the margin on AMD is good enough to have models go for below MSRP ever. Based on process / die size, it’s probably more expensive to make a 9070 XT than a 5080.",pcmasterrace,2026-02-08 18:18:54,1
AMD,o4dobb0,"Imagine spending YEARS glazing Radeon and shittin on Nvidia.... they're ""in too deep"" at this point.",pcmasterrace,2026-02-09 04:22:24,1
AMD,o496qpr,"Remember all those people that bought a 7900xtx instead of a 4080(super) cuz it was ""better value"" not only is that card worse now, but has also tanked in resale value while the 4080(super) can be sold for basically what you paid for it. Not to mention amd gives up driver support sooner, so ur amd card might potentially run worse in future games",pcmasterrace,2026-02-08 14:16:34,49
AMD,o4a12ii,"Older amd cards get better with time on linux, they have native support of things the old ones don't have on windows.   That's where it makes sense to have an old amd card. There is a channel that does all this comparisons with aging cards.",pcmasterrace,2026-02-08 16:53:39,2
AMD,o49dxts,"Seriously, it’s a red flag if one product depreciates much more quickly than the other.",pcmasterrace,2026-02-08 14:57:23,-1
AMD,o4937me,"Perhaps because their main ""gaming"" profit comes from consoles, they disregard PC gamers demand.",pcmasterrace,2026-02-08 13:55:20,12
AMD,o496uu7,Cousins,pcmasterrace,2026-02-08 14:17:14,3
AMD,o496y1f,"I just dont understand what theyre doing, they were the ones who made it, was it just gonna sit and collect dust, never to be released?",pcmasterrace,2026-02-08 14:17:45,6
AMD,o4a1svw,"Old amd gpus are fine wine in linux as they get all the support natively, fsr4 runs well there, for example an ancient rx 5700xt runs a whooping 40% better on average than on windows and even works with dx12 games.",pcmasterrace,2026-02-08 16:57:11,0
AMD,o4989er,Even the RTX 20-Series can use it. You probably wouldn't *want* to because of performance costs outside the 2080 or 2080Ti. Though just the fact that the choice is there is a great for the end user.,pcmasterrace,2026-02-08 14:25:22,16
AMD,o49k5tp,"Between DLSS 4 and 4.5, which would you rather use for the games you play?",pcmasterrace,2026-02-08 15:30:12,1
AMD,o498pi9,"... tell that to PhysX 32bit on the 50-series, which is still broken in most of them, tho they did 'allow' it in some..  and old(er) cards may be able to 'use' DLSS 4.5 - but not it's full feature set..",pcmasterrace,2026-02-08 14:27:59,-6
AMD,o4afles,">With UDNA as well they might even sunset RDNA4.  That's something I've been pondering for a while with Radeon. They've put themselves in a pickle. I assume the sunsetting of rDNA 2 and 3 drivers was in preparation for support of the upcoming uDNA architecture. They probably didn't wanna spread themselves thin supporting so many generations of cards and we all know how ""smoothly"" that ended.",pcmasterrace,2026-02-08 18:03:35,2
AMD,o49fcp6,"Yeah, not the same version as the 9000 series, but AMD accidently released source code that shows it works, and people have ported that to the 6000 series also.",pcmasterrace,2026-02-08 15:05:01,8
AMD,o49hyr1,"They  accidentally leaked a dlls file that showed it worked in RDNA 2 and 3 cards, people were quick to make optiscaler use it.  I have usee it on my 7900 XTX on games like Cyberpunk, Expedition 33 and while the performance takes a hit it certainly looks better than FSR 2.",pcmasterrace,2026-02-08 15:18:52,7
AMD,o4985cq,I have recommended Nvidia always and this is from working at team red for some time. The intention was always to force consumers to buy a new generation for FSR and also for driver updates as the plan was probably to sunset RDNA to focus on UDNA. Intel is really much a more compelling option now eith the better driver situation and lower prices if your needs are simple.,pcmasterrace,2026-02-08 14:24:43,-5
AMD,o4bbmy7,And yet my 30 series can't run framegen.,pcmasterrace,2026-02-08 20:37:27,-6
AMD,o4b4xdq,Eh. Idk if that's a great high horse alternative though. Spending more money to support an even worse company who also does shitty things (but at least they support their cards longer).,pcmasterrace,2026-02-08 20:03:43,3
AMD,o4aqvay,"That’s not the point.   They could release the version that runs on older cards. Yes it doesn’t work as well as the new cards, but it’s better than the current offering.   They didn’t include the AI technology in the cards, which maybe wasn’t the right decision in hindsight, but the guy who compiled the code from the source code leak proved it still works remarkably well on the old cards!",pcmasterrace,2026-02-08 18:55:08,21
AMD,o4b7v5l,Chuds butthurt they chose a bad card over an Nvidia card. The 9070xt is the only card amd has made that's worth buying since the 480.,pcmasterrace,2026-02-08 20:18:21,-6
AMD,o4ba17m,"As for older hardware people are angry because int8 has existed for many months now unofficially and works surprisingly really well. Yeah there's a larger performance penalty, I found it to be about 15% worse than 3. But visually even lower scaling options look better and brings that performance back up.   It's really not too different compared to the DLSS 4.5 situation on Nvidia gpus except they gave you the choice.  Newer gpus might not have this problem but now there's possible doubt that could push people away from future Radeon gpus. Hell it could be any feature they just completely drop on older cards.",pcmasterrace,2026-02-08 20:29:20,9
AMD,o49vznv,Far as I know the only difference is the data format. Intel does something similar with DP4A and XMX. They definitely have the resources for it.,pcmasterrace,2026-02-08 16:29:07,2
AMD,o499ws8,I mean its doesn't matter right there are some hardware differences between generations as long as ther previous hardware can run those models then those at least should be ported.,pcmasterrace,2026-02-08 14:34:54,8
AMD,o4agj25,"But the int 8 version of fsr 4 has been shown to work pretty okay on rDNA 2 & 3 cards already. It's not perfect, but quite a few people prefer it over fsr 3.1.",pcmasterrace,2026-02-08 18:07:54,4
AMD,o4ah9qb,"By that logic nvidia was selling obsolete hardware until the 40 series. Your 3090 can't ""properly"" do DLSS 4.5 either. No native FP8 support. Yet nvidia still allows you to decide if the performance hit is worth it to you or not. AMD could go that same route, or release the INT8 version which we know exists, yet they don't do it, because they are allergic to making good decisions.",pcmasterrace,2026-02-08 18:11:25,4
AMD,o4aibjd,Nvidia's older cards don't have FP8 support either. If I enable dlss 4.5 model on my 3090 I'll lose 30% performance compared to the older models but I can still do it. There is no excuse. Give people the option.,pcmasterrace,2026-02-08 18:16:17,2
AMD,o49jga9,>And upscaling will become more and more adopted in the coming years it looks like.  My hope is the games industry will crash and burn before we get too far in to this.,pcmasterrace,2026-02-08 15:26:31,3
AMD,o4cpeg8,"Yeah and every time a headline that say ""Nvidia is focusing on AI"" drops, tons and tons of people jump on it and criticize them and talk like AMD is a savior and ridicule people for buying Nvidia, even though literally every Nvidia headlines never matter shit for consumers, because at the same time Nvidia still giving out best features and best long terms feature supports while paying developers to optimize games for their hardware, Nvidia always goes above and beyond to support their users for the best gaming experience, who gives a fuck if they focus on AI, while AMD give NOTHING to consumers that buy them. ditch them and abandon them all, yet everyone still afraid to criticize or call them out, they've done far worse and far more evil than Nvidia and still get away with everything, the double standard is crazy",pcmasterrace,2026-02-09 01:06:36,2
AMD,o4a13s9,"Zen 5 was by all means a pretty major rework. Unless you think they invested billions into making a mediocre product you're dead wrong on that account. It's got some flaws but it's also implemented some neat ideas.  9800X3D stats got posted. It's not a perfected measurement but currently they're at 0.76% failure rate. That's actually pretty good. It seems more likely a motherboard issue instead of AMD.  RDNA 4 launch was poor and the subsidization was bad.  Redstone is up to developers to implement. Are you serious? The feature that they could implement themselves (AI based frame gen) got support pretty immediately in all games that have previous FSR frame gen versions.  Mate, you are twisting facts into getting justification for hating AMD.",pcmasterrace,2026-02-08 16:53:49,0
AMD,o49ui0b,"Fucking post-redstone W10 is basically the actual W11 MS happened to hand out for free; it's such a massive difference.  Redstone is the reason W7 holdouts were still such a strong market share all the way until the last couple years; users saw the shitshow it was in 2015 and got scared of getting into a W8 boogaloo. It was *steam and chrome* dropping support that finally tanked it out of relevance.  And now everyone has rose-tinted glasses for W10 as if it was a perfect system, lul.",pcmasterrace,2026-02-08 16:21:47,-1
AMD,o499fjs,They let you use the upscaling features which is by far the best part of DLSS as a whole. The Transformer model is a godsend for image quality and anti-ailiasing.,pcmasterrace,2026-02-08 14:32:11,21
AMD,o49sokt,"Around here, the 9070XT was $590 and the 5070Ti was $720. $130 was worth the extra for the better features and raytracing, but yeah, if it was $300+, I probably would've gotten a 9070XT or maybe gone with a 5080.",pcmasterrace,2026-02-08 16:12:52,2
AMD,o4acjwe,5070ti is a lot faster than the 9070 non xt,pcmasterrace,2026-02-08 17:49:14,0
AMD,o49r1fi,I feel like I got the last chopper out of Vietnam lol. I sold my 7900xtx in April for just as much as I bought it (around $800) and bought a 5080 for $1000,pcmasterrace,2026-02-08 16:04:42,10
AMD,o498oei,"Got my 7900XTX for almost half the price of a 4080S at the time. Or even 55% of the price (could easily be got for 22.5% the price of a 4080 (which is worse) or 40% of the price of a 4080S; ppl the 7900XTX came out well before the 4080S...). In what world is that not better value. When you play on 4K, you are playing on the Res, where upscaling is least noticeable as well. I've used both DLSS and FSR at 4K, the differance **AT 4K**, in most games, isn't much (most people who play 4K on either would agree). Or you know, you have a card that can play Native 1440/4K anyways.  Resale value for AMD was always bad, most people prefer Nvidia. Owned AMD cards before, I knew the resale value would tank. Didn't care, this is my GPU for the next 5-7 years.  Again... 55% of the price, for worst upscaler/RT performance (who even cares about RT at 4K) vs Nvidia slighly better offering, or 35% less than their slightly worse. Honestly really sad I didn't get a 4080S /s. Can people just enjoy their hardware damn, every other week, ppl on this sub complaining about something. And I do not regret getting a 4K card for a fraction of what Nvidia charges. And probably never will. Thanks for the concern, but it isn't needed.  'so ur smd card might potentially run worse in future games' gonna have to perform like trash before I start regretting paying 55% of Nvidia 4080S offering or 30% for their worse 4080 Offering, that will be Vram limited at 4k a lot quicker than a 7900XTX.  Edit: Average difference in cost once the 4080S released, as people where whining I got a good deal (which wasn't even that hard, but fair criticism).",pcmasterrace,2026-02-08 14:27:48,29
AMD,o499wmh,"lol, I’ll take a heavily depreciated AMD card over an nvidia card that holds its value because idiots are prepared to pay stupid money for it any day.",pcmasterrace,2026-02-08 14:34:53,-9
AMD,o4cf8gt,"The vast majority of people, including Radeon users are not on Linux.",pcmasterrace,2026-02-09 00:09:56,1
AMD,o49f417,"Happens often enough in software development. Someone made it, maybe just because they could, or maybe because management thought it was a good idea at the time.   But then management changed their mind. I wouldn’t be surprised if the int8 version “leak” was intentional from a dev or devs who think it’s incredibly stupid that their work is being arbitrarily held back.",pcmasterrace,2026-02-08 15:03:43,12
AMD,o4b8acc,It's probably for pisser 2. Since it just uses normal int 8 not wmma like the 7000 and 9000 series have for int 8. With the PS5 pro having the ai hardware of rdna2 just more of them which is only basic int8 well,pcmasterrace,2026-02-08 20:20:30,2
AMD,o49x1nk,"The model as probably been train with non legally own data. so they cannot open source it to make it available to everyone.  Radeon don't do proprietary tech. The goal is making FSR4++ available to all GPU manufacturer, Even the Chinese one. that the GPUopen philo.  Training a model always take several months, even a year. Now with human handpick data for training ewww...  Its will take time. Long time radeon user know to be patient with things.  Spoiled Nvidia user that switched to Radeon his past 6-7 years for what ever reason need to understand it.  Both ecosystems are hardly comparable.  Cutting Edge tech and Convenience is only on Nvidia side.  That FSR4 exist in his proprietary tech form is an anomaly that should be rectified.",pcmasterrace,2026-02-08 16:34:12,-5
AMD,o4bj67j,"Someone linked this video a few days ago in another thread regarding DLSS and for me, it helped understand why DLSS 4.5 is good and why the tech overall works amazingly well.  https://www.youtube.com/watch?v=iOY6wL6tv9w   Its more efficient, much more detail, less perceptible artifacts using the updated model (preset L)    Running modern games at native 4K is incredibly difficult - Maxed out, a 5090 at native 4K with all the features on can barely hit 30fps.   When you're pixel-peeping - you can tell there's a difference.  When you're sitting on a couch and just want to enjoy a single-player game, you're not really going to notice it, especially when you're running optimized settings on a per-game basis instead of simply running the highest available graphics preset.    You generally want to use the latest version of DLSS available due to improvements.",pcmasterrace,2026-02-08 21:14:57,1
AMD,o4b8mis,"AMD cards never supported PhysX to begin with. The only issue with 50-series cards in that comparison is that you had to manually toggle it off to get a proper frame rate (while it's automatically off on AMD cards). So it's a weird thing to obsess about when comparing vendors.  The 32-bit CUDA deprecation was announced like a decade ago, and GPU-side PhysX hasn't been a marketing point for many years. The way they went about removing it from the 50-series without announcement was kinda shitty, but it's understandable that it was dropped eventually (deprecation is a warning that a feature will no longer be supported and may be dropped entirely in the future).  The fact that they wrote an emulation layer to fix that mess is pretty neat.",pcmasterrace,2026-02-08 20:22:14,5
AMD,o4abq1q,Dropping PhysX is irrelevant for 95% of people and is in no way comparable to what we’re talking about here.,pcmasterrace,2026-02-08 17:45:19,5
AMD,o4cz5tu,"""DLL files"" is underselling what actually happened lol. They leaked the entire source code tree for FSR4, and under an irrevocable open license no less. The DLL files were compiled from that source code by a community member.",pcmasterrace,2026-02-09 02:01:07,2
AMD,o49tgm0,does it have multi frame gen?,pcmasterrace,2026-02-08 16:16:42,0
AMD,o4bcqvp,"i personally could not give a rat’s ass about frame gen, and its ideal use case is far more niche than having an upscaler with good, stable image quality.",pcmasterrace,2026-02-08 20:43:03,16
AMD,o4ccuev,"As mentioned elsewhere they make features available for older cards when possible. They make the upscaler available across every single RTX card, but the performance cost does go up significantly on the older cards. On a lower spec 20 series card running DLSS 4.5 upscaling has a **significant** performance cost to the point it might not be worth it over using the faster DLSS 4 or DLSS 3 model presets except on the 2080/2080ti. 20 and 30 series cards don't have the hardware to physically run DLSS framegen at the speed necessary, and is also why multiframegen is 50 series only as each new generation of cards have gotten more tensor hardware to deal with the increasing shift towards more capable upscaling and interpolation. It might be possible to shoehorn MFG onto a 40 or 30 series card, but it would have entirely unacceptable latency.",pcmasterrace,2026-02-08 23:56:13,3
AMD,o4cs1qe,"What worse and shitty things about Nvidia compared to AMD ?  Spending extra for a better product in everything, better performance, with much much better software, much wider game support, significantly better long term support which is proven, what ""shitty things"" about that, and why the fuck don't you want to spend more to support that, everything here is consumer-friendly and higher quality  Meanwhile the ""better"" company is abandoning all their previous gen, have worse performance, worse product, much worse software, FSR 4 is worse than DLSS4 which is even worse than DLSS 4.5, Redstone is non existence, FG is broken, game support is terrible, driver is a mess, and absolute dogshit longevity proven by how they treat their previous gens, how the fuck any of these not shitty, and why wouldn't you pay only $200 CAD to upgrade to a better product with none of these bullshits  AMD fanboys are stupid as hell, people don't give a shit about supporting a company, people want better product and not being treated like shit, and FACTS prove that Nvidia is a much more consumer-friendly company that treat their users much better, while AMD is sheer anti-consumer that treat their users like trash",pcmasterrace,2026-02-09 01:21:49,1
AMD,o4c8tyj,"Not only does the INT8 version of FSR4 work well on RDNA3, that's _without_ WMMA acceleration. If it's already running well on standard shader cores, it would run even better using the AI cores AMD included in RDNA3 (before they proceeded to gaslight us into thinking they didn't include them for this very type of thing).",pcmasterrace,2026-02-08 23:32:13,3
AMD,o49wvsd,">Far as I know the only difference is the data format.   What exactly do you mean by that?    They look different and generally yield different results.   >They definitely have the resources for it.  Never said otherwise.    All I did was giving an explanation on why it isn't just a simple ""okay, we allow it"" move from AMD but something that actually takes some investing from them.",pcmasterrace,2026-02-08 16:33:24,0
AMD,o4b1jni,"It's the ""pretty okay"" part that's the problem. They're trying to compete with DLSS and they sure as hell don't want FSR4 being seen as ""pretty okay"", but that's all it will ever be on RDNA2/3.",pcmasterrace,2026-02-08 19:47:02,1
AMD,o49a0xf,"The ""Dynamic Multi Frame Generation"" feature is the main feature of 4.5 tbh, and x6 dynamic framegen, and floating multiplier wont be available or work on older cards! :)",pcmasterrace,2026-02-08 14:35:34,-10
AMD,o4arpng,How long ago was the 9070XT at $590?,pcmasterrace,2026-02-08 18:59:06,1
AMD,o4b7cza,"For most of their lifespans, 4080 variants sold for around 1000-1200€ (launching with that shitty 1200€ MSRP that was officiall revised down to $999 with the Super version), while the XTX tried to get $1000+ but failed to sell for that price, so it dropped to 800-900€.  If you really got one at $500-600 to justify that 45% claim, it's a crazy bargain. But it sounds more like you just made a selective comparison with a great bargain vs a particularly bad offer, or at different times.  The XTX is mostly a case example which shows that people do not pay extra for more VRAM than they actually need in the current generation, but purely judge a card's value based on performance across relevant use cases.  AMD in that that case provided worse value because their card provided equal performance, but had notably worse upscaling but worse at some (albeit not all) ray tracing workloads.  And then Path Tracing released to Cyberpunk, showed what true next-gen graphics are, and AMD's high end aspirations were completely obliterated.",pcmasterrace,2026-02-08 20:15:47,3
AMD,o4a3rdm,Well I got my 4090 for free because my dad bought it as a gift to congratulate me on my first kid. Therefore the 4090 was the best value because it was 0% of the price of anything else on the market!!!,pcmasterrace,2026-02-08 17:06:44,11
AMD,o499q87,"No shit, ofc a certain card can be good value if you get it at a massive discount, your outlying experience isnt representative of the msrp which was 1000$ vs the 4080 super, also at 1000$",pcmasterrace,2026-02-08 14:33:52,16
AMD,o4aoxub,"We compare MSRP to MSRP in this house, not whatever you paid.",pcmasterrace,2026-02-08 18:46:16,5
AMD,o4b3q1r,What games are you playing at 4K without an upscaler ?! I have a 5070Ti and can't think of any game of the last few years I could play without one (unless your 4K is limited to 60hz)  Also people are being weird here. Dude is talking about their personal experience in this situation is all,pcmasterrace,2026-02-08 19:57:46,1
AMD,o49b9hz,"Look how much better the 2060 has aged than the 5600xt.  Same thing with the 3060 and the 6600xt. Same thing up and down the product lines. Make whatever purchase you want but ""AMD fine wine"" is spoiled as shit.",pcmasterrace,2026-02-08 14:42:39,8
AMD,o4cfrts,"I am aware, as a dual boot user, I'm only informing that old ones work really well there, it's a shame windows does not do the same kind of work.   Majority won't change due to anti cheat games or certain apps.",pcmasterrace,2026-02-09 00:13:01,1
AMD,o49g5hn,"Thats was ive been thinking aswell, their devs made it work on older cards, then the executives blocked it to make rdna4 more enticing, then a rogue employee leaked it because they knew it worked and amd was holding it back",pcmasterrace,2026-02-08 15:09:20,6
AMD,o49vdol,"I haven't tried it, usually I just use the upscaler. I have tried using lossless scaling frame gen and it works like a charm.   Played expedition 33 this way: 4k, 60 FPS, FSR 4 with optiscaler, everything on high and then lossless scaling x2, so it would be 120 FPS on my TV.   I found this was very stable and the input lag wasn't a problem for me, I could beat the game and post game.",pcmasterrace,2026-02-08 16:26:09,1
AMD,o4cdoor,"""20 and 30 series cards don't have the hardware to physically run DLSS framegen at the speed necessary""   And AMD could say the same about FSR4's int8 path on older GPUs. Just because people are ok with using it despite an 8ms frame time cost doesn't mean AMD would be (or should be) ok releasing the feature in that state.   The reality is (and we have ample evidence from other implementations now) that pretty damn good frame gen is possible on the 30 series, and that pretty damn good multi framegen is possible on the 40 series.   These features are used as upsells by Nvidia just like FSR4 was used as an upsell by AMD. Future features _will_ be used as upsells as well. Neither company is benevolent or your friend.",pcmasterrace,2026-02-09 00:01:11,-1
AMD,o4cwkyc,I don't think AMD is better necessarily lol. What I am saying is calling one corpo better because their software is good is kinda like saying I like getting stabbed by a knife as opposed to a spear cause the knife is efficient and the spear is just too clunky. Either way you're bleeding dry,pcmasterrace,2026-02-09 01:47:33,1
AMD,o49y5z3,"Yes, because Int8 isn't as precise as FP8 is. The algorithm is likely the same. They yield different results because 1 is officially made and the other is implemented by open source devs. Int 8 could likely run faster if AMD bother to optimize it. Int 8 could also not be as easy to accelerate as FP8 is.",pcmasterrace,2026-02-08 16:39:36,1
AMD,o4b3sfr,"Pretty okay beats the dogshit FSR3.1 looks like in comparison.   If ""pretty"" okay is the best RDNA2 and 3 cap out at, then AMD should give us that instead of leaving us with shit because the pretty okay isn't amazing like DLSS 4.5.  Done some testing in Cyberpunk when I bothered to tweak the drivers and yeah FSR4 INT8 is better than dp4a XeSS any day, and clears FSR 3.1  Besides yeah as another comment pointed out, RTX 30 (and 20) series don't have FP8 either yet they run DLSS 4.5... not fast, but Nvidia gave that choice even if their old tensor cores are doing it via INT8 that starts to strain under 4.5's algorithm, which FSR4 INT8 is not to that point either btw, it's relatively heavy but you won't lose fps at anything lower than 100fps.",pcmasterrace,2026-02-08 19:58:06,3
AMD,o49e9h7,Frame Gen is cool but not everyone likes using that. Upscaling is the more useful feature of DLSS overall.,pcmasterrace,2026-02-08 14:59:07,13
AMD,o49eid4,"I'm yet to see MFG discussions, everyone talk about M/L presets added with DLSS4.5. Which are, apparently, miles better than FSR4 and work on crap like 2060 from 2018.",pcmasterrace,2026-02-08 15:00:27,6
AMD,o4asxjn,A few months ago in November at Microcenter.,pcmasterrace,2026-02-08 19:04:54,1
AMD,o4c8cp5,"4080S was selling for $1300 when I looked, 4080 was \~$1000, and a 7900XTX could be had for $750-800. You could get the 7900XTX for 40-45% cheaper. Are you forgetting that a 7900XTX **was out for some time**, when the 4080S was released? And the 7900XTX was being heavily discounted, while the 4080S sold well above MSRP and was scalped post released.  I snagged a good deal, but either way most peopel could get a 30-40% discount vs 4080S for performance between a 4080 and 4080S, with more VRAM than a 4080, was a good deal. Or 20-25% less to buy than a 4080, while giving better performance, and having more Vram.  Even in your own numbers a 7900XTX is better than a 4080, and cost 35% less LOL... Bruh... Defo no value, in a card, being cheaper, having 50% more Vram, having better performance than a 4080, for worse RT/Upscaler, none at all...",pcmasterrace,2026-02-08 23:29:26,1
AMD,o4b4l0y,"Unironically yes, that is how it would work. What is the point you're trying to make here ? If you got it for free that would be the best value card you personally experienced.  Why is everyone treating that comment like they said ""this is my experience and therefore it is every experience"". They're quite clearly talking about a personal experience, god damn.",pcmasterrace,2026-02-08 20:02:01,0
AMD,o4a7zxp,The MSRP was set at $1000 when the 4080 MSRP was $1200. The 4080 Super came out over a year later so comparing MSRP pricing is disingenuous and not fair to the argument.,pcmasterrace,2026-02-08 17:27:13,3
AMD,o4b43cp,They never said their experience was THE experience though,pcmasterrace,2026-02-08 19:59:35,1
AMD,o4c7nhk,"Lol bro comparing a 5070TI to a 7900XTX at 4K, not even in the same league.",pcmasterrace,2026-02-08 23:25:17,1
AMD,o49byk0,"And as long as that ageing is reflected in higher prices, I don’t care. I’ll fight tooth and nail to avoid paying more than about AUD$400 (the lower the better) for a graphics card. Gaming is not important enough to spend stupid money in it",pcmasterrace,2026-02-08 14:46:31,1
AMD,o4cei9d,"Oh, I absolutely agree that int8 fsr should be made officially available.  As for non-DLSS framegen, like with Lossless Scaling, I gotta be honest, it's kind of bad unless you already have a high base framerate such that the time delta between frames is tiny. Like, REALLY bad artifacting, 2015 TV motion smoothing levels of ""I mean, I guess if you only look at smoothly panning camera shots"".  It works somewhat ok for certain types of gameplay, and is just a straight downgrade for others. Screenspace/postprocess framegen is just inherently not well suited for gaming.",pcmasterrace,2026-02-09 00:05:54,2
AMD,o49ywk8,What even is your point?,pcmasterrace,2026-02-08 16:43:11,2
AMD,o4b6oq6,"Sure but, again, they're not competing against FSR 3.1 or XeSS. They're trying to beat the notion that DLSS is the only worthwhile upscaling technology. ""Pretty okay"" performance doesn't help them do that, unfortunately.  I'm not excusing them to be clear. This is 100% their fault for the lack of hardware features in older GPUs.",pcmasterrace,2026-02-08 20:12:26,0
AMD,o4cpw7j,"It wasn't hard to get any of the 40-series cards (including the 4080 Super) at MSRP if you waited a bit after the initial release hype. It's still true that the XTX was a good deal at $800. I just see that as a 20-30% discount rather than 45%.  I looked at that comparison quite closely in late 2024, when I switched from an i5-13600K to a 9800X3D. Because that ment changing mobo and RAM as well, I decided to pair my old processor with a new GPU to build a PC for my brother for christmas.  I considered anything from a 4070/7700XT to 4080/7900 XTX for that build, but at the pricing at the time (1050€ for a 4080 Super, 850€ for an XTX) I still couldn't convince myself to go with AMD.  For a GPU in that price bracket, I found that path tracing was a major distinction. Cyberpunk Overdrive was the first true generational leap in graphics since Crysis. And other than Crysis, it could actually run at well over 60 FPS on current-gen hardware. Which ment I had also used DLSS extensively on my 4090 and saw the value in a better upscaler as well.   Even at 200€ below the 4080 Super, the XTX just seemed too expensive for not supporting that. Without caring about true current gen graphics, a much cheaper GPU at the 70-tier would have done fine anyway. And the extra VRAM of the XTX is not doing anything for games. There are no meaningful limits to 16 GB yet, and that doesn't seem to be changing any time soon.  I ultimately went with a non-super 4080 for around 800€ because I happened to get a 100€ coupon for used hardware on ebay, which has done a great job since.",pcmasterrace,2026-02-09 01:09:20,1
AMD,o4adtl1,"Well the XTX guy said they got it for half the price of a SUPER. That would make the regular 4080 less relevant and actually put the XTX at an advantage since it's been out long enough to get deeply discounted, no?  Man it's kinda wild talking about graphics card discounts, on the Nvidia side I don't remember seeing any since Pascal or that brief period after the covid boom where they were trying to clear out leftover 3090Tis.",pcmasterrace,2026-02-08 17:55:19,6
AMD,o49crx1,"I never said they were the same price the entirety of their lifespan, that was their msrp and could be bought at that price for a while",pcmasterrace,2026-02-08 14:50:58,3
AMD,o49dls6,People spend all sort of money on their hobbies.  I simply oppose the constant selling of AMD as the better buy for people who are budget constrained.  They are being mislead about how their purchase will actually pan out for them over time and that matters for more for someone who can't scrape up the money for their hobby often.,pcmasterrace,2026-02-08 14:55:32,4
AMD,o4a24ui,My point is that there's no reason for AMD to not release that model for RDNA 3/2. To pretend that it'll need to be trained on separate data or need more maintainence isn't true. It's the same model with a different datatype.,pcmasterrace,2026-02-08 16:58:48,2
AMD,o4dr7q7,"Well leaving the majority of their userbase with crap FSR 3.1 certainly won't aid them in that. FSR4 INT8 is at least a respectable, competent upscaler that isn't too far behind its legitimately good FP8 counterpart... it just has more overhead that reduces its high fps potential, but it's still valuable at sub 60fps as the clear winner to get the fps up.  FSR2 and 3 era are exactly when AMD got the rep for being behind, especially FSR3 era when Nvidia was putting on DLSS3 which was a truly good upscaler (FSR4 FP8 slightly wins over DLSS 3 after all, and FSR4 is said to be good). FSR4 INT8 is at least something comparable to DLSS3, HUB's testing had it behind FP8 but it was mostly just being a slightly less sharp and detailed reconstruction, which at 4K quality and such it was VERY hard to even tell the difference by their own admission. Lower res is when yeah full FSR4 gains a bigger lead, but ultimately that's also where DLSS4 gains over FSR4 as well because when there's less raw data to work with that's when more and more of the job is put onto the upscaler to make look good to begin with.",pcmasterrace,2026-02-09 04:42:14,1
AMD,o4aen2f,"The first guy referenced people that bought the 7900XTX because it was a better value.   The second guy gave his anecdote of buying the 7900XTX at a better value price.   The first guy then came back and tried to say $1000 7900XTX vs $1000 4080S because MSRP, which doesn't even make sense because the 4080S came out over a year later and caused the steep discounts on the XTX.  I dunno man, first guy seems to be talking out of both sides of his mouth.",pcmasterrace,2026-02-08 17:59:12,6
AMD,o4a5ivs,">My point is that there's no reason for AMD to not release that model for RDNA 3/2.  Apart from cost, which is a reason for a profit driven company.   >To pretend that it'll need to be trained on separate data  Never did that. I think you don't understand my comment.   > or need more maintainence isn't true. It's the same model with a different datatype.  I think you have no idea about how development work actually works and how maintaining two datatypes would affect that.",pcmasterrace,2026-02-08 17:15:15,2
AMD,o49dfzf,"Yeah in ur region, not eveywhere is the same",pcmasterrace,2026-02-08 14:54:38,0
AMD,o4crl5h,"As long as your house's wiring is up to modern code, and your surge protector is rated for the full amount of power, it's fine.   If both are true, the breaker will blow before you run into a power related issue.",pcmasterrace,2026-02-09 01:18:58,11
AMD,o4crqey,of course. your circuit breaker should trip before the outlet even gets warm. hopefully.   all the crap in my room runs off a total of 3 outlets via power strips and UPSes.,pcmasterrace,2026-02-09 01:19:49,5
AMD,o4cw9b6,"I run my main gaming rig and a lower power plex server off the same outlet. Most home outlets are on 15 amp circuits. 15 amp at 120v is 1800 watts, and it’s highly unlikely that your two computers combined will be pulling 1800W (obv also gotta factor in anything else plugged into the circuit)  Point is, you should be totally fine. The breaker will blow before it does any damage worst case. I had to go flip the switch on my breaker box once in my office cuz I forgot my space heater was plugged into the same circuit so that plus the two PCs was enough to overload the circuit   FWIW my gaming rig pulls at maximum like 450W if both my GPU and CPU are going full blast",pcmasterrace,2026-02-09 01:45:51,1
AMD,o4cwvat,"It depends on how many other devices are on the same circuit, and what that circuit is rated for.",pcmasterrace,2026-02-09 01:49:06,1
AMD,o4czch9,">Will it be safe to run two PCs from the same outlet?  Yes, 99% of the time  >First PC is an ABS Ryzen 7 9700X 32gb ram with a Radeon 9060XT 16gb Vram  I'd call this roughly a 300 watt load while gaming. Throw in another 100 watts for monitors, LED desk lamp, etc for a total of 400 watts  >Second PC is a BosGame mini PC Ryzen 7 6800H 32gb ram with Radeon graphics  Less than 100 watts?  >Bought the mini PC to make it easier for moderating/video/ stream editing so it really won't have any intensive tasks other than YouTube while the main gaming PC is doing the streams  Yeah I would hope definitely less than 100 watts  >Just wondering if it would be safe to have both PCs running from the same outlet  You need to consider the electrical breaker that powers that outlet, as well as the other devices that are on that circuit/breaker. In North America you will typically have a single 120 volt 15 amp breaker powering multiple outlets in a room/floor. Since power equals voltage times current 120V RMS * 15 amp is 1,800 watts per breaker before it should trip.   >and I do have a surge protector extension cord that I could use instead if that would be better  One surge protector plugged directly into the outlet with both computers plugged into the surge protector should be just fine. Do not plug a surge protector into another surge protector. Do not run other high wattage devices on that circuit like space heaters, electric kettles, etc. Plan to use around 1,200 watts of devices on a 120 volt 15 amp circuit and no more",pcmasterrace,2026-02-09 02:02:05,1
AMD,o4dgker,I've done it for 8 years with no issues. My girlfriend and I have out computer desk back to back and all that is plugged into same surge protector. Sometimes my laptop is in use and plugged in as well.,pcmasterrace,2026-02-09 03:33:22,1
AMD,o4csfjv,"Your second PC isn't much of a power hog, it shouldn't be an issue. Just get a good power strip with safetly features",pcmasterrace,2026-02-09 01:24:19,1
AMD,o4csf2t,"It's mostly down to the power supplies and monitors 2,000 wats is going to be pushing it for a 20 amp breaker.   If your power supply is less than 1000 per computer, you should be ok. Unless something else is pulling from that same circuit. Multiple outlets can be on the same breaker...",pcmasterrace,2026-02-09 01:24:14,0
AMD,o4cufa1,"Honestly if you're in America and on a 15 amp circuit I wouldn't dare do that! As someone who worked in electrical, there is a REASON we tell people NOT to exceed 13 amps on a 15 amp circuit! More AMPS bring more RESISTANCE. Wanna know what RESISTANCE brings? 🔥FIRES🔥  Unless each pc has a 450 watt power supply and you separate the monitors on a different circuit I wouldn't do it... Most modern pc's nowadays have at minimum an 850 watt power supply. Two of those is definitely exceeding safe operating standards. What happens if there is a transient spike? A normal circuit will not hold that!",pcmasterrace,2026-02-09 01:36:14,-2
AMD,o4crpyy,"Thanks, appreciate the answer.",pcmasterrace,2026-02-09 01:19:45,1
AMD,o4cxbq7,"This is not always true. If you are running off a power strip you need to make sure that power strip is rated for the full amount of your outlets. An underspecced power strip can cause problems, while your houses circuit breaker won't trip, but the power strip will have exceeded its limits and possibly cause a fire.   If the strip is up to proper standards then yes the main breaker will trip properly instead.",pcmasterrace,2026-02-09 01:51:32,3
AMD,o4ctyzm,"Fwiw a 20 amp breaker with 12awg wiring is rated for 2400 watts non continuous, or 1920w continuous load. Computers aren't a continuous load, even if they're always on.  A 1000w power supply doesn't use 1000w all the time. It provides whatever the computer needs and no more. I think the 9070xts are typically pulling between 300 and 400w. Big cpus are around 200 to 300. The rest of the stuff can be pretty minor... and they only use that much when they need the power to, not all the time.  Two such computers will be sitting under the 15amps or 1800 watts or a normal household circuit. Even with peripherals.",pcmasterrace,2026-02-09 01:33:45,2
AMD,o4cwb9r,"The power supply isn't the main issue, I have a 1kw psu in my pc and it's never pulling that much even if I tried.   Heck my little canister vac actually pulls 1kw and so does my little smoothie blender, the pc isn't even in the same ballpark.",pcmasterrace,2026-02-09 01:46:07,1
AMD,o4d4cof,That second PC probably draws less power than a big monitor would. Generally good advice but the mini PC is barely a PC in this context.  Edit: yeah the 6800h is a 45W chip and the rest of the computer isn't going to be more than that so like 100W total. My monitor draws about that.,pcmasterrace,2026-02-09 02:28:00,1
AMD,o4cysdn,"well, the use of good power strips and identifying what's trustable comes from just understanding how much power one can pull from a given circuit. many of the ones I use are made up of 3 quarter inch wide strips wired to a switch and a bit of cable and maybe some surge protection. cheap but they work.  obviously there's shady ones out there but if you stick with reputable brands like GE and Eaton and not rando crap then it should be fine.  none of my strips are pulling nowhere near their rated load capacity to be a concern. I wouldn't use them if I didn't trust it.",pcmasterrace,2026-02-09 01:59:15,1
AMD,o4d5522,"I just reread the original post and saw the thing about the mini pc, somehow I first missed it 🙄🤪. Yeah you're right. I was envisioning two fully fledged gaming pcs with full towers, 2×850 watt+ PSU's etc...  That's what I get for reading while being dead tired!",pcmasterrace,2026-02-09 02:31:52,1
AMD,o3ne0or,I completely forgot to mention which AIO I had that's the thermal right Frozen Notte duel fan,pcmasterrace,2026-02-05 02:44:55,1
AMD,o3qxvjf,"Try looking on ebay or contact the manufacturer, I lost mine, contacted Lian Li and they sent me replacement",pcmasterrace,2026-02-05 17:13:45,1
AMD,o3nsh3l,Did ya use Google before posting?,pcmasterrace,2026-02-05 04:12:54,1
AMD,o3nc18l,Not much info to go off of but I checked eBay and it seems you can replace the aio for around $20.,pcmasterrace,2026-02-05 02:33:44,0
AMD,o46lbsy,"9/10 I’ve contacted companies overseas about missing or broken parts they shipped replacements or accessories free no questions asked, monsgeek was the only outlier.",pcmasterrace,2026-02-08 01:57:40,2
AMD,o3nwmif,Ya 😢,pcmasterrace,2026-02-05 04:40:39,0
AMD,o43280i,"Surely Valve is going to lobby for this, considering that the Steam Machine is stuck with RDNA 3... right?",AMD,2026-02-07 14:31:23,170
AMD,o42uvty,"""You had your time"" -AMD to 7900xtx owners",AMD,2026-02-07 13:49:08,217
AMD,o42sxxo,Radeon Team:  https://i.redd.it/be3i6kxls2ig1.gif,AMD,2026-02-07 13:37:32,220
AMD,o43j7u2,"As much as I hate Nvidia for what they're doing with AI And fucking up the consumer GPU market, at least you can use dlss 4.5 all the way back to 20 series cards. They just said you're going to use a performance hit if you use it on 20 series or 30 series.",AMD,2026-02-07 15:59:20,51
AMD,o42w9y2,Amd never misses an opportunity to miss an opportunity.,AMD,2026-02-07 13:57:22,159
AMD,o42yl8i,The amount of people fanboying amd and trying to discredit hub here is actually insane. This is such an important topic yet all the people calling them paid intel and nvidia shills is hilarious lmao.   Just because I buy amd cpus and gpus doesn't mean they shouldn't be ridiculed from making stupid easily avoidable mistakes,AMD,2026-02-07 14:10:49,121
AMD,o44k63g,This is why Nvidia keeps dominating. AMD is actively fucking over previous gen buyers while Nvidia gave DLSS 4.5 to rtx 2xxx cards. Just insanity.,AMD,2026-02-07 19:01:25,30
AMD,o42s0g0,It would be nice to have FSR 4 on RX 6900 XT. I was planning to upgrade from my RTX 3060 but seeing how AMD kinda doesn't care about their older GPUs made me think about it. I still might do it but I don't know.,AMD,2026-02-07 13:31:49,40
AMD,o43bmtl,AMD really needs to bring FSR4 all the way down to 6000 series. Absolutely dumb not to,AMD,2026-02-07 15:21:56,16
AMD,o4336fq,I am not sure why this is being framed as support for legacy products but AMD is still announcing new Strix Halo APU in January.,AMD,2026-02-07 14:36:43,7
AMD,o42uzhl,"Valve is AMD partner. Steam Deck was RDNA 2 and sold a lot. Steam Machine is RDNA 3, and could sell well. Also, Sony announced that the [PlayStation 5 Pro will receive the full version of FSR 4 in 2026](https://www.techpowerup.com/338569/playstation-5-pro-to-gain-full-amd-fsr-4-integration-in-2026), and PS5 Pro is RDNA 2/3 hybrid. In my opinion, AMD is just working with Valve and Sony to optimize the model, which could be why it hasn’t been officially released yet, but will be released probably during 2026.",AMD,2026-02-07 13:49:43,59
AMD,o43y1xo,AMD was always the best at snatching the defeat from the jaws of victory.,AMD,2026-02-07 17:12:16,22
AMD,o44oajb,"FSR4 and DLSS4 are **great** features and IMO close to essential for long-term gaming use.  The 6800 XT and 3080 came out at about the same time at similar price points (but killed by crypto) with similar performance and the 6800 XT even had a killer feature of a lot more VRAM.  But that feature is negated as the 3080 runs DLSS 4 and the 6800 XT runs neither.  Making it pretty clear which company I'll be buying from in the future.  Pull your head out of your ass, Radeon division.  Yes I know you repeatedly show yourselves incapable of this but for some stupid reason I hold out hope that *someone* there will eventually figure out the simple logic of how to properly support a product, especially as your direct competitor repeatedly hands you the blueprints how to do so.",AMD,2026-02-07 19:22:17,8
AMD,o45cp1f,"this is why AMD has sub 10% dGPU share, why you shouldnt buy Radeon. Its hilarious that there wasnt ""high end"" this gen from AMD because they were supposed to focus on market share (their words). Absolutely nothing changed tho.",AMD,2026-02-07 21:32:47,9
AMD,o44gvoq,And this exactly why I upgraded to a 5070ti instrument of a 9070xt,AMD,2026-02-07 18:45:08,5
AMD,o43vhb4,"I know all corpos are bad, but this is definitely the last time I’m buying amd gpu. Got a 7900 xt and it’s pathetic how poorly this thing has been supported given it was a $1100 card. Thankfully it has the raw performance to do everything I need of it reasonably but headaches with drivers and power draw early on increased the cost of the build to what would’ve been a comparable 4080 super build. Typical case of a company forgetting who its original audience and demographic was - good riddance.",AMD,2026-02-07 16:59:34,13
AMD,o44cb43,"I recently used FSR4 in GoW with my 7900XTX, and can only agree how ridiculous this situation is.",AMD,2026-02-07 18:22:41,4
AMD,o474xuf,"AMD, release INT8 FSR4 for RDNA2 and 3 already!! Name it an ""experimental feature""/""FSR4 Lite"" and it'll be fine - I don't mind it costing 2ms on a 6800XT to create a 1440p image.  Also, stop leveraging Valve into not providing FSR4 support for RDNA3 in Proton.  First you try to axe RDNA2 driver support early, and then you're ditching a fully developed software solution that demonstrated it can markedly improve the experience of existing users. For what - trying to bully them into buying a new GPU instead?!  - **You're currently playing scorched earth tactics against your own customer base and they are rightfully mad about it.**  - **In comparison to the competition, you're not doing the bare minimum in HW and SW support plus feature set anymore!**   - **This laziness, together with ""Nvidia minus 50 bucks"" as a pricing stategy just amounts to a bad deal with poor value.**  - **Fix this ongoing PR disaster, undo the reputation damage before it becomes permanent. The next step should be obvious and doesn't even pose an economic risk. Stop being hostile towards loyal customers!**",AMD,2026-02-08 04:04:12,8
AMD,o43rho1,Imagine if it was just fully open sourced from the start and Valve provided anti-cheat so it could be allowed for online play.,AMD,2026-02-07 16:40:02,3
AMD,o44ypji,"It really seems like RTG is not being led by someone with a vision for gamers . . . it seems really detached from the market and consumers. Lack of strix halo products, lack of FSR4 on older GPUs, FSR4 in more titles . . . They have some great tech but it seems like they are floundering so hard. Very frustrating to watch because there is so much good potential there.",AMD,2026-02-07 20:17:05,3
AMD,o463lmf,Ditching AMD as soon as the RTX 50 super series drops if the 5080 Super has 24GB of ram unless they port FSR 4 to RDNA3.,AMD,2026-02-08 00:08:58,3
AMD,o46aea1,"I'm getting nvidia next gen, no matter what happens. I cease to believe that amd will change with support and adoption",AMD,2026-02-08 00:49:10,3
AMD,o46atra,"AMD had so much goodwill with the 7xxx series and the pricing of the 9xxxx series. IDK why they’re shooting themselves in the foot with gate keeping FSR 4. If it’s less performant on older cards, who gives a shit, just give people the option. That’s what Nvidia did with the newest DLSS and everyone liked it",AMD,2026-02-08 00:51:45,3
AMD,o43yit1,Yup. Went from RX 6600 XT to RTX 5070 Ti. Got tired of driver updates that crash my PC with black and blue screens.,AMD,2026-02-07 17:14:35,7
AMD,o44ys57,lol - one of the many reasons i went back to nvidia begrudgingly,AMD,2026-02-07 20:17:28,3
AMD,o4ab12u,"Pressure them, to release this... also pretty sure they don't care until they are selling all to AI.",AMD,2026-02-08 17:41:59,2
AMD,o42ux3d,"Man, I'm not even gonna watch it if they don't at least give me a hint of what the issue is in the title. Hate this click bait bs",AMD,2026-02-07 13:49:20,21
AMD,o46g3jg,I switched to AMD because they seemed to be doing the opposite of Nvidia's crap with this. And now they've betrayed me and other buyers? This might be my first and last AMD GPU (7900XTX).,AMD,2026-02-08 01:24:36,3
AMD,o436n32,People who think steam machine can get rdna 4 feature are day dreaming. The machine is already produced and near release. The specs were freeze long ago.,AMD,2026-02-07 14:55:44,1
AMD,o46rpv1,I openly defended AMD for months but man I was wrong. It's really so stupid. Get it together and finish it.,AMD,2026-02-08 02:37:51,2
AMD,o471tl4,FSR is a huge disappointment with how far behind it is vs DLSS. 😔,AMD,2026-02-08 03:43:11,2
AMD,o441s3d,"Certainly this year with its ridiculous high prices would be a massive show of good will to AMD fans.   Hopefully Valve is pushing on them at a higher level as it would greatly help Machine that is on RDNA 3, this also could be a reason to have not released yet as Valve has worked very closely with AMD and with their push to Linux OS and by extension AMD hardware... so maybe they are releasing Int8 with Machine to show solidarity with Valve?",AMD,2026-02-07 17:30:45,1
AMD,o45zc9w,The new leaders are incompetent but because AMD has such a CPU advantage for desktop it will mask their incompetence for a few more years. They will bring AMD back to where it was before Ms. Su saved the company.,AMD,2026-02-07 23:43:06,1
AMD,o46ejak,I hope that it happens. FSR 2 & 3 being supported by the RX 570 allowed me to get a ton of mileage out of it.,AMD,2026-02-08 01:14:44,1
AMD,o45iuk8,"I have been buying AMD at release for the last 20 years, my last 3 GPUs were an R390, an RX480, a 5700 and my current GPU is is the 7900 xt and its a similar situation with CPUs since the first ryzen generations to now the 9800x3d. I am never buying AMD ever again! This is the kind of practices why I haven't been buying Nvidea or Intel, locking out features that we all know are possible on hardware that is one generation behind instead of at least introducing it as an experimental feature or part of beta driver shows how much this company that was relatively consumer friendly went from you can flash a higher tier or even next gen GPU bios and keep the same Mobo platform with AM4 from 1700x to 5800x3d to the same anti consumer crap the other teams are doing, and since the other teams have better features and support AMD lost the one reason why I keep coming back to them. Never again!",AMD,2026-02-07 22:05:48,1
AMD,o44x2u5,"FSR4 is much slower than previous versions, especially without full hardware acceleration. Upscaling tech is literally only for squeezing higher framerates out of cards. Although I understand wanting features supported on old cards, porting FSR4 to older hardware is unlikely to achieve the one thing it needs to do; allow the card to render games at acceptable framerates by upscaling.",AMD,2026-02-07 20:08:24,1
AMD,o48962f,So where can we snag that DLL?,AMD,2026-02-08 09:51:55,1
AMD,o43r56z,"Haven't watched the video, but in AMD's defense. Smaller native floating point makes such a big difference in performance, quality and such a different beast to tune that it doesn't really make sense for AMD to invest in FSR yet.  FSR4 requires FP8, which is only natively supported by RDNA4. Nvidia already showed that FP4 gives you double the ai performance for visually the same quality as FP8. From a technical stance, the a lot of work the software radeon team will have to tune FP8 FSR4 will have to be redone whenever Radeon supports FP4 natively.  While it sucks that we don't get FSR4 on rdna 2 or 3, AMD themselves aren't willing to place a dead-end int8 version under the same label as their current stepping stone. Integer and floating point are different enough that optimizations in 1 don't translate to the other a lot of the time.  They also have to communicate with the hardware team to decide what is and isn't a viable development route. Tensor cores are essential for running transformer based upscaling. The hardware team not developing tensor cores held radeon back from developing transformer based FSR (FSR4) sooner. While Nvidia put tensor cores in their gpus since 2018 RTX. AMD only started with 2025 RDNA4, so AMD is playing 7 years of catch up. Even the first Intel Arc cards from 2022 had tensor cores.",AMD,2026-02-07 16:38:19,-2
AMD,o47732n,It's not exclusive to just AMD. Nvidia and even Intel are bugging out and selling higher. They are only aiming at the high VRAM units because they can't supply that unit until the market levels off. Any other company that had plans is now probably going to spend the next two years waiting on AI to make a profit before they shell out a new model and more than likely they are going to cater to the budget market because the market will be flooded with memory wafers.,AMD,2026-02-08 04:19:10,0
AMD,o43kna7,"I think the problem is that, the INT8 version of FSR4 is technically worse than the FP8 version, AMD can't release the INT8 version legally (otherwise that would be classified as false advertising).  They should either call it FSR 3.9 or something, or just make it open source permanently",AMD,2026-02-07 16:06:23,-5
AMD,o4718pm,Glad my eyes are shit. I'm happy with 1080p and and not needing any of this upscaling.,AMD,2026-02-08 03:39:18,0
AMD,o49ra0w,"It is probably because FSR 4 upscaling is worse on older models since they lack the AI cores to run it right. AMD is probably worried about people using the 7000 series cards in comparison videos.  You can see the Int8 version is a lot softer and lacks a lot of the sharpness and detail you get in the FP8 version. HUB also didn't include FPS numbers. I suspect the performance hit is notable.  Not a very smart move, but can I see why they might do it.",AMD,2026-02-08 16:05:54,0
AMD,o4a3dn2,I would buy a 9070 XT only if AMD would release FSR4 for my 7700XT.,AMD,2026-02-08 17:04:52,0
AMD,o45pavn,this green shill again....,AMD,2026-02-07 22:41:37,-3
AMD,o42xc1i,so much repost spam of this,AMD,2026-02-07 14:03:36,-16
AMD,o44042j,"I think Tim is missing the point a little here. AMD has opted (finally, and rightfully) to follow Nvidias example where it comes to their perception in the market.   Let me elaborate. Nvidia releases a software feature that doesn't work on the previous generation of GPU’s. Why do they do this? Is it because it simply wouldn’t work? Or because they just want to screw their customers with older cards? Or because they want to encourage sales of the new GPU? Maybe all 3, who knows.   But one reason that I find is often overlooked is that Nvidia knows and cares tremendously about brand perception. They learned that with the first version of DLSS. They were a laughing stock for a year because of how shit it was.  So Nvidia will not put out software for GPU’s unless it meets a certain quality standard. Now look at DLSS. The brand perception is so well regarded because they lock it down to only the cards it works well on.  To circle back, I think this is exactly what AMD has finally learned. They've been dogged and trashed for years because of their shoddy software features, and now they’re taking a page out of Nvidias book.   Yes Int8 FSR 4 could work on older cards, but it would muddy the waters of brand perception. RDNA3 and RDNA2 users would think it’s ok, whereas RDNA4 users would have a much better experience. Thus leading to inconsistent messaging from users, and the word of mouth effect is dramatically weakened.",AMD,2026-02-07 17:22:29,-11
AMD,o42tk58,"Bla bla bla AMD bad, Nivea good, our last 2 weeks videos only had 70k views.",AMD,2026-02-07 13:41:15,-45
AMD,o44auoy,People dumb enough to buy rdna2/3 seething. Cope harder lmao. Maybe the 9000 series will be amds rtx 2000 maybe it'll just be left when the 10 series comes. Ether way if ga dumb enough to risk amd then cope,AMD,2026-02-07 18:15:37,-10
AMD,o42v5lq,fsr4 is codeveloped with sony. i guess releasing it for older rdna will need sony approval,AMD,2026-02-07 13:50:43,-23
AMD,o43b5du,"Valve's latest update on the Steam Machine mentions that they are investigating ""improved upscaling,"" whatever that means. The most innocent explanation is that AMD is still actively working on it and don't want to release it until Sony and Valve deems it ready.",AMD,2026-02-07 15:19:29,97
AMD,o43fba2,You just get a free copy of Lossless Scaling in your steam library,AMD,2026-02-07 15:40:21,34
AMD,o447nau,"Not sure to be honest. vkd3d-proton doesn't officially support FSR4 on non-RDNA4 cards yet. The dev a while back said   \> This decision is above my paygrade and it's done to work toward being able to support FSR4 more officially in Proton.  Seems like AMD is blocking them from making it available for older cards for at least the official proton builds. Community proton versions like ProtonGE offers FSR4 for RDNA3 with an easy launch arg, but no such thing is possible yet for rdna2 where you still need to swap dlls.",AMD,2026-02-07 17:59:47,11
AMD,o46zmrk,AMD forced Valve into not bundling FSR4 support for RDNA3 with Proton…,AMD,2026-02-08 03:28:46,3
AMD,o434fzg,Me realizing my not even 4 year old top of the line card isn't considered good enough anymore:,AMD,2026-02-07 14:43:39,103
AMD,o43eknc,Just them skipping a generation of high end cards and leaving me with no upgrade path was enough to get me back to Nvidia after 3 generations of AMD.  And they've done nothing but vindicate that choice ever since.,AMD,2026-02-07 15:36:41,35
AMD,o44j2kh,At least Intel's XeSS is a very good option and leagues better than FSR 3.1,AMD,2026-02-07 18:55:56,3
AMD,o47jm80,? It still runs as good as new and I don't care about upscaling yet,AMD,2026-02-08 05:55:54,1
AMD,o444vvb,I was seriously considering that card. Glad I didn't buy it.,AMD,2026-02-07 17:46:12,1
AMD,o43qcki,More like the marketing and executives,AMD,2026-02-07 16:34:24,45
AMD,o44su3h,"Still don’t understand how it’s nvidias fault that AI is blown up, it basically uses their gamble tech from far before whole AI shenanigans started. As corpo they followed the profit and capitalized at their advantage, thats textbook capitalism",AMD,2026-02-07 19:45:59,10
AMD,o42xjrf,![gif](giphy|ZuZYKBB2rISt383i1T)  Guess they took Marge's advice.,AMD,2026-02-07 14:04:51,27
AMD,o43k8bt,Funny because they get called AMD shills a lot lmao,AMD,2026-02-07 16:04:20,47
AMD,o43s563,Basically any comment made that even remotely disparages AMD gets immediately downvoted by amd fangirls and flamed for being intel/nvidia fanboys. They’re legitimately insane,AMD,2026-02-07 16:43:14,24
AMD,o44r9hc,It's Reddit what were you expecting?,AMD,2026-02-07 19:37:45,6
AMD,o43yhjr,"AMD has spent a lot of time and resources at building their own cult.  The cultists will always defend it, no matter how bizzare or stupid decisions will they make.",AMD,2026-02-07 17:14:24,9
AMD,o45qxdw,"It's really not, most consumers have nfi what this shit is yet alone who makes the GPU in their gaming platform.",AMD,2026-02-07 22:50:54,1
AMD,o47wobs,"I've seen like 3 comments like that, disparaging Radeon is much more common in r/AMD than defending it",AMD,2026-02-08 07:53:41,1
AMD,o46ep0e,"And people are willing to switch teams, pay the premium, and tell their friends/families to buy Nvidia bc ""it's simply better, set and forget""",AMD,2026-02-08 01:15:44,3
AMD,o42y04y,"I had a 7900XTX and went to a 4090 because I got a really good deal. Honestly, ignoring the power, DLSS alone is worth it. The fact that they keep bringing out new models that work on older cards as well is a big plus. I loved my 7900XTX until I got a 4K monitor, then it went downhill being stuck on FSR3. I hope AMD has a change of heart, but the lack of a tensor core equivalent on RDNA 2/3 is always going to be a problem.",AMD,2026-02-07 14:07:28,22
AMD,o45igvl,"FSR 4 on 6000 series is one thing.   But gap between FSR 3.1 (driver override) and DLSS 2.0 (driver override) games is so insanely huge is not even funny, And it can be used on any RTX (so 2000 series and up) card.",AMD,2026-02-07 22:03:45,2
AMD,o42xtsl,"Matter of time, imo, see here: https://www.reddit.com/r/Amd/comments/1qya3yu/comment/o42uzhl/",AMD,2026-02-07 14:06:28,1
AMD,o46g5j8,"I was on a 6700XT and tried INT-8 FS4. It's pretty good except that I'd need to mod the drivers and DLLs into every game that used FSR 2 or newer. If I updated, I'd have to re-mod those drivers again. I ended up getting the 9070 bc the ML-accelerated models look better and get more support.  But I do agree the 6900XT still has a lot of power to run INT-8 FSR effectively well. Maybe they're afraid Nvidia will launch DLSS 5 that beats FSR 4 out of the water.",AMD,2026-02-08 01:24:57,1
AMD,o4cu8av,Especially they are still actively producing and selling mobile APUs based on rdna3 and rdna2.,AMD,2026-02-09 01:35:12,2
AMD,o42znzi,Hopium is strong,AMD,2026-02-07 14:16:58,28
AMD,o42xgxp,Doesn't the PS5 pro GPU have AMDs equivalent of tensor cores like the 9070XT does though?,AMD,2026-02-07 14:04:24,23
AMD,o42xr9w,"As you say they're most likely still working on it. However since it got leaked early, releasing it early as a ""beta"" feature would be very appreciated and prevent a lot of backlash.",AMD,2026-02-07 14:06:04,7
AMD,o45riy2,Steamdeck did not sell alot. It's sold 7/10ths of sweet f all over its life time. Be lucky to even be at 6m lifetime unit sales. That's not a lot.,AMD,2026-02-07 22:54:19,0
AMD,o461x6r,*Radeon,AMD,2026-02-07 23:58:48,3
AMD,o47x7k1,"more VRAM is not at all negated by upscaling tech, but rather DLSS can be an advantage for the 3080",AMD,2026-02-08 07:58:40,1
AMD,o46rvtn,"Indeed, why would you buy a new AMD card when FSR4 doesn't work on the old cards you don't have.",AMD,2026-02-08 02:38:55,-3
AMD,o46gosj,"I don't blame you, we all deserve the best for our wallets. For me, I'll stick with a $500-ish GPU idc who makes it. AMD, Nvidia, or Intel. Qualcomm?",AMD,2026-02-08 01:28:20,1
AMD,o47k482,what driver issues have you had?,AMD,2026-02-08 06:00:08,1
AMD,o47lhz5,"24? keep dreaming, especially now lol",AMD,2026-02-08 06:12:01,2
AMD,o4cxf2j,> AMD had so much goodwill with the 7xxx series and the pricing of the 9xxxx series.  What?!,AMD,2026-02-09 01:52:03,2
AMD,o46tgpi,"Everyone gave a shit on previous FSR iterations, forgeting that it was able to run even on 10 years old GPUs. AMD won't make thst mistake again.",AMD,2026-02-08 02:48:57,-1
AMD,o46gst1,"How much you got yours for? Is it a better investment for the next 5+ years, including reselling?",AMD,2026-02-08 01:29:03,1
AMD,o452ege,"The thumbnail says FSR, and the error was the refusal to officially support FSR4 on RDNA3/4.  On Linux, you can already run the full FP8 FSR4 model on RDNA3 with Proton-GE, and the INT8 model AMD itself released by accident works on both RDNA2/3.  However, AMD refuses to officially support FSR4 on RDNA3 with either the INT8 or FP8 model.",AMD,2026-02-07 20:37:20,8
AMD,o42vgvs,How is this even close to clickbait? This is a huge mistake radeon made when they didn't have to.,AMD,2026-02-07 13:52:34,12
AMD,o43kz0p,"Hate the game, not the players.  They'd be saying no to lots of money by labeling it 'correctly'",AMD,2026-02-07 16:08:00,3
AMD,o43okr4,Already got official FSR4 model on SteamOS though.,AMD,2026-02-07 16:25:46,2
AMD,o49bxuh,"Yeah on lower end and rdna2 cards, on the higher end like 7900xt/xtx, its the same speed as dlss4.5 on 40/50  series",AMD,2026-02-08 14:46:24,1
AMD,o442j87,the INT8 version works well enough,AMD,2026-02-07 17:34:30,5
AMD,o444oc5,"im not sure how this makes any sense... 9060xt 8gb is technically worse than 9060xt 16gb yet they are sold with the same name. you can just call it fsr4 lite, fsr 3.5, fsr4 int8, whatever just to differentiate.",AMD,2026-02-07 17:45:10,7
AMD,o4427x3,FSR4 Lite,AMD,2026-02-07 17:32:57,3
AMD,o43mjlj,"It's theoretically worse, technically better.",AMD,2026-02-07 16:15:45,0
AMD,o47lmwl,For the opposite reason I'm happy with 4k and not using upscaling,AMD,2026-02-08 06:13:12,2
AMD,o4cvn52,"2ms frametime cost to generate a 1440p image in quality mode in Oblivion Remastered on my setup using Linux.  For a lot of the HW that's out there, INT8 makes a lot of sense, since RDNA2 and up have 4x the INT8 throughput compared to INT32. FP8 would need RDNA3 or later to run somewhat performantly.  Thing is, the INT8 version doesn't really need to be better than the FP8 version. It already completely outclasses FSR3 + XeSS and comes close to FP8 - this is definitely good enough. AI cores aren't required on a technical level to reach better image quality btw., so who says that the INT8 version can't be improved upon?  IMO AMD should call it ""FSR4 Lite"" and release it already as-is.",AMD,2026-02-09 01:42:33,2
AMD,o44254m,Especially when fsr4 on older cards via optiscaler can have anything from minor AA issues to full blown shadows freakibg out or noise being everywhere,AMD,2026-02-07 17:32:34,2
AMD,o447b9h,"Yeah AMD really dropped the ball by not including the hardware for AI upscaling and strong raytracing from the beginning. So now they're in a fucked if they do, fucked if they dont situation. Obviously here in the AMD subreddit, there is a personal bias towards releasing FSR4 on older cards even if its not optimal to run the model on 6 and 7 series, and I get that considering the cards are not even 5 years old. But everywhere else they would get ridiculed for having shitty software, and people would use it as an excuse to not buy AMD. They've been trying to shake off the shitty drivers reputation for so long now but can't, even when now their drivers are completely fine (at least for me on RX480, Vega 64, and 6950XT). And even Nvidia isn't clear from driver issues either. Its just the nature when there are thousands of combinations of GPUs, CPUs, monitors, and applications to run.  Unless Radeon has a Ryzen moment, I doubt they will ever be popular with consumers. Which is probably why they decided to just focus on enterprise solutions instead.",AMD,2026-02-07 17:58:09,2
AMD,o44hs8b,"DLSS 4.5 runs significantly worse on 2000 and 3000 series cards, to the point where it can be worse than native, but Nvidia allowed it on those cards anyway just because they can technically do it. They at least giver users the option.",AMD,2026-02-07 18:49:36,2
AMD,o42vv7o,"That's an interesting take. I don't think I've ever seen HU being accused of having an axe to grind with AMD.  I have, however, seen many folks talk about how they're biased towards AMD for years both on Reddit and at large.",AMD,2026-02-07 13:54:55,21
AMD,o42wp7q,"Some of the dumbest shit I've read on this sub, and I've seen people wanting to upgrade from 9800x3d to the 9850   Tell me you've never actually watched their content lmao",AMD,2026-02-07 13:59:52,19
AMD,o42vpgy,"As an AMD user of over 25 years (since the original Duron), these criticisms are guaranteed and necessary. These blunders won't do AMD any good.",AMD,2026-02-07 13:53:58,29
AMD,o42vq6v,They have literally been shitting on intel cpus for well over a year lol. They have not said anything good about the core ultra series in term or performance or name. They said the core ultra name is absolute crap and nobody should buy them cause they suck.  But yea I bet intel pays them to trash on their products.,AMD,2026-02-07 13:54:04,12
AMD,o42ujxk,Is this a joke? They've been overly soft on Radeon over the years.,AMD,2026-02-07 13:47:11,31
AMD,o431mwl,Hilarious. Hardware Unboxed are the ones who pissed Nvidia off so bad that Nvidia said they weren't going to send them review samples anymore (although they backtracked after a bunch of other reviewers pilloried them for it). On other subs HUB is regularly accused of being biased toward AMD. Quit White Knighting for your favorite corp.,AMD,2026-02-07 14:28:09,16
AMD,o435sb9,"I pretty much hit all the large and medium sized tech reviewers for every review cycle, and I don't really see how one could reach that conclusion unless they don't often watch HU.  I've found HU to be pretty objective and fair, they call anti-consumer bull$hit when they see it, whether it comes from Intel, nVidia, or AMD. Maybe not as abrasively as Gamer's Nexus, but not very far off in egregious cases (like this one).  My biggest criticism of HU is that I think they're softer on nVidia than they should be, but I also think that about most big tech reviewers.",AMD,2026-02-07 14:51:04,6
AMD,o435rmz,"Hub is rather biased in favor of amd, not the other way around. Sometimes it's good to call them out.   Wait, are you the russian version of user benchmark? You know with the narrarive of intel paying hub and so on...",AMD,2026-02-07 14:50:58,0
AMD,o42wlvi,"I think they're desperate to prove they ain't in AMD's pocket.  The video lacks precision and is cutting corners.  FSR4 on older generations of AMD hardware is unreliable and slower.   More ghosting, less stable, and much slower on 4k rendering.  Maybe if AMD invests a lot of resources on it it'll get better.  But for now, I believe their aim should be to get it better for current gen hardware so that it's at least on par with DLSS 4.5.",AMD,2026-02-07 13:59:19,-10
AMD,o42u8l5,"I mean, AMD has all the ingredients for success, yet they keep making the most nonsensical choices time after time after time. And mind you, this is the opinion of someone who owns a 7900 XTX and 5800X3D. I'm glad the big channels are calling their behaviour out..",AMD,2026-02-07 13:45:19,23
AMD,o42u7uj,"The Nvidia bad rhetoric gets more views   If they wanted traffic, they'd glaze AMD, that always does well for the view counts",AMD,2026-02-07 13:45:12,22
AMD,o42u8gq,nivea does make my hands soft…,AMD,2026-02-07 13:45:18,7
AMD,o42w6pp,> Nivea  Nivea is some good stuff man you're just a hater.,AMD,2026-02-07 13:56:49,6
AMD,o44cyny,you say that as if we don't have it though...     I own a 7900 xt and I have been using FSR4 in most of my games for months,AMD,2026-02-07 18:25:52,2
AMD,o44bpuu,"What are you on about? RDNA 2 was the easiest buy ever, the 3080 had 10gb vram lmao.",AMD,2026-02-07 18:19:51,3
AMD,o42x98j,"It's not. This is nonsense. It's AMD tech for AMD hardware, Sony may have had some input but AMD has the final say over the tech. If AMD actually had a satisfying FSR4 version for RDNA2-3 they would've already released it for both PC use and for Sony. No fucking way Sony has any say in its release for PC",AMD,2026-02-07 14:03:08,14
AMD,o4566ew,"The Steam Machine runs Linux and Linux now has Goverlay and optiscaler thanks to enterprising open source developers!   https://np.reddit.com/r/linux_gaming/comments/1pmpvf0/goverlay_164/  Instructions for getting FS4 to work on the SteamDeck  https://www.gamingonlinux.com/2025/09/steam-deck-plugin-adds-amd-fsr4-support-to-improve-visuals/  The steam deck specifically has ""Decky"" which makes it easy to swap the dlls to inject FSR4 in games that support DLSS",AMD,2026-02-07 20:57:45,30
AMD,o4750gl,"I suspect that this is the answer, AMD announced the upscaling they are working on for the PS5 Pro as a joint project with Sony Project Amethyst. I am pretty sure we are going to see the improved scaling for the PS5 Pro as soon as AMD and Sony deem it to be fully baked. I seriously doubt we are going to see anything regarding FSR4 backporting until after Project Amethyst launches.",AMD,2026-02-08 04:04:43,7
AMD,o449lvu,I hate to admit it but sometimes I love my dirty little fake frames.,AMD,2026-02-07 18:09:26,9
AMD,o48a8zd,All upscallers are ass & I refuse to use them. Give me native or give me death!,AMD,2026-02-08 10:02:11,8
AMD,o4a4yt9,"Same. When I’m playing something like Space Marine 2 on a controller I’ll crank up that upscaling and frame gen. Barely feel the extra input lag. Considering how expensive hardware is these days, I’ll take any advantage I can get when it comes to extending my PC’s usability.",AMD,2026-02-08 17:12:35,1
AMD,o45bmph,Decky makes it easy to swap the dlls and inject FSR4 into DLSS compatible games.    Someone posted a video guide to the  SteamDeck subreddit:  https://np.reddit.com/r/SteamDeck/comments/1nlieb0/fsr4_steam_deck_lossless_scaling_full_setup_guide  (5 months ago),AMD,2026-02-07 21:27:04,3
AMD,o43ig4y,"The AMD Radeon RX 7900 XTX was released on **December 13, 2022**.   As of today (February 7, 2026), that is:  * **3 years, 1 month, and 25 days ago**",AMD,2026-02-07 15:55:38,84
AMD,o434nkj,"This is some bollocks. Though, our card does still destroy basically everything at any setting (excluding raytracing)",AMD,2026-02-07 14:44:50,61
AMD,o4ak0cs,"Manufactured obsolescence brought to us by ""AMD Fine Wine Technology"".",AMD,2026-02-08 18:24:00,2
AMD,o45th7v,Doesn't have to be good but it will last a very long time. People are still playing with RX470 and RX580.,AMD,2026-02-07 23:05:41,1
AMD,o486qsf,most gamers has no idea how powerful 4k cards are like the 9070xt or 6950xt (I have that old card).   In actual gameplay which youtubers like hardware unboxed sucked at to measure as their tests are for kids,AMD,2026-02-08 09:28:55,-1
AMD,o440u14,*looks at rtx launching 18 months after the 1080ti*,AMD,2026-02-07 17:26:02,-3
AMD,o47pwe0,"You have plenty of raw performance to keep up, but there are plenty of people with mid tier RX 7000 cards who can't and need to rely on upscalers to get the fps they want. I got all the raw performance I need, but I don't even use Native DLAA anymore at 3440x1440p since DLSS 4.0/4.5 in Quality mode has such negligible visual difference that the benefits (higher fps/lower power consumption at fps cap) always outweighs the minor tradeoffs.",AMD,2026-02-08 06:51:23,2
AMD,o4538ht,Never said it was their fault. Just said I don't like their part in it and what they're doing and how it directly affects the consumer market,AMD,2026-02-07 20:41:52,17
AMD,o43y4go,They might do something right then!,AMD,2026-02-07 17:12:37,14
AMD,o4ctu3i,"I think the pattern is that Steve gets called AMD shill, while Tim gets praises",AMD,2026-02-09 01:33:00,1
AMD,o44qig3,[https://imgur.com/a/oWUqIHl](https://imgur.com/a/oWUqIHl),AMD,2026-02-07 19:33:49,5
AMD,o47e48d,"I'm single and have an amd setup, where are those amd fangirls you're talking about ?",AMD,2026-02-08 05:11:21,3
AMD,o44rem5,"It's not a cult, just your average redditor doing average reddit things.",AMD,2026-02-07 19:38:30,10
AMD,o470v9w,Not for Linux.,AMD,2026-02-08 03:36:51,1
AMD,o42yq2e,I know most people don't like upscaling but DLSS 4 on Quality looks so good that honestly I can't even notice the difference between 4 Quality and Native. 4.5 is even better but it runs better on 40 and 50 Series.   I would love to have FSR 4 on the 6900 XT since it kinda has the similar or in some games better performance than RTX 3090.,AMD,2026-02-07 14:11:35,12
AMD,o487giw,"Moved from a 7900xtx to a 5090 myself, and the extra power plus actual raytracing performance with dlaa has unfortunately closed the door on ever going back to a Radeon gpu for me.  I still don't like ray tracing very much, but if I'm gonna be forced to use it in certain games then it might as well actually run well.",AMD,2026-02-08 09:35:46,0
AMD,o45lbia,You mean DLSS 4/4.5?,AMD,2026-02-07 22:19:21,2
AMD,o42vccn,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-07 13:51:50,-1
AMD,o42zcb1,"Nope, it doesnt support fp8, only expanded int8, etc.",AMD,2026-02-07 14:15:06,25
AMD,o42zc5t,"Yea, I think PS5 Pro is a hybrid (for compatibility reason): based on RDNA 2 with additional RDNA 3/4 hardware, like Matrix Cores for RT features. As [TechPowerup](https://www.techpowerup.com/gpu-specs/playstation-5-pro-gpu.c4232) notes:   * RDNA 2.0 and RDNA 3.0 for the Base GPU * RDNA 4.0 for Ray Tracing",AMD,2026-02-07 14:15:05,15
AMD,o44b9ck,Nope 5 pro is still at its core an rdna2 chip. Just with rdna4 RT and extra int8 hardware. Not even any wmma int8 or wmma fp16 like rdna3,AMD,2026-02-07 18:17:37,4
AMD,o42ynyp,Yeah. PS5 Pro's GPU is basically a 9060XT.,AMD,2026-02-07 14:11:15,-11
AMD,o42ybbs,I agree. AMD messed up (as always lol) releasing the source code. And now people are disappointed. They should have released it as a beta once the mess was made.,AMD,2026-02-07 14:09:14,5
AMD,o43v284,"Probably. But then again why the secrecy? They could just say it’s release date Q2 2026 or whatever.  AMD, and specifically Radeons marketing team is awful.",AMD,2026-02-07 16:57:31,5
AMD,o46f7cp,"True but wouldn't shareholders would invest less money into AMD if they made INT-8 FSR 4 available on RDNA 3/2? It wouldn't give those customers a reason to get RDNA 4 bc that would ""magically"" extend the life cycle on a platform already put on maintenance mode.",AMD,2026-02-08 01:18:57,1
AMD,o4644lm,"They keep doing it in the CPU segment too, although to a lesser degree.  Wanna example? 9850x3D. 9800x3D made with more refined manufacturing process and overclocked to the bone... 😂  Entire 8000G line, whether it's with or without GPU - the lower end models also have significantly cut PCIE lanes.  And so many releases of the same/similar CPUs as those already existing, especially for AM4. Or releasing new APUs, but naming them intentionally as confusing as possible (look at Ryzen 5700).",AMD,2026-02-08 00:12:06,-2
AMD,o4akqkx,"I should have been more specific.  DLSS4 makes more future games playable at a better frame rates while still looking good than the extra VRAM.  The extra VRAM will help to look better in the occasional game but DLSS4 will help to look better at better framerates in more games.  If AMD releases FSR4 INT8 for RDNA2 & 3, that will jump the 6800 XT back to competitiveness.",AMD,2026-02-08 18:27:23,1
AMD,o471pe6,Because I don't need upscaling and I use Linux. Any AMD card from the past 10 years can play 1080p great.,AMD,2026-02-08 03:42:24,-4
AMD,o49dtyk,"Frequent (as in almost every time they release a new driver version and it self-updates) driver timeouts that happen for no reason requiring fresh installs, hard crashes in certain games that won’t respond to restarting drivers with Windows Key + Ctrl + Shift + B to name a few. And in general there is definitely a lack of feature parity b/w the AMD drivers and nvidia ones. But as hardware unboxed mentioned, what is really annoying is that they clearly have time to bundle in this useless AI stuff that no one other than shareholders asked for, while ignoring something like fsr 4 which most if not all people would use most likely for planned obsolescence and artificial market segmentation.",AMD,2026-02-08 14:56:47,5
AMD,o49avry,"Ur talking about vram, man those cards arent even coming out anymore",AMD,2026-02-08 14:40:30,5
AMD,o4dejzp,"Bruh, people got so pissed at NVIDIA for the pricing of the 4xxxx series and fake frames. There was a massive push on Reddit and the tech media to go AMD. This is what led Linus to use a 7900 XTX over a 5090 in his updated personal system.   Then after the 9xxxx series launched, they were initially undercutting NVIDIA by ~$150-200 dollars. This was temporarily unavailable, but as recently as December you could get a 9070 xt for under MSRP, while most every 5070ti/5080 was slightly over MSRP and a way worse value. This led to pretty much every Redditor glazing the 9xxxx series, but AMD now seems hell bent on overturning all the goodwill.",AMD,2026-02-09 03:21:59,1
AMD,o4784kg,"Converting it from my currency to USD, it's 1,000 dollars. That price was already increased though because it was less during October which prompted me to upgrade before the price increased further.  Bettet investment compared to what though?  I think it is a good investment for 4-5 years. It'll have shown its age by then though with the way games have become more demanding. You'll have to lower some settings but hopefully not too much with DLSS.  You will still be able to use it. I'm just worried about how games have become more demanding and/or less optimized these days. Worried it'll be way more demanding in the future but hoping not because the current level of graphics are already insane to me.  For reselling I think it'll fare better than the 5060 Ti 16GB and the 5070 12GB because it has the performance and the VRAM but we will see what the next gen is capable of. Maybe it's way more power efficient and/or better in performance like the leap from 2000 series to 3000 series. We don't know. Maybe 6060 Ti will be near in performance but with better Upscaling and Frame Gen capability? Maybe not.  Upscaling and Frame Gen is here to stay and the improvements have been insane so I think the card's capability to use it properly will also be considered.  Maybe NVIDIA pulls another DLSS 4.5 level of quality increase in some way or magically improves latency that can only be utilized by the next gen.",AMD,2026-02-08 04:26:35,2
AMD,o42vpf8,What's the mistake?,AMD,2026-02-07 13:53:57,20
AMD,o439qsn,I have a 7900gre do I need to worry about this mistake?,AMD,2026-02-07 15:12:13,4
AMD,o42wui5,"Did their lead engineer stub their toe in the office? Did they snub customers who own previous-gen products by not officially supporting FSR 4 on those products? Did they put milk in Jennifer's coffee, despite the fact that she takes it black? Did they pivot their longer-term business strategy away from consumers at an inopportune time? Did they forget to defrost the chicken?  Help us out here and tell us the titular ""mistake""!",AMD,2026-02-07 14:00:44,9
AMD,o445v5c,Yep this is a YouTube problem at large. At least they dont post AI edited thumbnails of someone crying. I hate that shit.,AMD,2026-02-07 17:51:02,4
AMD,o4bn8c6,But how much faster is it than just rendering at native resolution?,AMD,2026-02-08 21:35:02,1
AMD,o444snv,"""it works well enough"", so AMD release it and get the exact opposite videos about all the times it isn't working well enough and then people shit on AMD for providing something that doesn't work well enough.",AMD,2026-02-07 17:45:46,3
AMD,o43p9r6,Intel's been shitting on Intel CPU's since Cannon Lake.,AMD,2026-02-07 16:29:08,1
AMD,o43dd7q,They shit on software support in NVIDIA cards like dlss and visual features like raytracing so much NVIDIA attempted to ban them from reviews lol.,AMD,2026-02-07 15:30:41,3
AMD,o438khh,"Hey — Your comment has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-02-07 15:06:05,0
AMD,o43pgi2,Intel actually has very strong ties to Russia; most of the GPU driver development was in Russia.,AMD,2026-02-07 16:30:01,1
AMD,o42x61x,> The video lacks precision and is cutting corners.  That's kind of par for the course for their coverage on upscaling. Compare their past coverage of DLSS where they had to be dragged practically kicking and screaming into acknowledging it and their early coverage on XeSS.   I just think upscaling and such isn't their jam. But AMD's recent moves on the GPU side are wholly deserving of critique either way.,AMD,2026-02-07 14:02:37,5
AMD,o42y6ox,Exactly I worked for them and no one I know buy theirs shit as they know the software around them sucks. Even with employee discounts discounts they are definitely not worth it.,AMD,2026-02-07 14:08:30,3
AMD,o433akq,Here u go : [https://i.imgur.com/QnaoH9w.png](https://i.imgur.com/QnaoH9w.png)   Not my problem you use Microslop Malware 10 or 11.,AMD,2026-02-07 14:37:21,-12
AMD,o43491q,with DLSS Vaseline,AMD,2026-02-07 14:42:37,-4
AMD,o42xzfr,Exactly my point I worked for that company and I'm not sure why other people spout these nonsense. FSR4 was able to run RDNA3 and older but they wanted to force consumers to buy their latest gen products. Such scummy behaviour from a company really there is no defending this.,AMD,2026-02-07 14:07:21,3
AMD,o447xj3,"They don't have a say in its release, however AMD is codeveloping their hardware and especially their software with Sony. This is what Project Amethyst is.      It's the same way that the PS4 Pro shipped with hardware/software that Vega eventually used (rapid packed math)",AMD,2026-02-07 18:01:11,0
AMD,o42zc2n,"if you havent known, sony tv has best video post processing and upscaling in tv market.   meanwhile, amd fully in house fsr3 upscaling is horrible.  so it is possible that most of fsr4 technical contribution comes from sony.   hence sony might have legal rights in approving fsr4 releases.",AMD,2026-02-07 14:15:04,-7
AMD,o476mod,"Maybe this is copium, but technically, AMD said to Hardware Unboxed that they have no updates to share at this time. At this time. If they have no intentions whatsoever of doing it, why not rip the band-aid off, declare it's not happening in no uncertain terms, and just get it over with? Why drag all this speculation and negative PR out? It is possible they're hamstrung by Sony. It is a little-known fact that Sony contributed to FSR 4, so it wouldn't shock me if Sony wants first dibs. A lot of GPU owners here think we're the Radeon division's main clients when in actuality Sony and Microsoft dwarf us on the balance sheets.",AMD,2026-02-08 04:15:57,7
AMD,o4aod2j,"The custom version of FSR that Sony’s releasing isn’t really a back porting of FSR 4. The PS5 Pro may have an RDNA 2 GPU, but the Matrix cores in it are the same ones you find in RDNA 4 GPU’s.",AMD,2026-02-08 18:43:39,1
AMD,o44rhrj,Sadly it’s nowhere near as good as ML FGs,AMD,2026-02-07 19:38:57,15
AMD,o48v3i2,![gif](giphy|pHb82xtBPfqEg),AMD,2026-02-08 13:02:54,3
AMD,o490ncx,"For years we were told that if you want best picture quality, you have to downsample from a higher resolution down to your monitor´s native resolution.   And now companies tell us the exact opposite, and that 80% fake frames (Nvidia MFG) is the new hot shit.   Nahhh, thanks. Build faster and efficient graphics cards not AI slop software.   :)",AMD,2026-02-08 13:39:30,5
AMD,o4c6hty,Amen,AMD,2026-02-08 23:18:27,1
AMD,o43il88,u/bot_sleuth_bot,AMD,2026-02-07 15:56:18,-35
AMD,o43owsx,"It can even do decent raytracing, if the game isn't too heavy to begin with.",AMD,2026-02-07 16:27:24,14
AMD,o44k3tl,"This isn't really true though, not even at 1080p for some games. UE5 games all require upscaling to hit decent performance. Being stuck with a 7900XTX just means you'll get an uglier game than someone with a 9070XT or Nvidia card.  Like in theory a 7900XTX is roughly equal to a 9070XT/4080/5070Ti if they run the exact same settings but in practice it's really not because you can run much lower resolutions on those cards and still get better visual results. I'd say that in hindsight picking a 7900XTX over a 4080 was a bad choice. Very similar to the 5700XT vs 2070S situation. Those cards feel very much like a stopgap.",AMD,2026-02-07 19:01:06,10
AMD,o44de2s,"1080ti launched 10 months after the regular 10 series and the Titan Xp, 1.5 years is a normal amount of time, the 1080ti was still shoulder to shoulder with the 2080 for a similar price and the 2080ti was much more expensive",AMD,2026-02-07 18:27:57,13
AMD,o49ewls,"But you’ll excuse AMD doing the same thing, just because they are less successful at it.  Lisa is out there pushing their AI accelerators and ROCm as hard as Jensen.",AMD,2026-02-08 15:02:36,5
AMD,o45hidj,Its also short form content creators spamming 'nvidia bad' on anything but refusing to do the same for AMD fuckups because it feeds the algorithm well and negative content generates more comments = even better for algorithm,AMD,2026-02-07 21:58:35,6
AMD,o460zjl,"You can't see the cult, if you're a member.",AMD,2026-02-07 23:53:08,1
AMD,o48coie,Yeah but 1.87% of Steam users use Linux so it's not really relevant in this context.,AMD,2026-02-08 10:24:59,8
AMD,o48a37z,wine 11 got released fixing dx12 performance issues on nvidia and intel. but yeah nothing beats open source amd drivers,AMD,2026-02-08 10:00:39,1
AMD,o47wl84,50 series wasn't (isn't?) even for Windows with their driver issues,AMD,2026-02-08 07:52:53,-1
AMD,o43edel,"> I know most people don't like upscaling  I don't think most people care actually as long as it works well enough without creating additional issues. It's really only on hobbyist communities where you get people whinging about upscaling, checkerboarding, ultra high refresh rates, free-range organic ""real frames"", etc.  The average person just wants to play their game and it to look and feel alright.",AMD,2026-02-07 15:35:42,9
AMD,o441psi,"DLSS has been really good since 2.6 or so, where they fixed most of the ghosting issues. 4.5 has some shimmering issues though. That said, since everything uses poor TAA nowadays, the ML upscalers tend to have a decent comparative image quality.",AMD,2026-02-07 17:30:26,0
AMD,o47jyrp,try native without TAA,AMD,2026-02-08 05:58:50,0
AMD,o45merr,"U can force dlss 4.5 on any game with DLSS 2.0 and higher via geforce experience (driver level) on any cards thats RTX 2000 series and above.   In case of AMD without using any 3rd party stuff its FSR 4 on any game that supports FSR 3.1  and higher but the list is small to begin with and often plenty of popular games dont really support FSR in the first place. FSR 3.1 list was famous for being half filled with very small, very indie games that didnt need it in the first place.",AMD,2026-02-07 22:25:24,1
AMD,o42w5wl,"Seriously? xDDDDDDDDDDDDDDDDDDDDDDDDD  I actually posted a comment that insults another person and that is fine, but slight insult to AMD and it's automatically removed.   Hilarious.",AMD,2026-02-07 13:56:41,2
AMD,o431sax,Nobody cares what’s in your system only people that do need therapy.,AMD,2026-02-07 14:28:58,21
AMD,o43l0f2,yet nvidia has supperior drivers on windows... Whats wrong with you ? AMD isnt your friend,AMD,2026-02-07 16:08:12,1
AMD,o43tea3,>  has a custom AI accelerator capable of 300 8-bit INT8 TOPS and 67 16-bit FP16 TeraFLOPS.   https://www.techpowerup.com/320490/sony-playstation-5-pro-details-emerge-faster-cpu-more-system-bandwidth-and-better  adding this here for clarity on the spec,AMD,2026-02-07 16:49:23,8
AMD,o42z9ib,"No, in performance almost. But architecture  It's rdna3 vs rdna4",AMD,2026-02-07 14:14:39,10
AMD,o434apc,"Pro’s GPU is RDNA2, with RDNA3 bits added to it, additional logic, RDNA4 RT, and custom AI solution.",AMD,2026-02-07 14:42:52,8
AMD,o430h2e,Doesnt support fp8 tho,AMD,2026-02-07 14:21:33,5
AMD,o441bjk,Maybe because they dont want to give a date because that could screw partners over,AMD,2026-02-07 17:28:27,3
AMD,o46fe6h,"Or that if they did, Nvidia can drop an even better transformer model that beats AMD FSR solution",AMD,2026-02-08 01:20:09,1
AMD,o468ebi,"I don’t see how the 9850x3d is a failure…like at all. It’s the same as a Super edition in Nvidia cards, a bit more of everything meant to lure in late adopters, it’s not supposed to be an upgrade path from the 9800x3d.   No idea about the mobile range   “The same CPUs” are generally failed higher SCU products, everyone does this. Perfect example is memory that doesn’t hit certain markers gets sold as a lesser SKU (3200mhz vs 3600mhz DDR4); they don’t just throw away the product. Intel also does this, as do Nvidia.   No idea about APUs, but a perhaps poorly named product isn’t “snatching defeat” either. Their APUs are in the 3 main consoles which sounds like they’re winning there too.",AMD,2026-02-08 00:37:13,4
AMD,o47w5ul,"APUs lack PCIe lanes because analog PCIe PHYs are actually quite large and there are set die sizes and transistor budgets to meet. The iGPU also takes the equivalent of x16 lanes (IF isn't 1:1, but it's close to this amount) for memory and IO access. Infinity Fabric is data protocol agnostic, so iGPU can use internal lanes without needing any of those large, analog PHYs and can report as a standard PCI bus.",AMD,2026-02-08 07:48:56,1
AMD,o42wv0a,"It's the mistake they made buddy, it's obvious /s.",AMD,2026-02-07 14:00:49,21
AMD,o44bqs3,"Nah cmon, Radeon have only ever made one mistake... Right? 😓",AMD,2026-02-07 18:19:58,-1
AMD,o43lzf2,It's in the video,AMD,2026-02-07 16:12:59,-10
AMD,o455xjg,"Unless the games you play run poorly while being actually optimized, my answer would be no.   Don't let companies that make unoptimized games gaslight you into believing that you are at fault for not using an upscaler with a **decent last-gen** card.  Sadly, the only choices you have is to either give, lower your standards or avoid the game entirely.",AMD,2026-02-07 20:56:26,3
AMD,o42ya4e,"Not releasing a functional fsr 4 for older radeon gpus when you've had a working build is a huge mistake and if you spent like 30 seconds watching the video you would know that ""mistake"" too  Nvidia is still supporting turing which is an architecture almost 7.5 years old at this point. The fact that amd refuses to support their previous generation with fsr 4 is crazy.",AMD,2026-02-07 14:09:03,26
AMD,o4buhjz,"In ratchet and clank at 4k the 9070xt gets a 67% boost using fsr4 fp8 balanced, and the 7900xtx gets 65% with fsr4 int8, almost identical",AMD,2026-02-08 22:11:41,1
AMD,o44dgon,"I haven't seen any situation where it works worse than FSR 3. They can just call it 3.9, or 4 Lite, or something. Or just release the source code officially, so people can do it themselves.",AMD,2026-02-07 18:28:18,7
AMD,o47ryf9,"Easy, mark it as opt-in beta and release it as-is.",AMD,2026-02-08 07:10:06,3
AMD,o49cary,"Dlss4.5 had tons of issues at launch aswell, but it was an opt in beta",AMD,2026-02-08 14:48:22,1
AMD,o4525uw,"That's exactly the point of the video. It already works well with mods, why don't AMD support this officially? FSR4 INT8 is their own code already!",AMD,2026-02-07 20:36:01,1
AMD,o447ik4,I believe that's called FSR 2.,AMD,2026-02-07 17:59:08,1
AMD,o4491ud,Yes but to insinuate that Sony has any say in the PC release of FSR4 for older arches is straight up silly.,AMD,2026-02-07 18:06:43,0
AMD,o432mcw,"If you haven't known, all TV processing is fucking awful and works on completely different principles than real time processing. Especially when it comes to upscaling.",AMD,2026-02-07 14:33:36,7
AMD,o4ajdfu,">If they have no intentions whatsoever of doing it, why not rip the band-aid off, declare it's not happening in no uncertain terms, and just get it over with?  Well, traditionally, AMD has always blundered their PR harder than anyone else, and this time probably doesn't want people to just wait for the next APUs that feature FSR 4 because their newest chips are still on RDNA 3.5 or whatever.  Essentially, they're still shipping older architecture without modern FSR support and want people to keep buying it. If they said it was DOA, nobody would buy into it.",AMD,2026-02-08 18:21:08,4
AMD,o49w7gf,"As the owner of a laptop with a 6850m XT in it, my sad suspicion is that when AMD announced they were separating the driver development for RDNA 3 & 4 from the driver development for RDNA 1 & 2, and then had that whole kerfuffle ""maintenance mode""  which they then corrected to, RDNA 1 & 2 continuing to receive new game support. What it was really about is that AMD is only planning on bringing the INT8 version of FSR4 to RDNA 3 and 3.5 based products and that the 6000 series GPU's are going to be left out in the cold.   I think Sony getting first crack at it has a lot more to do with the fact that they helped jointly create the code to run FS4 on older GPU's than the volume of SoC's they buy from AMD for their PlayStation division. But that is a good point, making one of your best customers angry at you is never a good idea.",AMD,2026-02-08 16:30:09,2
AMD,o4b0wwf,"I did not call Project Amethyst an FSR backport, I said I don't think we will see any  information about FSR4 being backported, until after Project Amethyst launches.   I believe the Viola GPU in the PS5 Pro is a unique custom blend of RDNA 2, RDNA 3 and RDNA 4 features, that pretty much requires it's own custom upscaler for best performance.   Having said that I still suspect that the INT8 files for FSR4 that leaked are in some way related to Project Amethyst.",AMD,2026-02-08 19:43:57,1
AMD,o45w0cr,They both stink,AMD,2026-02-07 23:20:27,-4
AMD,o495o1q,"And then youve got the goons who will gaslight you about the latency or image quality.  ""Its better than native "" they shout from the rooftops",AMD,2026-02-08 14:10:08,5
AMD,o43ppew,You look like a bot mate,AMD,2026-02-07 16:31:15,39
AMD,o43px7l,"Oh I agree completely it's great with ray tracing in a lot of cases. In fact the only thing that was realllly fucked was Wukong, which reduced my fps to 45 even on low RT. Where as cyberpunk runs ray tracing maxed out at a decent fps. Haven't had any issues with other games. (I think path tracing was an issue).",AMD,2026-02-07 16:32:18,3
AMD,o457z8v,Satisfactory runs in UE5 and runs better then it did when it ran in UE4.   UE5 isnt bad. Its just most teams using it just suck.,AMD,2026-02-07 21:07:34,6
AMD,o44qgci,"> I'd say that in hindsight picking a 7900XTX over a 4080 was a bad choice. Very similar to the 5700XT vs 2070S situation. Those cards feel very much like a stopgap.  Unless Radeon starts being forward thinking I'd argue the whole brand is falling into that category.   They're not innovating or even considering future tech. They're being dragged kicking and screaming into supporting things late, with no planning for the hardware or software support. Nvidia is now humiliating them on longer term support. Nvidia cards that launched before RDNA1 and the VII are better supported than some APUs and cards that have released in the last few years on AMD's side.   They can't do the Nvidia minus $50 model, while having worse long-term support and short-sighted planning. They shouldn't have even been blindsided on this tech direction. DLSS2.0 (the first big step for upscaling) pre-dates FSR1 by like a whole year.",AMD,2026-02-07 19:33:31,10
AMD,o44qfef,"Yeah I never understand what these people are talking about. Expedition 33 at 4K Epic has drops as low as 45fps on the 5090 and it's more than 70% faster. The XTX can NOT run, let alone destroy, anything at any setting on these heavy modern games, it needs upscaling in many of them and is stuck with a poor one.  The card isn't slow by any means but the lack of good AA/upscaling makes it tough in these stupidly heavy games.",AMD,2026-02-07 19:33:23,7
AMD,o44f2mg,And it couldnt do raytracing or dlss or ai workloads or etc etc etc,AMD,2026-02-07 18:36:13,-3
AMD,o4cibqf,"Where's the official ""AI"" upscalers for RDNA 3.0 and the current RDNA 3.5?   ""AI"" upscalers are available for Nvidia's Turing and above.",AMD,2026-02-09 00:27:27,1
AMD,o4cn7j2,Again never said that either but Nvidia has one of if not the biggest influence in what's happening in the GPU market. When they started pushing AI acceleration and making fuck tons of money.  Of course AMD and Intel are going to follow the bag to not get left behind.Nividia are the industry leaders right now and they are the ones pushing for this. But it's about influence in the industry and Nvidia has that.,AMD,2026-02-09 00:54:02,1
AMD,o46g9uc,"Yeah on AMD, you need to use optiscaler on FSR 2 games. FSR 3.1 games just use Adrenalin software",AMD,2026-02-08 01:25:43,1
AMD,o42zr6s,"Mark Cerny did say it has special acceleration cores for RT and PSSR upscaling. And Digital Foundry made comparison and came to the conclusion that the closest GPU is the 9060XT, hence my comment.",AMD,2026-02-07 14:17:28,-1
AMD,o43100l,Are we sure? If that's the case then it shouldn't be able to do PSSR efficiently with the raster power it got. Also doesn't make sense why the base PS5 doesn't support PSSR.,AMD,2026-02-07 14:24:35,-1
AMD,o48n1ph,Everyone else will continue to improve with or without amd.,AMD,2026-02-08 12:00:12,2
AMD,o449036,"No shit Sherlock, it's not in the title though",AMD,2026-02-07 18:06:29,2
AMD,o45jdsv,"I never use upscaling for performance personally. I use upscaling in every possible game because actually running DLSS over native AA/TAA implementations looks literally better in virtually every title (at least from 1440p and up baseline, i think its a little worse from 1080p baseline). And its been like that easily since 3.x version.   On the other hand, theres almost no reason to not use DLSS 4.5 on quality vs native.",AMD,2026-02-07 22:08:44,2
AMD,o431sdg,I absolutely agree that AMD's lack of support for previous-gen products is unacceptable. But it's not a matter of the time commitment to watch the video. I'd gladly watch a 30-minute deep-dive on the topic by a creator who respects his or her audience by using descriptive titles.  Video content should deliver value by presenting a thoughtfully-researched analysis of a topic that can't otherwise be communicated by a handful of sentences of text. Not by gatekeeping information behind an ambiguous title.,AMD,2026-02-07 14:28:59,-14
AMD,o4bwq1r,And how much with FSR3? Balanced is about 2/3 resolution.,AMD,2026-02-08 22:23:34,1
AMD,o44rqtm,"Yeah, they can really just call it whatever, don't call it FSR4. I don't care, I just want it, because it's so much better than FSR3.1. No comparison.   I've been using it lately in Linux with ProtonGE",AMD,2026-02-07 19:40:16,7
AMD,o44sm78,"FSR1, FSR2 wasn't vaseline it was aliasing city. It could have benefited from some blurring to soften some of the jaggies and shimmer.",AMD,2026-02-07 19:44:51,2
AMD,o489f0h,"when the game can run 60fps easy , and you make it double, you wont notice fake frames at all. I love the feature",AMD,2026-02-08 09:54:17,7
AMD,o463p59,"nah, ML FG has it's place. LL scaling FG is complete trash. There is no scenario I would run LL scaling over native.  I'm happily running NV FG 2x in X4 Foundations",AMD,2026-02-08 00:09:33,5
AMD,o48ae2d,"Nah, it’s great - i use it in every game where it’s available",AMD,2026-02-08 10:03:31,2
AMD,o44d41r,I have fond memories of trying Witcher 3 RT on release and getting something like 0.5 fps. Good times.,AMD,2026-02-07 18:26:36,7
AMD,o4520a2,"I moved to Linux last year and choose a 7900XTX along with a 9800X3D. Personally, I can't complain because I don't use that software stack. Though, that opinion changes with ROCm...  Overall, I don't care for RT, Upscaling, Frame Gen or going beyond 1440p. If a game runs shit to look decent at 1440p on a 7900XTX, it's very likely not my GPUs fault but a incompetent studio unable to optimize. RT is a choice, while Upscaling and FG has become a crutch for many.  AMD's choice over the past few years are definitely pretty disappointing, but we should not let that distract ourselves from those shitty game companies.",AMD,2026-02-07 20:35:10,1
AMD,o470nfw,"UE didn't even have proper multithreading until 5.7. There IS a lot of technical debt.  The engine tools and documentation are unfortunately designed in a way that encourages bad practices, which definitely makes it harder for teams to not suck.  The games industry regularly firing their most capable senior development staff to ""increase revenue"" doesn't exactly help either.",AMD,2026-02-08 03:35:26,3
AMD,o45g27j,"No Lumen, right? That, and Nanite to a lesser extent, are the main things that make UE5 heavy. It also makes sense that it's demanding as Lumen is pretty much an inefficient way to do RT. Early UE5 games skipped them but nowadays that's not really the case.  It's true that if you strip out the main features of UE5 it runs better than UE4.",AMD,2026-02-07 21:50:46,2
AMD,o471hvz,"> They can't do the Nvidia minus $50 model, while having worse long-term support and short-sighted planning.  Exactly. With AMD having tried to axe RDNA2 support early and going out of its way to not provide FSR4 on older hw (going as far as leveraging Valve into stripping support for that from Proton), it's essentially playing scorched earth tactics with its remaining customers - many of which have been very loyal up until now.",AMD,2026-02-08 03:41:00,6
AMD,o47mbid,Are you certain about that? I played ex33 maxxed out with an average fps well above 60 with only one specific area experiencing some stutter.,AMD,2026-02-08 06:19:08,1
AMD,o44fnr9,"Which at the time atleast were fads, it wasn’t until mid 30 series where dlss, rtx were in a good state and by that time the 1080ti wasn’t powerful enough for raster at the top anyways.",AMD,2026-02-07 18:39:06,14
AMD,o4cx8wr,"> Nividia are the industry leaders right now and they are the ones pushing for this  Dude, everyone is pushing for this.  Singling out nvidia out of all of them just exposes your bias.  > But it's about influence in the industry and Nvidia has that.  So exactly like I said :   But you’ll excuse AMD doing the same thing, just because they are less successful at it.  You literally reworded exactly what I said, proving me right, while thinking you were contradicting it.",AMD,2026-02-09 01:51:07,0
AMD,o441jsf,So you are mobile gamer or intel fan ? Id rather be more focused on AI bubble bursting and Nvidia learning a lesson.,AMD,2026-02-07 17:29:36,0
AMD,o43tdr6,Where is the video when they said so?,AMD,2026-02-07 16:49:18,3
AMD,o4363mw,"Well it has expanded int8 support, which is why mark cerny has been talking about pssr2 being a modification of fsr4, likely int8 or something similar, which is why some people are speculating that it might release for rdna3 gpus at the same time as ps5 pro in March.  The pro is a mix of rdna2 and 3, with rdna4 RT cores, and additional custom AI hardware that is nowhere nesr as capable as RDNA4.",AMD,2026-02-07 14:52:47,3
AMD,o44bf0g,Pssr is int 8. Like dlss 2/3 and xess,AMD,2026-02-07 18:18:23,2
AMD,o43fq3h,"Imo, you forgot something important about YouTube: YouTube videos has thumbnails, and in the thumbnail of this video you can quite easily read the text that was blurred out, and it will give you a clue on what the problem is about",AMD,2026-02-07 15:42:20,10
AMD,o434zyw,You’re gatekeeping information to yourself 😂,AMD,2026-02-07 14:46:46,10
AMD,o47ldod,Not everyone may have availability to watch it right now,AMD,2026-02-08 06:10:57,1
AMD,o4bzyz5,Why does fsr3 matter?  It gives 74%,AMD,2026-02-08 22:41:03,1
AMD,o48v1c5,">when the game can run 60fps easy , and you make it double, you wont notice fake frames at all. I love the feature  Please speak for yourself, the game immediately feels laggy because its not actually running at 60fps. It looks smooth but feels sluggish. If you can't tell I don't know what to tell you.",AMD,2026-02-08 13:02:28,1
AMD,o463rnk,">ML FG has it's place  Yeah, the trash can",AMD,2026-02-08 00:09:57,-11
AMD,o48ut0a,"Yeah, automatic input latency generation is the best!",AMD,2026-02-08 13:00:50,-2
AMD,o44dsx0,"lol shit ill have to try that at some point. Installing rdr2 at the moment, never played the first though...",AMD,2026-02-07 18:29:57,1
AMD,o489naj,upscaling makes slop run faster and look better,AMD,2026-02-08 09:56:27,2
AMD,o47m3jo,"Agreed ive never used upscaling tech either, haven't needed to i only run a 5k g9 it isnt exactly demanding compared to the 4 and 8k stuff about.",AMD,2026-02-08 06:17:12,1
AMD,o470yct,"UE5 without its most prominent, glorified and aggressively marketed features is actually OK.  Epic Fail.",AMD,2026-02-08 03:37:24,0
AMD,o4cszjr,"All the while mobile APUs based and rdna2 are still being actively produced and sold. Yes, “Rembrandt” aka 6000 series aka 7035 series has now been re-rebadged into Ryzen 100 series.",AMD,2026-02-09 01:27:45,1
AMD,o47ocf7,"Definitely, The Reacher, parts of Stone Wave Cliffs, Endless Night Sanctuary etc, you're not getting 60fps at native 4K Epic.",AMD,2026-02-08 06:37:21,2
AMD,o48h4i2,I've also had 60+ fps at 4k in this game.,AMD,2026-02-08 11:06:52,0
AMD,o44hibb,Frame gen and path tracing are as welk  Doesnt change the fact new features werent available to older cards,AMD,2026-02-07 18:48:14,-5
AMD,o4d3ykg,You just assumed my stance on something then when I clarified you're upset and just arguing with me about something clearly we both agree for no reason. Which is crazy cause I'm here for AMD fumbling the bag by not putting FSR 4 on rdna 3 and previous GPUs. For no reason.,AMD,2026-02-09 02:25:58,1
AMD,o49fpt5,"If FG feels that bad for you then you should definitely be setting your flip queue to zero/immediate since every queued frame adds that much latency. FG done correctly adds a bit over one half of one frame of latency but you get almost double fps, while queue 1 usually only adds like 20-30% more frames with worse latency. Better off in terms of latency and smoothness using zero queue plus FG in any games that let you change it compared to queue 1 (god forbid if the default is 2 or 3 frames).  But yeah playing with a fat queue and then adding FG on top from anything but like 120fps base feels pretty bad",AMD,2026-02-08 15:06:59,5
AMD,o475fct,You're entitled to your opinion on the matter.,AMD,2026-02-08 04:07:34,3
AMD,o4930s8,"Unnoticeable, actually latency with fg+reflex is lower than native with reflex off",AMD,2026-02-08 13:54:11,2
AMD,o4cn3ud,"it's like 10ms, most people in most games wont notice, there's way more than that in a lot of TVs",AMD,2026-02-09 00:53:28,1
AMD,o4819sy,I wouldn't call it OK as there's still the part where almost any game that doesn't work with loading screens has traversal stutter. Their automatic shader per-compilation tool also always seems to miss a bunch of them.,AMD,2026-02-08 08:36:53,1
AMD,o4cxhpa,Gotta milk the people some more.,AMD,2026-02-09 01:52:26,1
AMD,o47qisa,I guess I just got lucky then because although not quite 4k my fps had plenty of wiggle room and would likely run 4k at 60 fairly stable.,AMD,2026-02-08 06:57:03,1
AMD,o48hmqd,I should probably say too im running a 7900xtx so not even close to as powerful as a 5090.,AMD,2026-02-08 11:11:32,2
AMD,o48ibuv,[https://www.youtube.com/watch?v=XfRlqy0QKuI](https://www.youtube.com/watch?v=XfRlqy0QKuI)  Seems to be barely above 30. I guess you must've had a special 7900XTX that outperforms a 5090. Very lucky.  Edit: [https://www.youtube.com/watch?v=FB52eJbFKDE](https://www.youtube.com/watch?v=FB52eJbFKDE)  It's not even always above 60fps at 4K FSR Q with a 2900mhz 450W 7900XTX.,AMD,2026-02-08 11:18:00,1
AMD,o44hnb3,Cards which launched in 2016 and 2017…,AMD,2026-02-07 18:48:56,7
AMD,o493qfr,"Lower than native is literally impossible. Inputs are not being polled on the fake frames. You fell for the marketing. If you see some test result that claims otherwise you're being manipulated.   Reflex isnt a catch all anyways, it only works if your GPU bound and it just ultimately amounts to throttling your CPU instead. Its a performance hack that would be better achieved by tuning your system better.",AMD,2026-02-08 13:58:30,1
AMD,o48l3fy,Oooph,AMD,2026-02-08 11:43:09,1
AMD,o47v3y7,"Depends what not quite 4K is but there's no getting lucky lol. I just saw a benchmark showing it drop into the 30s during combat at native 4K. FSR Quality does keep it closer to 60fps but even then it still drops under.  It's not like the entire game runs like that but there are quite a few areas that do unfortunately, just is what it is. I mean I have to use DLSS Q with the 5090 to always stay above 60fps too, it's sad.",AMD,2026-02-08 07:39:12,2
AMD,o47qrcg,I wanted to check again to be sure but unfortunately I deleted the game after my second playthrough. I am playing shadow of the tomb raider at the mo though and I get 125 fps average with that maxxed out no upscaling.,AMD,2026-02-08 06:59:13,1
AMD,o48hoei,Ditto,AMD,2026-02-08 11:11:58,1
AMD,o44i42m,">Just them skipping a generation of high end cards and leaving me with no upgrade path was enough to get me back to Nvidia after 3 generations of AMD. And they've done nothing but vindicate that choice ever since.  So nvidia does it its ok, amd does it and youre mad? Very hypocritical",AMD,2026-02-07 18:51:14,-4
AMD,o493xk1,"Still nonissue, fg is great",AMD,2026-02-08 13:59:42,4
AMD,o47vusz,"I use a samsung g9 ultrawide 5k, pixel count is maybe 10% less than a 4k? Must make a bigger difference than I expected. For me only the gestral village had fps drops, and I think that was just some kind of bug.",AMD,2026-02-08 07:46:05,1
AMD,o49e6mm,Shadow of the tomb raider is 2018 game dude.  Why are you bringing it up ?,AMD,2026-02-08 14:58:42,3
AMD,o46stpy,"Two different guys you're arguing with. I'm the guy you're quoting but not the guy whose been replying after that.  I bought their products because they were competitive on Raster starting with the 5700xt and 6900xt and I could look past fsr and RT performance for a while longer on a 7900 xtx.  But I was not willing to grab a 9070xt just for finally tolerable fsr, and instead grabbed a 5090 for a real upgrade during the period where they could be had for near msrp.  I now have zero reason to ever go back to an amd gpu since they will always be behind the curve and it's unlikely that they'll actually put out a card that competes with the 5090 within the next 2 or 3 years.",AMD,2026-02-08 02:44:55,5
AMD,o44rfym,"Nvidia launched some new features that had little traction and no demand, that older hardware couldn't do.   AMD 5-6 years after DLSS2.0 launched decided MAYBE just maybe people do care about upscaling, while having a decent version be locked solely to their latest arch. Leaking a version that sort of works on older archs. And just a complete lack of forward thinking in RDNA2 and RDNA3's designs. They aren't innovating or pushing a new tech, they're catching up 6 years late to the party.",AMD,2026-02-07 19:38:41,9
AMD,o3by5cb,Nice,AMD,2026-02-03 12:35:53,3
AMD,o3mtsfy,"7840H, you again?",AMD,2026-02-05 00:49:05,3
AMD,o3m0bmy,Get this in Techweeb’s hands ASAP!,AMD,2026-02-04 22:10:45,2
AMD,o411jy7,Ohh this is a good one,AMD,2026-02-07 04:30:28,1
AMD,o3fwep8,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-04 00:18:07,1
AMD,o3ho3wc,They said this months ago but in Australia prices have decreased instead and we're currently getting 9070XTs for under US MSRP and almost $100 AUD under Australia's MSRP...,AMD,2026-02-04 07:05:01,39
AMD,o3i9d1f,**JUST**   **DON'T**   **BUY**   **IT**,AMD,2026-02-04 10:22:00,25
AMD,o3ibipo,already increased over 100 euro,AMD,2026-02-04 10:41:31,6
AMD,o3r2f5p,Got my nitro+ 9070xt for $550 I'm good. Just patiently waiting to sell my 7900xt for break even or small profit.,AMD,2026-02-05 17:35:04,3
AMD,o3rynte,Im on rx 6800 rn. Wating 9070 drop down to 300$ hehe.,AMD,2026-02-05 20:03:46,3
AMD,o3lekwi,"I'm pretty sure they already are doing that. By the time it gets to us they're already doing it, and pricing shows it.   Could not be happier I got a 9070xt this week.",AMD,2026-02-04 20:26:52,2
AMD,o3mja02,Intel be then,AMD,2026-02-04 23:51:21,2
AMD,o40wnz9,"Sigh and I just returned a gigabyte card so I can get the mercury. Tried it out, returned it and waiting for the money to hit my bank. Wondering if I should just buy the mercury 9070xt now...",AMD,2026-02-07 03:55:54,1
AMD,o3lh5yh,8gb models? browsing in firefox without any 3d stuff running gpu ram is at 3.6gb at 4k,AMD,2026-02-04 20:39:11,-4
AMD,o3md6jn,"Same here in NZ. But I remember looking at GPU prices during the early days of the Covid shortage and marvelling at how we were unaffected while prices were shooting up in the US and Europe.  Spoiler: we were affected, and badly. It just took an extra month or two to reach us. So for that reason I just bought a 9070 XT last week...",AMD,2026-02-04 23:17:27,11
AMD,o3ohs0b,Hardware Unboxed said to expect raised prices for Australia,AMD,2026-02-05 07:30:08,7
AMD,o3i8h8y,Thats wild. What is causing it? Because most people will assume that means there's too many 9070XTs and not enough demand. But the prices in AUS should be even higher not lower.,AMD,2026-02-04 10:13:51,7
AMD,o3sieol,"That's wild , here in Germany basicly everything jumped +200 or +300.  Like an 5060ti which did cost weeks ago 499 Euro costs now 699-720€",AMD,2026-02-05 21:38:37,2
AMD,o3jjakr,"Just don't buy ngreedia, don't use shame  A-nti soul I-ntelligence and perhaps chaos-time of human being will fail.",AMD,2026-02-04 15:16:11,8
AMD,o3m49q5,"Are you talking about playing video?  I run Firefox on workstations without GPUs all the time. You can run a half dozen or a dozen tabs on 128 MB VRAM no problem.  Hell, a dozen tabs is only about 0.5 GB system RAM.",AMD,2026-02-04 22:30:36,4
AMD,o3p64pz,I think I recall him saying the same the last time these rumours came up which obviously didn't hold up :/,AMD,2026-02-05 11:19:21,2
AMD,o3il85c,No-one is buying new PCs due to RAM prices so demand for GPUs likely dropped a tonne. Even CPU prices are at an all time low (9800X3D's are going for ~$420 USD if you exclude tax).,AMD,2026-02-04 12:01:40,27
AMD,o4cwhcu,"NVidia has the better upscaling, so for most people, they have the best performance/$ on the high end and mid end.  Low end Intel still has the crown.  AMD needs to cut prices to be more competitive, or vastly improve the upscaling.  But nothing will get done until NAND prices normalize.",AMD,2026-02-09 01:47:01,1
AMD,o3qu4lx,over a hundred open tab for me. windows dekstop sits at 1.3gb vram,AMD,2026-02-05 16:56:04,-2
AMD,o3ibibc,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-04 10:41:24,1
AMD,o3sg377,"Close some tabs. Genuinely, why the fuck would you need 100 tabs? You cannot process all that information at once. You're obviously just sitting on a pile of 'Look at this later'... So either work through it, or just get rid of them knowing you'll never make it there.",AMD,2026-02-05 21:27:26,4
AMD,o3wx0by,"yes, they are constantly changing sometimes less sometimes more depends on time spent at the computer. but this is just 1 workflow, i can do something else in the meantime still eating vram. but all this is irrelevelant in an age where they say 8gb is enough. for what? turning off the computer? cos on the software side its not possible with ever incrasing ai coded slop games and software, developed for the future tech around 30 years in the future still unrelased.   windows 10 for example install it on a server baremetal with 512gb ram. nothing installed just at first boot systam ram was around 10-15gb used by the system itself. for what?   windows 11 went a bit further it was over 20 on the same machine. not mentioning the disk space....",AMD,2026-02-06 15:16:28,0
AMD,o42qk7y,"Impressive, very nice. Now let's see the MSRP",AMD,2026-02-07 13:22:42,41
AMD,o43aa6q,So it's just a painted backplate and a different sticker on the middle fan?,AMD,2026-02-07 15:15:03,17
AMD,o461te8,But it still has a 12 pin so hard pass.,AMD,2026-02-07 23:58:11,13
AMD,o42wmea,Crimson connector.,AMD,2026-02-07 13:59:24,18
AMD,o43hyb3,![gif](giphy|3o6UB0tWKBDSJlIHRK),AMD,2026-02-07 15:53:13,5
AMD,o45qzvx,Channelling that 3rd party accessory sticker wrap aesthetic. If it wasn't a GPU I'd expect to find it on clearance in 6 months time for a quarter of the price.,AMD,2026-02-07 22:51:18,3
AMD,o48k029,Sapphire can get fucked. Won't honour a warranty for my dead xtx because the store i bought it from went into administration.  Claim it has no manufacturer warranty,AMD,2026-02-08 11:33:17,3
AMD,o42sixh,Nice,AMD,2026-02-07 13:34:58,2
AMD,o43evyk,What about that power connector sapphire demoed last year.  It's similar to Asus BTF?,AMD,2026-02-07 15:38:14,1
AMD,o44vpaz,What about the corruption desert?,AMD,2026-02-07 20:01:03,1
AMD,o4bz48b,"I have the normal version of this card, this is reassuring for spec requirements. Still deciding between PS or PC",AMD,2026-02-08 22:36:27,1
AMD,o42wz4n,Only way they can sell this shitty game is by bundling it with stuff people actually want,AMD,2026-02-07 14:01:29,-9
AMD,o42syke,Probably $1000,AMD,2026-02-07 13:37:39,12
AMD,o43ev2q,I've wanted to just spray pain that back plate a s the front grill... white build and the gpu is gray,AMD,2026-02-07 15:38:07,6
AMD,o477isl,"What’s wrong with 12 pin, is it good or bad?",AMD,2026-02-08 04:22:16,1
AMD,o43e7f7,It glows,AMD,2026-02-07 15:34:53,4
AMD,o4664l3,Watch it blow up.,AMD,2026-02-08 00:23:56,2
AMD,o43goe8,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-07 15:47:00,1
AMD,o43bg8y,You ok?,AMD,2026-02-07 15:21:02,1
AMD,o44ampx,![gif](giphy|uk0OE1jkLXwUkHSGlk),AMD,2026-02-07 18:14:31,6
AMD,o44z6um,That's less than the price of the regular one where I live,AMD,2026-02-07 20:19:41,3
AMD,o4cqm4k,Just $1000?,AMD,2026-02-09 01:13:23,2
AMD,o47pu2k,https://i.redd.it/yojq1mdxw7ig1.gif,AMD,2026-02-08 06:50:48,13
AMD,o49zu3d,https://www.techpowerup.com/344543/sapphire-amd-radeon-rx-9070-xt-nitro-melts-connector-after-almost-a-year-in-use?cp=3,AMD,2026-02-08 16:47:41,3
AMD,o43i5vx,It was my opinion on the latest AMDs graphic cards!,AMD,2026-02-07 15:54:14,1
AMD,o43cg9f,Yep just saying the truth,AMD,2026-02-07 15:26:05,2
AMD,o4cyb7k,Yes  we in a ram crisis here,AMD,2026-02-09 01:56:40,1
AMD,o2ikgsy,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-30 00:45:41,1
AMD,o2janon,"Really stretching here. 9950X3Dv2 != 9950X3D2; 9950X3D version 2 can be anything, like improved frequency and cache CCDs, not unlike 9850X3D. It can still be a hybrid chip. Creating one 9950X3D2 will cost two 9850X3Ds in practice because really good CCDs don't just magically appear. There's a limited supply of very good bins.  Even if 9950X3D2 (the dual V-Cache one) exists, it's not for gamers. It'd be for parallel workloads that require cache and execution consistency. That'd be better under a Pro variant that offers enterprise security features for workstations or even under EPYC 4000-series that can combine higher frequencies and cache use in database operations that are hitting system RAM a bit harder than usual.  Gamers can't have everything.",AMD,2026-01-30 03:11:01,86
AMD,o2l53j7,"It says ""v2"". Pretty sure that's revision 2 of the folder under it. Maybe they screwed up v1 or v2 shows better numbers than v1.",AMD,2026-01-30 11:46:42,15
AMD,o2ivzwv,How is this a rumor when there’s a video of it. wtf?,AMD,2026-01-30 01:49:21,42
AMD,o2qh2ju,If i worked at one of these places i would name folders like this  just to cause confusion,AMD,2026-01-31 04:15:49,6
AMD,o2j4spq,multiplier 65x... damn,AMD,2026-01-30 02:38:16,5
AMD,o2oivqz,I also have a folder on my desktop. I just created it. It is called: AMD Ryzen 9 9990X3D with a whopping 4 cache CCD's. Absolute monster of a CPU. Created custom silicon and took 4 9800X3D CCD's and put them on 1. Benchmarks are off the charts.,AMD,2026-01-30 21:39:42,3
AMD,o2mxuz2,That's cool.   What about 6Ghz 9990X3D?,AMD,2026-01-30 17:19:54,2
AMD,o2w1ljh,I’m more interested in what happened to R5 9600X3D to be honest.,AMD,2026-02-01 00:54:54,1
AMD,o2x76o6,Link to video please,AMD,2026-02-01 05:17:32,1
AMD,o30aoxk,"why would u want that... it is a two chiplet design, and for gaming it would have issues like all the other dual ccd cpus where u loose perf when a core access a level 3 cache on the other ccd.",AMD,2026-02-01 17:54:56,1
AMD,o361pdl,It's the v2 version which doesn't catch fire on the asus and asrock boards.,AMD,2026-02-02 15:19:25,1
AMD,o3pbx5v,Having 2x 3d cache CCDs prob won't benefit games much as any spill over to the 2nd cache will be too far away anyways and double the latency.,AMD,2026-02-05 12:05:32,1
AMD,o3140yf,Ehh AMD. I use it but I also am team blue,AMD,2026-02-01 20:09:34,0
AMD,o2rgb3n,"There were rumors about 32GB VRAM 9070 XT, they denied it, 6 months later we got the AI PRO 9700.   I think there has been too many independent rumors about the 9950X3D2 as dual V-Cache. Whether it will end up as a real, purchasable product or not, might be a different story. But at this point, I'm fully convinced the dual V-Cache CPU exists.",AMD,2026-01-31 09:10:31,7
AMD,o2w1ew0,Yeah and why would they call it 9950x3d2? That’s such a clumsy name compared to e.g. 9990x3d,AMD,2026-02-01 00:53:50,1
AMD,o2n7496,Or the Ryzen 9 Pro 9965X3D,AMD,2026-01-30 18:01:05,0
AMD,o37xfu7,"We don't even know what is in that folder. Maybe it is a draft for a presentation they have to give on the 9950X3D, and that folder is the second draft of the presentation.",AMD,2026-02-02 20:32:41,1
AMD,o2j0p5b,"We do not know that the folder refers to model number. Note that they have 9950X3D and 9950X3Dv2 folders right next to each other. These could simply be v1 and v2 of something to do with that existing chip. We also don't know if the 9950X3D2 or whatever it gets called (please be XT3D) will be released. It could be, but AMD could also opt not to, finding it unprofitable or just unnecessary.",AMD,2026-01-30 02:15:37,41
AMD,o2j2dkw,Because you can name a folder anything. I create content for Enterprise IT customers and partners and regularly slip in little easter eggs. None of them are based on anything real.,AMD,2026-01-30 02:24:50,25
AMD,o2j7q1u,video of what? a fucking folder?,AMD,2026-01-30 02:54:21,9
AMD,o2t1ax2,You mean the second time they ran the test?,AMD,2026-01-31 15:53:30,1
AMD,o2j80dp,Frequency go brrr,AMD,2026-01-30 02:55:58,3
AMD,o2xp5do,"They will release 9999X3D with 6.9 GHz, instead.",AMD,2026-02-01 07:47:10,2
AMD,o2w5qrp,9950x6d,AMD,2026-02-01 01:19:16,1
AMD,o2j8982,"But the title is literally a folder name on desktop, yeah It can be fake, but it's no way rumor, the folder with that name is there",AMD,2026-01-30 02:57:20,-8
AMD,o36jts0,I'll allow it.,AMD,2026-02-02 16:44:03,1
AMD,o2zkkgu,"Its missing ""MAX AI"" or some other bullshit marketing suffix",AMD,2026-02-01 15:54:57,2
AMD,o2j8gik,Yes but the implication of a leak IS a rumour,AMD,2026-01-30 02:58:29,9
AMD,o328z1n,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-01 23:35:56,1
AMD,o4cnjkd,The 5 buyers of this device will be quite frustrated,AMD,2026-02-09 00:55:53,54
AMD,o4d18w6,"Spending $4300 on a handheld is legitimately insane. Even with Strix Halo, it's a PS5 or Series X running at 1080p instead of 4K. At least throwing that kind of dough at something like a 5090 actually gives you a premium top of the line experience.",AMD,2026-02-09 02:12:11,24
AMD,o4dhf0h,"Absolute absurdity. I initially predicted this would be around $2,800-$3,299 MSRP (which is still crazy), but paying $4,299 for a 4060/4070 laptop tier performance is lunacy.  At that price point I rather buy a 5090 laptop, handheld portability be damned. 🤣 Or better yet, buy a 5080 desktop…",AMD,2026-02-09 03:38:22,6
AMD,o4d3xfc,Cool but these handheld slop needs to calm down.,AMD,2026-02-09 02:25:48,10
AMD,o4ddpk0,"*Your scientists were so preoccupied with whether or not they could, they didn't stop to think if they should.*",AMD,2026-02-09 03:17:20,3
AMD,o4drlx8,All that money for no FSR 4 is wild. AMD refusing to release anything RDNA 4 on these platforms is pathetic. Come on now,AMD,2026-02-09 04:45:01,3
AMD,o4dty4u,"the price is high, would be justifiable if the portability is better than laptops, but this is not the case, 1.4kg is almost the weight of an asus G14",AMD,2026-02-09 05:01:35,2
AMD,o4dyndj,April Fools? This has to be a joke,AMD,2026-02-09 05:37:00,1
AMD,o4dsnmg,"Dang, that's almost half the price of a Predator 21X.",AMD,2026-02-09 04:52:23,1
AMD,o4dxolz,Weight approaching the lower end of thin gaming laptops with vastly faster gpus🤷‍♂️,AMD,2026-02-09 05:29:29,1
AMD,o4dzsla,I already feel 608g Ally quite heavy. This feels like weapons from Monster Hunter.,AMD,2026-02-09 05:46:07,1
AMD,o4e27ma,"Just $4300? Come on, that's not nearly enough. We all know handheld buyers have unlimited budget to feed their dumb fetish. Gotta milk them for everything!",AMD,2026-02-09 06:05:53,1
AMD,o4ddrw0,Buying ayaneo product at this price is like buying a used condom and putting it in your mouth. the qc and the software are the fucking worst!,AMD,2026-02-09 03:17:41,0
AMD,o4dila8,Lawl. Just buy something cheap and use moonlight.,AMD,2026-02-09 03:45:29,0
AMD,o4dpffm,"This headline is purely to stir up the online community.   The 2299 option isn't a bad deal there.  Have a look at HP workstations with 395, except this also has a super bright OLED with built in controls.  Edit: price",AMD,2026-02-09 04:29:56,0
AMD,o4d4e7l,Their parents own the airlines so it will be alright,AMD,2026-02-09 02:28:12,14
AMD,o4d0bw5,"Cope, it's literally the best handheld on the market. Which means there's a built in market for it. 5090 places higher on steam hardware survey than any amd cards. Also the only strix halo oled.",AMD,2026-02-09 02:07:22,-36
AMD,o4db0qn,The 32gb ram AI Max 385 at 1999$ or 1799$ preorder does seem alot more reasonable. Still overpriced but maybe it wouldn't be half bad if it had fsr4 support without dll swapping or using linux. It should've had the AI Max 388 instead as well.,AMD,2026-02-09 03:02:32,4
AMD,o4d53gz,It's not at 1080p it's 2400x1504. Same screen as the Redmagic Astra with 313 ppi. And ps5 have no screen. You can plug this into a 4k TV too,AMD,2026-02-09 02:31:39,-6
AMD,o4dy5m7,Yeah none of these nee handheld devices are IMO worth it until FSR4 native support.,AMD,2026-02-09 05:33:11,1
AMD,o4dmjcw,And what if you carry your handheld computer away from your ethernet dock?,AMD,2026-02-09 04:10:44,1
AMD,o4dxfa1,">Have a look at HP workstations with 395, except this also has a super bright OLED with built in controls  Which is also a colossally overpriced machine *for gaming* because it games like a low power laptop 5060.",AMD,2026-02-09 05:27:28,1
AMD,o4d27rk,"i have a 5090. i'm one of those people. i ain't spending four grand on a handheld, and i fucking love handhelds. i'll just buy a tablet and do moonlight via that. steam deck for when im offline. that's fine.",AMD,2026-02-09 02:17:14,19
AMD,o4d1xmr,Well you're just wrong about the 5090. There are several AMD cards that beat the 5090 on the steam hardware survey.,AMD,2026-02-09 02:15:45,7
AMD,o4dhh38,Im very happy with my Steam Deck OLED.,AMD,2026-02-09 03:38:42,1
AMD,o4dx947,> You can plug this into a 4k TV too  You can plug anything into a TV.,AMD,2026-02-09 05:26:08,1
AMD,o4dp634,?  Don't need an ethernet dock. Or any dock.,AMD,2026-02-09 04:28:09,1
AMD,o4dv0o7,Anywhere with a decent internet infrastructure can handle streaming at 100 Mbps over 4G/5G?,AMD,2026-02-09 05:09:21,1
AMD,o4d43lf,Then you ain't the target audience. I'd rather buy a mate xt but people will say buy a phone and a tablet. Cool. I don't want a tablet. People looking for the best PC handheld aren't looking for a tablet.,AMD,2026-02-09 02:26:42,-22
AMD,o4dhxsq,ok?,AMD,2026-02-09 03:41:33,-1
AMD,o4dz9kf,A rug?,AMD,2026-02-09 05:41:53,1
AMD,o4dzot1,I think he's trying to imply the hypothetical where not even cellular works and only offline games are available.  I'd think anyone who has 4.3k to spend on a handheld can easily afford to get a mobile starlink satellite receiver or something similar if they do want to travel to such obscure places however.,AMD,2026-02-09 05:45:17,1
AMD,o4d5t1e,"and you're very right that i'm absolutely not the target audience but the original point that marclapin was making is that its not exactly a massive audience to begin with. to say otherwise IS cope lol. four grand in this economy, jesus dude",AMD,2026-02-09 02:35:19,16
AMD,o4d9nzd,"The target audience for this is tiny though. At the highest end you're spending over 8x the cost of the steam deck, and you're definitely not getting 8x the value, not to mention the ergonomic drawbacks that come with being over twice as heavy.",AMD,2026-02-09 02:55:44,8
AMD,o4e098i,You can definitely do that too,AMD,2026-02-09 05:49:50,1
AMD,o4d93ky,"It's anyone who doesn't care about the money and is looking for the best handheld. The mate xt was up to 3400$, initially only in China, and reportedly sold quite well.",AMD,2026-02-09 02:52:48,-16
AMD,o4dalmf,"Yeah don't get the 128 gig version. Get the 64 for 2300. Lego 2 is 1500$ USD, and never in stock. Going for 1850 on aliX. Or even the 385 for 1800 since the screen is better.   And that just has the z2e which is just the z1e which we've had for like 3 years. Pre orders sold out in like an hour in Canada.  Everyone was saying how no one will buy that either. But being the best handheld on the market sells.",AMD,2026-02-09 03:00:26,-6
AMD,o4dcgqw,"Frankly any of those prices are still just not worth it over the steam deck for me. It made sense back before the steam deck was released that Ayaneo and the other handheld makers were charging premiums for these kinds of devices because they were relatively niche and not so mainstream, but not now. The target audience for things this expensive are people who can afford to not care about price to performance.",AMD,2026-02-09 03:10:19,4
AMD,o4dd9i9,"Well... yeah. As well as people who want the best handheld. This can play cyberpunk at 100+ fps or comfortably with RT. Or run portal RTX at all, games over 8 gigs of VRAM. It has the performance of an RTX 4070m but w/o being gimped by 8 gigs of VRAM like the 4070m.  The steam deck literally cannot do things this can so the price to performance in theory becomes infinity in those scenarios. And the screen is way better too. Brightness, ppi, 165 hz. Should be better in sun too",AMD,2026-02-09 03:14:48,-3
AMD,o47wf7n,$300 more for HX470 vs 370?   That's $100 per 100mhz increase - 100mhz in CPU and 200mhz in iGP,AMD,2026-02-08 07:51:19,10
AMD,o455pxb,1k for a handheld? JUST TOPKEK,AMD,2026-02-07 20:55:16,12
AMD,o478csl,Windows 🤦‍♂️,AMD,2026-02-08 04:28:15,4
AMD,o4bvo60,Good luck with that.,AMD,2026-02-08 22:17:58,1
AMD,o441s5v,I like the price tbh,AMD,2026-02-07 17:30:46,0
AMD,o447xsu,WHERE ARE THE LAPTOPS,AMD,2026-02-07 18:01:13,0
AMD,o48ikkv,Double the ram is why.,AMD,2026-02-08 11:20:12,9
AMD,o4cqqt6,AMD totally lost the plot in terms of prices.,AMD,2026-02-09 01:14:07,1
AMD,o45s1w5,It's one of the cheapest PC handhelds on the market and more powerful than most of the others other than the 2-3k ones,AMD,2026-02-07 22:57:22,-16
AMD,o45yw7u,"Yes finally, first HX370 device under 1000$. It is just a patience game. Let's wait more and the prices will drop.  And I like that they have 16GB/512GB options which is all I need since I can replace the 512GB drive by one I have already.",AMD,2026-02-07 23:40:23,3
AMD,o4cqsta,They should just offer a HX470 option at 16GB ram.,AMD,2026-02-09 01:14:27,2
AMD,o47w4zm,"Lmao, 890m handhelds have been commoditized and we know they all perform the same.",AMD,2026-02-08 07:48:42,-1
AMD,o488o3o,sd cards are very cheap too,AMD,2026-02-08 09:47:14,1
AMD,o49sdyz,with 144 hz oled and 800 nits so whats a better handheld for less,AMD,2026-02-08 16:11:25,1
AMD,o4bal0z,Yeah but they aren't that fast,AMD,2026-02-08 20:32:06,1
AMD,o2tgtlb,how can it be a better binned 9800x3d when the layers are above or under which made them run cooler?,AMD,2026-01-31 17:07:46,3
AMD,o2haazr,So I need help here is the 9850x3d safer to use than 9800x3d ?  By that I mean it won't get fucked as easily! Because I seen a lot of people talking about the 9800x3d getting burned or not working and so on  So is it better quality?,AMD,2026-01-29 20:51:20,2
AMD,o34ysn3,The 9800X3D is exactly the same. This is not new for the 9850X3D.,AMD,2026-02-02 11:18:47,2
AMD,o2hkoh2,"Doubtful but who knows at this point, it’s just a better binned 9800x3d so in theory it should still have the same issues unless AMD tweaked something from the original",AMD,2026-01-29 21:40:47,3
AMD,o2hnodm,I think it would be better. Later batches of CPUs should be more reliable.,AMD,2026-01-29 21:55:02,2
AMD,o2z4rgx,Where do you hear about 9800x3ds getting burned?  Those are only with ASROCK motherboards killing them.,AMD,2026-02-01 14:36:00,-1
AMD,o2i3rmr,I see because in my country it's 50$ more expensive than 9800x3d  So it makes 0 sense to get the new one if it's just a overclocked version lol,AMD,2026-01-29 23:15:49,2
AMD,o2zak7r,"its not just asrock, there are reports from msi, asus, gigabyte. asus put out a statement not too long ago. seems to be something wrong with the 800 series motherboards",AMD,2026-02-01 15:06:22,5
AMD,o2zb2j6,"Oh I didn't know about that, why not use the b650/ x670s then?",AMD,2026-02-01 15:08:56,2
AMD,o2zc0ye,"yeah those are the safer bets right now. 800 series is better if you need USB4, otherwise they're effectively similar",AMD,2026-02-01 15:13:44,3
AMD,o2a7dta,"Have any of the reviewers addressed if the IMC is better with heavier overclocks? Most of them seem to have gone the other way to test the claim that the 3d cache makes binned memory less important.  I just wanna know if this one is better at heavy memory OC, because memory OC is fun!",AMD,2026-01-28 20:34:42,50
AMD,o2b1z9j,its just a KS from AMD instead of intel.,AMD,2026-01-28 22:51:18,38
AMD,o2afhzw,tldr: it’s not worth the premium.  edit: I am a hypocrite I just bought it for my new system.,AMD,2026-01-28 21:10:16,24
AMD,o2a2qwq,"The best reviews are those from Hardware Canucks and Der8auer, slamming the CPU for being power inefficient and running hot for just a bunch of additional frames... on a 5090.",AMD,2026-01-28 20:14:05,40
AMD,o29uujg,cool so basically “don’t bother”,AMD,2026-01-28 19:38:45,22
AMD,o2duedw,"Manufacturer makes a newer model clearly based on last years model. Calls it the same, but with a 50 in the end instead, which is widely understood as „refresh at best“ in the field. People are livid because it is just marginally better than last years model.  but_why.gif  If anything, the 9950x (and the other 16C parts) are the odd man out here, they should’ve been called 3999 and so on imho.   Don’t get me wrong, I’d very much rather see a new 10000 series roll out right now, which would suck in many games like 9000 did before the x3D variants released.   I am just saying they did not, in fact, call this here chip 10800x3D.",AMD,2026-01-29 09:50:58,3
AMD,o2bagk3,Was it released in UD today ? I can’t find any listings .,AMD,2026-01-28 23:34:32,1
AMD,o2g7t9y,"Nice up-bin, but it is still the same chip, so a bit of a nothingburger. If you were still planning on upgrading (with today's RAM and SSD prices, lul) and targeting 9800X3D, this is a fair improvement to buy instead if you not tight on money, but otherwise this does not matter in the slightest.  The actual upgrades are about an year away. Non-X3Ds maybe bit earlier, but Zen 6 X3Ds, actual improvements over what already exists, maybe an year from now. And Intel may have something useful again by then as well.",AMD,2026-01-29 17:52:14,1
AMD,o3t60cj,It seems like Linus was super wrong in his review. He said it doesn't take more power and it's not just a binned 9800 lol,AMD,2026-02-05 23:40:32,1
AMD,o2abeu8,Asking the real questions here. Also curious if these chips are more consistent in their memory OC across multiple chips. It seems hit or miss with the IMC on regular 9800X3D’s,AMD,2026-01-28 20:52:23,21
AMD,o2d25ms,I wouldn't really expect that kind of content for day one reviews. That's more of a few-weeks-after-launch type of content. It also has the propensity to be silicon-lottery dependent so you can't really go by a single sample anyway.,AMD,2026-01-29 05:43:26,6
AMD,o2aycby,"you are so brave, with how expensive ram right now, i don't think i want to take that risk",AMD,2026-01-28 22:33:43,3
AMD,o2bp1yl,Exactly my question it’s the weakest part of my 9800x3d by far and I’m just hoping they made it better for the 9950x3d2 I’m eventually going to waste my money on lol,AMD,2026-01-29 00:50:25,0
AMD,o2chwm3,"At least it's at a reasonable price point, instead of the way overpriced KS strategy of Intel.",AMD,2026-01-29 03:29:49,15
AMD,o2b5e6u,it is if you plug your 5090 to a calculator screen,AMD,2026-01-28 23:08:23,14
AMD,o2a8tfi,Intel says hi,AMD,2026-01-28 20:41:06,16
AMD,o2dza64,Eh okay. I'll keep my 7800X3D then.,AMD,2026-01-29 10:34:44,2
AMD,o2anaqb,It’s not a bunch though compared to the 9800,AMD,2026-01-28 21:44:23,3
AMD,o2dahsm,Techpowerup also noted the increased power consumption and temperatures. The small gain in performance is not free lol,AMD,2026-01-29 06:50:02,1
AMD,o2e5rmh,❤️,AMD,2026-01-29 11:29:02,1
AMD,o2gsc65,"> refresh at best  I don't mind refreshes to be honest, as long as the price is roughly the same.",AMD,2026-01-29 19:25:13,2
AMD,o2d2dp8,Tomorrow.,AMD,2026-01-29 05:45:09,3
AMD,o2o943x,Would it be worth $20 more assuming you're upgrading anyways?,AMD,2026-01-30 20:53:39,1
AMD,o296ggk,Why would you be looking at a refresh part for gaming at 4k??  Makes zero sense you don't need to upgrade your cpu until Zen 6 X3D is out.,AMD,2026-01-28 17:53:06,9
AMD,o2alnh3,"I can tell you that the 9950x3d appears to have a slightly better IMC, and I've seen others say the same. If the 9850x3d is similar then it serves a niche which ironically is the opposite of the marketing push for ""it's okay to use cheaper RAM.""",AMD,2026-01-28 21:37:11,5
AMD,o2grqmw,"Yeah, maybe we could get der8auer to do some follow-up content like that.",AMD,2026-01-29 19:22:24,2
AMD,o2bxpbw,RAM is hard to break. Like really hard.  The system will crash 99.99% of the time before you get even close to doing damage.,AMD,2026-01-29 01:38:01,7
AMD,o2bvobz,"I typically pick a conservative voltage which allows me to get a decent OC, but isn't breaking any world records. I don't think I've ever killed RAM that way.",AMD,2026-01-29 01:26:44,1
AMD,o2ftbfu,Wait for Zen 6 X3D 12 core that will be a better use of your money fight the FOMO.,AMD,2026-01-29 16:47:01,2
AMD,o2gs0fk,"Yeah, then again the 9800X3D isn't sold at its MSRP right now, so more AMD trying to 'reset' the price a bit for each sold.",AMD,2026-01-29 19:23:40,2
AMD,o2cm4m9,"your 1000$ oled calculator screen that runs at 720hz. lol.  But yeah, it's a waste of money for a cpu that won't even be the fastest for a year when zen 6 comes out.",AMD,2026-01-29 03:55:19,4
AMD,o2ge3m5,Prescott anyone?,AMD,2026-01-29 18:20:17,1
AMD,o2b556m,"to which 9800? 9800X3D PBO + 200 ? 9800X3D eco mode ? 9800X3D 720p ? 9800X3D 1440p ? 9800X3D 5070 Ti ? 5090 ? Medium settings? Ultra settings? 4% average in best case scenario 5090 and medium settings at 1080p, 4 or 5 frames on top of 100, 105 vs 100, a bunch (never a lot).  look at this: [https://i.imgur.com/SD3LxBc.png](https://i.imgur.com/SD3LxBc.png)  That is a 5090 running at 1440p, 2 fps difference between the 9850X3D and 9800X3D, the very moment you swap that 5090 for a 5080 or a 5070 Ti level GPU (far more realistic scenario) the difference between both CPUs will become null and void, except for the power draw difference, which is nasty in the case of the 9850X3D, 108 watts vs 75 watts just for two extra frames no one is going to notice, shame.  There's also a +10c delta hurting the 9850X3D compared to the much cooler and power efficient 9800X3D. It would be silly to buy the new part if you're going to play on something else than a 5090 or at 1440p ultra. Use those 20 bucks to buy a game on Steam.",AMD,2026-01-28 23:07:06,10
AMD,o2pemrk,"Many who buy 9800X3D or 9850X3D are just going for the fastest gaming CPU. At that point the price does not matter.  If you are price sensitive, 9800X3D is better value, since the difference is tiny.",AMD,2026-01-31 00:25:28,1
AMD,o2ij15v,"You should probably mention that its hard to kill ram with clocks, or timings. But voltage, ya its easy to fry with too much voltage.",AMD,2026-01-30 00:38:07,2
AMD,o2debod,"tell that to my B450 Carbon that overvolted my RAM by 0.07v after a BIOS update - 1.54v, all 4 sticks instantly dead - thankfully Patriot honored my limited lifetime warranty with no receipt or packaging",AMD,2026-01-29 07:22:34,1
AMD,o2cne26,"I think they're just milking all those enthusiasts suffering from FOMO, and they're a lot. They will buy this overclocked 9800X3D, then they will get the 9950X3D-2 but at the same time they will be keeping an eye on Zen 6 pre-orders.",AMD,2026-01-29 04:03:08,3
AMD,o2by2mg,"I'll look at the reviews eventually, but I'm wondering if there's an analysis available 1:1 with 9800X3D - if there's any efficiency gains at the same clockspeeds and voltages (etc.).  Cause it looks like they just pushed the envelope a bit with better silicon in terms of stability but they haven't gained any efficiency.",AMD,2026-01-29 01:40:07,1
AMD,o2di9rm,">the very moment you swap that 5090 for a 5080 or a 5070 Ti level GPU (far more realistic scenario) the difference between both CPUs will become null and void  This is CPU review, not GPU.",AMD,2026-01-29 07:57:32,1
AMD,o2cln95,"Wow, doing a CPU test in a GPU limited scenario...what a revelation that the CPU doesn't really matter.  The entire goal of a cpu test should be to differentiate the cpu. That means min settings at 720p (thank god for that new oled) or 1080p resolutions in esport games with uncapped fps.   We know what the result will be like in GPU limited scenarios already. It's a waste of a test.",AMD,2026-01-29 03:52:21,1
AMD,o2illpt,Even that. It's not impossible - really depends on the dies used and quality - but *most* RAM will just keep on trucking while crashing the system.,AMD,2026-01-30 00:51:41,1
AMD,o2ecc6s,How does that kill RAM? I've run 1.55 volts to my DDR4 for 5-7 years. Others daily at 1.65v.  I would understand if it was 1.8-2.2 volts.,AMD,2026-01-29 12:17:18,4
AMD,o2pd0ah,bro personally attacked me,AMD,2026-01-31 00:16:39,2
AMD,o2byfql,"It's not better silicon, at least not premium silicon, if it was then they wouldn't have needed to bump the stock core voltage to 1.3v, they might have done some basic binning but at the end of the day it's the same 14 month old CPU but factory overclocked, you can manually overclock your 9800X3D, it's unlocked.",AMD,2026-01-29 01:42:10,6
AMD,o2cmlzq,"Yeah if you want to actually check the performance increase of an overclocked 9800X3D you should run a 720p test or lower, but the thing is the vast majority of 9850X3D buyers are running their games at 1440p or higher on a 4080/90 5070 Ti/80/90 or AMD 7900 XTX / 9070 XT.",AMD,2026-01-29 03:58:15,1
AMD,o2ftix9,You also had active cooling on that ram.,AMD,2026-01-29 16:47:56,2
AMD,o2ee79z,"those are DDR3 voltages, nobody other than people with watercooling daily 1.5+v",AMD,2026-01-29 12:30:03,0
AMD,o2pd8pm,"sorry, I also replied to this other comment you just made, if you're happy with it then just keep it, even if doesn't make any sense for anything 5080 and below.",AMD,2026-01-31 00:17:54,1
AMD,o2efz1o,"> if it was then they wouldn't have needed to bump the stock core voltage to 1.3v  The spec VID range for all zen 5 CPU's, vcache or not, goes up to 1.42 VID. There is no voltage range change on the 9850x3d.",AMD,2026-01-29 12:41:50,3
AMD,o2bzjby,"I mean I agree - I have no idea if my 9800X3D can handle the higher speeds as I'm content with it stock (still under the cheapest TR dual-tower I could find at MicroCenter, haven't gotten my custom loop built yet).  I think it's more just that these CPUs can reliably handle the speeds, whereas not all 9800X3D CPUs could.  Also still wondering about the memory controller, but I understand that that's an insensitive thing to ask about these days lol",AMD,2026-01-29 01:48:15,1
AMD,o2coxlq,"And the result will be obvious: almost zero difference in performance.  If you have a speed limit of 60 mph on the highway, don't be surprised when the ferrari drives at the same speed as a toyota.",AMD,2026-01-29 04:12:57,3
AMD,o2g9d4i,"Mm. I started out with 1.5v. Ran that for a short while. Then 1.55v and a 120mm fan, hanging from the case with zipties. If voltage would have killed RAM instantly, it would have killed it with or without fan/cooling.",AMD,2026-01-29 17:59:08,2
AMD,o2eh7fm,"BS. Very few watercool their RAM. All you need is ANY kind of fan blowing on the RAM when going past 1.5v. And that is not to keep them alive, it is to keep them stable. RAM will spit out bit-errors way before they die. Normal DDR4 is rated for 85 C. Generally, it is best to keep them under 50-55 C if you overclock them.    DDR3 default is 1.65v, yes. What do people that OC ram on DDR5 run? Is it 1.4-1.5v? And default is 1.1-1.2v. Of course, XMP profiles for even DDR5 often runs at 1.4v. Have you heard of millions of RAM sticks dying?",AMD,2026-01-29 12:49:47,4
AMD,o2ehh1a,While running medium/ light loads (gaming) the core voltage now jumps to 1.3v+ while the 9800X3D stays around 1.1v,AMD,2026-01-29 12:51:28,2
AMD,o2c0pc9,"The memory controller is also the same, they're not developing a new IMC or upgrading the current one just for a factory overclocked 9800X3D, they're doing that for Zen 6. We've had 9850X3D users here stating that their CPU can't handle certain RAM configurations that they could achieve with a 9800X3D. It's just a silicon lottery. You might swap your 9800X3D for this factory overclocked CPU and get a worse IMC compared to what you had.  If the memory controller was any better AMD would have marketed the fact non-stop while charging extra money for it.",AMD,2026-01-29 01:54:41,4
AMD,o2elzj3,"DDR5 is irrelevant here, I'm talking about DDR4 voltages - and honestly I'm barely acquainted with DDR5 OC. DDR4 EXPO/XMP generally doesn't go over 1.4v and I don't know where you get the idea that many people daily 1.55-1.65v.",AMD,2026-01-29 13:18:33,2
AMD,o2ekofz,"Especially DDR 5 and crazy so dual rank DDR 5 is next level sensitive to temperature for single errors (classic 10k Karhu single errors)  So you're dead on, at least on AM5, no one in their right mind would daily sticks voltage high enough to kill them.",AMD,2026-01-29 13:11:02,1
AMD,o2ei8b4,"The CPU's won't use more VID than they need to hit the maximum frequency that is on their boost table, even if it's safe and available.  For the 9800x3d out of the box, that means that whatever voltage that it takes to hit ~5225mhz is the maximum that you'll see. You may have had a 9800x3d which would be happy at 5500mhz with normal gaming voltage, but since 5225 was an arbitrary limit, the CPU would sit at 5225. It will then use the voltage that it thinks that it needs for 5225mhz, rather than using the full voltage.  Removing fmax and other limits so that you only hit the voltage/reliability limiters exposes the actual voltage range.  Likewise, applying an fmax limit to another CPU like the 9850x3d or the 9950x3d's vcache CCD brings behavior in line with 9800x3d (just deleting all of the frequency options near the top of the curve).  If you're riding the safety limiter at 1x scalar, then about 1.34 VID on zen5 (x3d or standard) is typical in games.",AMD,2026-01-29 12:56:09,2
AMD,o2c1zro,"Ah, cool. I wasn't sure if they improved over a stepping or whatever else (been known to happen).  Yeah I'm aware that they're redoing it for Zen 6, which will be perfect timing-wise. Zen 5 can't really make use of faster memory outside of niche cases.",AMD,2026-01-29 02:01:47,1
AMD,o2erkj1,"[https://pcpartpicker.com/products/memory/#ff=ddr4&b=ddr4&B=1550000000,4250000000](https://pcpartpicker.com/products/memory/#ff=ddr4&b=ddr4&B=1550000000,4250000000)  What does that look like? You can get DDR 4, running 1.55v, straight  from the shop and a wide selection. Have you even looked?  Where do I get the idea? Watching RAM OC content like Buildzoid, being on the Overclocking subreddit, [overclockers.co.uk](http://overclockers.co.uk) etc for the past 10-15 years.  DDR5 at 1.4v, OOTB: [https://pcpartpicker.com/products/memory/#b=ddr5&B=1400000000,4250000000&ff=ddr5](https://pcpartpicker.com/products/memory/#b=ddr5&B=1400000000,4250000000&ff=ddr5)  Another popular voltage for DDR5 is 1.45v. Expo/XMP.",AMD,2026-01-29 13:48:55,0
AMD,o2fubzg,advanced users that tweak and run active cooling on ram sometimes forget the average casual users don't run fans on their ram and what they are doing applies to a very small percentage of people.,AMD,2026-01-29 16:51:31,0
AMD,o2ek9g0,I'm doing scalar x1 and never seen anything beyond 1.15v,AMD,2026-01-29 13:08:35,1
AMD,o2f0hcx,"I didn't know they made DDR4-4800/5333 sticks, do people actually run these in ""regular"" builds?   Browsing OC-centered sites won't give you a realistic image of the general population, most people don't OC much and run at <1.5v (they're also afraid of the numbers turning red in the BIOS).    >Where do I get the idea? Watching RAM OC content like Buildzoid, being on the Overclocking subreddit, overclockers.co.uk etc for the past 10-15 years.   Cool, I've been on the internet for a little while too :\^)",AMD,2026-01-29 14:35:10,2
AMD,o2elf05,"If that is the case, then your CPU is restricted in software to a maximum boost frequency which is well below its capabilities, and it just never botheres to use a medium-high voltage because it's always running at a frequency which is stable with a low voltage. The scalar will allow for ~1.34 VID and there is hundreds of mhz of stable clock gain between 1.15v and 1.34v.  The 9800x3d can buy +200mhz via the PBO controls, but otherwise you need async eclk to fully bypass AMD's frequency lockouts.",AMD,2026-01-29 13:15:19,1
AMD,o3vlpx3,Yeah because there isnt a problem. RMA rates for the 9000 Series are in line with those of the 7000 series. Its a product that sells ten thousands of units so there will be dead CPUs every day or so.,AMD,2026-02-06 10:10:18,134
AMD,o3vjfct,"Oh well, I sure hope my MSI board doesn't kill mine, already had a 5800X3D die on an MSI mobo",AMD,2026-02-06 09:48:47,28
AMD,o3vzp1w,"I thought that's what they've been doing. Weren't they already looking at the issue like a year ago? Wtf have they been doing since then?? I have a 7800X3D and an Asrock board, I hope there won't be any issues.",AMD,2026-02-06 12:07:30,13
AMD,o3z4gu7,"The reddit to article to reddit to article to reddit to article to asrock eventually responding followed by reddit to article endless circlejerk continues, pretty much every single one of them referencing the same data but then each step ""ADDING"" them together rather than recognizing them as the same.",AMD,2026-02-06 21:40:00,3
AMD,o3xhnqz,"Just… now? Was there a recent spike in volume? This has been an issue for a while, and some BIOS updates have moved the needle in the right direction. They told GN that they were investigating this like 7 months ago.      The article also uses… cherry picked data from one megathread on one subreddit.",AMD,2026-02-06 16:54:07,3
AMD,o3xfkel,who murdered these processors?,AMD,2026-02-06 16:44:19,2
AMD,o3w8z4n,Meh. A dead cpu is bound to happen.,AMD,2026-02-06 13:08:55,2
AMD,o45a0hc,"So what are the symptoms of these dead mobo/CPU combos?   A friend of mine has an asrock B850 motherboard and a 9600X and it just froze up mid-game, and is now completely dead (CPU+DRAM lights turn on, won't post). How likely are these related?",AMD,2026-02-07 21:18:32,1
AMD,o47rvd9,That’s a lot of words to say nothing,AMD,2026-02-08 07:09:20,1
AMD,o4bmtfl,"I have exactly the same problem, my 9800X3D was destroyed by an Asrock A620I...",AMD,2026-02-08 21:32:59,1
AMD,o3ywuor,One year passed and they are still saying same shit again? Cpus even year later are still dying on their mbs.,AMD,2026-02-06 21:02:13,0
AMD,o3xachi,These all could be from boards they sold before replacing the faulty bios chips. Did they ever announce a full recall for those boards at least?,AMD,2026-02-06 16:20:13,0
AMD,o3vm8gg,"Crazy how nobody is losing their shit like they did with Intel 2 years ago, considering AMD is also using the ""It's the motherboard manufacturer's fault"" line at this moment.  Look, I'm aware I'm in the AMD subreddit, so feelings matter over facts (like with any specific subreddit), but please look at the whole picture before downvoting. AMD and Intel are not your friends, and both are handling their respective issues horribly. I'm purely commenting on the community's response.",AMD,2026-02-06 10:15:02,-28
AMD,o3w1chi,"It’s been a clip farm topic. People still discussing it while RMA rates are lower than their competitor are a joke really.   It’s a fascinating problem that deserves to be investigated, but the destructive intent with their videos is so petty.",AMD,2026-02-06 12:19:20,56
AMD,o3vt3lv,"I don't know if you have been here for long but this has been going for years (since 3000 family where they degraded fast, we used to have 2 or more reports every week and then a year passed the next arch released and new symptom appeared where the chips simply burned the pads of various rails) and AMD and board vendors have acknowledged the issue and have been actively looking into it. Most MB manufacturers seem to have successfully fixed while others continue the same.  I still remember when AMD was going back and forward with agesa bioses and MB manufacturers were deleting them due to critical voltage measurement and regulation like the f5a.  https://arstechnica.com/gadgets/2023/04/some-ryzen-7000x3d-processors-are-burning-out-high-voltages-may-be-to-blame/",AMD,2026-02-06 11:15:59,13
AMD,o3vjiq7,It seems to be mostly Asus and Asrock at this point.,AMD,2026-02-06 09:49:43,13
AMD,o3vvap1,MSI has the lowest failure rate on my data.,AMD,2026-02-06 11:34:10,3
AMD,o3winsj,I’m on a 5800x3d and msi B450. Use PBO2 tuner and under volt it. I did -15 or -20mv on all cores years ago and I’ve never had an issue.   I also disabled PBO and game boost.,AMD,2026-02-06 14:02:35,1
AMD,o3w8q18,my ex Gigabyte B850M Aorus Pro has killed my R5 9600X,AMD,2026-02-06 13:07:24,0
AMD,o3zxjgc,The endless cycle of click farming,AMD,2026-02-07 00:18:10,5
AMD,o3vnbeg,Probably because the intel ones were 100% going to fail due to manufacturing issues. The 9800X3D failure rate is nowhere near as high.,AMD,2026-02-06 10:25:02,36
AMD,o3vn1q7,"Didn't intel eventually take somewhat of the blame when they started doing the whole ""it's fixed with the new microcode"". And i also believe it was made a big deal there because server providers had very abnormal failure rates, not just consumers on reddit.",AMD,2026-02-06 10:22:34,18
AMD,o3wmrc0,"Everyone hears about the same handful of failures, and we think there are everyone x handful = total number of failures.",AMD,2026-02-06 14:24:16,15
AMD,o41leul,a large part of the problem with hardware reporting going on are this website that OP linked and this website we are posting on. lol.  It's an unstoppable woozle machine and videocardz is at the center of a lot of it.,AMD,2026-02-07 07:13:02,5
AMD,o3xw4zm,"> clip farm topic  YouTubers gotta get those views, being negative helps",AMD,2026-02-06 18:03:12,4
AMD,o3vxezd,Manufacturers running CPUs out of spec is not an AMD problem.,AMD,2026-02-06 11:50:36,31
AMD,o3w4p5t,"Not like the other boards dont kill any. Even MSI Godlike has issues. So I say buy Biostar, Biostar da best cuz they dont even kill any 7800X3D, you can't go wrong with Biostar.",AMD,2026-02-06 12:42:09,-13
AMD,o3whieg,MSI boards have had so many issues that one of the largest PC manufacturers in Germany has removed them as a supplier.  Source: a friend is working there,AMD,2026-02-06 13:56:26,10
AMD,o3vpj6c,"Your comment accidentally proves what I said. There was so much misinformation going around due to outrage that even though the oxidization issues concerned only a small batch of 13th Generation processors manufactured at the end of 2022, somehow it got crossed with the seperate issue of voltage misregulation causing failure. So people thought that the issue happened because of oxidization and that meant that EVERY Raptor Lake (and Refresh) chip was going to have this issue. The oxidization apparently was caught very early on and affected only some early 13th gen chips (meaning 14th Gen was only ever affected by Vmin shift and not oxidization). Thing is vmin shift was eventually addressed, and you only hear about issues from people that have outdated bioses, or people with RMAs running them on motherboards that probably got damaged from the first bad CPU.  Despite that, Intel still handled it like dogshit, and the first few months they also pulled the ""it's the manufacturer's fault"" card, and AMD seems to be doing the same",AMD,2026-02-06 10:45:06,-41
AMD,o3w7cft,Yup they extended the warranty to 5 years if i can recall,AMD,2026-02-06 12:58:58,8
AMD,o3y2m8h,"Exactly, but they are also setting up a false narrative that it is a very bad situation, while RMA rates are well within the appropriate range.",AMD,2026-02-06 18:33:51,1
AMD,o3wonaf,That's what people and Intel said about 13th and 14th gen. Turned out that gaming bioses just exacerbated the Intel flaws. As an enthusiast you can't say for sure that it's only a bios problem.,AMD,2026-02-06 14:34:01,16
AMD,o3xfxbf,-Users.  FTFY,AMD,2026-02-06 16:46:00,2
AMD,o41sval,"It is because AMD provides AGESA as a binary ""black box"" to manufacturers to put in their BIOSes and it's ultimately AGESA that's responsible for controlling critical parameters related to the CPU and memory like limits to vCORE and vSOC.   Manufacturers can tune their BIOSes to an extent, but only within the limits defined by AGESA.",AMD,2026-02-07 08:23:37,1
AMD,o3wrkef,"But the ""data"" shows that MSI is teh most reliable board manufacturer by far.",AMD,2026-02-06 14:49:07,4
AMD,o3ylz8i,"It's not misinformation? You're soloing out the oxidization issue but that wasn't the only one. Writing microcode is part of the manufacturing step. Intel can't blame mobo manufacturers if it's literally every single one of them, and every single 13/14th gen CPU that has been degraded will never work as intended and fail no matter what ""fix"" you install after the fact. If the AMD issue was on the same scale we'd have heard already just like how server hosters and hardware retailers rang the alarm bell with the amount of failures.  I literally know 4 different people that are on their 2nd or 3rd RMA at this point, and while the latest microcode claims to have fixed it it's just impossible to tell. They might have just been able to slow down thee degrading to the point it'll take a couple more years, I mean they've ""fixed"" it like 10 times already now. Meanwhile the breaking news is that there's 10 confirmed cases of 9800X3D's having issues on ASUS boards when it's one of the top selling CPUs in the world.",AMD,2026-02-06 20:07:42,2
AMD,o3wpv1v,"The Intel failure rates were much higher, and the issue was down to architure, running out of spec only made the issue that was already there worse.  This has nothing to do with AMD, when less than 0.1% of the CPUs sold are failing is within margin of error.  We can't fix stupid, stop following content creators telling you to overclock and use PBO with ridiculous values.",AMD,2026-02-06 14:40:23,13
AMD,o42f0vy,"Considering AMD AGESA limits are high and intentionally outside of spec to allow for overclocking (which is for advanced users).  It is still the motherboard manufacturers fault for running CPUs out of spec, which is why AMD had to lower the AGESA limits in the 7000 Series.  I don’t know of a single motherboard manufacturer that doesn't try to run hardware out of spec to claim they provide better performance than the competition. There is nothing worse than a motherboard at default settings.  Adding insult to injury, people listen to content creators to enable and tweak overclock settings as if there were a universal standard for ""best settings"" (when, in fact, it’s different for everyone).  So no, it is neither AMD nor AGESA fault. Unless you want AMD to lock their CPUs like Intel (NO THANK YOU), you are free to go buy Intel and leave AMD alone for the advanced users",AMD,2026-02-07 11:56:34,1
AMD,o3wqmcj,You are presuming that they're running ridiculous values.,AMD,2026-02-06 14:44:16,4
AMD,o3wsd85,"There is not a single content creator video related to overlock and PBO that is accurate, cause everyones hardware and silicon has different ceiling, so yes they are running ridiculous values.  Those of us who actually know what they doing, never had a CPU fail once, and the numbers speak for themselves, all have you have to provide as evidence is ""trust me bro"".",AMD,2026-02-06 14:53:10,3
AMD,o3wvlwb,"You're literally the trust me bro guy in this exchange. There are no numbers speaking for themselves, just logically deficient presumptions after you beat the easy odds.  I'm not claiming the opposite, I'm saying it's unknown.",AMD,2026-02-06 15:09:32,6
AMD,o3wwpg0,"Since you wanna try hard at getting embarassed, [https://videocardz.com/newz/puget-systems-reliability-data-shows-core-ultra-200-and-ryzen-9000-x3d-with-near-identical-failure-rates](https://videocardz.com/newz/puget-systems-reliability-data-shows-core-ultra-200-and-ryzen-9000-x3d-with-near-identical-failure-rates)  The shopping list can go on, [https://wccftech.com/amd-ryzen-7-9800x3d-has-reportedly-one-of-the-lowest-rma-rates-in-ryzen-9000-7000-series/](https://wccftech.com/amd-ryzen-7-9800x3d-has-reportedly-one-of-the-lowest-rma-rates-in-ryzen-9000-7000-series/)  Now you free to take your ""trust me bro, there are issues with AMD"", and go out the door.",AMD,2026-02-06 15:14:59,1
AMD,o42u8v8,Now that you pointed it out it's hilarious 😂😂,AMD,2026-02-07 13:45:22,1
AMD,o49754n,"“Certain CPU configurations.”  That reminds me of these certain DDR5 modules that are certified at ultra-high speeds, but you can’t buy because they’re only engineering samples.",AMD,2026-02-08 14:18:54,27
AMD,o48yywb,Certain?,AMD,2026-02-08 13:28:52,18
AMD,o49017u,I installed it will tell you when I get home it happens most common for me and always after a cold boot.,AMD,2026-02-08 13:35:38,10
AMD,o493yrf,Totally. This time will be so different than all the other times.,AMD,2026-02-08 13:59:54,6
AMD,o49jelu,I mean thats what all the previous versions were for no? Or is it because the recent ram prices and press are causing a drop in sales and this is their answer to stop the bleeding ... and yet they are still probably taking a risk to blame manuf defect probabilities if it continues .. hmm,AMD,2026-02-08 15:26:16,2
AMD,o4b47ph,Think I dodged a bullet by not choosing AsRock. If the taichi was available I would’ve gotten it over my Aorus master.,AMD,2026-02-08 20:00:11,2
AMD,o4aeru6,"Also released for my X670E steel legend, didn't get a new bios since 3.5.",AMD,2026-02-08 17:59:48,1
AMD,o4dkw73,Obi-wan BIOS: From a certain Point of View,AMD,2026-02-09 03:59:56,1
AMD,o4dvmvv,On my Asus x870-E it sometimes does not detect 1 of my nvme's requiring a restart not sure if the boot failures on asrock are similar may just be agesa issue then.,AMD,2026-02-09 05:13:56,1
AMD,o4ckmqc,"So it looks like its still not fixed.   Got stuck on bios code 97 after initial cold boot.     then restarted got stuck on bios step 39 but this time i did reach my boot launcher before it locked up, so it was better.    on third reboot it failed in a more traditional way and immediately locked upon bios step 3.     on my 4th boot reached bootloader again and then finally was able to boot into windows.   so still not fixed and having problems.",AMD,2026-02-09 00:39:49,3
AMD,o490ehc,Let us know what happens,AMD,2026-02-08 13:37:57,2
AMD,o4de6yf,That was the motherboard I got.  I wanted it becouse it had the ability to run PCIE 8x4x4x witch allowed me to add additional M2 drives.,AMD,2026-02-09 03:19:57,2
AMD,o4boysq,"Did you installed it already, it's worth it?.",AMD,2026-02-08 21:43:39,1
AMD,o4br9sg,Nope. I'll wait for some time for it to be tested.,AMD,2026-02-08 21:55:11,1
AMD,o2imecr,"I grabbed one in a Micro Center bundle.  The pricing works that way (though is comparatively the same).  I didn’t need it and know it doesn’t matter, but I needed a bundle anyways and we have 3x AM5 systems in the family, soon to be 4.  Everybody gets a bump.",AMD,2026-01-30 00:55:58,16
AMD,o2itg9g,Opinions on 9850x3d? Even for people who dont own a 9800x3d this refresh chip takes a pretty huge hit in efficiency for its minor performance uplifts.  I was planning on upgrading to this but considering I build in SFF is might just be better to pick the 9800x3d instead (especially with the price drops)  Id say this itteration is mid. Luckily it doesnt cost much more but thats about it,AMD,2026-01-30 01:35:07,26
AMD,o2p20n7,"I'm jealous of these bundles with CPU, RAM, and mobo you can get in US here in UK there is nothing like that.",AMD,2026-01-30 23:16:06,2
AMD,o2j3640,.44,AMD,2026-01-30 02:29:15,1
AMD,o3gckwt,$499 for the CPU.. $1500 for RAM,AMD,2026-02-04 01:48:00,1
AMD,o40t7rf,"Its crazy.   In Germany the 9800x3d is now 425€, with a solid mb that would be around 580-600€.  The 5800x3d is on amazon right now for 570€. like what the f is going on",AMD,2026-02-07 03:32:45,1
AMD,o2iw9uj,"Yeah, IMO the margin doesn't justify the gains, just like most modern TDP-boosted chips. It certainly has its place, though, and I imagine it does better than the 9800X3D when given the 9800X3D's power budget.",AMD,2026-01-30 01:50:54,18
AMD,o2klrqi,If the 9800X3D didn’t exist I’d imagine no one would be really talking about the power draw of the 9850X3D for the level of performance delivered.  What will be interesting to see is how well it undervolts. It’s such a highly binned version of this chip I would not be surprised if it can be made to run cooler than a 9800X3D if set close to 9800X3D performance.,AMD,2026-01-30 08:58:51,5
AMD,o2kcl57,"MSRP vs MSRP it's fine ($20 more). With the 9800X3D at $449 though, that's a much better value. I think with how the prices change, the value of each other will fluctuate. At most I'd pay 10% more for up to 5% performance bump. For comparison a 5090 is 3x to 4x the price of a 5080 at 50% higher performance. A 10% price increase for a halo product is nice, but not essential.   Hidden value king from this though is definitely the 7800X3D. Can sometimes be found even cheaper than a 5800X3D.",AMD,2026-01-30 07:36:13,2
AMD,o2kdq1f,waiting for zen 6 still rocking a 7500f,AMD,2026-01-30 07:46:11,2
AMD,o2k8bc2,I wouldn't pay more for it than the 9800x3d,AMD,2026-01-30 07:00:07,1
AMD,o2wu890,In Canada we have similar bundles but the pricing is insane on all parts right now.  Its a major turn off.,AMD,2026-02-01 03:48:53,1
AMD,o2j8xik,"I just built a 9800x3d over the weekend 449 at microcenter. Coming from intel the 9800 power usage is amazing. I could return and get the new 50 but I’m not worried about it.  If it was already out sure I probably would have gone 9850 but I’ll take my power draw win and enjoy it for a long time. I’m so used to seeing high temps and fighting throttles, this cpu is awesome.",AMD,2026-01-30 03:01:11,7
AMD,o2kn0qb,"Hmmm. True, I'll wait and see if there anything that comes out with more testing. But frankly stock for stock, Id pick the cheaper one right now. After a point, a better cpu doesnt net huge gains, speaking for myself at the very least",AMD,2026-01-30 09:10:14,2
AMD,o2kf2g4,Amazing dedication. Im not so patient apparently even though I have a 7700x haha,AMD,2026-01-30 07:57:57,1
AMD,o3c128h,Its crazy the price differences depending on the place your buying.,AMD,2026-02-03 12:55:05,1
AMD,o2jltyd,"don't do it unless you're playing on a 5090 at 1080 high settings, wasted power efficiency and thermals for barely any gains at 1440 high/ultra.",AMD,2026-01-30 04:17:21,3
AMD,o2jmont,Nah I don’t plan to. I’m on 3440x1440 with 7900xtx and quite happy. Looking forward to running the 9800x3d for at least 4 years.,AMD,2026-01-30 04:22:41,2
AMD,o2pcfn5,Should I return my 9850X3D and get the 9800X3D? It was a tiny price difference.,AMD,2026-01-31 00:13:32,1
AMD,o2qr3pt,Linus said the trade off isn't that bad at all,AMD,2026-01-31 05:27:45,1
AMD,o2jnne3,"Yep, and if you didn't get a lemon then you can always manually overclock it if you wanna have a 9850X3D for free, I've sometimes set my 9800X3D to 5700 MHz at 1.25v and it's rock solid stable with the same power draw and thermals than the factory overclocked 9850X3D, but I'm playing on a 4080 at 1440p ultra settings so the difference was laughable, I reverted back to PBO+100 and scalar x1, incredibly power efficient with per-core curve optimizer and gaming at 41-47c.",AMD,2026-01-30 04:28:45,3
AMD,o2pd0gu,"if you don't care about the increased power draw and thermals then no, it's not the end of the world and will just hurt it in the summer time if your cooler is mediocre, but I wonder why you decided to go with that CPU to pair with a 6950XT.",AMD,2026-01-31 00:16:40,1
AMD,o2tv4zb,Looks like you bought it lol,AMD,2026-01-31 18:16:14,1
AMD,o2qt8t8,"This is a good read: [https://www.tomshardware.com/pc-components/cpus/amd-ryzen-7-9850x3d-review/5](https://www.tomshardware.com/pc-components/cpus/amd-ryzen-7-9850x3d-review/5)  ""The Ryzen 7 9850X3D was never set up for success. From the moment it was announced and AMD revealed its internal benchmarks, it was clear that we were dealing with a CPU that offered marginal, single-digit performance increases over the Ryzen 7 9800X3D. Even when being charitable to the Ryzen 7 9850X3D, it fails to meet muster.  It’s a worse value than the Ryzen 7 9800X3D, consumes more power, and just barely claims a new top slot in our gaming rankings. The extra juice isn’t worth the squeeze here. That’s even more true when you factor in PBO, which itself can bring up the clock speed of the Ryzen 7 9800X3D by 200MHz and likely close the performance gap"" /Quote",AMD,2026-01-31 05:44:23,2
AMD,o2r0be3,Well that depends if your cooling is overkill enough to not matter. Otherwise its 10c° or more in heat,AMD,2026-01-31 06:42:49,1
AMD,o2pd5jz,I haven't changed my flair yet. I upgraded my GPU too a couple of days ago to 9070XT. My CPU cooler is the Peerless Assassin.,AMD,2026-01-31 00:17:26,1
AMD,o2tvnyn,Hahaha yes I couldn't resist. It's a freaking monster.,AMD,2026-01-31 18:18:39,1
AMD,o2pdjhj,"you won't see any difference compared to a 9800X3D, but who knows, maybe you will get a 5090 or 6090 at some point, but if you have deep pockets for a 5090 or 6090 then you should have bought / waited for a 9950X3D-2 instead.",AMD,2026-01-31 00:19:32,1
AMD,o2pdync,The 5090 is on a league of its own. It costs 4000 euros minimum here compared to the 9070XT which cost me 799. I have the money for the 5090 but I really cannot justify it in any way shape or form considering I mostly play CPU bound games.,AMD,2026-01-31 00:21:50,2
AMD,o2pecfn,"just enjoy your new CPU, it should be a noticeable difference compared to the 7700X playing those CPU-bound games at 1080p high, or 1440p with DLSS. More playing, less Reddit doom-scrolling.",AMD,2026-01-31 00:23:54,2
AMD,o3gth3x,"Reminds me of AMD'S old reference cards for RX 570, RX 580, and Vega 56 and 64",AMD,2026-02-04 03:24:17,98
AMD,o3h4igc,Anytime I see a relatively new blower card I think about my dream of building a modern trash can mac pro. Maybe one day when I've got more money to blow.,AMD,2026-02-04 04:34:03,31
AMD,o3ga0aj,Is sapphire using internet explorer or sum?,AMD,2026-02-04 01:33:32,50
AMD,o3gvwx5,Why would anyone buy this over a 9070XT?,AMD,2026-02-04 03:38:57,16
AMD,o3hd7tj,"Looks like the same cooler used on the new 9700 Pro AI. I personally enjoy blower coolers, they are not affected by case cooling and don't heat up other components. 2 Slots is also a pro.",AMD,2026-02-04 05:35:55,4
AMD,o3i2qe9,I can hear it  ![gif](giphy|1MbbcMkVoCpayQHUbW|downsized),AMD,2026-02-04 09:19:33,4
AMD,o3gsnv6,New old new old new stock now coming to a Newegg near you,AMD,2026-02-04 03:19:29,4
AMD,o3id1vy,"Noise and cooling performance aside, I just think blower cards are really neat and look much better than fan style.",AMD,2026-02-04 10:55:10,2
AMD,o3jpavu,Nice choice for local AI,AMD,2026-02-04 15:44:33,3
AMD,o3iwsfv,What year is this,AMD,2026-02-04 13:17:57,1
AMD,o3jaie8,Man I love the power connectors on the end like that. I wish more consumer cards had it like that but they don’t do that too often due to people using consumer cards in servers.,AMD,2026-02-04 14:32:00,1
AMD,o3kjzej,"man i got one of these for like 550 at microcenter a few months ago, crazy that this is a deal now.",AMD,2026-02-04 18:05:28,1
AMD,o3kvvfq,Don't buy amd has no plans of adding fsr 4 to rdna 3,AMD,2026-02-04 18:58:48,1
AMD,o42fgyz,Anything but SFF from AMD,AMD,2026-02-07 12:00:25,1
AMD,o3i8d8p,Terrible deal.,AMD,2026-02-04 10:12:51,1
AMD,o3h1afb,LOLWTF,AMD,2026-02-04 04:12:39,0
AMD,o3hs1gz,Gross. I bought my 7900xtx pulse for 850......,AMD,2026-02-04 07:39:43,0
AMD,o3i4h5u,I have nightmares after hd6990. PC start was like a plane take off. Blowers suck.,AMD,2026-02-04 09:36:17,0
AMD,o3hfn6h,The ones that sound like your computer is about to take off? Ahh I miss my Vega 64,AMD,2026-02-04 05:54:35,50
AMD,o3hyt56,I was thinking 5870.,AMD,2026-02-04 08:42:26,2
AMD,o3hw6r8,"iirc there was no reference model for the RX 500 series, since the RX 480 reference model (the one you probably mean) came with a single 6 pin for 150w and burned out the pcie connector on lower end motherboards by overpulling power from it, so they gave up on making 500 series reference cards.",AMD,2026-02-04 08:17:32,0
AMD,o3ge2dp,why?,AMD,2026-02-04 01:56:21,10
AMD,o3gyhzg,I'd imagine the extra vram if using it for workstation use.,AMD,2026-02-04 03:54:54,16
AMD,o3j9sd1,4 extra gb and 30% higher bandwidth. Plus strong courageous blower vibes.,AMD,2026-02-04 14:28:17,9
AMD,o3jvess,"For gaming, I wouldn't.  I have a 7900xt, but if given the choice today I'd probably get the 9070xt.  I still don't play anything with raytracing, but I'm sure in a year or two that'll change.",AMD,2026-02-04 16:12:40,1
AMD,o3ita4k,"That's the one! I used to undervolt my Vega 56 and crank the fan speed way up. Really loud, but the temperatures were quite good",AMD,2026-02-04 12:56:41,10
AMD,o3izd44,Man my old MSI RX580 armoured card was like that. I remember playing Battlefield 1 (I think it was) and hearing the GPU sound like a jet when the load was getting heavy and it was hitting higher temps.,AMD,2026-02-04 13:32:32,6
AMD,o3lkoca,"I went from a reference Vega 64 to a whisper-quiet Nitro+ Vega 64, to yet another blower-style RX 5700 XT. I hope I never end up with a blower-style card again. 🤣",AMD,2026-02-04 20:55:58,5
AMD,o3m6crv,I still have my sapphire vega 64 limited in a work computer. The one with all brushed aluminum blower shroud. For some reason the more I look at it the more the aesthetic grows on me. Was simple and elegant with its silver and red tones. Now in days cards are way too fat/long and kind of ugly with the rainbow throwup lighting.,AMD,2026-02-04 22:41:13,2
AMD,o3itcem,"It didn’t actually melt any PCIe slots. It actually drew more power through the 6-pin PCIe power instead, but it was technically above the specification limit, which became a big deal. Somewhat ironically, the 960 (one gen older) was even worse about this, but that never became news.   In practice, those old power connectors were overdesigned for the task, so you could draw a lot more power through them. That’s not the case for 12VHPWR connector…",AMD,2026-02-04 12:57:05,9
AMD,o3gem72,"Probably because they forgot that server/workstation cards are typically blower, and not just older gpus",AMD,2026-02-04 01:59:29,33
AMD,o3jc9oj,I suppose if the main bottleneck for you is VRAM this is the cheapest option. Was the 7000 series actually popular with the workstation/productivity crowd?,AMD,2026-02-04 14:41:05,4
AMD,o3pja0f,"Opposite of you, I will never go back to 3 fan cards lol. Blower everyday for me.",AMD,2026-02-05 12:56:19,1
AMD,o3j7fq9,"I have an old Seasonic power supply. The PCI-E x8 cables it shipped with split from the supply to the card.  After almost 18 months with my 6800, I started getting reboots under load. Figured my 15 year old power supply had kicked the bucket. Nope.  Melted the PCI-E x8 harness. Went to buy a new one after installing the new supply I bought and in big letters on the suppliers site it says not to use this harness if feeding a single high power GPU and to use dedicated 1:1 harnesses, that did not come with my supply.",AMD,2026-02-04 14:16:04,5
AMD,o3mnwlt,"They overdrew the PCIe slots.  This was a real problem that caused widespread stability issues.  (overdrawing the PCIe connectors happened on plenty of higher-end models, but not Polaris, the thing didn't draw that much power in the first place!)",AMD,2026-02-05 00:16:46,2
AMD,o3vut7x,> those old power connectors were overdesigned for the task  Actually they weren't that overdesigned. Most consumer PSUs just used thicker cables and better pins than the spec mandated.,AMD,2026-02-06 11:30:14,1
AMD,o3iw0ea,"I remember hearing a lot about them frying the slots back in the day but i actually couldn't find anything online about it right now. I'm pretty sure the vbios update to pull more from the 6pins came after launch. I found this article from toms hardware where they say the card pulled 6.74a from the slot. 12 x 6.74 = 80.9 watts, which is above the 75w spec.  https://www.tomshardware.com/reviews/amd-radeon-rx-480-power-measurements,4622-2.html",AMD,2026-02-04 13:13:23,-2
AMD,o3gg9x0,Manufacturers didn't stop putting blowers on consumer cards because they were bad for desktops. They stopped putting them in consumer cards because datacenters were buying consumer cards instead of server cards (that often cost several times more).,AMD,2026-02-04 02:08:54,42
AMD,o3gqs6n,I was more referring to them releasing an rdna3 card in 2026,AMD,2026-02-04 03:08:34,12
AMD,o3jda33,The 7900s were the only consumer cards supported by ROCm for a while so yes. The 7900 xtx is a viable contender to the 3090 in terms of value. XTX has 1 gbps memory bandwidth so its much faster than a 9070 xt. The 7900 xt was a good value but the 24 gb class has more model support and so on so it wasnt as popular.,AMD,2026-02-04 14:46:16,3
AMD,o3p8458,B60 is 24GB for 700€,AMD,2026-02-05 11:36:04,2
AMD,o3ytoh9,fan of jet engines?,AMD,2026-02-06 20:46:21,1
AMD,o3j7yp4,"That is bad, but not really a GPU problem per se. If the manufacturer puts two connectors on a single pigtail, they better spec the wire dimension to support that.",AMD,2026-02-04 14:18:51,5
AMD,o3vxkxt,"Im talking about the connector itself. The 6-pin PCIe power connector has 2 pairs that provide power, 1 sense pin and one empty (which is essentially always connected anyway). This means that there is 75W power over 2 pairs, or 37,5W per pair. At 12V, the connector can handle 12*8=96 W of power per pair (according to Molex, who makes the connectors). The PCIe spec is little more than a third of that.   PCIe put in a LOT of safety margin back in those days, and they have gradually reduced it. The 8-pin draws 150W over 3 pairs or 50W per pair, with the same 96W spec from Molex. Still almost 100% safety margin. The 12VHPWR has almost no safety margin. Nvidia plays fast and loose with safety, it seems.",AMD,2026-02-06 11:51:51,3
AMD,o3jae5q,"The total power draw from the slot in the spec is max allowed 75W, +8% for tolerance which is 81W. The draw in that test by Tom's is 6,74*12=80.88W. If you think that that is a coincidence, I have a bridge to sell you. AMD pushed the power draw to the absolute max allowed by the spec, but they used all of the power from the 12V rail which is not what the standard specifies - the standard says 66W+8% from the 12V rail, and the rest from 5V and 3.3V. The slot isn't going to melt because the total power draw isn't over the total spec. This is not good engineering (good engineering would be to switch to an 8-pin) but it isn't going to melt the slot.  Note, by the way, that the PCIe standard specified at the time that no card can EVER use more than 300W total, yet both AMD and Nvidia went gleefully past this number. I don't know if the spec has been changed now, but I would hope so given the 5090 and similar card.",AMD,2026-02-04 14:31:24,4
AMD,o3gp43a,That would certainly explain why no modern consumer cards have a rear exhaust anymore. They use to be a thing with some non-blower cards but I see none these days.,AMD,2026-02-04 02:58:58,13
AMD,o3h5097,"The only reason blowers lasted for as long as they did is because of SLI/CF.  I'm not sure if the kids here remember, but there's a reason most variants of FX5950 and 7800GTX had 'regular' axial fans, not blowers.   SLI just wasn't a (big) thing with Rankine and Curie.   After all, when you've two hot cards propped up together, you've no choice but to use a blower to exhaust the hot air outside.   Most people hated the noise, or at least I did, but everyone suffered because of the few SLI nutcases who only cared about FPS and bragging rights, even if it came at the cost of horrendous frame pacing and screen tearing issues, not to mention the headache of setting up SLI.   ...  Unrelated but I was so fed up with the noise of my Fermi back then, I deshrouded it and cooled it with a 120x38mm Delta fan that peaked at 4,000 RPM.   Even at 800ish RPM (PWM'd), that chonky Delta dropped temperatures by almost 20 degrees (70c > 50c) and was whisper quiet in comparison.   So yeah, fuck blowers... unless you need 'em!",AMD,2026-02-04 04:37:24,6
AMD,o3gggh0,More like heat and noise reasons,AMD,2026-02-04 02:09:56,5
AMD,o3gtjbu,"Excess stock, and still powerful hardware",AMD,2026-02-04 03:24:39,4
AMD,o3yujcj,"Ironically, my blower is less annoying (lower pitch noice) than my 3 fan card (both are 3080).  My blower card i in my PC in a rack in an other room anyway.",AMD,2026-02-06 20:50:40,2
AMD,o3mo2hs,They do - if the PSU itself is in spec.,AMD,2026-02-05 00:17:40,3
AMD,o3w7a0i,Yeah that 12VHPWR connect sucks. I wish Nvidia would focus on making their graphics cards more power efficient. Maybe then they wouldn't melt the connectors,AMD,2026-02-06 12:58:32,2
AMD,o3wwlm9,"> the connector can handle 12*8=96 W of power per pair (according to Molex, who makes the connectors). The PCIe spec is little more than a third of that.   The original minimum spec had 6,5A per pin for the 6 pin iirc or might even have been lower. Not sure if it has been updated since then, but it was not 8A when originally launched.   The pins themselves comes in different specifications. So you can't look at the pin spec itself, because multiple different pins can be used for the same connector.",AMD,2026-02-06 15:14:27,1
AMD,o3gtoil,Its quite annoying imo. I've been looking for some blower cards for some of my sff pcs and servers,AMD,2026-02-04 03:25:29,7
AMD,o3idx99,"Some new budget GPUs still use rear exhausts, despite having two regular fans. Like MSI RTX 5060 Ventus 2X or ASUS Dual RTX5060. The heatsink fins going horizontal instead of vertical.  My older MSI RTX 3060 has that too.",AMD,2026-02-04 11:02:48,1
AMD,o3iggi4,"My asus prime 9070 XT has rear exhaust, many still have. Only a few models, like the 6/7000 series reference cards are blocked there",AMD,2026-02-04 11:24:24,1
AMD,o3hz801,I remember setting up two 3870x2 in crossfire for benchmarks.,AMD,2026-02-04 08:46:19,4
AMD,o3ilt36,> Rankine and Curie    Wait they always used scientists? TIL,AMD,2026-02-04 12:05:59,3
AMD,o3j7udt,In games that supported it I'd get damn near 100% scaling with my RX480's.  Upgraded to them from R9 280x's.,AMD,2026-02-04 14:18:13,2
AMD,o3p7im7,I have a blower 1080 in my rig (OEM Dell?/HP?). I want new cards with a blower in my rig. I want the air out of my case (Define 7 XL) not inside.  I also don't want 400W GPUs.,AMD,2026-02-05 11:31:05,1
AMD,o3gl5us,"They don't care about the noise, and they can also stack them together so blower is the only way to do that.",AMD,2026-02-04 02:36:40,8
AMD,o3gu4rp,Yet amd has abandoned that hardware,AMD,2026-02-04 03:28:10,-5
AMD,o3zyq3x,"It’s not per pin, it is per pair. The 6-pin uses two pairs of 12V. The spec for the CONNECTOR is 8A from the manufacturer, so 2x8Ax12V for each 6-pin. The 75W that PCIe specifies means approximately 3A per pair, so even if you say that Molex used to say 6,5A per pair (and I have never seen that before), the PCIe spec is still way below that.",AMD,2026-02-07 00:25:01,1
AMD,o3gulur,"It seems like they are predominantly in the workstation and server channels. Like for instance, every single one of the R9700 cards have a blower or no fan at all.",AMD,2026-02-04 03:31:00,5
AMD,o3gu9wi,There is no abandonment. Still getting normal driver updates (same branch as rdna4 in fact) and still being produced. Far from end of life,AMD,2026-02-04 03:29:01,1
AMD,o3gv8ax,"Yeah its in the driver branch where amd promised new features, but then later state that RDNA3 wont get any, RDNA3 without FSR4 is basically DOA when sold alongside RDNA4",AMD,2026-02-04 03:34:47,3
AMD,o3hdb96,"what are you talking about, the rx 7900xt is not being produced anymore, its literally at end of life  [https://www.techpowerup.com/gpu-specs/radeon-rx-7900-xt.c3912](https://www.techpowerup.com/gpu-specs/radeon-rx-7900-xt.c3912)",AMD,2026-02-04 05:36:40,2
AMD,o3hn3kj,It's not getting FSR4 because it has no AI accelerators. There won't be any performance benefit.,AMD,2026-02-04 06:56:20,1
AMD,o3gvjcs,"Amd never promised more features for rdna3, and upscalers are not required for everything. Xess is very passable and more than sufficient",AMD,2026-02-04 03:36:39,-1
AMD,o3hjlxj,"End of production does not always mean end of life  Until there are no more driver updates are coming for rdna3, it is not EOL",AMD,2026-02-04 06:26:41,1
AMD,o3hnn1g,"https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html  This clearly states that rdna3 has dedicated ai accelerators.  And if you look ok any techpowerup page on rdna3 they very clearly have matrix cores.  Have you been living under a rock, the leaked version has been tested many times and delivers a great performance boost?",AMD,2026-02-04 07:01:00,0
AMD,o3gza5u,"They did promise more features for rdna3, its in a literal official statement on their website.  [continued support for every radeon gamer](https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html)",AMD,2026-02-04 03:59:49,4
AMD,o3hdjq2,"huh? what ur saying is false  they stated they were working on new features for both rdna3 and 4 when they split the drivers, they very much promised new features for rdna3  [https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html](https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html)",AMD,2026-02-04 05:38:29,2
AMD,o3gw9v0,"Its not about promises, nvidia never promised dlss4 to rtx 20 when it came out, its about keeping up when your competitors are supporting multiple generations.  And its about supporting older hardware when we know its capable. If intel can literally provide xess for these cards, amd can provide fsr4  Do you expect RDNA4 to not get FSR5 when that comes out?",AMD,2026-02-04 03:41:09,-1
AMD,o3hk3cm,"im talking about production, if you look at the link its literally stated as end of life in terms of production, ur ignoring ur initial comment where you said its still being produced.   and again amd did promise new features for rdna3 if you look at the link",AMD,2026-02-04 06:30:47,2
AMD,o3czpuh,Cool to see cool designs back to GPUs.   Too bad this one will probably use the 12pin.,AMD,2026-02-03 15:58:25,23
AMD,o3c970c,Hopefully the connector isnt as hot as a desert,AMD,2026-02-03 13:43:11,57
AMD,o3c95zj,"We need the motherboard X870EA Nitro+ Phantom Link to enjoy the new feature associated to the graphic card to check the thermal statut of the connector, too bad for nitro+ users...",AMD,2026-02-03 13:43:02,6
AMD,o3cgmhk,Still the same bs connector with their bad support? No thanks,AMD,2026-02-03 14:23:48,20
AMD,o3dpdz4,The should remove the crappy connector.,AMD,2026-02-03 17:57:10,2
AMD,o3ekt9i,If it still has the 12 pin hard pass. Grabbed a power color because it uses 8-pin connectors.,AMD,2026-02-03 20:21:49,2
AMD,o3gcgdt,I’ll never buy a card with the housefire connector.,AMD,2026-02-04 01:47:17,2
AMD,o3cwvyf,I just hope AMD beats earnings today lol,AMD,2026-02-03 15:45:03,1
AMD,o3d48zf,What I need is white nitro+,AMD,2026-02-03 16:19:52,1
AMD,o3mf7k1,Desert as in... Flame Generation cable ready?,AMD,2026-02-04 23:28:40,1
AMD,o3wqxp1,The best looking 9070 xt card now got a golden colorway . 👍,AMD,2026-02-06 14:45:53,1
AMD,o3zoorv,With a crimson hot burning 12V power socket.,AMD,2026-02-06 23:26:56,1
AMD,o3cdypb,Only way they can sell this game is by attaching it to everything else they can think of.  Massive red flag,AMD,2026-02-03 14:09:28,1
AMD,o3skxxh,So happy with my TUF 9070XT for this exact reason,AMD,2026-02-05 21:50:54,3
AMD,o3ckwk0,"It's the Nitro+, it'll have the melty connector",AMD,2026-02-03 14:46:21,30
AMD,o3e045c,"As someone who just bought the regular Nitro+ 9070 XT, these comments aren't very confidence inspiring lol",AMD,2026-02-03 18:45:19,4
AMD,o3cutct,"I dunno dude, I think it might be a hit. Time will tell.",AMD,2026-02-03 15:35:11,3
AMD,o3egikj,Don't use the 3-1 cable adapter. The only burnt nitro+ cards I have seen have used the 3-1 cable adapter.,AMD,2026-02-03 20:01:27,11
AMD,o3ebpau,"You will probably be fine, the chances of the connector melting / burning are lower on lower power draw cards  But I will be honest. I hope for you that you won't need the support. Their support at least in Germany is abysmal to a point where they insulted me, and required me to test the GPU in another PC, because who doesn't have a spare PC that can handle a 7900 XTX. So after a lot of fighting I was able to send it back, Then I just bought a 9070 XT and plan on selling the 7900 XTX",AMD,2026-02-03 19:39:01,4
AMD,o3ghgx7,"Also you can give undervolt+underpower a try. 9070XT,-80mV,-30% Power(220W),Vram fast timing 2700Mhz, still performs almost the same as stock and cut the power draw by 100W. At 220W I don't believe this cable can burn even if we try to.",AMD,2026-02-04 02:15:38,2
AMD,o3flsgh,"i got lucky i guess. my power supply already had a 12v2x6 cable, so i simply plugged that in rather than any adaptor. never had an issue with my card. runs like a beast",AMD,2026-02-03 23:20:02,1
AMD,o3cv1qg,"It's pa.  This is what they do.  They make a big show and then when you actually play what they implement, it's shit.",AMD,2026-02-03 15:36:18,2
AMD,o3et5tm,"That’s good to know. I have a PSU that has the 12v connector, so hopefully no issues!",AMD,2026-02-03 21:00:50,3
AMD,o3hihao,I’m definitely going to tinker with undervolting. Thanks for the tips,AMD,2026-02-04 06:17:22,1
AMD,o3d87oq,"Maybe it's good, maybe it's shit. Either way, I don't think that simply having a marketing budget is a red flag.",AMD,2026-02-03 16:38:13,5
AMD,o3ex2u0,"Yeah doing the exact same thing. Had it for 10 months, zero issues.",AMD,2026-02-03 21:18:52,2
AMD,o3ze3rb,And you just know they're going to pair it with Zen 2,AMD,2026-02-06 22:28:48,60
AMD,o40m2f6,"Zen 5, or even Zen 4 with RDNA 4 is gonna be a killer mobile chip. But nooooo. gotta use the 5 year old architecture and let Intel catch up.  AMD loves to shoot themselves in the foot.",AMD,2026-02-07 02:47:07,22
AMD,o40f1ih,"hello RDNA 3.7, nice to meet you.",AMD,2026-02-07 02:03:24,21
AMD,o4004wj,">Over the GFX115x targets, GFX1170 adds the SALUFloatInsts and DPPSrc1SGPR features. But it doesn't have all of the other GFX12 ISA features found with the complete RDNA4 GPUs.  >This commit introduces that initial AMD GFX1170 target to LLVM Git. It will be interesting to see what comes of this ""RDNA 4m"" graphics for APUs/SoCs.  >Update: There is also now a merge request adding new FP8/BF8 conversion instructions for the GFX1170 target.  I guess they market it as RDNA 4m because it's using RDNA 3.5 but with updated ISA?",AMD,2026-02-07 00:33:08,9
AMD,o41e61u,Probably Medusa Point if I had to guess,AMD,2026-02-07 06:08:26,3
AMD,o419bwg,"Wait, how do we know that this isn't just referencing a new set of RDNA4 laptop dGPUs?",AMD,2026-02-07 05:28:40,2
AMD,o40ncfl,Let's hope Gorgon point and Gorgon halo isn't what the leaks say and include RDNA 3.5 igpus but actually use rdna4 that would be pretty sweet,AMD,2026-02-07 02:55:02,2
AMD,o44b7dx,Sigh... AMD needs to toss everything 2x generations old away... Nobody likes current gen CPU sprinkled with moldy Radeon leftovers.   Whoever is hired at AMD that keeps doing this generation after generation should get fired. I hear burger King is hiring.,AMD,2026-02-07 18:17:21,1
AMD,o47qngq,"Is this what Zen 6's IOD is going to use? Timing lines up. It's newer than RDNA3.5 (gfx1150). Also gives AMD a way to port (or simply convert FP8) FSR4 to ""RDNA4m"" but not RDNA3/3.5. 🫠   (Allowing FSR4 on gfx1170, but not anything lower)  I'll die of cringy laughter if this happens.     &nbsp;   - Now I'm really curious to see Samsung's RDNA4 IP in Exynos 2600/Xclipse 960. Like, does it have all of RDNA4's features? Granted, this is being fabbed on Samsung 2nm, so CUs will be a good chunk smaller than N3P or N4P, if density improved and is competitive to TSMC N2. Mobile designs are heavily constrained by power/thermals, so wonder what design choices were made.",AMD,2026-02-08 06:58:13,1
AMD,o3znwgn,Typical AMD move.  Be hold the all new Ryzen 9000 mobile series! Ryzen 9525U (Zen 2 + RDNA 3),AMD,2026-02-06 23:22:26,34
AMD,o3zn85q,AMD never misses a chance to miss!  One thing RDNA4 is good at is scaling down. I could see a RDNA4 APU paired with fast (soldered) RAM as an amazing little low-power system. Plus you get FSR4 which even further sweetens the pot. If they could give us this with a decent modern-ish CPU core I think it'd be a big win in a market that feels almost abandoned at this point. It's either Strix Halo for 2500 dollars or a refresh of a 5yr old SoC.,AMD,2026-02-06 23:18:31,0
AMD,o41e02j,because it reads APU in the file,AMD,2026-02-07 06:07:00,10
AMD,o40ri96,"That would be great ""fun"" if they did that since in addition to the Ryzen AI 300/400 stuff they also have the Ryzen 200 series (like the Ryzen 7 250, Zen4 and RDNA3), Ryzen 100 series (such as the Ryzen 5 150, Zen3+ and RDNA2) and the Ryzen 10 series (yep, that's what they call the series, for things like the Ryzen 5 40 which has Zen2 and RDNA2), the names already make no damn sense anyway (for example that Ryzen 5 40 only has 4 cores, so does the Ryzen AI 5 330 and 430, despite the Ryzen 5 naming) so I wonder how they'd shove a Zen2 and RDNA3 combo in there if they do actually decide to make that lol.",AMD,2026-02-07 03:21:35,11
AMD,o40glyi,2CU,AMD,2026-02-07 02:13:01,4
AMD,o3zpaf3,> One thing RDNA4 is good at is scaling down.  Are there any examples of RDNA4 at low power?,AMD,2026-02-06 23:30:24,18
AMD,o41qpqm,"Looking at Navi 44 and 48, it doesn't look like RDNA4 scales down all that well. If anything, it's scaling up better than Blackwell.",AMD,2026-02-07 08:03:00,2
AMD,o41fhy7,"Ah, gotcha. Dumb comment I made there. Tis a shame AMD doesn't want to (or maybe actually can't) bring real RDNA4 to APUs, and so we're stuck with this instead.",AMD,2026-02-07 06:19:54,2
AMD,o40w0wg,What a bunch of nonsense models. I wonder why they are so fond of zen 2 after so many years.,AMD,2026-02-07 03:51:35,9
AMD,o3zqnxb,"Not SUPER low power, but with my 9070 xt i can set the power limit to 213W (-30%) down from 304W, and only lose 5-10% performance",AMD,2026-02-06 23:38:27,4
AMD,o3zy30c,"Just undervolting. It tends to keep a lot of the performance while really tanking the power needed. Look at even 9060 undervolts. I suspect a 16CU part with decent RAM would do quite well. Perhaps not though, maybe most of the winds over 3.5 aren't there when scaled to an APU with pedestrian RAM vs GDDR.",AMD,2026-02-07 00:21:18,2
AMD,o41jfnl,"RDNA4 seems to be bigger than 3.5, so makes some sense AMD wouldn't want to use it. RDNA5 will use chiplets to allow to share them across segments, so it is less of an issue.",AMD,2026-02-07 06:55:07,4
AMD,o46pwk0,probably incredibly high yield on mature manufacturing techniques to maximize use of materials,AMD,2026-02-08 02:26:25,4
AMD,o41r5lj,"That's very easy to explain: AMD pushed the 9070XT beyond what's reasonable in an attempt to compete with the 5070 Ti.  By scaling down, I would expect the architecture to have poor area scaling, meaning a doubling of functional units would increase the chip size by more than 2x and give a minor performance improvement. Instead, it's the opposite, and RDNA4 seems to scale better to larger sizes than Blackwell.",AMD,2026-02-07 08:07:12,6
AMD,o419xp2,"only +23% in Linux feels like bad drivers, when the B390's getting 2x performance on Windows",Intel,2026-02-07 05:33:34,21
AMD,o41k2xt,Good. AMD has been rehashing same crap lately. This is why competition matters,Intel,2026-02-07 07:01:00,15
AMD,o426cd8,AMD 14nm moment,Intel,2026-02-07 10:36:23,7
AMD,o42ncwr,"Nice, I'm tired of AMD rebranding old APUs and charging for them as if they were brand new.",Intel,2026-02-07 13:01:43,3
AMD,o420dq7,What about 8060s? Though?  Who fucking cares about 890M? It’s made for business laptops…,Intel,2026-02-07 09:37:29,0
AMD,o3zmils,I want Intel to win but sadly they only win against the HX370 which is a generation behind the 395+. So intel is still behind. They better price it well below the HX370 to regain market share. AMD lost the plot in terms of pricing and HX370 handhelds are well over 1300. If they can partner with MSI and produce a handheld for under 1000 they have a winner. Anything above that is DOA.,Intel,2026-02-06 23:14:29,-24
AMD,o41ay0s,considering sr-iov and passthrough on linux are completely busted in the xe drivers for the B50 and that has been out for months it wouldn't surprise me if these are rough as well.,Intel,2026-02-07 05:41:39,5
AMD,o421h3i,RDNA 3.5++++,Intel,2026-02-07 09:48:29,13
AMD,o45uzpk,"Yes since they have this new loser as the head of client they don't build anything new. They just recycle old shit with new marketing name. I am sure he will kill the business in 2 years. Saving grace is AMD is untouchable right now for desktop, and Intel sadly is full of useless VPs that spend their days in meetings.",Intel,2026-02-07 23:14:29,1
AMD,o42u6kw,"8060s still comfortably beats out the B390, but it's designed for a different power class and draws a lot more power.",Intel,2026-02-07 13:44:59,6
AMD,o45a3to,"The 890M is in the Z2 Extreme. It's not just for laptops, it's their prime handheld GPU chip.  The 8060S only starts beating the B390 around 33 W, making it unfit for a handheld device where battery life is very important",Intel,2026-02-07 21:19:03,2
AMD,o3zrsvz,the 395+ is a different segment of product imo. They start at $2200+ while the intel chips start around \~$1200 so far. The battery life also isn't even close.,Intel,2026-02-06 23:45:02,37
AMD,o3zrwwh,Panther lake and 395+ can’t be compared the 395+ is a much bigger die and with double the bus they’re completely different tiers,Intel,2026-02-06 23:45:41,24
AMD,o40cvka,"Strix Point and Strix Halo are the same generation, one is just much bigger and more expensive and for an entirely different performance tier. You're coming into a 5050 vs B580 comparison and saying that a 5090 is faster than the B580. Duh, of course it is.",Intel,2026-02-07 01:50:11,11
AMD,o40lyhs,The price of panther lake is gunna be expensive too,Intel,2026-02-07 02:46:26,-3
AMD,o424dp4,"Exactly. I was really disappointed with their igpu tactics. Rehash, rehash, rehash.   I do think we need DDR6 for next major igpu scaling, as memory bandwidth is a problem too.",Intel,2026-02-07 10:17:09,5
AMD,o40emvu,Which laptop is releasing at $1200 with the B390,Intel,2026-02-07 02:00:58,2
AMD,o45uscn,"Yes agreed, but if the machines offering them are in the same ballpark then it is a failure on intel. A machine sporting the B390 should be cheaper than an equivalent machine with the HX370. We need handhelds with B390 below the 1000 mark for example.",Intel,2026-02-07 23:13:16,1
AMD,o40fa3x,"[https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780](https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780)     there's this $1400 one. there's also only like 5 available series 3 laptops. if this 16"" is starting at this price, I'd expect the 14"" laptops to be a little cheaper.",Intel,2026-02-07 02:04:52,11
AMD,o41rwbo,This seems like a significant outlier from what I can see and it's unavailable so unclear if it's an error.    I also can't see this on the hp website.   Guess we'll have to wait and see. From what I've researched the x /h processors are significantly more than the base,Intel,2026-02-07 08:14:15,2
AMD,o416ch9,it's unavailable .....possibly it's a subsidized product.....,Intel,2026-02-07 05:05:43,-3
AMD,o428rdn,"yep, this  basically all the new B390 laptops lists around a same ballpark price as the 395+ ones  just over 2k USD",Intel,2026-02-07 10:59:32,2
AMD,o41au1b,"it ran out of stock a couple days ago. There's a few open box ones you can still pick up though. most of the panther lake laptops just aren't released yet. They might go up in price due to ram, but everything else will as well unfortunately.",Intel,2026-02-07 05:40:45,4
AMD,o45ul90,"That is my point. Not worth the price if it barely beats the HX370. And just the fact we need to have this conversation shows how Intel Marketing miserably failed. If the B390 devices are cheaper, please make sure you plaster it everywhere and communicate it well. Even the people interested in it like me do not know or believe it.   Again if anyone at intel reads this, you better price this lower than the HX370 or you have no chance. I do not like AMD but sadly they have the best overpriced products right now.",Intel,2026-02-07 23:12:08,1
AMD,o45vxqk,this sounds about right.  Intel just said that the margins are PTL are below corporate average. which means yields on 18a are abysmal.  Expect prices to be high and supply to be low until yields improve (which the CFO head said wont be till 2027. But they should gradually improve through 2026.  AMD and intel are in a race. Intel need to improve 18a yields. AMD needs to get zen6 out. Whoever gets there first will be the winner. Until then it kinda looks like the laptop market is just a continuation of what it was before. AMD gets a slight pickup from the 400 series. Intel gets a slight bump from PTL.,Intel,2026-02-07 23:20:02,1
AMD,o431u16,well why don't they restock it ??,Intel,2026-02-07 14:29:14,1
AMD,o2chc3i,What’s insane is the IPC of the Darkmont E cores in Pantherlake seems to be better than AMD’s zen5 P cores.,Intel,2026-01-29 03:26:27,55
AMD,o2k7huk,"Now they just need to put this in a desktop package, slap some BLLC (Big Last Level Cache) on that mfer and we're talking. And by the time they've done that, Zen 6 is here. So we'll see. The end of the year will be interesting.",Intel,2026-01-30 06:53:26,11
AMD,o2gkb7q,"I'm very hopeful for Intel on this gen and Nova Lake I'm very excited about.    Intel in benchmarks though was always hit or miss due to cache issues and lack of avx 512. Canned benchmarks they did good on, but a lot of apps were starting to get avx512 optimized more and they would lose HARD there. Not only that but there were certain games where it was just catastrophic, and not due to windows e core issues.    Look at Homeworld 3 CPU benchmarks (I know it sucks).",Intel,2026-01-29 18:48:07,12
AMD,o2fk8dr,Give me barlett lake for lga 1700. Its all I care about.,Intel,2026-01-29 16:06:35,11
AMD,o2xo1k6,"Great to see good competition, we need to wait for Zen 6 for more equal comparison (18A vs 2nm cores)",Intel,2026-02-01 07:37:01,1
AMD,o2e7a5q,"AMD doesn't really have P cores  Great job Intel though, this is what we like to see",Intel,2026-01-29 11:40:54,24
AMD,o2dxto2,It's Mobile Zen 5 Cores that have lot less cache,Intel,2026-01-29 10:21:41,9
AMD,o2egfae,"Why? AMD still uses 5/4nm, that is a tech from 2020.  They can move to 3nm, and their performance will be in pair. They just did it to make as much profit as they can.",Intel,2026-01-29 12:44:46,-7
AMD,o2ksewg,Yeah. We gonna have a 28 core with bllc cache with novalake. They also making a 28 core with no bllc cache.,Intel,2026-01-30 09:59:36,2
AMD,o2fwqez,"Intel has this golden opportunity to just rule the new DDR4 world while AMD plays pretend like people are still buying DDR5, but instead Intel wants to play pretend, too.  It's sad how stupid Intel's strategy people are.",Intel,2026-01-29 17:02:11,6
AMD,o2llp44,"This. As a early adopter of the 12900K, all the shait we had to go through with Intel/W11 software just to make it work and not to mention 13/14th gen issues.. I mean sure, now 12900K is superb, but still. We deserve Bartlett Lake K.  Would love to OC that juicy chunk of a CPU.",Intel,2026-01-30 13:33:01,1
AMD,o2h0i6b,still much bigger than darkmont E core,Intel,2026-01-29 20:04:06,13
AMD,o2e5xiz,18mb vs 16mb so a pretty fair comparison.,Intel,2026-01-29 11:30:21,18
AMD,o2h9ta3,The e-cores have no micro-op cache.,Intel,2026-01-29 20:48:58,0
AMD,o2em3lb,"Except it's not as easy as cramming more transistors into the same space... You can increase perf/W that way, but not really an IPC, that's all about architecture.",Intel,2026-01-29 13:19:10,28
AMD,o2esiez,"> They can move to 3nm, and their performance will be in pair.   A pure node shrink doesn't give you any IPC benefits. IPC is dictated by the underlying architecture. It might help their boost clocks and energy efficiency, but it doesn't improve IPC.",Intel,2026-01-29 13:53:52,19
AMD,o2g16jj,Except we are running out of ddr4 stockpile and the prices are starting to reach parity unfortunately.,Intel,2026-01-29 17:22:20,17
AMD,o2jj1t8,"TF? AMD literally just released 5900XT which is basically a renamed 5950X not too long ago and still selling. Also, don't bet on DDR4 as those prices are hiking too now.",Intel,2026-01-30 04:00:00,4
AMD,o2egwrm,He's saying mobile zen 5 has less ipc than desktop zen 5,Intel,2026-01-29 12:47:54,13
AMD,o2idkwe,Panther Lake is also Mobile. So it’s a fair comparison.,Intel,2026-01-30 00:09:02,9
AMD,o2kcbrx,Yes I agree but does ipc differ between intle mobile and desktop with the same core?,Intel,2026-01-30 07:33:55,0
AMD,o2ken0q,Spec is dependent on Cache size and Memory Latency outside the core IPC,Intel,2026-01-30 07:54:12,6
AMD,o2kep89,Yep,Intel,2026-01-30 07:54:45,3
AMD,o1wy1s0,"Let's just wait for products to hit, maybe Intel is going to hit it out of the park, but they have a long way to go before I will trust their marketing slides.   AMD is no better in that regard, I'm not just shitting on Intel.   In all honesty, I'm pretty happy Intel is doing better on the iGPU side, cause it was so bad for so very long.   My laptop is currently an i7-1185g7, not their best showing, but was a step in the right direction on the iGPU side.   Not anything I would have bought brand new, but being a refurbisher in my spare time has perks.   My laptop is always 3-6 years old.",Intel,2026-01-26 23:15:08,5
AMD,o1xyg0n,Intel's been down this road before. It's not worth it. 14900HX is the only reminder you need. How fast does it need to be before we start talking about just putting a GPU in? If I'm getting a laptop why would I get a power hog collapsing star? It's called mITX.,Intel,2026-01-27 02:26:55,13
AMD,o1yqu4g,Spinning a delay of building a competitor into this? Good try,Intel,2026-01-27 05:18:16,2
AMD,o23ohno,"To completely compete with Strix Halo (really just the graphics side) would require intel to increase the die size just like AMD did. In practice, even in gaming, it isn't worth it. Yes, you get between 20-30% more performance with Strix Halo up to 65W, and then pushing 80w+ Strix Halo opens up further to closer to 40-50% better performance. Thing is, it just doesn't make sense to create a large APU with those power requirements for full performance in today's market... especially at the prices of Strix Halo products. You can just get a solution with a dGPU at those prices.  I just wish Intel would push for Linux compatibility. Panther Lake is being held back by the suck that is current Windows 11 imo.",Intel,2026-01-27 22:14:03,1
AMD,o2mvyy4,"Hmm, that's a shame.  I really like Strix Halo.  They're niche, no doubt, but with NVIDIA having DGX Spark, and AMD releasing a ""competitor"" later this year, kind of surprised Intel is going to ignore the compact AI development market.",Intel,2026-01-30 17:11:27,1
AMD,o1wzz6y,"From what I've seen today on reviews, they really trade blows at different TDP ranges. I'm yet to see benchmarks that come out with the 70 something percent gains Intel claimed.",Intel,2026-01-26 23:24:58,-7
AMD,o1yy03n,"LPDDR tends to be more power efficient than GDDR, especially when shared with the CPU. Not to mention you lose less power to communication. In other words, an iGPU will always be more power efficient than a dGPU.",Intel,2026-01-27 06:11:47,10
AMD,o1ywlnk,It still might not be a competitor because it's going to be expensive as hell. If one wants the bleeding edge chip they have to fork it out.,Intel,2026-01-27 06:00:59,5
AMD,o1xi2af,The 70% claim is against Strix Point which is the similarly sized offering from AMD. It only trades blows with Strix Halo because that's a far larger (and expensive to manufacture) chip whose gpu portion is as large as a desktop RX 7700. More cores at lower clocks means better efficiency.,Intel,2026-01-27 00:57:44,19
AMD,o1xhl0p,iGPU?,Intel,2026-01-27 00:55:13,7
AMD,o245brf,"It’ll use less power, and it’ll do less work per watt too.",Intel,2026-01-27 23:36:54,2
AMD,o27qbi3,"By ""this"" I meant the quote. As much as Pantherlake is great, I don't believe they were able to pull an AX launch off in time but simply didn't",Intel,2026-01-28 13:56:02,1
AMD,o1xxwb1,It only trades blows at low power. Once you get higher wattages in it does not. But efficiency wise I think id trade my strip halo for an extra hour or battery gaming even with the lesser performance as I won't feel it much in the type of games I play. Panther Lake is a strategy gamers dream chip. Thin and light that can play the games at 1080p for 3 hours on battery.,Intel,2026-01-27 02:23:52,3
AMD,o24uoot,This is more based on die size than work per watt.,Intel,2026-01-28 01:48:07,3
AMD,o28kg6w,Nah,Intel,2026-01-28 16:17:34,1
AMD,o1yfxob,"I intended to say its large size was what allowed Strix Halo to also be competitive at certain low power ranges, at higher ones it completely dominates, you are absolutely right.",Intel,2026-01-27 04:06:44,3
AMD,o24vwrb,"Die size is going to limit channels more than anything though, right?",Intel,2026-01-28 01:54:40,2
AMD,o1ufz7y,Only 6yrs later than the M1 launch and about 9.5yrs after Intel learned that Apple was developing their old SoC independently.,Intel,2026-01-26 16:38:04,47
AMD,o1wjkyo,"dear god pls give me a zenbook with an OLED screen and all day battery life 🙏🙏 I don't need m5 performance, I just need to be able to have my terminal and browser open all day without needing to plug it in twice a day.",Intel,2026-01-26 22:05:48,24
AMD,o1u7jyg,Nice! They will be sweet to run Linux. 30h battery life?,Intel,2026-01-26 16:01:54,12
AMD,o1uctja,"If only Windows could catch up with Apple MacOS  Panther Lake development was chaotic and last year, they were not sure if 18A was going to be ready, but was worth the risk  I am more excited about Nova Lake ( wont be on Intel node)",Intel,2026-01-26 16:24:35,17
AMD,o1ughit,"Panther Lake is very impressive, but this title is clickbait. Apple still is the leader overall, in both performance and efficiency.",Intel,2026-01-26 16:40:12,8
AMD,o1uhyjv,And it’s deleted before i read 🤣,Intel,2026-01-26 16:46:25,2
AMD,o1uqg9h,I get a 404 error clicking that link. Is it pay-walled? I’ve searched for it on the main page and it doesn’t come up.,Intel,2026-01-26 17:22:49,2
AMD,o1vuxkh,QCOM and their pipe dream of Windows on ARM are done for. There's literally no reason to buy a Snapdragon X chip,Intel,2026-01-26 20:16:11,3
AMD,o1vte62,"No it won’t. At least until the manufacturers get their shits together. I was looking for a mobile and somewhat capable, second laptop and every single brand has some serious shit. Especially related to usb-c ports and mainboards.",Intel,2026-01-26 20:09:23,1
AMD,o1w1k0z,"They have to, otherwise they will be out of business bc arm",Intel,2026-01-26 20:45:33,1
AMD,o1wzkmp,I’m more curious about ultra low power CPUs and how they’ll perform. I really want a low power truly fanless laptop for light tasks.,Intel,2026-01-26 23:22:54,1
AMD,o1z60rh,"So a high-end 16 core Intel chip beats out Apples lowest end chip with 10 cores in multitasking, is that really an answer? Apples chip is 50% faster in single core, and they have two tiers of chips above their low end.   Intel is very far from where Apple is, if these metrics are a good example of where they stand.",Intel,2026-01-27 07:17:17,1
AMD,o1zpf3q,"lol, its way behind even m4 and under desktop can't beat ryzens. I do wish them good luck though, getting bored with AMD(slacking a lot) becoming intel..",Intel,2026-01-27 10:14:54,1
AMD,o2687wm,"No, it's not. 🤔",Intel,2026-01-28 06:56:23,1
AMD,o28grp0,My laptop is slow as hell and has a hard drive 8gb ram Intel i5 8th gen and a integrated graphics card how I make my laptop fast,Intel,2026-01-28 16:01:42,1
AMD,o2dnbq7,"I honestly can't recall the last processor(s) I was anticipating as much as this, except perhaps Lunar Lake and the Snapdragon X Elites. Those were primarily focused on battery life and Panther Lake and The X2 Elites are the processors we really wanted the prior gen to be. I believe this will be the year of lightweight, long battery, performant notebooks with incredible screens, that you can game on. It's quite awesome. I expect this to go down as probably tbe biggest jump in compact performance ever.",Intel,2026-01-29 08:44:12,1
AMD,o4aqlk5,@ title     plus it games✌️,Intel,2026-02-08 18:53:53,1
AMD,o1v0ce7,"lol this is like the 3rd time I’ve seen headlines calling something the windows answer to apple silicon. First it was the latest windows ARM, then it was AMD Strix Halo, and how its intel Panther lake. Don’t get me wrong, I’m stoked for Panther Lake, but it is diminished by their reliance on Windows for the operating system.",Intel,2026-01-26 18:05:58,1
AMD,o1vv3cn,I very doubt,Intel,2026-01-26 20:16:53,0
AMD,o1uh7xf,Eh as per the article the only thing that stands out is the iGPU and that's it. The rest is on par with a mid-range Ryzen AI 7 350. And you pair that with a 5050 and boom: Double the FPS of the Intel.,Intel,2026-01-26 16:43:16,-8
AMD,o1vlq7a,Better late than never. It's a very solid response. I do hope they keep pushing as we really need CPU and GPU competition.,Intel,2026-01-26 19:36:03,19
AMD,o1xa9il,"Move slowly, but surely",Intel,2026-01-27 00:17:39,7
AMD,o1vffx8,Apple switched because of Intel node issues and now they are considering using Intel fabs for their own chips. Could have just stayed on Intel,Intel,2026-01-26 19:09:07,-11
AMD,o1wmdz8,Oled screens tend to burn battery faster.,Intel,2026-01-26 22:18:47,13
AMD,o1y37z4,"It already exists. The Lunar Lake will still have better battery life than Panther Lake, UNLESS you get the 4-core GPU which will result in worse performance for the GPU than the current lunar lake in the zenbook",Intel,2026-01-27 02:53:03,3
AMD,o1yhrze,This is all I want/wanted I caved and got a macbook air a few months ago. Damnit. Some of these machines look incredible,Intel,2026-01-27 04:18:11,2
AMD,o36okur,"Exactly! Windows 11 is horrible, Apple Silicon/ARM's Linux compatibility is not good. Linux + Intel = YES",Intel,2026-02-02 17:05:56,1
AMD,o1uvxb2,>If only Windows could catch up with Apple MacOS  I have 3 wishes.    Mac: better emulation layer for games like with steam OS using proton. A more seamless experience.    Windows: better ARM support.   Linux: easier emulation of other applications like adobe suite/teams/(insert windows program not on linux),Intel,2026-01-26 17:47:07,5
AMD,o1ul68c,Do we really have any confirmation on Nova Lake node at this point in time?,Intel,2026-01-26 17:00:01,3
AMD,o22hdnr,Windows doesn't need to catch up so much as stop ruining itself.,Intel,2026-01-27 19:01:18,1
AMD,o25ybqw,"Same if it has AVX10.2 it will likely be the best desktop CPU, and make most HEDT irrelevant",Intel,2026-01-28 05:39:35,1
AMD,o2cq8jn,TBH i like windows over Mac. only think i like about apple is their M series chips. Never interested when they had intel chips,Intel,2026-01-29 04:21:24,1
AMD,o2b8d7q,And Mac OS for those who swing that way ... mostly good on the outside (not loving the latest design tho) and Unix is just a terminal session away. I use Mac OS and Windows about 50/50 but vastly prefer Mac OS.,Intel,2026-01-28 23:23:40,1
AMD,o1uqin0,Same for me.,Intel,2026-01-26 17:23:06,1
AMD,o1us3oe,"Seems like it got deleted, don't know why.",Intel,2026-01-26 17:29:55,2
AMD,o1wpzwv,"Unless you want better battery life, reliable wake from sleep, better cpu and multi-core perf.   Apple proved you don't have to be gaming first to be successful. Intel only wins in gaming, nothing else, and only against a almost 2 year old Snapdragon SKU. Lets compare to X2 when it comes out.  This is like saying Intel just destroyed AMD, maybe in some ways, until AMD introduces their next gen, that's the way it works.",Intel,2026-01-26 22:35:43,-8
AMD,o1vulvx,You already own a laptop with a Panther Lake chip? 😂,Intel,2026-01-26 20:14:45,1
AMD,o1ulb4v,"It’s way ahead of midrange Ryzen, it’s basically on par with high end Strix Point while delivering better battery life.",Intel,2026-01-26 17:00:36,10
AMD,o1ui763,Don't forgrt the amazing battery life.,Intel,2026-01-26 16:47:26,4
AMD,o21n3g4,What do we possibly need cpu performance for or gpu?  All software is bloat. Its debloating we need.,Intel,2026-01-27 16:51:49,1
AMD,o1vmtvy,Not really true. Apple switched so they could use ARM and control the design and vertical integration and intel wasn’t a foundry.,Intel,2026-01-26 19:40:51,11
AMD,o1wvi1p,"oh definitely, it's just that after having it I can't go back. the thing is I don't even do any heavy compute on my laptop so I don't think I'm asking for anything unreasonable. most of my day is spent in an ssh session to my home server and a browser, discord, spotify. I just need like 8 hrs screen on time and I'd be happy. I get like 3-4 right now on my zenbook 14X and that's just laughably bad.",Intel,2026-01-26 23:02:15,7
AMD,o1x7x56,"If you gonna look at something all day, better make it good. Same thing with your bed, you spend so much time on that thing, might as well be good.",Intel,2026-01-27 00:05:48,10
AMD,o25y65o,"Panther lake is more efficient at all power bands, why would lunar lake have a longer battery life?",Intel,2026-01-28 05:38:28,1
AMD,o1v50a4,Proton is translation layer for windows directx api to vulcun  in Linux.  The translation layer for x86 to arm is way more difficult than what proton does,Intel,2026-01-26 18:25:28,6
AMD,o1x5fv9,> Do we really have any confirmation on Nova Lake node at this point in time?  Intel's publicly confirmed they're also using TSMC for compute. Only one thing makes sense.,Intel,2026-01-26 23:53:06,2
AMD,o1uq0o2,"18A is not in the same league of N2.   Even 18AP is not on par with N2   14A is not ready  So unless they want inferior product to AMD (Zen6) which will be on N2, NovaLake will be on TSMC N2  14A is when Intel expects to match or surpass TSMC nodes",Intel,2026-01-26 17:20:59,-8
AMD,o1wr9b8,"Tell me you haven't watched the reviews without telling me. Lol ""better battery life"" has been QCOMs only argument vs Intel/AMD and now with Panther Lake's superior battery life, QCOM has nothing more going for it. The X2's efficiency is not going to be that much of any more than the X.",Intel,2026-01-26 22:41:39,6
AMD,o2v617d,"Snapdragon has no advantages. It is not cheaper, it is not faster and it does not have better battery life at equal performance. Add the ARM performance translation tax for x86 apps and they are donezo.  Qualcomm needs to heavily discount and increase performance at the same time to have any chance to win.",Intel,2026-01-31 22:03:32,1
AMD,o1vgx5e,Keep on dreaming. Numbers speak for themselves.,Intel,2026-01-26 19:15:21,-5
AMD,o1uld9q,"I've mostly been picking up AMD as of late, but this is a great win Intel's needed for years. About the only potential downfall is pricing, but given that AMD has nearly zero design wins for Strix Halo on laptop, Intel has a pretty wide open field to work in.",Intel,2026-01-26 17:00:51,5
AMD,o1uirjk,The Ryzen will be better. Always been,Intel,2026-01-26 16:49:51,-11
AMD,o21ptj8,In theory you are correct in practice that isn't happening so...,Intel,2026-01-27 17:03:28,2
AMD,o280xud,"You can easily disable 90% of the bloat. It's especially easy if you have Windows pro. Removing bloat freed up over a GB of RAM on startup for me. But yes, the bloatware is ridiculously atrocious",Intel,2026-01-28 14:49:50,1
AMD,o1vn0ec,No that’s the spin. Apple was mad about battery life and performance on laptops.,Intel,2026-01-26 19:41:38,7
AMD,o21wkh6,Actually having a mostly back background is good for OLED as it can actually save battery. So console or terminal is good.,Intel,2026-01-27 17:32:51,1
AMD,o21wbym,"Actually for just text rendering, IPS is still better. But for colors and media and gaming, OLED wins.",Intel,2026-01-27 17:31:47,1
AMD,o263gj0,The 17W Panther Lake has a worse GPU than Lunar Lake (17W) is what I'm saying. If you want the good GPU then it will have worse battery life because that's 25W+,Intel,2026-01-28 06:18:28,1
AMD,o1vc3ax,Argh! My mistake!,Intel,2026-01-26 18:54:58,4
AMD,o1v4ocs,Does it really that matter? I mean intel 12th and 13th  gen was on par or even better than zen3 and zen4 .  Both intel 12th ans 13th gen were on intel 7 or 10nm while zen  3 was 7nm and zen 4 at 5nm.  If anything I don't get how intel dropped ball so hard with arrow lake . Going from 10nm(intel 7) in 14th gen to 3nm of tsmc in arrow lake should produce both efficiency and performance gain like how nvidia got going from samsung 8nm to tsmc 4nm in 30xx to 40xx.,Intel,2026-01-26 18:24:06,8
AMD,o1wt2oz,"X2 is already rated 45% more efficient than X1, I've watched the reviews, but more than that I've had my hands on real hardware. Try actually informing yourself before responding. I'll just leave you with that, obviously on an Intel reddit people are going to be super defensive about their ""team"".",Intel,2026-01-26 22:50:25,-3
AMD,o1vhj64,"Yes, the numbers speak for themself on NotebookCheck’s review of PTL, which places it ahead of Strix Point’s multi core and single core, and way ahead in efficiency.",Intel,2026-01-26 19:18:00,6
AMD,o1ul00o,Nothing indicates that,Intel,2026-01-26 16:59:17,7
AMD,o1vcrkw,Is this a troll? It's the X7 and X9 are going to be quite a bit better than a 350 are you joking? Do you read benchmarks or reviews at all? The ultra 5 with b 370 is probably gonna be at or above a 350-360.,Intel,2026-01-26 18:57:46,3
AMD,o1wtita,AMD Glazer spotted 🤡,Intel,2026-01-26 22:52:34,2
AMD,o25c96y,Well we have to buy new devices just cause some company is too lazy to make a native app and use a web wrapper like whatsapp,Intel,2026-01-28 03:21:16,1
AMD,o1vnubu,Apple was also mad about margins. It cheaper for them to make their own than beholden to Intel’s pricing.,Intel,2026-01-26 19:45:15,7
AMD,o26idnu,"Where did you see that? I’m not aware of any regressions vs Lunar Lake, and Intel is specifically releasing a handheld sku to deal with the super low power : high gpu performance regime.    SKU differences aside, take a 388H and throttle it to 17W and it greatly outperforms lunar lake.",Intel,2026-01-28 08:24:03,1
AMD,o1vkgcn,"Intel 7 was competitive in peak performance, but lacked efficiency. Also note that arrowlake utilized N3b which was notorious for poor yield and over complexity and quickly replaced with N3e in Apples line of products. Arrowlake also followed the 13th/14th gen debacle and Intel reduced ring bus clocks significantly to play it safe.  Most notably, going from a monolithic design to chiplets leads to increased latency which reduces gaming performance and makes it harder to hold higher clock speeds. Monolothic designs will always be faster in a 1:1 comparison but chiplets make up for it by being modular and cheaper to scale, something intel hasnt been able to really take advantage of until Pantherlake.",Intel,2026-01-26 19:30:31,7
AMD,o1veick,"The node helps, obviously, but you still need to design a product that works. The choice of node is not a magical solution.  Arrow Lake had flaws that showed themselves in certain workloads due to design. They \*did\* still get an overall efficiency boost but thats not a high bar given what they were coming from.",Intel,2026-01-26 19:05:08,5
AMD,o1vg7we,"They were experimenting with tiles and it’s an early iteration. Not to mention they have more experience with their own fab.   Arrow lake smokes 14th gen in compiler workloads. Those faster e cores help a lot!   Outside of gaming workloads, it’s fine. Even with gaming workloads it does ok, just not a real jump from 14th gen. It was never going to beat x3d",Intel,2026-01-26 19:12:21,2
AMD,o1wk8a6,It hardly does. Mid range chips from seven or eight years ago can still do everything that modern chips can. All you have is a bunch of tech consumers who convinced themselves that they need the latest tech.,Intel,2026-01-26 22:08:50,1
AMD,o1x5cjb,"> Does it really that matter?  Yes, you're looking at something like 15% perf for a full node. That's very significant. A node shrink alone could constitute an entirely new gen in perf.   > If anything I don't get how intel dropped ball so hard with arrow lake  They redesigned the SoC fabric, and it's shit.",Intel,2026-01-26 23:52:38,1
AMD,o1wtbca,No way! You've had your hands on real Panther Lake hardware already?? But it hasn't even been released yet! You must be a special redditor. Give me a break.,Intel,2026-01-26 22:51:34,5
AMD,o1vizts,I'm talking about Krackan,Intel,2026-01-26 19:24:15,-2
AMD,o1umpjo,This test does: https://www.computerbase.de/artikel/prozessoren/intel-core-ultra-x9-388h-test.95776/seite-5  It looses quite strongly against the 288v (which is not anymore that efficient and far less so than the Ryzen 7 350),Intel,2026-01-26 17:06:37,-5
AMD,o2809da,Any spec sheet will show you it will have only 4 cores for the GPU. Lunar lake has 8. Panther Lake cores would have to be twice as powerful as lunar lake which we know they aren't.,Intel,2026-01-28 14:46:32,1
AMD,o1wr13m,Thanks for explaining,Intel,2026-01-26 22:40:35,1
AMD,o1vjsl5,They still lose handily in efficiency and their multi core isn’t better than Strix Point,Intel,2026-01-26 19:27:42,4
AMD,o1uyy9f,Not really. It loses against the 288v in the first test and wins against in the second test.  There is no direct comparison against the Ryzen 7 350 and I don't know how you even reached the conclusion that the Ryzen 7 350 is more efficient than lunar lake?  [Notebookcheck](https://www.notebookcheck.net/Intel-Panther-Lake-Core-Ultra-X9-388H-performance-analysis-Outpaces-Arrow-Lake-and-exceeds-Zen-5-in-efficiency.1212583.0.html) conducted fairly good efficiency tests that compare the single and multicore CPU perf/W. The Ryzen 7 350 is actually included in their list and ranks dead last in the multicore comparison so there's that and in the single core comparison Lunar Lake generally performs better than both but once again the Ryzen 7 350 is beaten by panther lake.  They even include a nice graph for different PL which also happens to include the Ryzen 7 PRO 350.,Intel,2026-01-26 18:00:04,2
AMD,o28grba,"You are discussing SKUs. The reason lower stack PTL has fewer Xe cores is because they are nearly 2x efficiency.   However, should you take a higher SKU and configure TDP to 20W (or even 15W to account for external memory) it will be more efficient than Lunar. Lunar Lake is great, but PTL is much better.",Intel,2026-01-28 16:01:40,1
AMD,o1uz9i9,You'll see but not even in your wettest dreams the Ryzen will be beaten. Intel remains Intel after all.,Intel,2026-01-26 18:01:22,0
AMD,o28nv98,"It needs to be at least 2x as efficient to be as powerful because it has HALF as many cores. It isn't, therefore the GPU will be weaker. You can do that same TDP configuration on Lunar Lake. Not sure why that matters.  ""Nearly 2x more efficient"" translates to ""nearly as powerful"". But not quite.",Intel,2026-01-28 16:32:22,1
AMD,o299e0v,It is 70% faster. You’re comparing different SKUs. Compare top Lunar Lake to top Panther. How the TDP is configured is up to the OEM,Intel,2026-01-28 18:05:34,1
AMD,o29cjlo,"We don't know how efficient those 28W Panther Lake chips run at 17W. Just because you can run it at such low power does not mean it will work well. It will certainly be much less efficient if it were to be run that way otherwise Intel would be gloating at how powerful and efficient their Panther Lake GPU is at 17W. The fact that they don't means it is well below their sweet spot of efficiency. You are basically starving it, similar to stalling an engine.",Intel,2026-01-28 18:19:00,1
AMD,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer can’t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,11
AMD,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,15
AMD,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
AMD,nz4rv7t,"“Takes on strict halo” at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,3
AMD,nzi9w6l,"Amd hasn’t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen “AI 7” 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
AMD,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
AMD,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,9
AMD,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,3
AMD,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4 times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,14
AMD,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
AMD,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
AMD,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,16
AMD,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
AMD,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
AMD,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
AMD,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
AMD,nz9efnf,"We’re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
AMD,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
AMD,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,5
AMD,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
AMD,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,12
AMD,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
AMD,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,4
AMD,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
AMD,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
AMD,o21p2wl,https://youtu.be/X9eYQTkzxqU?si=Uzdz-E3QJzzVM42N  Not the only one comparing. Intel actually outperforms at lower wattage.,Intel,2026-01-27 17:00:16,1
AMD,nz610q1,">GPU's will take all the bandwidth you can feed them.   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,2
AMD,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
AMD,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.   https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
AMD,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
AMD,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
AMD,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
AMD,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
AMD,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
AMD,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mm² to 300mm² when compared",Intel,2026-01-13 18:08:18,1
AMD,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
AMD,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
AMD,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
AMD,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,4
AMD,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
AMD,nylfre3,"While I agree their naming scheme is a mess, yours is far worse.",Intel,2026-01-09 13:49:14,17
AMD,nykrwmo,"TBH, as long as the Ultra 5 338H is actually called an Ultra X5, it'd make the entire thing a lot more consistent  as in now X always means ""the one with the good GPU""",Intel,2026-01-09 11:10:20,19
AMD,nykqc9r,"You have no understanding of Intel's business and thus are not qualified to advise them what to do   Intel doesn't sell these CPUs to the end consumer, they sell them to their customers - the PC manufacturers. And that is the reason why there is so much choice, because the PC manufacturers want it.   Also, you clearly have never heard of vPro.",Intel,2026-01-09 10:57:07,38
AMD,nylcqp0,"The SKU count is roughly doubled because you have each step with/without vPro - these get a 100MHz max turbo frequency bump, but the main benefit is you can run the corporate firmware with vPro support, so you get additional security and manageability features. Exception is the Ultra 9 where they just do it with vPro support as standard.  These have a higher cost because you are getting more features.  You could make it so you just have one CPU and then the manufacturer pays a license for corporate firmware per device, but that's more work to then ensure manufacturers are licensing machines correctly, and more confusing for end-users where now if you're buying a corporate device with vPro support you know you are looking at Core Ultra 236V and 268V for vPro support whilst 226V and 258V don't have it.",Intel,2026-01-09 13:32:44,6
AMD,nykrcxa,"Because intel’s customers work with thin margins and want the wide product stack with lots of performance and price steps. For them it matters if they get 4.4ghz for $300 or 4.6ghz for $350. You are not Intel’s customer unless you ordered a pallet of 1000 CPUs, which I doubt.",Intel,2026-01-09 11:05:44,10
AMD,nyo7z62,Apple really isn't better. They leave out lots of the important performance information. They just don't tell you at all.,Intel,2026-01-09 21:31:03,3
AMD,nykykh6,I agree for those cpu that have no alphabet denomination at the back as that just looks like how desktop cpu is.  But for X7 and X9 is just even easier CPU differentiation,Intel,2026-01-09 12:02:34,2
AMD,nywwcvg,I only agree with the title. Your naming scheme is much worse lol,Intel,2026-01-11 03:59:55,2
AMD,nylenad,"At this point even S3, S3 Pro and S3 Pro Max would be a great improvement.",Intel,2026-01-09 13:43:16,1
AMD,nylwbke,"Totally don’t get it, miss the 13900k 14600k type names.",Intel,2026-01-09 15:11:59,1
AMD,nyneqru,"Brah I don't care about the naming conventions, it is what it is.  It is petty to argue about all of this.  I need the B770 and C880 to be released.  I need more Intel Arc Pro cards to be released, there is no hope for humanity otherwise.  I need Battlemage and Battlematrix everywhere but TSMC is the bottleneck.  Hopefully there is more for 2026 where Intel IFS shines.  God help us all!",Intel,2026-01-09 19:16:03,1
AMD,nyo686v,Are you saying they are all locked???,Intel,2026-01-09 21:22:59,1
AMD,nyqfh80,I only care about the top level sku so the names don't matter.,Intel,2026-01-10 04:43:17,1
AMD,nykt7v1,Samaung Galaxy S3,Intel,2026-01-09 11:21:10,1
AMD,nyky6jo,Intel to $100 guaranteed,Intel,2026-01-09 11:59:46,1
AMD,nymfe1x,Just Josh presses them hard on this issue at about 7:30 in this video https://youtu.be/AzGFbkKZE7A?si=yq1pmpRv7exQ-7i5,Intel,2026-01-09 16:38:15,0
AMD,nyo3uhv,"Check the Just Josh interview! Dude criticizes exactly that to an intel executive... For me, they should drop the ultra naming scheme altogether... it hasn't stuck yet... they should go back to de i3/i5/i7/i9...use the X for the B390... and an S for the 16 core variants...",Intel,2026-01-09 21:11:54,0
AMD,nyps0n6,Apple always nails the small stuff,Intel,2026-01-10 02:21:54,0
AMD,nyn9p1s,"Just because it sounds similar to Apple does not make it ""bad"".  I chose ""S3"" because they literally market them as ""Series 3"" (sounds awfully similar to M-series 🤨). Could fiddle with it but my idea stands:  Different core count = different name, Better gpu = add X",Intel,2026-01-09 18:53:22,-5
AMD,nyoegef,"Nope, its the Ultra 5 338H: [https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html](https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html)  The Ultra X7 and X9 are listed as such on Ark: [https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html](https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html)",Intel,2026-01-09 22:00:46,3
AMD,nz4b2zo,"the 338H doesn't have ""the"" good GPU, it has a B370, with 10 cores",Intel,2026-01-12 06:22:28,2
AMD,nylk5nn,"Btw, OEMs love the fact that the Ultra 5 336H and 338H are vastly different products with hugely different performance when it comes to graphics. Why? Cause they can market the 338H to you, and then sell you the 336H at a fraction of the cost, and if you are not very tech-savvy, well, that's too bad for you.",Intel,2026-01-09 14:12:07,15
AMD,nynb0an,"Yes because PC manufacturers want a ""choice"" to get a CPU with 100MHz higher clock speed as if that will make a difference in a mobile device at all.  If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.",Intel,2026-01-09 18:59:08,-8
AMD,nyn93or,"I see, that does complicate things.",Intel,2026-01-09 18:50:47,-2
AMD,o1nu420,Why are you getting downvoted 😭  For consumer products it makes complete sense to go for more simplistic naming schemes,Intel,2026-01-25 18:16:47,1
AMD,nz8mprz,"It still should have the X IMO. They're still bothering to call it a B3xx chip rather than just ""Intel Graphics"" like the <=4 Xe chips. The handheld chips running downclocked GPUs get the B360 and B380 names as well. Those should all be Core Ultra X.  As of right now only the ending 8 differentiates the 338H from the small-GPU SKUs, which IMO is not clear enough. Also, if the X became the standard for all big-GPU chips, that ending digit can be used for something else, such as noting the actual GPU performance within the stack. Perhaps just using the 6-9 from B360-390 or something like that.",Intel,2026-01-12 21:47:30,2
AMD,nynhyn2,"You are placing way more importance into the naming than any normal customer would.  The names that matter to normal customers are Core Ultra 5, Core Ultra 7, Core Ultra X7, which is what you will also find on the stickers that Intel has the PC manufacturers put on the device. The model numbers are just for the PC manufacturers and customers who want the exact SKU.  Seriously, your obsession with this is weird. Just accept that the naming is not meant for you and move on. Not everything has to be like Apple.  > If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.  Yes, because vPro doesn't matter for normal consumers, but only for big enterprise customers.",Intel,2026-01-09 19:30:44,5
AMD,nymf8n3,It’s too hard to be a smart consumer and Google the names of the processor(s) and compare?,Intel,2026-01-09 16:37:35,3
AMD,nyo54xn,"""Obsession"" as if multiple YouTubers, some with millions of subscribers, haven't said the exact same thing I did",Intel,2026-01-09 21:17:55,-2
AMD,nynq0up,If you're a smart consumer you aren't buying windows laptops.,Intel,2026-01-09 20:07:44,-4
AMD,nyou0lx,"Right, those guys are surely the ultimate authority on anything and not just engagement driven outrage machines /s",Intel,2026-01-09 23:18:05,4
AMD,nyviuhi,That's some serious credentials you're bringing up,Intel,2026-01-10 23:32:07,5
AMD,nypm046,Ah I should instead buy Apple laptops and/or Arm laptops that don't work with my programs. Genius!,Intel,2026-01-10 01:49:37,6
AMD,nykpkf2,Probs the same reason tire companies don’t make their own cars. It’s a lot of work. Plus there’s plenty of competition  What you’re missing is companies like intel have initiatives such as the ultrabook initiative to help manufacturers make better laptops.    Besides it’s way more profitable for them to just make the cpu and not deal directly with consumers as much as possible.,Intel,2026-01-09 10:50:30,38
AMD,nykpm6e,">Another common complaint I hear with non Apple laptops is battery life on suspend  This is still a problem, and it's because of windows. Microsoft claim to have fixed it ONLY for snapdragon laptops, but I have heard some still having the issue. Ever since apple made the M chips the blame has shifted to X86 processors being the problem when it was and still is windows the whole time. This whole thing makes x86 seem worse than it actually is.",Intel,2026-01-09 10:50:55,23
AMD,nykulsp,"They did. Intel produced NUC desktop and laptops for quite a while, then they sold the business to Asus. The desktops are absolutely excellent.",Intel,2026-01-09 11:32:25,22
AMD,nymeqg2,Because then theyre essentially *competing* against their own customers. In a market that would require a lot of effort for small margins.   Their effort is better spent horizontally expanding rather than vertically at this point.,Intel,2026-01-09 16:35:22,5
AMD,nym7ev6,"As someone who owns the Intel nuc 12 enthusiast, the engineering on them is phenomenal, Intel would do an extremely good job resulting in low margins and financially it doesn’t make sense to make laptops stick and perfect the chips is probably the best financial decision seeing as neither Qualcomm, amd, nvidia make laptops themselves. Even reference designed are outsourced. I worked on intels Arc program those Intel branded cards are expensive AF as they don’t have the bargaining power of getting low cost components like Asus or Lenovo would.",Intel,2026-01-09 16:02:39,3
AMD,nykpdew,"They used to, NUC was acquired by Asus.",Intel,2026-01-09 10:48:51,6
AMD,nykquc9,Making and selling CPUs is more profitable. Apple has a different business model.,Intel,2026-01-09 11:01:21,2
AMD,nymjurr,I really like Intel NUC. Too bad they sold it to Asus,Intel,2026-01-09 16:58:00,2
AMD,nykrk5a,"Why don't tire companies make their own cars? Why don't window companies build their own houses or office buildings?  Why don't the semiconductor tooling companies like ASML, LAM, Applied Materials just build their own fabs and make their own chips? Hell why can't they just build their own laptops too??   Do you really think that Apple manufactures their own M series chips? Or really any of the components in their products? Because the truth might shock you.   I recommend you look up supply chains and maybe learn a lil something :)",Intel,2026-01-09 11:07:26,4
AMD,nyq0x6r,"You are not Intel’s or AMD’s customer  Dell, Asus, Lenovo, MSI, etc are all customers of Intel and AMD in that they are the ones who actually buy chips  If Intel or AMD go into the laptop business then they are competing with their customers which tends to make their customers upset  The same problem exists with Microsoft and their Surface line which is one of the reasons the Surface line often seems to struggle - Microsoft has to be careful walking the line between demonstrating what they would like the hardware companies to do vs actually competing with them  As for differences in performance that can often come down to what price point the manufacturer is aiming for with the laptop. Higher price can often mean better quality components (which in a laptop can mean lower power consumption)",Intel,2026-01-10 03:12:36,1
AMD,nyszed9,A mid ground solution would be to make a reference laptop and mandate manufacturers to meet or exceed the criteria like battery life and thermals.  Intel did have an initiative called Ultrabook serving a similar purpose before.,Intel,2026-01-10 16:05:13,1
AMD,nyvag38,"They did, but got out of it  https://www.intel.com/content/www/us/en/ark/products/series/196845/intel-nuc-laptop-kits.html  They used to make mobos too, but got out of that",Intel,2026-01-10 22:47:52,1
AMD,nzmt7q4,"low margin, and competing with customers is no good",Intel,2026-01-14 23:25:58,1
AMD,nyljl73,"Please, don't.  Once, a long time ago, when dial-up was a thing and used by business, I've purchased an Intel modem. 9600 bps connection speed. It was *the* *worst modem I've ever witnessed* \[despite the fact that it was expensive\], because the firmware was expected to be run in sterile laboratory conditions. The real world with unperfect landlines drove this child into confusion and madness, it was repeatedly entering renegotiation on every disturbance, often ending in just dropped connection.  I assume that Intel entrusted the design to talented engineers who had no idea about real-world operating conditions and did not bother to study them. Even in the US at that time, analog telephone lines did not always have negligible levels of interference.  \* 9600bps was the bleeding edge at the time; the old modem at the company where I worked had a speed of 2400 bps.",Intel,2026-01-09 14:09:12,-4
AMD,nyl7kr6,"\> It’s a lot of work.  Smaller companies like Framework are able to come up with new designs from scratch, though. Slimbook too I think?  \> Besides it’s way more profitable for them to just make the cpu  That pretty much sums it up & answers my question.",Intel,2026-01-09 13:02:37,-20
AMD,nyrb7mi,It's both.,Intel,2026-01-10 09:04:04,0
AMD,nzbd8ey,I have an Intel NUC 9 Extreme [LAPQC71A](https://www.intel.com/content/www/us/en/products/sku/196641/intel-nuc-9-extreme-laptop-kit-lapqc71a/specifications.html). It’s built like a tank (magnesium alloy chassis) and I’ve always been very happy with it. It’s still my main laptop.,Intel,2026-01-13 07:27:53,2
AMD,nys69j0,It's not cause SDXE also suffers from this issue Intel CPU Didn't have the issue with MacOS,Intel,2026-01-10 13:25:30,1
AMD,nyt7d08,"There are issues, but SDXE still did better on battery life than comparable Intel chips, and that's with a number of its own issues. Only with LNL/PTL has Intel meaningfully started to close that gap, the first such push since HSW-ULT.",Intel,2026-01-10 16:42:50,2
AMD,nyt7wqy,Some software on windows will break it you are one update away from Things breaking in windows be it WoA or X86_64,Intel,2026-01-10 16:45:26,3
AMD,nxtihn4,The biggest issue was that it was crippled by the ported meteor lake memory controller dies its that simple,Intel,2026-01-05 14:12:18,27
AMD,nxtjtfv,"Very good explanation, I own a 285k and I can say the stock experience is average, but the platform is great and coming from 14900k, the temps and power efficiency are impressive. Once fully tuned, 9000c38 A-die, 36 d2d and 34 ngu, gaming is on par with 14900k, but more efficient. I think nova lake will be amazing.",Intel,2026-01-05 14:19:43,36
AMD,nxvnqr5,">if you judge Arrow Lake solely by the frame rate counter in Cyberpunk 2077 at 1080p  Am I allowed to take into account that Intel went all the way from ""7"" to ""3"" lithography which is more than 2x improvement to achieve almost nothing?",Intel,2026-01-05 20:16:32,9
AMD,nxti8go,It’s not necessary for consumers to buy an inferior product from a multibillion dollar company now backed by the global superpower’s government.,Intel,2026-01-05 14:10:52,41
AMD,nxugtl3,">We need to stop looking at the Core Ultra 9 285K through the lens of a typical generational refresh  That's all what consumers care about. They don't care if ARL on paper or on theory is some great reset. Perf, power, and cost is what's important.   >he 285K is suffering from the acute growing pains of decoupling the compute complex from the uncore in a way that creates a distinct latency penalty that enthusiasts are mistaking for regression.  It's not being ""mistaken"" for a regression, it quite literally is one.   The problem is also that AMD also has disaggregated their compute from their IMC, and yet has *better* latency on *less advanced* packaging.   >The controversy here isn't that Intel failed to push frequency; it is that they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.  Nothing about ARL's current design prioritizes ppw or area efficiency over RPL's design from a chiplets vs monolithic perspective. ARL isn't enabling higher core counts from going chiplets, that seems to be left to NVL according to rumors. And chiplets carries an area penalty over monolithic designs anyway.   >The removal of Hyper-Threading from the Lion Cove P-cores is the most contentious yet logically sound decision engineers could have made given the thermal constraints of modern silicon.   This makes no sense   >By removing the simultaneous multithreading logic, specifically the duplication of architectural state and the complexity required in the reorder buffers and schedulers to handle two threads, Intel was able to physically widen the core and increase the L2 cache per core to 3MB without blowing up the die size  SMT costs Zen 5 less than 5% in area btw. Just throwing that out there.   >The result is a P-core with significantly higher IPC than Raptor Cove  It's not though. This has been a significantly worse ""tock"" in terms of IPC uplift compared to something like SNC or GLC.   >but this raw single-threaded throughput is being masked by the interconnect latency.  Maybe gaming or some benchmarks are, but for the large part, no.   You can see this two ways, LNC's structural gains (core width, ROB capacity, etc etc) have smaller gains, percentage wise, over their predecessor versus something you would see in GLC vs SNC, or SNC vs SKL.  And also LNL's uncore is dramatically improved over ARL, and yet you see the same unimpressive IPC gains over MTL/RWC (which is the basis for ARL's mid mem fabric).   >the architectural overhead of the Foveros packaging means that ring bus latency is higher.  No? The ring runs at a different frequency than the D2D?",Intel,2026-01-05 17:00:52,13
AMD,nxtflnr,"https://chipsandcheese.com/p/skymont-in-gaming-workloads  None of the youtubers mentioned about core-to-core latency, improvements on the schedulers and execution ports setup.  The e-cores are really great in a 4 group cluster.",Intel,2026-01-05 13:55:51,16
AMD,nxv9k2w,Designing a consumer product line around the niche of top of the line workstation is not a good decision for the average consumer.  I only once met a person with video production workstation that has more than an I7 or R7.  Previous gen I5 were amazing combo of multi core and single core . They rivaled amd r7 in preformance . Now a person wanting mid level cpu's would pay preformance tax due to it being made for the few people needing extreme amount of cores since those people would not buy dies that actually were designed for it like Xeon.,Intel,2026-01-05 19:10:59,5
AMD,nxtg5aw,"this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)   i would agree this architecture is very much limited by d2d and without 200s or just pushing d2d can be kinda underwhelming in performance but for sure as first gen product is very solid and makes me personally excited for nova lake as it seems they plan to fix and improve on their current architecture  also i believe clockspeed difference was merely responsible to 13/14th gen failures which were caused by excessive idle voltage.  i would say adding DLVR was kinda smart as well as it reduces your power consumption significantly at idle especially with proper tuning.  Edit: fixed typos, autocorrect being silly with me",Intel,2026-01-05 13:58:56,10
AMD,nxtm1ov,as a person who made an upgrade from lga1700 13950hx ES laptop mutant to 265kf this cpu feels soooo smooth in win 11 despite on a huge 75 NS the ram latency and not ability to reduce latency this new E core with 800 score at 5 ghz in cpuz single core make using the pc so nice and ofc in single core game like cs2 I've loose a lot of performance with 13950hx at 5.6ghz I've had 980fps in dust 2 fps benchmark and on 265kf only 800... BUT in a multi core load game I got fps improvement but any way this upgrade is worth it for ppl who is not a pc enthusiast and don't want to tune the lga1700 CPUs,Intel,2026-01-05 14:32:00,3
AMD,ny3llfj,That's one big wall of excuses,Intel,2026-01-06 23:17:53,3
AMD,nxto7vg,"It smells like slop in here. Shit post rather than a shitpost, congratulations.  I like my 265K. It performs well for my purposes and has reduced my personal power consumption considerably vs AM4. It is behind AM5 in my testing for broad term ""gaming"" when specifically chasing framerates, but compared to a 9700X or 9800X3D it is absolutely stellar at doing stuff while doing other stuff.",Intel,2026-01-05 14:43:46,5
AMD,nxtkrwq,Yea with a z bord and the 200s boost and fast ram 15th gen is finally matching or supasing 14th gen,Intel,2026-01-05 14:25:03,4
AMD,nxwaygs,Wall of slop.  Make your points in a more concise manner.,Intel,2026-01-05 22:04:26,5
AMD,nxtzro3,"AMD had a very similar experience with their first generation of Ryzen CPUs. One of the differences here is Intel had a competitive product prior to their architectural shakeup. Had Intel totally croaked for years and not been competitive in the CPU space the narrative would be completely different and everyone would be singing their praises.  Aside from that I think the biggest problem with this new architecture is simply there's little reason to buy in. It was only recently (if memory serves, I could be wrong on this) announced that the next generation of CPUs will share this platform and later ones will be on a new socket. When AM4 was announced we knew that it would persist for multiple generations and now with AM5 - why would you buy a board that will be obsolete when its time to upgrade when you could buy into a platform that will support your next 1-2 CPUs? Especially with the old intel socket performing just as well on a more mature platform, for most end users this first core ultra series just isn't worth investing.",Intel,2026-01-05 15:41:28,2
AMD,nxua32l,"""they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.""  That good for them, but we don't want that. I would take better latency over improvements that pretty much only save money to the companies.",Intel,2026-01-05 16:29:42,4
AMD,nxu3n2t,Love my 285K rig.,Intel,2026-01-05 15:59:37,3
AMD,nxu27tg,"IMO corrupt tech sites and tech YouTubers are behind ARL’s failure for two reasons.   The first: AMD MCM processors without 3D cache are bad for gaming, and it’s hard to find this information in 90% of charts to warn intel about the consequences of going down this path, even though they should have done their own tests and experiments.   The second: in my own tests, the 265K was 15% faster than the 9700X on launch BIOS. Through BIOS updates, that gap extended to 20%, making it only 7%-10% slower than the 14700K. Yet on some very questionable charts, the 265K is shown as slower than the 9700X.   For experienced users, RPL CPUs’ temperatures can be lowered by 20°C by turning Hyper-Threading off and undervolting. For inexperienced users, ARL is better, as it runs 20°C cooler out of the box.   I believe that in 2026 we deserve raw, unedited video benchmarks that start from the desktop, show full system specs, then enter game-by-game benchmarking—no more charts with zero evidence to back them up.   When I tried to confront HUB with my benchmarks, I got blocked to cover up their lies and corruption. If they truly cared about which CPU is faster, they would share their in-game benchmark scores and discuss them in a scientific way, but that’s not their goal. The numbers go up and down for the highest bidders.   Lastly, I believe Nova Lake, with its expected 144 MB cache, will be faster than Zen 6 X3D by 10–20% as 9700x in real world is 20% slower than 265k and if both got the same IPC uplift NL will end up on top.   My own tests    14700k vs 9700x 30% faster https://youtu.be/1f6W6nkDS4o?si=chFUAeBWzybQopaL   265k vs 9700x 23% faster https://youtu.be/PuB0Dg-Jvyk?si=SmGJUFtYj-OjjpQh   Tech sites got same results that clearly show RPL and ARL CPUs are only slower than 9800x3d and faster than everything else from AMD    https://www.pcgameshardware.de/Ryzen-9-9950X3D-CPU-281025/Tests/Benchmark-Release-Preis-vs-9800X3D-1467485/2/     https://www.purepc.pl/amd-ryzen-9-9950x3d-test-recenzja-opinia-cena-wydajnosc-gry-programy?page=0,55",Intel,2026-01-05 15:52:58,5
AMD,nxts3mv,"For me.   I will go with Intel because of reliability (I know about chip degradation) but for me I bought 13900K from first day I undervolted using offset and but Max turbo frequency to 5.5Ghz . so my chip never tried to boost 5.9Ghz with crazy voltage.  Next is the most important is Everything just works. The boot is faster. wakes from sleep. its a more mature. I have another amd pc with r5 2600, where I found some stability issue.  Another one that is important to me. is idle power consumption. My 13900K can idle at 6 watts. Imagine 24 cores idling at 6 watts. where as 6 core zen idle at 15-20watts.(and that is low side many user reported 25+watt). its all because of chiplet.  I didn't test the new Arrow Lake as this uses tile. so i cant comment on idle power draw. if anybody has test, let me know.",Intel,2026-01-05 15:03:58,4
AMD,nxwilo7,"Intel's latency problems have been around for a while now. Arrow Lake just threw gasoline on a fire that was already burning. [The 14900K had a latency of about 90ns for memory access, which is awful compared to the 10900K's 66ns and the 3950X's 73ns latency.](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the). The 285K sits at 106ns.  The 9900X sits at 82.43ns, so it seems like latency is going up across the board in general.",Intel,2026-01-05 22:41:53,2
AMD,nxmkdsp,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2026-01-04 14:07:07,1
AMD,nxxn31i,I compare Arrow Lake to Zen 1 or the first iterations of Ryzen. It is quite the techinal hurdle but it allows for future generations to bake very well. Intel was just playing catchup from having 14NM^6 nodes lmao.,Intel,2026-01-06 02:15:07,1
AMD,nxyburv,the problem for me the platform cost value over lga1700 isn't where it needs to be. namely I'm not interested in trading 32GB of DDR4 for just 8GB of DDR5.,Intel,2026-01-06 04:39:27,1
AMD,ny7t3re,"Since you are talking about latencies, best intel gaming cpu, so far, is 14700k. Excellent gaming performance and also great cpu for productivity.  Second best, is 14900k, same as 14700k, but cause of price it gets to second place.  Third place, you name it.  Period.  PS: I'm with intel since ..... Pentium 3 at 800MHz. Also have AMD cpu.",Intel,2026-01-07 15:44:56,1
AMD,nyabjt5,"End-user workloads at midsized businesses are almost entirely single-threaded. Even when the applications are multi-threaded, they often become single-threaded when they get shuffled through corporate antimalware. While AMD chases gamers and Apple chases artists, Intel is hitting a sweet spot here not perfectly served by any other single-user processor.",Intel,2026-01-07 22:22:37,1
AMD,nyduqy4,"I've been saying similar in a couple of PCMR threads.  Arrow lakes problem is chiplet to chiplet latency. Which is understandable as this was a major departure from monolithic dies this generation. This explains both why games particularly suffer, and why very high speed memory can mitigate the issue somewhat.  Does this mean they aren't bad chips? No, they are (or at least for gaming performance they are). But it's quite exciting in the sense that it's a specific issue holding them back that can be solved. The rest of the architecture has a lot of potential.  I think of these chips as in a similar place to AMD Zen 1 technology wise. A huge shift, not actually delivering much performance boost yet, but with a whole lot of potential for updates to make big improvements.",Intel,2026-01-08 12:08:37,1
AMD,nyf32oe,"Yes yes, I remember saying similar things about my FX-8350.",Intel,2026-01-08 16:03:36,1
AMD,nysqs8m,"285K seems to have fantastic workstation performance, I didn't think much before selecting Intel over AMD parts for work to be honest, considering the efficiency gains. We landed on 265K, it benches very favorably compared to AM5 parts in nearly every workload except for gaming, and even then, that's mostly the X3D parts, and even then, that's when there is virtually no GPU bottleneck.   Most initial ""gaming"" reviews were done exclusively with 5090s, which is extremely unrealistic for most gamers and gave an extremely false impression that you would see a massive uplift by buying this part compared with the alternative CPU parts in gaming.   A lot of reviewers such as HUB have done a lot of work to correct this impression with a variety of testing scenarios and explaining their reasoning behind removing the GPU limiting factor as much as possible in benchmarks. Incidentally, these reviews typically showed that anything less than a 5080 sees virtually no benefit with a 9800X3D versus, say, a 9700X, and incidentally the two parts that saw no discernible benefits were the Radeon 9070XT and the 9070.  Obviously, if you have a 5090 and an unlimited budget with price being no factor, your best bet for top tier performance in all categories is a 9950X3D, but that just isn't what most people are looking at.  I myself bought into the future proofing mentality with my CPU and bought a 9800X3D, and ironically bought another AMD Radeon 9070XT to pair it with, a GPU that seems literally no performance gain compared to a 9700X, let alone a 285K. The same 285K that takes a dump all over the 9800X3D in nearly every other productivity benchmark.  I guess the good news is that when I upgrade my GPU in 2-4 years, at least I'll take advantage of the 9800X3D in games, lol. But considering it cost me over $900 for CPU, RAM, and motherboard, that's cold comfort. Ironically I probably would have made out a lot better buying a 5700X3D or a 5800X3D and the 9070XT and keeping my AM4 board and DDR4 RAM and saving $600.",Intel,2026-01-10 15:22:09,1
AMD,o2v1ko1,It’s a stepping stone … the efficiency is super and performs the same as a 14900 drawing a lot less power. Impressive.,Intel,2026-01-31 21:41:48,1
AMD,nxtna7j,"You are correct that they needed to make a big architectural change as the 14th gen was clearly having issues and was being pushed too hard to make up for them, and ARL is the first step to that reset.  From their engineering perspective, it makes sense.  I wouldn't buy it though.  NVL... different story.",Intel,2026-01-05 14:38:45,1
AMD,nxvbwve,"No 285k is not shaving off 80-100W from 14900k, Set 100W PL1=PL2 on both and then compare. 285k is not even that much better than 9950x in MT. No amount of words can explain ARL flop, it took intel 3 gens after rocket lake fiasco to catch up, just to waste all that effort on ARL. Show me any other silicon design with advantage of 2 nodes and a new architecture just to be slower than the predcessor.",Intel,2026-01-05 19:21:42,1
AMD,nxwb6x7,"""If you are buying a 285K solely for (1080P) gaming, you are buying the wrong product for the wrong reason. ""  Fixed it for you. I game in 4K and I didn't buy a Ultra 9 285K to play games in 1080P to get high(er) framerate at the expense of stuttering. The ultras are outstanding CPUs.",Intel,2026-01-05 22:05:33,1
AMD,nxtfm4y,I guess.,Intel,2026-01-05 13:55:55,-1
AMD,nxte2yd,No one has time to read all that,Intel,2026-01-05 13:47:04,-11
AMD,nxvicu7,This is a very long way of saying Intel has chose to prioritize competing with Apple’s SoC and ignoring gamer’s and PC enthusiast’s wants/needs.,Intel,2026-01-05 19:51:27,0
AMD,nxuwzmt,"I’m just waiting for Nova Lake and if it gets delayed into 2027 and if TSMC does the packaging for some chips there may be issues  with supply. TSMC does not like that Intel has IFS and they play dirty, real dirty.  Nova Lake flagship could be at or above $1000, I may go with Ultra 9 285K with the fastest pcie5 nvme and ram, by then hopefully ddr5 is accessible.",Intel,2026-01-05 18:14:53,0
AMD,nxtrgru,My same experience with both of my 265k systems. They have been extremely stable for me and very efficient. Never have to worry about temps and they perform well with a 5080 and 9070XT. Contrary to all the media rhetoric I enjoy gaming with them.,Intel,2026-01-05 15:00:40,19
AMD,nxu6pg9,Basically the same or even less FPS than a 9800X3D consuming 80 watts,Intel,2026-01-05 16:13:55,3
AMD,nxvj13w,"Hi I'm sorry to ask but what does this mean?     9000c38 A-die, 36 d2d and 34 ngu     is that an app or something?",Intel,2026-01-05 19:54:33,2
AMD,nxuzcbk,"Exactly, tuned 285k is just 2-3% away from max tuned 14900KS + DDR4 at 4300MHz CL16.  All Z890 boards are so cheap so I grabbed an APEX with a 265K. Only costed me $600",Intel,2026-01-05 18:25:21,2
AMD,o367lks,"In gaming specifically, how smooth would you say the 285k is compared to the 149k? in the sense of frametimes and shader caching.",Intel,2026-02-02 15:47:34,1
AMD,nxvtdhm,"that is exactly the point, if it was just architecture we could live with it and consider it an scurve of innovation, but the process proves that this design is going nowhere",Intel,2026-01-05 20:42:56,7
AMD,nxwwv5w,"New process nodes don’t improve  performance on desktop processors when clock speeds and core counts stay the same. A 10nm monolithic ARL could have performed better and cost them less, although the newer process does improve power consumption which is essential for laptops.",Intel,2026-01-05 23:55:52,2
AMD,o1y3fmp,Stopgap design and their first major foray into chiplet. I think they learned a lot of things on the way and Intel is famous for their processes getting better with age after a shrink. Compare Alder Lake to Raptor Lake. Compare Broadwell to Comet Lake.,Intel,2026-01-27 02:54:11,1
AMD,nxum4ri,"And nobody was saying you're required to. The entire point of the post was to say regardless of your thoughts and purchasing habits, ARL was deliberate, and even smart. Those who look for single metrics by which to judge ARL are missing the point.   Yes, if that single metric is all you care about, by all means go spend your money elsewhere, but ARL is a step sideways so future generations can take leaps forward.",Intel,2026-01-05 17:25:38,8
AMD,nxtkrhd,"This is way too true. I want a healthy Intel and AMD, but I'm also not going to act like Intel being now backed by the US Government and MAGA, as well as securing a well funded partnership with nVidia, doesn't make me feel a lot less like they need consumers to pity buy things from them to encourage innovation.",Intel,2026-01-05 14:24:59,6
AMD,nxuiree,">In highly parallelized rendering workloads like Blender or Cinebench, the 24-thread Arrow Lake design is often matching or beating the 32-thread Raptor Lake parts, which proves that the removal of Hyper-Threading was not a net loss for total throughput  So matching perf with a last gen part, after you hit a double node shrink **and** a massive E-core IPC gain and a P-core tock too is fine?   >The ""rent"" paid in silicon area for HT was no longer worth the ""yield"" in multithreaded performance,  This was a mistake according to LBT himself.   >This implies that Intel’s next step must be an aggressive overhaul of the interconnect topology, perhaps moving towards a mesh or a more direct active interposer solution for desktop parts if they want to reclaim the gaming crown from AMD’s X3D parts  Moving to a mesh wouldn't help much, and Intel's mesh's have a reputation for being insanely slow on their Xeon parts.   How much more advanced packaging does Intel have to use to match the latency of AMD using iFOP?   > But if you analyze the architecture, the Lion Cove P-core is a marvel of width and prediction capability that is simply being strangled by the packaging logistics  It's not. LNC is both not that all that wide, all the ARM cores beat it in that metric, and the prediction capabilities of LNC is bad- it's a literal regression vs RWC (last gen) in accuracy. It's worse than the E-cores branch prediction accuracy. And it's much worse than Zen 5's as well.   >and the floating-point performance is stellar.  This specifically is not the case. While in previous generations Intel was very competitive with AMD in spec 2017 FP, with ARL vs Zen 5 we see an almost 15% gap.   >The 285K is the cooler, more efficient, strictly professional grown-up in the room that unfortunately forgot how to play games because it’s too busy trying to figure out how to talk to its own memory controller across a microscopic bridge.  Idk why you are trying to trivialize gaming when it pushes a huge percentage of the market, and is why Intel has been repeatedly telling investors they have lost the high end DT market.",Intel,2026-01-05 17:09:57,6
AMD,nxtic59,Yes because they do core to cores transfers via their shared l2 rather than the l3/ring. It's sick. Skymont in general is so underrated for how enormous of a performance gain it was. They literally fixed all the ecore issues it's the pcores that flopped,Intel,2026-01-05 14:11:26,17
AMD,nxu08ua,"As someone who “writes like AI” in part because of a learning disability that made it hard for me to write, I tend to organize my thoughts very deliberately. Using lots of punctuation, dashes, etc is now often interpreted as having used AI, although I have no idea whether OP did or didn’t.",Intel,2026-01-05 15:43:44,10
AMD,nxviton,">this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)  It doesn't point out a bunch of BS, it introduces a bunch of new BS that is just straight up, factually wrong.   >i would agree this architecture is very much limited by d2d  Not single core performance like he is implying it is.   Just look at ARL-H vs MTL-H for example.",Intel,2026-01-05 19:53:36,3
AMD,nxv2i2x,"Soon you will not be able to tell what is AI and what was before, where you were living in the Stone Age. Fooz better buckle their seat belts and get rekt, it’s about to get a Bit-Funky.",Intel,2026-01-05 18:39:29,2
AMD,nyi3r20,"\> For experienced users, RPL CPUs’ temperatures can be lowered by 20°C by turning Hyper-Threading off     \> 265k vs 9700x 23% faster   \> 5 games    You are cute.",Intel,2026-01-09 00:12:42,1
AMD,o0ie5bv,at least you have proved that i did make the right choice on my 265k,Intel,2026-01-19 17:29:31,1
AMD,nxzan7c,AMD Unboxed are not serious and should be ignored at every opportunity. They have a long history of doing what you've said they've done to you. They get in childish arguments on twitter when they aren't blocking people who have data that disagrees with their clear bias.,Intel,2026-01-06 09:33:39,1
AMD,nxu6lhk,I have one 13900k since day one... But like all enthusiastic guys I did some benchmarks and the voltages were a concern. So I tuned the bios after just a few hours of working.  3 years have passed and I have 0 problems. It's like a rocket 🚀 very fast and reliable,Intel,2026-01-05 16:13:24,7
AMD,nxzllb5,"Very nice, do you also use a contact frame?",Intel,2026-01-06 11:12:32,1
AMD,nyi50z0,> so my chip never tried to boost 5.9Ghz with crazy voltage.  You have no idea what cause degradation and undervolting does not save CPU.,Intel,2026-01-09 00:19:10,-1
AMD,nxwmvqc,For sure.  I still use 10900k which benefits massively from memory frequency. When running 4500 cl16 it sits in 33-36ns territory. Lowest latency cpu around was 2020.  Doesnt hold up vs today's cpus but the user experience is great.,Intel,2026-01-05 23:03:28,1
AMD,nxxq0wj,"Really, 90ns on DDR5-3600, what else can go wrong here...",Intel,2026-01-06 02:31:08,1
AMD,nxx7vlh,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:14,1
AMD,nyf7a0y,"""Wait till games leverage multicore"".",Intel,2026-01-08 16:22:24,1
AMD,nxz46az,"Could also add 1440p into the mix, at max settings with ray tracing, the CPU is going to matter less and less.",Intel,2026-01-06 08:31:05,1
AMD,nydtdxm,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-08 11:58:45,1
AMD,nxuz1x1,"Are you using a B580, I’m wondering how it performs.  I have a 5070 OC and an army of Alchemist and Battlemage cards.  I may try to get a 9070 this year and OC / solder it to XT performance. There is a great deal of fake hype around the 9070 XT, Lisa and her army of gawkers really pulled out the hype train for the 9070 XT.  Many influencers (somebody who can post a video to snoozetube) received 9070 XTs for free so they could gimp primp them out to the masses. It has left a stale, rotting smell in my loins.",Intel,2026-01-05 18:24:04,2
AMD,nxwegrj,"Did you used just ""stock"" profile ie NGU and D2D at auto and Intels default profile? Or Intel 200s boost?",Intel,2026-01-05 22:21:22,1
AMD,nxuzm9f,"Both 14900K and 285K won't get frametime spike when the L3 Cache is maxed and no random stutter/issue.  If I want a good gaming experience, I would go for the 14600k instead of the 9800X3D. Way cheaper and works way better.",Intel,2026-01-05 18:26:34,1
AMD,nxvwkhz,"These are the bios overclocking settings, ram speed is 9000mhz, d2d die2die is 3600mhz, ngu chip fabric 3400mhz fully manually tuned",Intel,2026-01-05 20:57:48,5
AMD,o36dni3,"Honestly I don't feel any difference, both are max tuned and there's no stutter or dips, 285k is the one I prefer, lower power and temps for about the same performance. And probably 285k is the winner for stability also.",Intel,2026-02-02 16:15:47,1
AMD,ny1yrwf,"> New process nodes don’t improve desktop performance when clock speeds and core counts stay the same.  This is correct. The node shrink itself does not give IPC gains, what it gives is energy efficiency and area. The reason why people associate node shrinks with better performance is because when they switch a processor to a newer node they typically increase cache sizes and/or improve base/turbo clocks.",Intel,2026-01-06 18:43:59,2
AMD,ny0mjxi,> New process nodes don’t improve desktop performance when clock speeds and core counts stay the same.  Netburst was awesome!,Intel,2026-01-06 15:03:34,1
AMD,nxvi8ry,The problem is that MTL for all means and purposes should have been that test bed product. Or even lakefield tbh.,Intel,2026-01-05 19:50:56,5
AMD,nxxqpo4,"Nothing smart about it.. they just couldn't do any better than release a half unfinished product because the company is corrupt as hell, fully relying on US gvt injecting  tons of money that ends up in a few top manager pockets instead of being used for restarting the core architecture from scratch.",Intel,2026-01-06 02:34:57,1
AMD,nxubozd,"We were backed strongly by Biden admin too, the CHIPS act money was what Trump gave us, but demanded the stock in return instead.   Which I think is ultimately good for us American citizens. Too many times companies just got hand outs, even Bernie Sanders approved the Intel stock deal. https://www.reuters.com/world/us/us-senator-sanders-favors-trump-plan-take-stake-intel-other-chipmakers-2025-08-20/",Intel,2026-01-05 16:37:08,14
AMD,nxu3xx9,Brotha everyone is tryina get a piece of that maga pie. Or vice versa. It would probably be unlawful to go against maga on fiduciary responsibility alone,Intel,2026-01-05 16:01:00,3
AMD,nxtmb1f,This!,Intel,2026-01-05 14:33:25,7
AMD,nxv74ph,"On my 13600k I also see a shared L2 for each 4-core E-core cluster. Was there a regression between then and now that they've resolved? Or is there some hidden behavior where this shared L2 couldn't actually be used to core-to-core communication without going through the ring?  If you have a source with more info, I'd greatly appreciate it.",Intel,2026-01-05 19:00:02,2
AMD,nxvzfjz,"AI is the aggregation of all the rules and examples fed to it.  You write according to proper ""rules"" or clear organization (which is very much not ""vernacular"" level writing), then boom, you and AI aren't all that different.  It's bloody annoying to try organizing thoughts or points to be easily digested instead of a wall-of-text like you've been doing since Mavis Beacon taught you typing only to have people bitch about the style and ignore the content.",Intel,2026-01-05 21:11:12,3
AMD,nxv1yn0,There are so many of us lol,Intel,2026-01-05 18:37:05,0
AMD,nylpv7p,5 CPU bound games enough.,Intel,2026-01-09 14:41:06,1
AMD,nxzllwo,Do you use contact frame for the cpu?,Intel,2026-01-06 11:12:41,1
AMD,nxzluhl,"yes, i do",Intel,2026-01-06 11:14:44,1
AMD,nyi5ufb,"Please, High life form, what causes degradation? enlighten us, mere mortal",Intel,2026-01-09 00:23:20,2
AMD,nxv14y0,I do have a B580 but haven't paired it with the 265k cpus yet. The B580 is currently in a i5-13600 system.,Intel,2026-01-05 18:33:23,2
AMD,nya2yya,I have a 265k and b580. Had it about 6 months. First desktop I've had in about 20 years. Works flawless for everything I need. Very stable. Very fast for my needs.. I pretty much only play warcraft though.,Intel,2026-01-07 21:44:30,1
AMD,nxwoxfl,"Stock. One system uses a Z890 mb with 8200mhz cudimm and the other uses a B860 MB with 8000mhz cudimm. I tried 200S boost on the Z890 and it didn't benchmark much faster at all for the games I play, plus I had occasional lockups. It wasn't worth leaving it on for me so everything is at the Intel default profile.",Intel,2026-01-05 23:13:58,1
AMD,nxzc32p,Lmao,Intel,2026-01-06 09:47:24,4
AMD,nxv0qyc,"Lol, Userbenchmark guy making things up.",Intel,2026-01-05 18:31:40,15
AMD,nxw9ntc,thank you,Intel,2026-01-05 21:58:18,2
AMD,o36h3on,any tips/resources to tune/troubleshoot? ive been chasing stutters from 2 different 14th gen systems and no luck lol,Intel,2026-02-02 16:31:42,1
AMD,ny41irt,"Clock speeds haven’t meaningfully improved since 32 nm Sandy Bridge. The 2500K and 2600K could overclock to around 5.0 GHz, and modern CPUs still run at roughly the same frequencies. Core counts are also similar—Intel’s 14 nm 7980XE had 18 Skylake cores. Cache increases are possible on older nodes as well, so newer process nodes mainly improve efficiency these days, which is contrary to what most expect of them.  A 10nm monolithic ARL could have performed better at least in gaming on desktop.",Intel,2026-01-07 00:40:20,1
AMD,nydvpul,Apart from straight up increasing frequency improved nodes also make improvements to CPU possible even at the same clock.,Intel,2026-01-08 12:15:24,1
AMD,nxvyiba,"I suspect they just didn't have enough time to change anything. MTL releasing in Dec 2023, internal testing I'm sure yielding some set of data, and external reviews giving other feedback, even by the release of MTL, ARL has probably been in the pipeline for years and probably locked into certain design choices regardless of the feedback and testing.  Additionally I suspect that on mobile chips/laptop gaming rigs there's less focus on each individual part because  a) few people hardcore game on laptops b) the latency can be blamed on other things since laptops are a prepackaged consortium of parts and it's harder to isolate, and  c) therefore laptops tend to be evaluated as a whole rather than the individual pieces that comprise them.   Therefore the ""latency issues"" only became a massive kerfuffle when the offending cpu could be isolated and tested alone, and reviewers needed something to complain about.   That's even if Intel was considering latency as the issue everyone made it out to be. Intel could have looked at it and considered the latency worth the cost to improve in other areas and serve as intels seminal desktop entry utilizing disaggregated silicon. Then gamers came and lost their shit that the newest Intel chip deprioritized something that impacted their precious fps- even though the impact was ""the new one is approximately the same to maybe a little worse as last gen in most games"".   Notwithstanding the fact that the 200 series chips retain healthy gen-to-gen perf uplift and a _massive efficiency improvement_ in productivity and general computing, and boosting the NGU and D2D clocks (which are _very_ low from the factory, and can be done with the app that Intel _has specifically designed for tuning their chips_ and is freely available (XTU)) brings the gaming performance to ""approximately the same to a little better in most games"". Contrary to what some people may think these chips are not solely or even majority used for gaming and there are other use cases Intel can to think about/chose to prioritize.   To be fair, gamers and tech enthusiasts are the ones who will care the most and therefore have a disproportionately large impact on the kind of publicity the chip will get. So, was this intels smartest PR decision? Maybe not. But I think it was still a sound engineering decision, regardless of whether or not they had feedback or data on the issue, even if it didn't go over very well with their loudest customer base.  And this is all overshadowed by the fact that if you stuck even the most hardcore of gamers on a blind trial and told them to identify what kind of chip they were gaming on, I have a hard time believing any of them would be able to tell with any certainty which is which.   Once you get to 60 fps, the vast majority of people stop caring. Whats five fps going to do to radically change your gaming experience so much that it's unplayable? Which brings me back to one of my original points: If you care _that much_, you can go spend your money _elsewhere_.",Intel,2026-01-05 21:06:54,3
AMD,nxun1ug,"I'm aware of CHIPS.  Like I said, I want Intel to be competitive. I hope if their new layouts mature into something that can retake marketshare that the same care will be taken to preserve AMD. AFAIK, no one came to lend them money or sign large multifaceted partnerships at that time. This ""must save Intel,"" movement is something that is borne out of how big Intel is. They're too big to fail, it would have too many implications for the economy. It's not tit for tat, but as they are both American companies and AMD has significantly more invested in my country, I don't feel particularly motivated to help bail out Intel when they have the equivalent of the iron rice bowl rolled out for them at the moment. I'll buy what works best for my needs at the time I'm buying, as always, and if Intel can make a better gaming focused, with some imagery stitching on the side, processor than AMD can at the time I'm buying, that's the direction I'll go.  Ultimately the 200 series is what the Ryzen 1000 series was, but more stable as Intel has always had better support both internally and from vendors. I have no doubt Intel may catch AMD. My worry will be, what happens when they put their boots on AMDs throat, especially now that nVidia has more control over the situation than ever before?",Intel,2026-01-05 17:29:53,2
AMD,nxxrp83,Pouring free gvt money into Intel is only making the company high managers less interested in restructuring and improving the company when instead they can just keep going at snails pace while cashing in. Gvt money is only supporting growth of corruption inside the company and stalling progress.,Intel,2026-01-06 02:40:23,1
AMD,nxv6i3j,"It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor. It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.",Intel,2026-01-05 18:57:13,1
AMD,nxty6pw,I upped the tREFI of my memory in my MSI Vector with Intel Ultra 9 275HX and got some fps gains in Fortnite/Valorant/Hogwarts Legacy at 1200p,Intel,2026-01-05 15:33:56,2
AMD,nxv7w1f,Yeah pre arrow lake it was particularly shit as it would instead go through BOTH l2 and then to l3 to communicate between cores like a shitty ISP route to a game server   That is obviously worse than normal but core to core communication being done through even shared l2 is very rare so even without that quirk it's not expected  Go to chips and cheese.com they have a ton of information about this stuff. Like their skymont article in this case it details all the huge upgrades,Intel,2026-01-05 19:03:27,3
AMD,nxx7gwg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:51:03,1
AMD,nylunna,Every respectable benchmark is CPU-bound (including HUB whom you try to diss) because every sane reviewer uses a fat 5090 and 1080p to show CPU differences.  Even if you did not cherry-pick the games the review with fast GPU and more games is much more indicative of CPU performance than your benchmarks.,Intel,2026-01-09 15:04:11,1
AMD,ny47bmi,No I don't need it... You need a 360mm AIO that's all... After that it's all about bios settings.,Intel,2026-01-07 01:11:04,1
AMD,nxzm8r1,which brand do you use?,Intel,2026-01-06 11:18:02,1
AMD,nyi87lr,"1. tvb working incorrectly 2. unlimited current set by motherboards which gets only higher when undervolted because same power limit at lower voltage necessarily means more current   Motherboards were undervolting CPUs with wrong LLC calibration intentionally   4. ~~motherboards having wrong LLC calibration~~ EDIT: Intel CPUs themselves requesting abnormal voltage in anticipation of frequency boost resulting in abnormal idle voltage (just remembered it correctly) 5. motherboards having wrong LLC calibration resulting in abnormal load voltage  Not a single smart ass on the planet could have predicted that all of the above can happen with ""stock"" settings and foresee all of this.  No frequency limit will save CPU when it dies in idle state.",Intel,2026-01-09 00:35:27,-1
AMD,nxyej0v,"Thanks so in short:  You use only ""intel default profile"" and enabled XMP on your CUDIMM's right?   No further manual tweaks under NGU or D2D and RING values or any other critical tweaks pertaining to voltages no?",Intel,2026-01-06 04:57:20,2
AMD,o1y237m,"If you don't understand overclocking and don't see how a CPU with multiple millimeters physically between the IMC and cores can not possibly perform as well as the monolithic CPU I don't know what to tell you. Fully tuned 285K and 9800X3D benchmarks exist btw, check Blackbird PC Tech. 14900K is the best 1440p CPU and the 285K is the best 4K CPU.  Benchmark bar graphs of 10 games aren't the whole story anyway. There are far too many variables in various applications which will make the IMC island CPUs perform worse than the monolithic CPUs, sometimes substantially. It's enough for some of us to want to stay on monolithic just so that it always works. AMD knows this, next gen they won't put the IMC so far away and their die shots will look more like Arrow Lake.  For now, 50% more power consumption on 8 P core loads vs 9800X3D is a price I'm willing to pay. I don't want an 8+8 core CPU either so AMD doesn't really compete with the 24 core monolithic 14900K currently.",Intel,2026-01-27 02:46:57,1
AMD,o36t36x,"Sync all cores 55/57 depending on your cooler, undervolt also, ram oc is the most important for intel, frequency and tuned timings.",Intel,2026-02-02 17:26:56,1
AMD,ny5j3g0,"Bruh my 2600k couldn't hit 5ghz. I struggled to get to 4.4. same with my 5820k, which would do 4.2, both with voltage bumps and good cooling.   Modern processors definitely run faster. The 12900k in my media pc will happily sit at 5.2 on lightly threaded loads   Tho you are right about the core counts. High core numbers have been around for eons, they were just far too expensive for mainstream use",Intel,2026-01-07 05:57:02,1
AMD,nxuzom9,"It's because AMD doesn't manufacture chips, we do. That's what the funding was for, to revive manufacturing leadership in the US not to save failing architectures.  And yes as a consumer buy what works best for your needs and budget. That's the best thing about Ryzen and AMD's resurgence.  I don't get your ""boots on throat"" comment, but to think AMD hasn't done anything mischievous in the past is, well a lot has happened between the two companies in 40 years.",Intel,2026-01-05 18:26:52,5
AMD,nxv19d7,"A strong Intel IFS is a strong US. Many people get disillusioned and deceived through all the cognitive dissonance on social media, especially gamers. Unfortunately they are easy to manipulate.  Many people are buying INTC in the US to retire on, we will see this more and more as we approach 2030. INTEL IFS has to succeed otherwise the US will be doomed in this century. Even India is getting into the semiconductor industry now and Intel is working with them. We need IFS to be on top, cream of the crop, I need a taste.  You are defeating yourself by getting wrapped up with all the geopolitical propaganda, go take a walk in the woods and get away from it all.  Checks are in the mail.",Intel,2026-01-05 18:33:57,0
AMD,nxypf2c,"What? The deal under Trump was for 10% of the stake in shares. The government can sell the shares at any time to get a return. Since the stock has doubled since, it looks like it was a great deal for tax payers.",Intel,2026-01-06 06:19:05,2
AMD,nxv1ubv,Which way is up...?,Intel,2026-01-05 18:36:33,1
AMD,nxw36zg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.  Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2026-01-05 21:28:35,1
AMD,nymqb9s,"Here are few more games  [https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U\_](https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U_)  [https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI](https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI)  [https://youtu.be/mQ80rNg0k3c?si=sgXQJ\_kh0Nb22BIM](https://youtu.be/mQ80rNg0k3c?si=sgXQJ_kh0Nb22BIM)  [https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU](https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU)  Non 3ds are bad for gaming, thats a fact.  Here is my latest test for 7800x3d vs 14700k  [https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2](https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2)",Intel,2026-01-09 17:27:05,1
AMD,ny1tgj8,thermalright.   Any frame will do. get whatever is cheap,Intel,2026-01-06 18:20:04,1
AMD,nyimhf2,"Let me explain why it will not degrade my CPU.  1. **TVB (Thermal Velocity Boost) is disabled when I set the turbo limit.** 2. **Current is not the issue.** First, even Intel chips were failing at idle. Second, you stated that ""the same power limit at lower voltage necessarily means more current."" That is incorrect, if it drew more current, then why does lowering voltage reduce power consumption? Have you missed basic physics? 3. **CPUs follow a voltage-frequency (V-F) curve.** The main issue was that either the CPU or the motherboard was supplying excessive voltage, or the CPU was requesting too much. Higher voltage is required for extreme single-core boosts, such as 5.9 GHz. This is why i3 and i5 CPUs were not affected. When I limit my boost and apply an undervolt, the CPU no longer requests high voltage. It might request slightly higher voltage for 5.5 GHz, but that is nowhere near the voltage required for a 5.9 GHz boost. 4. **The same principle applies.**  Its seems you are a failure of high life form.   Maybe read here more. it was already to high voltage   [https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239](https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239)  Maybe you havent read much. if you undervolted only using AC/DC loadline and didnt limit max frequency. Then your CPU could have degraded.   While I used offset and set the max turbo limit.  I know how silicon works. I tune every CPU and GPU I buy.",Intel,2026-01-09 01:50:53,2
AMD,nyx4wv5,"Yeah, I do use XMP to get 8000/8200hz but no tweaks and everything is Intel default.",Intel,2026-01-11 04:51:00,1
AMD,nxx7x5q,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:28,2
AMD,ny6b0of,"Probably just bad chips. In almost every review, 4.7–4.8 GHz was achievable on the 2600K, and 4.6–5.0 GHz on Haswell-E. 5.0ghz wasn't hard to do on 8700k-9900k-10900k. Also, ARL runs at lower clock speeds than RPL, so in some cases newer process nodes actually clock lower.   this happened many times before like 32nm SNB OC better than 22nm IVB and haswell or even 14nm 6000 series skylake, so no in most cases new process nodes don't improve clock speeds or at least not on desktop.",Intel,2026-01-07 10:04:24,0
AMD,nxv4op7,"Okay, so we've gone right off the rails of a logical discussion. I fail to see how we went from talking about a failing architecture, which I don't think of Intel's product line as, to the whole bailout discussion. Maybe because I said AMD didn't get bailed out? I think maybe you forget that AMD had to offload global foundries, I don't think anyone even blinked when that happened and it's likely because that was a different era.   My post had nothing to do with the bailout versus architecture (or saving it) , apart from mentioning that I don't believe as a consumer I should feel motivated to buy Intel over AMD, or any other American firm, at the moment.   To be very clear, as this is personal for you, I do own Intel equipment including an Arc graphics setup for one of my children. I'm not anti-Intel or anti-American at all.   I know the history between AMD and Intel fairly well. Oversimplifying, AMD as it is doesn't exist without Intel. AMD only grew because they were the most successful second source producer for Intel, and the most successful at riding the thin grey line between patient infringement and unique implementation of similar IP that kept them alive while everyone else in the X86 space either died or became irrelevant to the consumer or enterprise space. After Itanium, the story levels out with both companies becoming effectively unwilling AMD64 codependents, and I'm saying that humorously.   Intel would gladly own the X86 market outright. So would AMD. At the end of the day, we need the two driving innovation through competition. Even if the CHIPS Act, the government stock acquisition, and the nVidia partnership are solely aimed at bringing more, needed, chip manufacturing to the US, it could create a situation where that amount of leverage puts Intel into a hyper dominant position again in the near future. Honestly, I hope I'm wrong.",Intel,2026-01-05 18:49:13,2
AMD,nxwdvf9,"Hey sorry to interject like that, can you ask around what ppl in the team think the safe VCCSA voltage for raptor lake is beyond standard spec? You can dm the answer if u want. I'm from Russia so it's not like I'll go run RMA'ing this stuff just because you told me that info",Intel,2026-01-05 22:18:29,1
AMD,nxv1r9g,"Did you even read my post, or are you a bot? Seriously asking. I didn't bring up geopolitics. I live in Canada my dude. AMD has their graphics office still right beside the TO airport, and that's only a side comment. AMD is American too. My concern isn't about anything you just said lol.",Intel,2026-01-05 18:36:10,0
AMD,nxv9flb,65535,Intel,2026-01-05 19:10:25,1
AMD,nyk009z,"\>then why does lowering voltage reduce power consumption?  I have no idea what is the workload you are using which is guaranteed to never hit the limits. Solitaire?   Undervolting does not guarantee lower power usage with modern boosting CPUs. It very obviously (to any sane person) depends on workload.  \>Maybe read here more.   \>Thomas\_Hannaford, Employee   ‎ You are so cute.  \>Motherboard power delivery settings exceeding Intel power guidance.  Literally what I said in 2, 3, 4 but in stupid terms.  \>Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity.  Literally what I said.  \>Intel® reaffirms that both **Intel® Core™ 13th and 14th Gen mobile processors** and future client product families – including the codename Lunar Lake and Arrow Lake families - are unaffected by the Vmin Shift Instability issue.  This is verifiably bullshit because nothing about mobile processors protects them from wrong voltage received because of wrong LLC settings.  \>CPUs follow a voltage-frequency (V-F) curve  If you do not know that Intel is requesting high voltage to avoid insufficient voltage before frequency boost you do not know jack shit. It is literally what happens under description ""**Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity**"" and there is a video proof of that with an oscilloscope.  No settings which you mentioned are preventing irreversible damage.   Claiming that you use Intel for reliability after you did not wish to use your own Intel at stock settings is laughable.",Intel,2026-01-09 07:01:30,0
AMD,nyjz7xj,"You skipped the 8600K, it also did 5ghz",Intel,2026-01-09 06:54:55,1
AMD,nzt2k76,"Anything over 4.7ghz on Haswell-E was NOT very common. I would say 4.4-4.5 on those chips was more realistic. 5 GHz we are talking golden godly chip.  I had a 5960X for 8 years, one of the better ones manufactured in Costa Rica and not Malaysia which was better for OC, and mine did 4.7 ghz.",Intel,2026-01-15 21:49:25,1
AMD,nxv805h,"My initial comments were on, ""I'm also not going to act like Intel being now backed by the US Government and MAGA"" and what the funding was for.  Edit: and my response was about who also backed the funding.",Intel,2026-01-05 19:03:57,1
AMD,nxwhiw8,"All I can recommend and say is follow whatever guidelines you're given officially and make sure your BIOS is updated. All those teams work to make sure it's delivered to the customer. Otherwise it's all random, some parts can handle higher voltage, some can't.  The term silicon lottery is real and just a nature of small scale manufacturing, EM and quantum effects these days.  I will say, with our new CEO a lot of these customer issues are now streamlined internally. Used to be layers between engineering and customer interaction, so I expect better responses and reliability than before.",Intel,2026-01-05 22:36:31,1
AMD,nxv9vyk,"Ah, so you mean *lower* the TREFI then, from what JEDEC or XMP specified?",Intel,2026-01-05 19:12:28,1
AMD,nzv5tob,"I believe that refined process nodes (the “+++” stages) are what really improve clock speeds, not the first generation of a new node, or at least this is the case for the last 15 years.   Here are some examples:   -Intel’s 32nm Sandy Bridge came after the early 32nm CPUs (dual-core i3/i5 300/500 series and the 6-core 980X), and Sandy Bridge turned out to be an excellent overclocker.   -22nm Ivy Bridge and Haswell were mediocre in that regard, while Haswell-E (a more mature implementation) overclocked better. Broadwell-E and early Skylake also weren’t great for OC.   -Once the node matured, things improved again. Coffee Lake (refined 14nm) made 5 GHz relatively easy. The same pattern repeats with 10nm: Alder Lake (12th gen) came after Tiger Lake and could already reach ~5 GHz, while Raptor Lake pushed clocks even higher, up to ~5.7 GHz.   Overall, higher clock speeds tend to come from node maturity and refinement, not from the first use of a new manufacturing process, and I believe 5.8ghz could have been possible if intel used its own 10nm process node on ARL considering its HyperThreadingless.",Intel,2026-01-16 04:43:25,1
AMD,nxv944s,"On that side, it'll depend on what the US does with the stock it has. Canada bought GM stock during the 2008 crash, and then quietly sold it off as GM recovered. If the US does that, I don't really see an issue.  The MAGA part was a half hearted comment, made to follow the weirdness of the whole situation and comment I was replying on as well. As I said in a separate post:  ""It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor (speaking about how decisions beyond national security are now driven by politics and trying to be on the right side at any moment when it matters). It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.""  I'm going to add, light heartedly, I do hope we see AMD/nVidia/Qualcomm/etc manufactured and final packaged products coming out of Intel Foundries one day. I'm not sure how or if it will work, but the situation seems dead set not to allow significant further nodes beyond 2 nm or packaging to occur outside of North America. If the US wants viable national chip production, Intel is the better option, I hope it works out in a way that maintains design level competition while meeting national security goals.",Intel,2026-01-05 19:08:59,1
AMD,nxwo9wf,"I get that. But it’d be nice to have a “this voltage is safe for 99% of the cpus and this voltage is the LD50 for the cpu”. That would probably improve customer relations but your legal team might be very, very unhappy with that lol.   Anyway cheers for the response, with the way things are looking up for intel, i might be buying some stocks soon",Intel,2026-01-05 23:10:36,1
AMD,nxvnm9c,JEDEC 5600 cl 40 kingston fury sodimm 2x 16 gb,Intel,2026-01-05 20:15:57,1
AMD,nxws02u,"I mean, you can just figure that out yourself and buy new CPUs till you get one working :P",Intel,2026-01-05 23:30:06,1
AMD,nxvpz7b,"Yeah, but what was TREFI before you lowered it?",Intel,2026-01-05 20:27:01,1
AMD,nxvvs3t,10000 ish,Intel,2026-01-05 20:54:09,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
AMD,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
AMD,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
AMD,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
AMD,o29lgdq,"i've got an i7-14700kf on an asus rog strix b760-a with a corsair h100i elite capellix xt (240mm aio) and when i run after effects my cpu temps shoot up to 90 degrees, it probably wouldve gotten higher but i closed it cause it felt too high for what i was doing, i'm looking to undervolt my cpu but i dont know anything about it. i just want something safe and simple (im not looking for an extreme undervolt, just one that would lower my temps & possibly keep the same performance)",Intel,2026-01-28 18:56:54,1
AMD,o3j0qr3,"Killer Ethernet keeps asking me to update, i already uninstall, reinstall, safe mode and reformat my laptop and keeps asking me to update.   [Killer Ethernet](https://imgur.com/a/G3YuvJN)",Intel,2026-02-04 13:40:14,1
AMD,o3v6d3v,"Hi all, if i were to buy a series 2 intel CPU laptop with accompanying NPU and iGPU (likely arc 130 or 140). I wondered if anything mentioned on CES 2026 about the Panther lake software improvement is being guaranteed to get to the older CPUs via software/driver updates. I want to buy a cheaper older laptop now while still getting any supposed/promised gains from Intel than buy a panther lake laptop amid the nonsense ram pricings",Intel,2026-02-06 07:44:05,1
AMD,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
AMD,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
AMD,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!  You can find and apply for all of our jobs online at [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We don’t currently accept submissions via social.  Good luck!",Intel,2026-01-07 00:05:20,2
AMD,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
AMD,nya3rq0,Hi u/ConspiracyPhD **Post** a question on [Intel® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud) forum for further investigation.,Intel,2026-01-07 21:48:00,1
AMD,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for Intel® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
AMD,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is built‑in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intel’s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If you’re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to Intel® Platform Trust Technology (Intel® P…](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
AMD,o2kc1kd,"Individual\_War\_129, we do not provide typical temperature operating ranges for each processor or each core, as it can vary based on the system design and workload. Processors have internal protections to prevent against excessive temperatures. Operating ranges below the protection points are highly dependent on system configuration and workload.  In case you haven't come across it yet, you may check the articles below:  [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [What Is Undervolt Protection and How Does It Affect Overclocking in Intel® Extreme Tuning Utility (…](https://www.intel.com/content/www/us/en/support/articles/000094219/processors.html)  [Thermal Design Power (TDP) in Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html)",Intel,2026-01-30 07:31:28,1
AMD,o3o6m00,"nfsanton, please be advised that the product you are reporting is an OEM (Original Equipment Manufacturer) device. As such, our support may be limited, since we do not have full visibility into the specific technologies, settings, or customizations implemented by the system manufacturer on your device.  For laptop systems, we strongly recommend installing and using the drivers provided by the system manufacturer, as these drivers are customized and validated to ensure full compatibility with your hardware.  That said, you may also choose to use the Intel generic driver if needed, which is available here: [**Intel® Killer™ Performance Suite**](https://www.intel.com/content/www/us/en/download/19779/intel-killer-performance-suite.html). Please note that functionality and behavior may vary when using generic drivers on OEM systems.  You may also find this public article helpful: [Intel® Driver & Support Assistant (Intel® DSA) Keeps Showing Available Driver Update Notificati…](https://www.intel.com/content/www/us/en/support/articles/000090127/software/software-applications.html)",Intel,2026-02-05 05:53:34,1
AMD,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
AMD,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
AMD,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
AMD,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like it’s up and running. Could you try accessing it again using your Intel account?  [Intel® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
AMD,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
AMD,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
AMD,nz3b0gd,"u/ConspiracyPhD Please check your inbox, I’ve sent you a personal message. I’ve already coordinated your concern with the respective team, and as per their instructions, you’ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,4
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,16
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Intel,2025-12-19 09:41:00,6
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,12
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,11
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,3
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nym6t8e,"Hm, we will see what happens with the stock price soon, but so far so good",Intel,2026-01-09 15:59:57,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-17
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,11
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,16
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,4
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,10
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month 😂,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Intel,2025-12-19 05:37:45,1
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 🤞",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,3
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,2
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,5
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,4
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,8
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,14
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,6
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,6
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250M Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,3
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,9
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,4
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,3
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,-1
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,1
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,3
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,2
AMD,nuu7dr5,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,2
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed 😔,Intel,2025-12-24 15:43:21,2
AMD,nybmp7b,did you buy one? I have owned two. A A750 and now a B580. They are great cards for the price paid. I'd like to upgrade to a B60 PRO. 24GB VRAM sounds amazing especially for $600. but I can't find one in stock anywhere.,Intel,2026-01-08 02:20:39,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,56
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,9
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,5
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,2
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,7
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,2
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,28
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,27
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,8
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,7
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme 😘,Intel,2025-12-11 20:12:02,5
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,4
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,3
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Intel,2025-12-11 21:24:52,13
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,4
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,6
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,52
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,10
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,5
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,11
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,6
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,1
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,1
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-14
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,8
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,8
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isn’t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,o2qkn2t,Omg a positive review for Intel Lunar Lake 😭  https://www.reddit.com/r/laptops/s/RYInPJfnAd,Intel,2026-01-31 04:40:30,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,4
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,1
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,-1
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,29
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,19
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,5
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,4
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,4
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-10
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,o2ql7sb,Panther Lake fixed it,Intel,2026-01-31 04:44:35,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,8
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,5
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
