brand,comment_id,text,subreddit,created_utc,score
Intel,nzd1gfc,"that amt of hair in a clean room, steve should wear a hooded rain coat lol.",hardware,2026-01-13 15:00:01,120
Intel,nzcvzjv,I thought that was Denis lol,hardware,2026-01-13 14:32:09,48
Intel,nzfe7p7,"Pretty good video with a lot of detail I haven't seen in previous factory tours. I probably still won't use an Intel GPU for gaming, but for a media server it's probably sufficient.  Side note: It's funny how whenever a GN video gets posted, the same cast of characters comes out and writes essays criticizing the video, apparently without watching it. I guess that's what it means to ""make it"".",hardware,2026-01-13 21:41:25,7
Intel,nzd7t05,"**GN:** NVIDIA and AMD abandoned this segment!  #Reality:  **B580:** $249 USD  **9060 XT 8GB:** $299 USD  **9060 non-XT:** $259 USD  **5060:** $299 USD  **5050:** $249 USD  I wouldn't say this is abandoned. I will say though the actual factory tour is cool, great content. But a dumb headline/title for the video.",hardware,2026-01-13 15:31:02,95
Intel,nzd3sjp,"Yeah this is a really surprisingly open factory tour, at least compared to when Sapphire took Linus and Alex along. It feels unprecedented to see this much access and insight, but that's the goodwill nurtured by Steve paying off in spades. The lament about AMD and Nvidia ditching the sub-250 side is real, so having Arc actually be a usable option down here is going to be significant later on.",hardware,2026-01-13 15:11:40,23
Intel,nzf5m3z,"Amazing video, GN is so good with content like this",hardware,2026-01-13 21:01:37,7
Intel,nzgtt65,"Love the Sparkle heatsink aesthetic, just wish there were some higher end card offerings from them instead of just Intel's lower midrange GPUs.",hardware,2026-01-14 02:14:38,2
Intel,nzd8jmh,"rtx 5050 149 mmÂ² 128bit has similar performance to B580 272 mmÂ² 192bit. Selling for same msrp, winning by not ""trying"". Love how 5050 is actually at msrp now cheapest b580 is $349+  ""intel's gpu division seems like the only place in tech right now where the customers arent getting shafted these days""  People just want their Nvidia gpus cheaper  Nvidia crashouts making people hype an even worse product at current prices lol  [https://imgur.com/a/8iRI87Q](https://imgur.com/a/8iRI87Q)",hardware,2026-01-13 15:34:35,11
Intel,nzd585j,He probably just single handedly killed 6 wafers worth of intel GPUs.,hardware,2026-01-13 15:18:37,3
Intel,nzd6pvl,AKA 'NVIDIA and AMD are going where the real money is.',hardware,2026-01-13 15:25:50,-1
Intel,nzcwvps,Finally a video that isnâ€™t â€œAI badâ€ or â€œcompany X badâ€,hardware,2026-01-13 14:36:46,-21
Intel,nzdfxes,That hat is so useful,hardware,2026-01-13 16:08:37,-4
Intel,nzdz3jo,Making them isn't important selling them is.  The reality is that this segment doesn't actually exist it has no buyers in it.  Seems intel is truly doomed trying to win segments that if they even exist aren't big enough to pay back their R&D even if the dominate them.,hardware,2026-01-13 17:48:25,-7
Intel,nzhjk4i,"Woah, a GN video that is actually interesting and informative for once instead of just ranting about the fact hardware companies exist to make money, not please gamers.",hardware,2026-01-14 04:51:57,1
Intel,nzdogu2,"To be fair this is not a clean room, just SMT assembly. Basically, soldering components. A lose hair might be a bit of a problem (kind of like in any factory), but his hair is tied and doesn't seem to be an issue.  At an actual clean room (where the actual silicon is processed and etched), the standards can become very crazy. Some parts of it aren't even accessible to humans, just automated lines to avoid contamination.",hardware,2026-01-13 16:47:26,70
Intel,nzd8zmt,"Gaming Jesus could walk on wafers without corrupting a single tile, His body is that pure.",hardware,2026-01-13 15:36:40,-18
Intel,nzd5ucm,"same thought, just feels disrespectful",hardware,2026-01-13 15:21:37,-23
Intel,nzdzd0h,"He desperately needs to learn how to take care of his hair. I have no idea why nerds think a dry, frizzy mess of hair is some kind of enviable quality.   I feel like I'm walking into friday night magic every time he pops up.",hardware,2026-01-13 17:49:38,-15
Intel,nzd0sl2,"who knew he can into GPU manufacturing? I thought he was a video editor and first real ""why are you employed"" guy on LTT.",hardware,2026-01-13 14:56:41,11
Intel,nzhet37,"Intel hired him on the spot when they saw the ""Live, Laugh, Liao"" sign",hardware,2026-01-14 04:19:24,1
Intel,nzd9rkp,"Wasn't that the B310? That was supposed to be the $100 bracket. To be fair that's an almost useless tier nowadays because iGPUs are capable of similar performance or even outperforming the cards in those brackets. Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.",hardware,2026-01-13 15:40:18,51
Intel,nzgaa45,He(Lucas) stated $100 was the market that was abandoned.  10:37  > Steve: So why still making A310?  > Lucas: Because what's the competitor have? Nvidia? like... GT710? GT1030? (laughs) No way. So literally Nvidia AMD already give up the segment of this like... $100 price card.,hardware,2026-01-14 00:24:50,6
Intel,nzdtxiv,"They're talking about the A310 and A380, of which Nvidia doesn't have anything made in this decade to compete with and AMD has the 6400 that came out 5 years ago.  The only cards with modern features in that price segment that consumers can directly buy are the A310 and A380.",hardware,2026-01-13 17:24:22,24
Intel,nzfp313,This guy was talking about the A310 which is a $100 GPU. Basically said that the only other options at that price bracket are either a GT 710 or a GT 1030. And from AMD you can still get an old RX 550. The A310 may be slow but it beats those two gpu's by a mile.,hardware,2026-01-13 22:32:49,7
Intel,nzejfd4,Now do SR-IOV and 16GB RAM for under $400.,hardware,2026-01-13 19:18:41,3
Intel,nze3uy1,> 9060 non-XT: $259 USD >  >   LOL. Good luck finding it. It's OEM exclusive,hardware,2026-01-13 18:09:42,7
Intel,nzdcsu3,The title is a literal quote from Lucas,hardware,2026-01-13 15:54:17,17
Intel,nzejmb9,Found the guy who didn't watch the video and is making assumptions based on headlines.,hardware,2026-01-13 19:19:33,6
Intel,nzdb9mw,The title is in quotes. That is from Sparkle.,hardware,2026-01-13 15:47:16,7
Intel,nzenlzf,Go to r/PCMasterrace and they will downvote you into oblivion for even MENTIONING the possibility of gaming on a 5060 let alone 5050 XD,hardware,2026-01-13 19:37:44,0
Intel,nzdm48p,The title is just playing the YouTube algorithm game. It's stupid but they have to do it.,hardware,2026-01-13 16:36:44,-4
Intel,nze22z8,"> Love how 5050 is actually at msrp now cheapest b580 is $349+  Just an FYI but B&H has the [Acer Nitro B580 for $249.99](https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html) and the [Intel Limited Edition model for $259.99](https://www.bhphotovideo.com/c/product/1869297-REG/intel_31p06hb0ba_arc_b580_limited_edition.html). So you can get B580 for MSRP, but for how long who knows.",hardware,2026-01-13 18:01:44,19
Intel,nzdnidn,"die area is not the only cost indicator. B580 actually uses N5 fab, which is likely cheaper than N4, used by 5050. In reality, B580 only has about ~15% more transistors and if we assume N5 is cheaper per transistor and N5 has higher yields (by being more mature) i'd say their die cost might be very similar.  While yes, it has wider memory, the chips are clocked lower, so they can buy slower bins, reducing per chip cost.   B580 has more power draw, which in turn costs more for power delivery and cooling.  All in all, AIB manufacturer likely has lower margin per card as it stands, so i wouldn't be surprised if intel is taking a lower margin on the gpu/gddr combo to get more market share.  Nvidia on the other hand optimized their cost REALLY well, as they have been doing GPUs for almost 30 years.",hardware,2026-01-13 16:43:04,7
Intel,nzhfs1m,"Nvidia is getting such good performance out of such a small die because they're the best. Simple as. They've been doing GPU's for decades. It's not unexpected that at this stage Intel needs to use a larger die to match the performance - it would be incredibly surprising if that wasn't the case.  But a small *part* of that die size advantage comes from that narrow 128bit bus, and *part* of B580's appeal is its wider bus and subsequently more VRAM.",hardware,2026-01-14 04:25:57,1
Intel,nzd4pls,"Isn't the title pretty much ""AMD & NVidia bad""?",hardware,2026-01-13 15:16:07,50
Intel,nzd1pn1,Although even then the thumbnail is framed negatively.   But I much prefer these to Steves endless negativity ragebait.,hardware,2026-01-13 15:01:19,4
Intel,nzcyycp,> finally a company that isnt bad   ftfy,hardware,2026-01-13 14:47:24,-27
Intel,nzgji2p,"Did you even watch the video? they are selling, and they are selling out, so much so that they want to ramp up production so they can push out more.",hardware,2026-01-14 01:15:59,4
Intel,nzfbt5x,I do not know if the numbering scheme from my workplace is common across the industry but the smt assembly would be in a class 5 or 6 clean room and the fabrication itself would be a 1 or 2 class clean room,hardware,2026-01-13 21:30:27,13
Intel,nzglawf,"I think it still matters to a certain point, thats why everyone in the factory is wearing a hat. The Factory boss decided to roll RNG dice and say *""Fck it, that small hat is fine, even tho wearing it is pointless now; I'll just pray nothing bad happen*"". lol  What hilarious is when you think about what going through uninformed factory-employee's head, after they saw some guy(Steve) walk-into the factory like that. Definitely a lot of ""WTF"" moment going through their mind lmao.",hardware,2026-01-14 01:26:15,3
Intel,nzdb445,"The factory boss told me not to bother tying my hair (""ä¸ç”¨ä¸ç”¨ä¸ç”¨, æ²¡äº‹å„¿æ²¡äº‹å„¿æ²¡äº‹å„¿. è¿™æ ·å¯ä»¥çš„"") when I started to put it under the hat... and after asking for a larger hat or hairnet.",hardware,2026-01-13 15:46:33,121
Intel,nze10ie,"> same thought, just feels disrespectful  It's a good thing you were there to personally witness the interaction so that we'd all know exactly how disrespectful Steve was being before holding everyone at gunpoint to force them to let him shoot the video without first fixing his hair.",hardware,2026-01-13 17:56:59,7
Intel,nzdfs8o,Nothing really surprises me with regards to gamers nexus at this point.  Edit: Downvote me all youâ€™d like. Theyâ€™ve been leaning incredibly hard into the rage bait type content of late.,hardware,2026-01-13 16:07:57,-50
Intel,nze0k30,I'm sure getting beauty tips from random redditors who have never left the basement is of utmost importance to Steve.,hardware,2026-01-13 17:54:57,15
Intel,nze76iz,yeah there is a reason the a310 cards they are making are 4 hdmi out.  Gotta know your market,hardware,2026-01-13 18:24:10,15
Intel,nzdm2vc,"Might be just the thing for older machines and a GNU Linux (or BSD like) migration. Or as a pass through GPU to Jellyfin, Emby, or Plex for media transcoding. Edit: or Small form computing tied together with a iGPU enhancing game performance...",hardware,2026-01-13 16:36:33,11
Intel,nzeeiko,I assume those are being marketed to OEMs who make digital billboard systems or something. No idea why else you would want 4 HDMI ports on a card.,hardware,2026-01-13 18:56:32,4
Intel,nzddzc6,"> Wasn't that the B310? That was supposed to be the $100 bracket.  It's the A310, which is Alchemist and it's pretty much dogwater for anything beyond being a 'display out' card. The claim that NVIDIA has abandoned that segment is stupid... They've had offerings in this segment for years, plus anyone smart will just go buy a used GPU, your money goes way further. For example, the GTX 1650 performs basically 10-15% better, has better drivers, better encoding and generally is better supported. It's older, but I mean Alchemist wasn't exactly impressive either when it released and pretty much Intel has moved onto Battlemage and Celestial driver optimisations instead.   Plus let's be real here I went and searched and I found only weird places tend sell the brand new A310, the only local computer shop I found selling it in Australia for instance is a big one which is good surprisingly, but they had it for $189 AUD, a total rip tbh. A used 1650 is like $100 AUD and a used 1650 SUPER is like $120 AUD. No reason to buy an A310 tbh, pocket the cash and move on. Or if you're really intent on spending around that much buying a used RTX 2060 for like $20 AUD more, so a total of $200-210 AUD is better. Then on the AMD side you have the RX 6400 which had an MSRP of $159 USD and it's again a solid 10-15% faster, but much better off buying a used 6500 XT or 6600. Neither company has abandoned the segment, they had offerings for years and the used market basically obliterated any point to buying a brand new card like this.  >  To be fair that's an almost useless tier nowadays because iGPUs are capable of outperforming the cards in those brackets.  Yep this too. Honestly, I mean it's cool they're showing how they make cards on this factory tour, but to be like ""NVIDIA and AMD abandoned this segment"" is stupid when it comes to the A310. Almost anything these days is better than an A310.  >  Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.  GT710 hasn't made sense for like 8 years at least, even when it was relevant people laughed at it, but it did the job for 'display out' and such which was all that mattered. GTX 1630 was okay but it was supposed to be $149 USD MSRP and it came out for like $200 USD in most stores due to GPU shortage at the time, not much NVIDIA could really do about that.",hardware,2026-01-13 15:59:39,1
Intel,nzdyz47,They haven't made anything because the market has moved on. Intel might be making these but are they selling them?,hardware,2026-01-13 17:47:52,-12
Intel,nzdhs0m,"[Is it in reference to this moment in the video?](https://youtu.be/YwrUxG26ulk?t=648) If so, he doesn't say that as a literal quote, he says ""give up the segment"". Unless there's another quote somewhere else which I missed which may be possible or maybe it was edited out or cut from the video? I can't remember everything he said tbh but there was a lot of good information in this video and I think the title is better off without it. If it was called ""Intel Arc GPU Factory Tour with Sparkle"" I would have insta-clicked to watch anyways.",hardware,2026-01-13 16:17:04,15
Intel,nzdelag,You know what you're doing with the title... It's honestly unnecessary to use it on a factory tour video tbh.,hardware,2026-01-13 16:02:26,35
Intel,nzejqso,Stop defending your clickbait.,hardware,2026-01-13 19:20:08,14
Intel,nzf28eu,do you wanna address this then? Its kinda cringe ignoring the rest of the post  >**GN:**Â NVIDIA and AMD abandoned this segment!     >**B580:**Â $249 USD  >**9060 XT 8GB:**Â $299 USD  >**9060 non-XT:**Â $259 USD  >**5060:**Â $299 USD  >**5050:**Â $249 USD  Why include that in the title then too?,hardware,2026-01-13 20:45:49,-1
Intel,nzg52uc,"Obligatory ""lol stupid pcmr amirite"" comment.",hardware,2026-01-13 23:56:47,0
Intel,nzdplnj,"The RTX 50 and 40 series are using the TSMC 4N node which is a custom version of the N5 node for NVIDIA. But anyway the N5, N5P, N4, N4P, N4X are all 5 nm class node, so have around the same price for the wafer. And I wouldn't be suprised that NVIDIA is paying less for these considering the volume compared to Intel orders.",hardware,2026-01-13 16:53:27,15
Intel,nzdpf4g,Well their gpus are much more expensive than amd & nvidia who arent even trying. When they try Intel wouldnt even have a chance  Nvidia increasing their entry gpu volume  [https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026](https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026),hardware,2026-01-13 16:52:06,3
Intel,nzdqnhb,Yep,hardware,2026-01-13 17:07:48,4
Intel,nzdueea,Thanks Steve,hardware,2026-01-13 17:26:35,29
Intel,nzdyjcv,But you still didn't to say hi to me at PAX West 2016 in front of the LEGO USS Missouri battleship...,hardware,2026-01-13 17:45:51,2
Intel,nzdtnpi,"makes sense, I coulda been more charitable in the way I said it",hardware,2026-01-13 17:23:03,1
Intel,nze79m5,"relax dude, steve already replied, no need to whiteknight",hardware,2026-01-13 18:24:32,-7
Intel,nzdtsrk,"he replied in a comment to me to say that he was told to leave it alone, I guess assembly isn't as careful as the initial production is.",hardware,2026-01-13 17:23:45,7
Intel,nzdjfsy,"Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN, and their recent consumer advocacy and stepping on some very powerful toes, your comment sounds an ***awful*** lot like an astroturfing smear campaign meant to breed sentiments against Steve & GN.",hardware,2026-01-13 16:24:37,9
Intel,nze6q5q,Imagine defending the hair of a guy who looks like he judges anime conventions in his spare time.,hardware,2026-01-13 18:22:13,-6
Intel,nzdq8iu,"I get it, for those with old boxes. But intel has great transcoding according to self hosters, and the powr consumption is much better vs old i5 pairing with those dedicated cards.",hardware,2026-01-13 17:05:17,8
Intel,nzej077,day traders love having a zillion stock tickers running.  i'm sure there's more applications where a heap of monitors is useful.,hardware,2026-01-13 19:16:47,8
Intel,nzeua57,A lot of digital displays make use of DP MST to avoid the use of home run cabling.,hardware,2026-01-13 20:08:25,3
Intel,nzelrbi,The a310 and a380 are fantastical for a media server!,hardware,2026-01-13 19:29:14,12
Intel,nzdi8nv,Your wasting a lot of words defending a company about to rerelease a 4 year old GPU (3060) because they canâ€™t get memory for the current model.,hardware,2026-01-13 16:19:11,11
Intel,nzdjgo9,"Sure it was paraphrased for the title, but that's just semantics.   AMD and Nvidia ""giving up"" vs. ""abandoning"" the segment mean the same thing either way, given Lucas' intention behind the statement.",hardware,2026-01-13 16:24:44,5
Intel,nzejtyf,Did you watch the video? I'm thinking not.,hardware,2026-01-13 19:20:31,-2
Intel,nzfgwnz,He is ignoring the post because the poster didn't watch the video and is spreading BS. The segment they are talking about is $100 cards.,hardware,2026-01-13 21:53:45,10
Intel,nzdqmql,"maybe the cost for the raw wafer, but that's not all TSMC will charge nvidia for. You also need to account for yields, which could be different depending on the type of node.",hardware,2026-01-13 17:07:41,0
Intel,nze7n1w,But then I won't earn my free toaster after the 11th white knight attempt.,hardware,2026-01-13 18:26:08,-1
Intel,nzdxfeg,"> Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN  You have to be joking, right?",hardware,2026-01-13 17:40:46,7
Intel,nzdlwqs,Yeah. Iâ€™m definitely an astroturfing bot account. You got me. My profile certainly *reeks* of botting / astroturfing ðŸ˜‚,hardware,2026-01-13 16:35:48,-10
Intel,nze72qg,"Imagine being as shallow are you are while still posting on reddit behind an anonymous username.  Let's see how your hair looks, mate. You're giving off pure incel vibes here.",hardware,2026-01-13 18:23:43,9
Intel,nzdii5s,"You know I also talked about AMD right? Not just NVIDIA. Regardless, you think Intel isn't also going to have memory issues soon? They might just divert all memory they have to the SKUs that are selling.",hardware,2026-01-13 16:20:22,6
Intel,nzedwr3,>  a literal quote from Lucas  Does not line up with  > paraphrased,hardware,2026-01-13 18:53:49,9
Intel,nzdz5pq,Yields wouldn't meaningfully differ within the same family. Certainly not by enough to remotely cover for the die size difference.,hardware,2026-01-13 17:48:42,7
Intel,nzfzkw5,I'm starting to think that r/hardware is the circlejerk sub and I just haven't yet found the actual hardware sub that it's parodying,hardware,2026-01-13 23:27:14,1
Intel,nzdw1w0,You do it for FREE? Are you regarded?,hardware,2026-01-13 17:34:20,1
Intel,nzegtti,Intel B570/580 already use GDDR6 which is what Nvidia is trying to achieve with the 3060 release. Intel presumably won't be effected.,hardware,2026-01-13 19:06:58,3
Intel,nzel47j,"...Yes, that's why I said it's a semantics issue.  The meaning is the same: The paraphrased quote isn't a statement made by GN like u/KARMAAACS implies.   What makes it worse is that Lucas said that in response to Steve's question about ~$100 A310 cards and why they're still producing them (adding that they see a healthy demand for them from their customers).   It's like he didn't watch the video and just reacted to the title.",hardware,2026-01-13 19:26:20,-6
Intel,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,113
Intel,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,38
Intel,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,89
Intel,nysrktd,Iâ€™d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  Whatâ€™s the point of hardware if you canâ€™t apply it to what you need.,hardware,2026-01-10 15:26:18,13
Intel,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,18
Intel,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,28
Intel,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,31
Intel,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,8
Intel,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,15
Intel,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,11
Intel,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,6
Intel,nyrhl2f,Website doesnâ€™t load with adblocker,hardware,2026-01-10 10:04:14,4
Intel,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,4
Intel,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
Intel,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
Intel,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
Intel,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
Intel,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
Intel,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
Intel,nys0jf3,Needs a conroe vs fx62 moment. It doesn't look promising.,hardware,2026-01-10 12:46:32,-1
Intel,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
Intel,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-6
Intel,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-9
Intel,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,37
Intel,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,3
Intel,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
Intel,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,16
Intel,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
Intel,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,29
Intel,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,35
Intel,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,23
Intel,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,10
Intel,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,14
Intel,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,11
Intel,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,6
Intel,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
Intel,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,14
Intel,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
Intel,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,14
Intel,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,16
Intel,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
Intel,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,24
Intel,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,12
Intel,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,10
Intel,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,3
Intel,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
Intel,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,7
Intel,nyt7vgu,"It seems the revived Dell XPS 16 will have the â€œB390â€ and no dGPU, as another option. LTTâ€™s video on it said Dell is quoting 27 hours of battery life in â€œgeneral tasksâ€ and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but hereâ€™s hoping.",hardware,2026-01-10 16:45:16,4
Intel,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,39
Intel,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 â€œLow Powerâ€ efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobileâ€™s 285H (not HX!), trading 2 performance cores for 2 â€œLow Powerâ€ efficiency cores.  Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intelâ€™s normal lineup.",hardware,2026-01-10 16:28:50,16
Intel,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
Intel,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
Intel,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,39
Intel,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,10
Intel,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
Intel,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
Intel,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
Intel,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,17
Intel,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,0
Intel,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,7
Intel,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,4
Intel,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,7
Intel,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,17
Intel,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
Intel,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,13
Intel,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-3
Intel,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,7
Intel,nytp8ri,"Huge step forward, I just wish the didnâ€™t struggle with older and brand new games. Itâ€™s a great card if you are willing to do troubleshooting and know computers but I wonâ€™t recommend them to family yet.",hardware,2026-01-10 18:07:45,6
Intel,nz0sex5,I hope they start supporting dx11 stuff. Thatâ€™s a ton of games.,hardware,2026-01-11 19:10:35,1
Intel,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,4
Intel,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
Intel,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,15
Intel,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,11
Intel,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,4
Intel,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,2
Intel,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,16
Intel,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
Intel,nyvvmm4,"> Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,15
Intel,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,5
Intel,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
Intel,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,3
Intel,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,0
Intel,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,7
Intel,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
Intel,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,2
Intel,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
Intel,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,9
Intel,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
Intel,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
Intel,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,8
Intel,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,3
Intel,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,29
Intel,nyuw1ry,"To be frank, high end mobile gaming is also pretty niche.   There are edge cases for AI.   However, the pricing for strix halo was just awkward. At the price levels their SKUs were coming, you might as well go with a nice M4 max and call it a day.",hardware,2026-01-10 21:36:09,6
Intel,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,5
Intel,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,10
Intel,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,11
Intel,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,6
Intel,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,22
Intel,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45Â°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
Intel,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
Intel,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,6
Intel,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,38
Intel,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,20
Intel,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,12
Intel,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,2
Intel,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-3
Intel,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,5
Intel,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,8
Intel,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,3
Intel,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,7
Intel,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
Intel,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
Intel,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
Intel,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
Intel,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,6
Intel,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
Intel,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,10
Intel,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
Intel,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
Intel,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,8
Intel,nywmg0a,"Depends on what you consider to be â€œhigh end mobile gamingâ€, the laptop 4060 is currently the 2nd most popular gpu on steam, and thatâ€™s the level strix halo targeted",hardware,2026-01-11 03:03:40,0
Intel,nyxpn7n,"What youâ€™re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. Thatâ€™s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,0
Intel,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-3
Intel,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,5
Intel,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,8
Intel,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
Intel,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
Intel,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,7
Intel,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,10
Intel,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
Intel,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,9
Intel,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,9
Intel,nywgma9,"QC last year had a bad product.Â  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200â‚¬   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,2
Intel,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,3
Intel,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
Intel,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.Â   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,4
Intel,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.Â  They have 5 total. AMD, Nvidia, ARM, Qualcomm,Â  Apple",hardware,2026-01-11 13:19:34,1
Intel,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,12
Intel,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,6
Intel,nysq6pk,"Not true. All you have to do is use iGPU and disable dGPU. On linux this is a non issue, on Windows I have no clue",hardware,2026-01-10 15:19:02,6
Intel,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,6
Intel,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
Intel,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
Intel,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,8
Intel,nz565l5,"Donâ€™t even get me started! My current HP laptop straight up doesnâ€™t support any type of fan control on Linuxâ€¦  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
Intel,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,6
Intel,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,1
Intel,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
Intel,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
Intel,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
Intel,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
Intel,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,3
Intel,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,5
Intel,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
Intel,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,2
Intel,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-3
Intel,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under Â£2k then maybe.",hardware,2026-01-10 16:20:10,3
Intel,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
Intel,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,2
Intel,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
Intel,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
Intel,nytd8yk,"On Linux you can use tools like supergfxctl to completely disable dGPU. It doesn't require a reboot but a logout. Pretty happy with G14 2023, dGPU is simply off and doesn't consume any power at all.  On Windows, it looks like on Asus Laptops you can do the same with G-Helper.",hardware,2026-01-10 17:10:47,3
Intel,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
Intel,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
Intel,nzbx72p,Medusa halo isnâ€™t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
Intel,nxxeocb,Great.  Does this mean AMD will finaly stop pricing Strix Point as if it was made out of gold ?,hardware,2026-01-06 01:29:45,312
Intel,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,107
Intel,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,32
Intel,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,9
Intel,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,23
Intel,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,51
Intel,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,17
Intel,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,18
Intel,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,25
Intel,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
Intel,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,6
Intel,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
Intel,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,4
Intel,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,1
Intel,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,4
Intel,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-26
Intel,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-10
Intel,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-12
Intel,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-10
Intel,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-16
Intel,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,55
Intel,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,82
Intel,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,6
Intel,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,23
Intel,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,3
Intel,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,3
Intel,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-10
Intel,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-13
Intel,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,65
Intel,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,24
Intel,nxz85mh,"With everything happening around NVIDIAÂ´s price increases and AMDÂ´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,10
Intel,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,1
Intel,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,13
Intel,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
Intel,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,52
Intel,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only â€œArc Graphicsâ€.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arcâ€™s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,9
Intel,ny7ef2l,It's a mid-gen refresh of battlemage.,hardware,2026-01-07 14:33:10,2
Intel,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,53
Intel,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,20
Intel,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,9
Intel,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,5
Intel,nxy6cgs,"At best, itâ€™s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,1
Intel,ny0ce80,What do you mean that a refreshed Strix Point canâ€™t compete with an updated architecture?,hardware,2026-01-06 14:10:24,4
Intel,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,9
Intel,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,8
Intel,nxzf0cx,I think theyâ€™re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,5
Intel,nxxj6sm,Intel charts for their gpus have been pretty on point   Msi claw with lunar lake is one of the best handhelds,hardware,2026-01-06 01:54:01,63
Intel,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,9
Intel,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,4
Intel,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,12
Intel,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,16
Intel,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
Intel,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,66
Intel,nxxh29b,Example?,hardware,2026-01-06 01:42:33,16
Intel,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,26
Intel,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,19
Intel,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,5
Intel,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,23
Intel,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,18
Intel,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,27
Intel,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,34
Intel,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,17
Intel,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,3
Intel,nxy0jsm,Eh? It's a standard 128 bit memory bus.â€‹,hardware,2026-01-06 03:29:57,35
Intel,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
Intel,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,7
Intel,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,5
Intel,nxz7zsg,"Reddit isnâ€™t indicative of anything really, most casual laptop buyers donâ€™t even know what AMD is.",hardware,2026-01-06 09:07:48,10
Intel,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,35
Intel,ny7dxyt,AMD is planning on again using RDNA 3.5 on their next mobile chips as well.,hardware,2026-01-07 14:30:42,3
Intel,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,15
Intel,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,7
Intel,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,6
Intel,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,1
Intel,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,20
Intel,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,12
Intel,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,4
Intel,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xeÂ³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
Intel,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,2
Intel,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,1
Intel,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,23
Intel,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,19
Intel,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,28
Intel,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,2
Intel,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,23
Intel,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,11
Intel,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,5
Intel,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that itâ€™s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,14
Intel,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,6
Intel,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
Intel,ny0na4n,"Itâ€™s a super strong generational gain though, itâ€™s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,12
Intel,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
Intel,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,4
Intel,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,51
Intel,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
Intel,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,2
Intel,nybz77q,"I don't think that will really be an issue, laptops can be configured with soldered 8 channel RAM like AMDs Z2 extreme, or they can still manage easily with regular DDR5 6400Mhz sodimms, which run at 102.4GB/s  Plus the CPUs that have intels new B390 iGPU are 4P/8E CPUs, so I doubt there will be much issues from low ram speeds. Something like the Radeon 890M have done fine with such speeds",hardware,2026-01-08 03:26:50,1
Intel,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,6
Intel,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
Intel,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-2
Intel,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-19
Intel,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-19
Intel,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
Intel,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-8
Intel,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,9
Intel,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,3
Intel,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,82
Intel,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,14
Intel,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,5
Intel,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,37
Intel,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
Intel,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,5
Intel,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,32
Intel,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
Intel,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isnâ€™t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
Intel,ny0wejw,"> most casual laptop buyers donâ€™t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
Intel,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,25
Intel,ny7wgeg,This is unfortunate news  (â•¥ï¹â•¥),hardware,2026-01-07 16:00:09,1
Intel,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,7
Intel,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-3
Intel,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.Â   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.Â   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.Â    > and better process node   Very much unproven.Â    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,3
Intel,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,7
Intel,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,3
Intel,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,3
Intel,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,0
Intel,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,14
Intel,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,25
Intel,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,30
Intel,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,3
Intel,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,3
Intel,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,8
Intel,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,6
Intel,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,6
Intel,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isnâ€™t more performance difference.,hardware,2026-01-06 04:18:43,4
Intel,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,0
Intel,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
Intel,ny7dl7u,"Nvidia's graphics have shown to be more efficient for space than both Intel and AMD, so whatever they use it will likely be better than what Intel can currently put out.",hardware,2026-01-07 14:28:52,2
Intel,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,3
Intel,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,9
Intel,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide thatâ€™s suspect.  Itâ€™s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,5
Intel,ny7raf2,I think they meant that the chip is very pricey which sucks because the handheld is already low-margin otherwisr and can't be priced too high else it got undercut by its competitors.,hardware,2026-01-07 15:36:28,1
Intel,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
Intel,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,10
Intel,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,10
Intel,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,8
Intel,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,18
Intel,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,16
Intel,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
Intel,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,12
Intel,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,2
Intel,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,16
Intel,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
Intel,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,7
Intel,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,7
Intel,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-11
Intel,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-7
Intel,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,18
Intel,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,17
Intel,ny6k0v5,Except with UDNA it might be the first time over a decade AMD isn't phoning it in.,hardware,2026-01-07 11:22:51,1
Intel,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,4
Intel,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,6
Intel,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,-1
Intel,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,4
Intel,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,7
Intel,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,14
Intel,nxysgiw,Mobile rtx 4050-70 maxes out at roughly 90-100 watts due to voltage limitation,hardware,2026-01-06 06:44:37,5
Intel,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
Intel,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
Intel,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
Intel,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,10
Intel,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
Intel,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,23
Intel,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,5
Intel,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
Intel,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,28
Intel,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,6
Intel,nxy622y,"It needs dedicated GPU hardware to run faster, but theyâ€™ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,4
Intel,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,8
Intel,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
Intel,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
Intel,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,11
Intel,ny6ka8x,"Yeah but basically unusable on pre 40 series. But at least NVIDIA gives users the choice.  AMD should just stop the BS pretending and just enable the full FP8 model across RDNA2-3 with FP16 emulation. But it prob runs so bad that they won't, far far worse than DLSS 4.5 on 20-30 series.",hardware,2026-01-07 11:24:58,2
Intel,ny6ob4j,"It would be good if that is true, but so far ive seen nothing that would inspire me confidence in AMD. And yes i remember the AMD patents you posted last year.",hardware,2026-01-07 11:55:47,2
Intel,ny7ebjk,"Yeah, it's DLSS4>FSR4>XESS (Intel)>=DLSS3>XESS (fallback)>FSR3      quality wise.",hardware,2026-01-07 14:32:39,1
Intel,nxzj5en,"First custom design OEM are fast, here 4400â‚¬ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAXâ€¦ one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
Intel,ny5eqws,"When I talk about ""design targets"", I'm not referring to an arbitrary TDP. There are very specific decisions each SoC made that have tradeoffs at different power envelopes.   Also, the context was LNL which is an entirely different beast from MTL.",hardware,2026-01-07 05:25:06,2
Intel,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,4
Intel,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,3
Intel,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
Intel,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,3
Intel,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,6
Intel,ny6po5w,"If you're referring to the April dump, heck even the August dump (analysis of Kepler\_L2 patents) then that's not close to the complete picture. A lot of new patents have surfaced since that expand upon the design in many ways, but I'm waiting for the last RDNA5 to be made public before making a potential follow up post.  But regardless even if they fix HW situation completely they'll prob fail spectacularly with SW stack as they've done so far with FSR Redstone and FSR4 game adoption. Even hear a lot of people complaining about having to use Optiscaler, even in newer games.   Also NVIDIA will no doubt move the needle a lot nextgen yet again. They already did with DLSS 4.5 and DFG and something tells me that DLSS5 is gonna be even worse for AMD. They better prepare for what's to come.  Worst case it's a complete massacre. I can see the following scenario happening:  **HW:** NVIDIA invests all their silicon budget into fixing 5090 scaling bottlenecks (16 GPCs instead of 12, revamped scheduling etc...), fixes other problems with 50 series (redesign cachemem mostly) + goes Brr on ML and to some extent RT. Raster goes up 35-40%, everything else goes up multiple times.   Worst case ML HW gets bumped to 4-8X NVFP4 rate, although 2-4X sounds more likely.  **SW:** NVIDIA uses this new insane ML HW to make new DLSS models. DLSS5 goes all in on NVFP4 and is faster than DLSS4.5. DLSS5 SR and RR for 50 series + 60 series which is lightweight and fast on new GPUs (high FPS), and a new DLSS ULTRA SR for 60 series (released across stack but painfully slow for anything pre 60 series) striving for maximum Image quality. The smaller model will be better than DLSS4.5 and the big model another tier entirely (DLSS3 -> 4 leap easily on top of DLSS4.5).   They also make DRS compatible with DLSS SR and RR so users get greater flexibility here similar to DFG for framegen.   FG will also release in two versions one light and heavy. Will also work with Reflex 2. It's possible only the big model will be frame extrapolation + limited to 60 series. Should make FG result in lower ms instead of higher + overall image quality far superior and basically all issues solved up to at least 4X.   Oh and a flood of MLPs and a demo showcasing the absurd visuals the 6090 can push. Moves goalpost past ReSTIR PT and will look borderline offline render quality. Very close to Blender renderers. IDK how they'll do it but MLPs are borderline magic, so prob doable.  Thinking about it more you're prob right and even if RDNA5 HW is amazing even beats 6090 in PT, a DLSS5 feature suite this impressive + moving goalpost to MLP based neural rendering will make RDNA5 irrelevant. As always SW and marketing will kill any momentum from HW side. Really hope I'm wrong but don't think so.  Sorry for the rambling.",hardware,2026-01-07 12:05:46,2
Intel,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,5
Intel,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.Â    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.Â    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,1
Intel,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
Intel,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
Intel,nycu2b3,"I enjoy reading your optimism. I hope it all comes true, but it sounds a bit too good to be true given the recent hardware developements. The 5090 scaling issue is that we stopped resolution scaling. If you go beyond 4k the 5090 scales a lot. VR resolutions report the 5090 being as much as twice the framerates of 4090.",hardware,2026-01-08 06:52:50,1
Intel,nydd2fq,I've rewritten prev reply to provide more info.  I'll also link the scheduling patent here in case anyone reading this thread is interested: [https://patents.google.com/patent/US12153957B2](https://patents.google.com/patent/US12153957B2)   It sounds like gains in workgraphs scenarios will be be even greater.,hardware,2026-01-08 09:42:10,2
Intel,nyd5bcu,"Yeah prob not realistic. I just tried to outline a nightmare scenario for AMD. As for the RDNA5 stuff we'll see how good it ends up being.  Agreed serious issues fs. RTX 5090 scheduling is brain dead. 16 SM GPCs, one central scheduler for 170 CUs. The smaller the internal res the harder it is to keep things going. Someone smarter than me could prob make a core scaling efficiency chart for different resolutions clearly showcasing how RT > raster and derive different formulas for 1080p, 1440p, 4K etc... . There's simply no reason why it has to be this bad moving forward.       But it's also interesting to entertain that RDNA5 could be a nightmare for NVIDIA. If NVIDIA doesnâ€™t fix scheduling AMD's nextgen could be a real nightmare scenario for them. The modular and decentralized scheduling will be a gamechanger and based on what patents have said scaling is almost perfect and can scale to [arbitrarily large configurations](https://patents.google.com/patent/US12153957B2), yes they used that wording. AT0 will function like 8 x AT4 instead of running into massive scaling issues. In fact based on what the patent has said it might be even better. Consider each scaling domain with a local cache independent of the L2, where the global command processor only acts as a distributor of work, not an orchestrator. Gains will be observed across the stack but expecting IPC gains to scale with number of CUs. Is this a big deal for RT and 4K native? Yeah but even more so for lower res gaming.    And assuming they reduce CPU overhead even further in new uarch AMD will easily take the max FPS crown although I suspect NVIDIA can finally address their driver overhead issue after booting Maxwell-Pascal. Weâ€™ll see who comes out on top in CPU overhead nextgen.  I thought most of that gain vs 4090 was due to extra BW? But yeah high end perf scaling falls apart at sub 4K internal res.",hardware,2026-01-08 08:30:40,1
Intel,nyjq3tx,"I dont think much can be done with overhead. AMDs overhead is already small, basically letting the API go directly to GPU as it is. While for Nvidia side, isnt most of the overhead related to how Nvidia handles DX12? in that case i dont see it going away for a long time.",hardware,2026-01-09 05:42:46,2
Intel,nyfqcxo,"Interesting, GN usually gets Tom to do discussions like these but instead decided to publish whatever that previous video was on 'Intel pulling an Nvidia'. I bet GN will probably have their own video with Tom, but I appreciate DF a little bit more with this discussion.  At around 21min, it's interesting to hear his talk on cross-vender SR, mentions how they'd like to work more on Nvidia's Streamline and a candid talk about DirectSR and how it isn't really the concrete solution for the work on cross vendor SR. At around 23min, Alex brought up something interesting about research they've published before on joint denoiser and SR. He kinda skirts around it, but continues on suggesting they have more plans on it. He also then continues on the state of PT, DXR 1.2, obviously it isn't a real focus with something on their iGPUs, but any future HW, will be their primary goal to tackle. Alex mentions Valve/Linux, and Tom says it isn't entirely their focus right now, at least for gaming.",hardware,2026-01-08 17:45:35,41
Intel,nyhyq7q,"Super interesting that he randomly announces that Intel will be dropping a pre built shader program for Panther Lake. And not build with the new Microsoft framework/infrastructure, but just on their own?? How can Intel randomly drop this, but nvidia and amd canâ€™t??",hardware,2026-01-08 23:46:46,12
Intel,nyhbk2i,"Seems like we're not the only ones that think FG isn't ideal rn. I really hope Intel succeeds in their efforts to pair Framegen with reprojection, but it'll prob be NVIDIA that gets there first. Might be the killer app for 60 series, but pure speculation of course.  The stuff about using AI to smoothe frames is interesting as well.  Things prob gonna change a lot in the coming years. We'll see if it's for the better.",hardware,2026-01-08 21:55:47,19
Intel,nyfhyad,Are PC games becoming more stuttery or we're just paying more attention to it?,hardware,2026-01-08 17:08:37,26
Intel,nyfezgd,This will probably piss off MLID since he hates Tom Peterson,hardware,2026-01-08 16:55:39,13
Intel,nyikqbo,This future of gaming is ridiculous.  Aggressive upscaling (360p) and one-in-four frames actually rendered and the rest FG?   For what?  Path tracing?  Nanite?,hardware,2026-01-09 01:41:36,-5
Intel,nyhaj0j,You still watch GN? Dude only farms drama after realizing how much clicks they generate.,hardware,2026-01-08 21:51:23,41
Intel,nyhmk4k,Thank you for the summary!,hardware,2026-01-08 22:45:36,3
Intel,nyjni69,"He's talked about it before I believe in a previous interview, might have been with PC World from memory or perhaps GN. Regardless, it wasn't exactly new iirc. [Anyways this is definitely old news.](https://overclock3d.net/news/software/intel-plans-to-make-shader-stutter-a-thing-of-the-past-with-arc/)",hardware,2026-01-09 05:24:04,10
Intel,nyja51l,"What do you mean? The Shader delivery program was launched on an AMD handheld, so I presume they will use the MS advanced shader delivery infra.",hardware,2026-01-09 03:59:46,7
Intel,nyimpss,"Intel already talked about this a couple months ago. It's not an announcement here, but it seems people are more interested in AMD and Nvidia news so those threads don't get as much traction.",hardware,2026-01-09 01:52:08,6
Intel,nyflgv9,I feel like more people paying more attention since the marketâ€™s grown a lot.   I remember old games I played in the early 2000s having micro and regular stuttering depending on the game. I chalked it up to â€œhuh guess itâ€™s loading in data as I playâ€ when I didnâ€™t know much.,hardware,2026-01-08 17:24:06,44
Intel,nyfmkrk,We are paying more attention to it but I suspect as we push higher frames with new engines and techniques the micro stutter is getting worse. It just starts becoming less perceptible to people than say the micro stutter from SLI and other stuff that caused issues in the past.,hardware,2026-01-08 17:28:57,22
Intel,nyfuxga,"I think digital foundry answered that question on their podcast. Compared to 15 years ago the frame rate is higher on average for most gamers, but it also stutters more. So it's huge fps with huge drops and hangs.",hardware,2026-01-08 18:05:15,21
Intel,nyfrkdx,"Worth remembering no one even gave a shit about stuttering enough to measure it until someone at I think it was anandtech back in the late 00s was so fed up with shitty perf on his crossfire system he started doing 1% lows on benchmarks.   Back in the 90s people would run like 6x SLI voodoos and not even care that the game hitched every 10 seconds down to sub 30s fps lol...  Edit: it's also key to remember that a lot of the games that stutter on PC these days stutter in the exact same places for the exact same reasons, and worse considering the lack of CPU power, on consoles. Console gamers just don't give a fuck lol....   DF did a good video showing this truth with the Silent Hill 2 remake.",hardware,2026-01-08 17:50:48,29
Intel,nyg5v26,"With a lot of PS3 games dipping to 20s, I guess we are just paying more attention to it now",hardware,2026-01-08 18:52:01,10
Intel,nyj9uk1,There was DF Clip of this exact question.  https://www.youtube.com/watch?v=pxsfT4c-F-Q,hardware,2026-01-09 03:58:01,5
Intel,nyfihv5,They're more surgery stuttery.,hardware,2026-01-08 17:11:01,2
Intel,nyfksy4,Everyone hates MLID,hardware,2026-01-08 17:21:10,40
Intel,nyfg44c,Funny piece of lore. Why so?,hardware,2026-01-08 17:00:30,15
Intel,nykk9lw,Who's MLID?,hardware,2026-01-09 10:03:38,3
Intel,nyjx60j,If the end visuals are better who cares. we already did a lot of such things in engine just didnt tell the players about it. One in four frames actually rendering shadows is a thing for example. Heck some games go as bad as once a second shadow updates.,hardware,2026-01-09 06:38:04,17
Intel,nykocr4,"Well, I think that most people (myself included) don't really care how it's done if the end result is looking good and feel good to play, off course something like 30fps based with MFG to 120 is bs, but I quite regularly use 60 -> 120 using frame gen because It feels okay input wise to me at 60, and the added visual smoothness is quite nice. So it all depends how it's implemented and talked about.",hardware,2026-01-09 10:40:00,11
Intel,nykneij,"yeah i had to tap out a couple months back. it's a shame, they did great work, but i refuse to support ragebait.",hardware,2026-01-09 10:31:33,15
Intel,nyhh357,"Doesn't help that there isn't any particular new hardware to review or anything new besides AI, the PC industry has hit stagnation in the consumer market.",hardware,2026-01-08 22:20:10,25
Intel,nyickvj,"In other news, guy who is being served turds at restaurant loudly complains about receiving fat dooks instead of food.     *""man that guy is such a whiner""*",hardware,2026-01-09 00:58:06,0
Intel,nykpeuy,"No it didnâ€™t sound like they were going to use the MS shader delivery program. Or no, they said they want to, but they have their own solution that theyâ€™ll launch before that.   And I also donâ€™t think the MS solution is ready. The ROG ALLY XBOX also doesnâ€™t have this feature already as far as I know",hardware,2026-01-09 10:49:11,2
Intel,nykoz3l,"> The Shader delivery program was launched on an AMD handheld  i assume you're referring to the steamdeck, which to my understanding was done by valve... so the point kinda stands, you just add valve to the preamble.",hardware,2026-01-09 10:45:26,2
Intel,nyg0pvl,"I dunno I remember many games, especially based on Quake engine, being buttery smooth if you could get to reasonable FPS",hardware,2026-01-08 18:30:02,18
Intel,nyjwwd6,also more cause for stutering. Back then we could compile shaders real time with no siginficant issues because they were small. Now we have to compile shaders real time that are huge to the point where we pre-compile half of them before we even start the game.,hardware,2026-01-09 06:35:52,7
Intel,nyfy60k,Console gamers *of certain genres* care.   The FGC rejected the idea of playing Street Fighter IV & MvC3  on PS3 because the Xbox 360 port had better input latency.   Even the PS4 port of USFIV wasn't liked.     Part of why I stopped going to tournaments is because SFV & Tekken 7 on PS4 always felt *off* compared to PC.,hardware,2026-01-08 18:19:10,11
Intel,nyg711t,"Not really. In many cases, console versions of games just don't stutter whereas PC games do because modern games are mainly designed for consoles and then ported to PC.  A couple of good recent examples are Wukong and Outer Words 2. Neither of those games on consoles have the horrid stutters that are prevalent on PC.",hardware,2026-01-08 18:56:57,12
Intel,nymcty0,MLID more like MID,hardware,2026-01-09 16:26:52,2
Intel,nyfkp1n,Heâ€™s been calling Tom a â€œsnake oil salesmanâ€ for performance claims on Alchemist,hardware,2026-01-08 17:20:43,16
Intel,nzdp1mq,"Moore's Law is dead, he's a YouTuber who makes predictions and purports to have insider information from Nvidia/Intel/AMD but in reality he is better described as FanFiction for PC enthusiasts.",hardware,2026-01-13 16:50:02,1
Intel,nym0alm,"I worry about things like latency.  Or what happens when you move suddenly, or fire, and the whole image falls apart.  If the hardware can't do it all yet, then wait a few years instead of using these... methods.  And just to be clear, I think very highly of  Mr. Tom 'TAP' Petersen; I'm just not liking where this is all heading.  Are you happy with Borderlands 4 or Outer Worlds 2?  If you are then we live in two different worlds.",hardware,2026-01-09 15:30:18,-2
Intel,nyjn1ql,"There's plenty of old stuff he needs to review. I've said it before but I will say it again and beat it like a drum: He STILL has not reviewed the 9060 XT 8GB for instance. Plenty of content he could have farmed off that, especially BEFORE the RAM shortage where having an 8GB card was some sort of sin in his eyes (except for some reason he ignored it but raile roaded the cheaper RTX 5050, lol did someone say bias?). Instead, he just tore the 9060 XT 8GB down and never touched it again after that teardown. I'm sorry but there's GENUINELY stuff he could be doing instead of farming clicks about 'NVIDIA bad' and 'Intel pathetic', but that won't get him views from drama farming NVIDIA which is what he craves these days. So sad to see a big channel like his who got big off doing solid technical content become a drama-hype ""news"" channel.",hardware,2026-01-09 05:20:56,23
Intel,nyk71tg,No wonder that strategy has proved successful. There are a lot of people that only care about complaining and bitching. Which I get up to a point but it starts to feel childish and pathetic quickly.,hardware,2026-01-09 08:02:47,19
Intel,nysczdh,"I love how reddit down votes you,  reddit posts all day complaining about the industry, GN does a video on it   Redditors ""whiney cry baby engagement farmers """,hardware,2026-01-10 14:05:38,-2
Intel,nyogfa3,"Valve does have one on Steam, but Microsoft announced a store agnostic, eventually hardware vendor agnostic one launched with the Asus ROG Xbox Ally X last year.  It was supposed to be working already but there's no sign of it just like everything else Microsoft releases about gaming half-baked like their gaming UI, their attempt to unify upscaling, and Directstorage.   So I imagine they're referring to what Microsoft called ""Advanced Shader Delivery"" that they've done little with but name and announce to sell more Asus Pretend-Xbox's.",hardware,2026-01-09 22:10:03,3
Intel,nyoi6ny,Iâ€™m talking about the Xbox Ally as the other commented said,hardware,2026-01-09 22:18:25,1
Intel,nygmjs6,We considered solid 60 or 75 smooth back then. Now at least I complain as soon as I can't stay above 100.,hardware,2026-01-08 20:05:12,13
Intel,nyhs8ie,"Shader compilation stutter is a PC problem thatâ€™s been especially bad in UE4 and 5 since the transition to DX12. Other types of stutter and bad frame rates used to be equally bad on consoles, or even worse in the 360/PS3 era.",hardware,2026-01-08 23:13:28,10
Intel,nygf3pi,Outer Worlds 2 is fairly consistent for a UE5 game. To me its just very heavy,hardware,2026-01-08 19:32:17,5
Intel,nygrcy6,"funny coming from MLID, notorious snake oil salesman",hardware,2026-01-08 20:26:46,49
Intel,nyflbj9,"Eeeegh, considering MLID overall story - can see that, can see that.  OTOH usage of such terminology - snake oil salesman - reminds me of that very infamous Intel presentation...",hardware,2026-01-08 17:23:27,14
Intel,nymda4b,You say that as if we will reduce our base frame rate. But the direction of travel is to use MFG to drive the 480Hz and above monitors that will become more and more common in the future.,hardware,2026-01-09 16:28:52,5
Intel,nyqwqj9,I dont like borderlands franchise and i havent played Outer Worlds 2 yet so i cannot comment on them from personal experience. However things like Reflex/Reflex2 has actually decreased latency for me.,hardware,2026-01-10 06:52:45,1
Intel,nylgvk7,"I havent really seen it as some sort of drama farm, I think a lot of GN's videos do show how bad the current computer hardware hobby is doing and Im glad someone is focusing on that",hardware,2026-01-09 13:55:07,1
Intel,nysckbg,Whata wrong with calling companies out? Like redditors cry all day but stop at GN when the channel brings up issues within an industry.  Sorry a person isn't running their channel the way the reddit collective wants.,hardware,2026-01-10 14:03:14,-2
Intel,nysv24y,"honestly, i had completely forgotten that was a thing.   i wish i had more faith in microsoft to pull this off, but i remain skeptical in basically anything they try until proven otherwise.",hardware,2026-01-10 15:44:02,1
Intel,nyguz4e,"People on gaming forums used to go around insisting the human eye could not distinguish greater than 60fps.  It was accepted as gospel in many places/by many people.  Granted, that was in the age of CRTs and almost no one had actually seen >60, but funny to think back on.",hardware,2026-01-08 20:43:03,13
Intel,nyjx1ak,snake oil salesmen dont like competition.,hardware,2026-01-09 06:37:00,10
Intel,nyjz26x,"MLID aka ""RTX 50 super will be released november 2025""",hardware,2026-01-09 06:53:37,5
Intel,nyknw8m,this just makes me like tom more,hardware,2026-01-09 10:35:57,7
Intel,nyjwmn8,"Replace that 60 with 30. Yes, people insisted we could not see more than 30 even while playing on 85hz CRTs.",hardware,2026-01-09 06:33:40,8
Intel,nyhcljr,"Meanwhile quake players kept dropping resolution to hit triple digit refresh rates :) But yeah, it's wild that 30 was the norm for so long on console",hardware,2026-01-08 22:00:09,7
Intel,nyko1gh,That's far from the worst prediction he's missed.  Remember SMT4 for future Zen architectures? 24 and 32 cores on desktop Zen4? L4 cache for Zen 4?,hardware,2026-01-09 10:37:14,11
Intel,nyrvn0z,Which Tom?,hardware,2026-01-10 12:08:40,1
Intel,nyjbd32,"console used to lock at 60, dont know why they choose to drop down to 30.",hardware,2026-01-09 04:06:56,3
Intel,nyjivip,"Because when the hardware cant handle all the sprites, it slows everything down including game logic which was tied to frame rate back then. It happens in plenty of hardware from snes to arcade cabinet.",hardware,2026-01-09 04:52:53,8
Intel,nyjwrq4,"also happened the other way round, older games would run faster than they should because they tied it to framerate rather than delta time. This bad practice was so common you can still find studios like bethesda do this.",hardware,2026-01-09 06:34:49,4
Intel,nz6q876,"for context, this G14 was the one that gave us [the first ever Geekbench scores for Panther Lake](https://www.reddit.com/r/hardware/comments/1ocaslx/panther_lake_geekbench_leak_its_good/) over on r/hardware",hardware,2026-01-12 16:33:10,34
Intel,nz6w6zw,"The price probably wasn't going to be low enough compared with other SKUs.  In the past, Intel skuss with Iris HD etc were so expensive and were only included in $2000 ultrabooks. This is probably going to be same.",hardware,2026-01-12 17:00:15,14
Intel,nz73dw5,When I first learned about I was confused why it existed. Not surprised Asus decided to cancel it.,hardware,2026-01-12 17:33:23,10
Intel,nz6r3wa,"It makes sense, a B390 only G14 kind of defeats the whole purpose of the Zephyrus, even as a base model.     Basically brings the GPU performance down to 3060 levels (actually a little worse than that), which is roughly 4 years ago at this point.",hardware,2026-01-12 16:37:10,34
Intel,nz6q33b,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-12 16:32:31,1
Intel,nz770nj,Itâ€™s also the one that didnâ€™t have a dedicated GPU in Geekbench,hardware,2026-01-12 17:49:53,13
Intel,nzcyo9s,"I don't think so, they're making it available on more than just the top 9 series, they also confirmed a custom chip for gaming handhelds which we'll prolly see at computex",hardware,2026-01-13 14:45:59,2
Intel,nz7invn,It probably made more sense as an low-end gaming Zephyrus when fitting it with 32GB LPDDR5x wasn't half the BOM,hardware,2026-01-12 18:42:14,15
Intel,nz7xzli,esp since on CES 2026 they launched an even more premium 14 inch laptop that actually does have the B390 (ExpertBook Ultra),hardware,2026-01-12 19:52:01,5
Intel,nz6vvy1,esp since Asus ended up launching the ExpertBook Ultra at CES  an even more premium 14 inch laptop compared to the G14 which does have a B390,hardware,2026-01-12 16:58:51,11
Intel,nz6ryqs,"But it actually can be used on battery unlike normal laptop gpus which make your laptop die in 5 seconds or performance is awful and it dies in 10 seconds, so what's the point of having a powerful GPU on a laptop if I got to plug in all the time it sucks.",hardware,2026-01-12 16:41:01,29
Intel,nz6tste,"A 3060 is still useable imo. If the efficiency is right, they can make usb-c powered gaming laptop and completely remove the dumb dc brick.",hardware,2026-01-12 16:49:18,17
Intel,nzcpz2s,"It would have made _some_ sense, but the Arc B390 mustâ€™ve been real good. But as you say, with the raise in RAM prices, you can forget about it.  I wonder whether Intel themselves would pair a mid-range Panther Lake with a top-notch iGPUâ€¦ It would make sense, since the high-end CPUs tend to have dGPUs.",hardware,2026-01-13 14:00:36,1
Intel,nz6tn0v,"For the Zephyrus you can also just disable the dGPU to game only on the iGPU, and the customers of the Zephyrus are minimum looking for a decently powerful dGPU",hardware,2026-01-12 16:48:35,18
Intel,nz6snj4,I hope their plugged out power scheme is permanent and doesn't vary based on application,hardware,2026-01-12 16:44:08,3
Intel,nzaau06,"> so what's the point of having a powerful GPU on a laptop if I got to plug in all the time  I have a laptop for travelling. I travel to places that have plugs, and I don't need to use a laptop while I am actually in transit.",hardware,2026-01-13 03:03:12,1
Intel,nz8xpz8,"It still pulls 60W (and apparently 80W, briefly) at full pelt...  That is to say an hour and a half battery life when gaming should be expected. It's not magic.",hardware,2026-01-12 22:40:05,0
Intel,nz71x4x,Usb c powered gaming laptops are more common now. The new ideapad 5 pro with panther lake and 5060 (combined 110w system power) uses usb c exclusively,hardware,2026-01-12 17:26:41,11
Intel,nz753k0,"> A 3060 is still useable   Nvidia seems to agree with you, which is why they are looking to put them back into production.  /I'm sorry, I couldn't resist.",hardware,2026-01-12 17:41:14,10
Intel,nz6wes7,"Useable, yes, but what customers of the Zephyrus are looking for, no.",hardware,2026-01-12 17:01:16,11
Intel,nz6vzj4,"Yeah, and it gets pretty good battery life on that and in power saving mode/60fps screen mode. Like 8 hours. Best I've had on a gaming laptop- not a high bar, but it's nice to be able to take it into the living room and use it light a normal laptop instead of it dying instantly.",hardware,2026-01-12 16:59:19,8
Intel,nz920o1,Still better than 45 minutes  Also it can obviously be put in a lower power mode to save battery while not hurting performance too drastically.,hardware,2026-01-12 23:01:47,2
Intel,nz6x4bl,"The G14 used to have gtx1650, so not sure what do you mean.",hardware,2026-01-12 17:04:35,0
Intel,nz724pz,Which was the 5050 of that time which is significantly more powerful than panther lake's igpu.,hardware,2026-01-12 17:27:38,9
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,115
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375â‚¬, seeing that the 9060XT is going for 350â‚¬ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,45
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,20
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,4
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,5
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,10
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,6
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldnâ€™t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,10
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,6
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,17
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,27
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,12
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,4
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,11
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,4
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,1
Intel,nu2970r,Iâ€™d be happy if they didnâ€™t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-22
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,22
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,6
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,24
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-4
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,6
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,17
Intel,nubvjrs,"Itâ€™s not but the B50 is the only Arc Pro that isnâ€™t gated behind a bad vendor like Hydratech.    If they canâ€™t properly launch the B60, why should I trust Intel or itâ€™s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,19
Intel,nt8w4et,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",hardware,2025-12-10 06:01:38,27
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,14
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,ntafnry,How does this compare to the Snapdragon X2 Elite?,hardware,2025-12-10 13:56:04,1
Intel,nuahcx5,4 pcores  8ecores 4 lpcores..,hardware,2025-12-16 06:05:52,1
Intel,ntcj3az,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:10,1
Intel,ntbju2x,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",hardware,2025-12-10 17:23:27,1
Intel,nte6d5x,"Iâ€™m sorry, but thatâ€™s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, thatâ€™s pathetic",hardware,2025-12-11 01:36:38,0
Intel,nt8wwpg,"Probably because GeekBench 6 only scales to a certain point, where more cores wonâ€™t help with improving performance compared to improving core IPC",hardware,2025-12-10 06:08:21,8
Intel,nt9ghvf,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",hardware,2025-12-10 09:13:58,5
Intel,ntcj6hz,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:37,-1
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,17
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, Iâ€™d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,15
Intel,ntdv8jz,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",hardware,2025-12-11 00:29:31,1
Intel,ntcpemm,"Itâ€™s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process â€” they use TSMCâ€™s then-most-advanced N3B node, Intelâ€™s first time adopting it. Meanwhile, Panther Lake uses Intelâ€™s own **18A** process.  Based on the current benchmark results, Intelâ€™s 18A appears to outperform TSMCâ€™s N3B by at least the same margin that **Intel 4** trailed behind N3B â€” which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how â€œoutdatedâ€ Intelâ€™s nodes are, how AMDâ€™s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMCâ€™s top process from just one year ago.",hardware,2025-12-10 20:47:39,-1
Intel,ntep84w,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",hardware,2025-12-11 03:32:35,3
Intel,ntfpawi,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",hardware,2025-12-11 08:27:12,2
Intel,nthte2d,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",hardware,2025-12-11 17:03:33,2
Intel,ntckb4u,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:22:19,2
Intel,ntgubsm,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",hardware,2025-12-11 14:03:57,1
Intel,ntfhexr,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isnâ€™t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary itâ€™s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",hardware,2025-12-11 07:10:24,2
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,15
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,30
Intel,ntfm9jb,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,hardware,2025-12-11 07:57:01,4
Intel,ntco5ms,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10â€“11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4â€“5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is averageâ€”around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40â€“45 W**.  I also estimate that the Cinebench R24 score should fall around **1300â€“1400**. Compared with Qualcommâ€™s X Elite 2 at **1950**, there is still a significant gapâ€”but the two products differ drastically in scale.  Overall, Panther Lakeâ€™s greatest achievements lie in several aspects:  1. **Energy efficiency** â€” likely the best among all x86 products. 2. **Performance per mmÂ²** â€” excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tileâ€™s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcommâ€™s processors. The X Elite 2 has **18 cores**, including **12 â€œvery largeâ€ cores**â€”similar in size to Intel P-coresâ€”and **6 large cores**, each larger than Intelâ€™s E-cores. The die area of this chip is **2.5Ã— larger** than Panther Lake 484â€™s.",hardware,2025-12-10 20:41:27,-2
Intel,nthu15b,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",hardware,2025-12-11 17:06:47,2
Intel,ntd3lso,">The benchmark is very friendly to ARM and least favorable to AMD.Â   How so?   >The only valid reference isÂ **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",hardware,2025-12-10 21:57:00,13
Intel,ntfgs1k,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",hardware,2025-12-11 07:04:32,2
Intel,ntsl9xc,My old 14900hx gets 35k multi core in cinebench r23,hardware,2025-12-13 10:15:38,1
Intel,ny53e0z,better go for cb2024 cuz r23 is being less relevant these days,hardware,2026-01-07 04:09:43,1
Intel,ntgxa5r,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",hardware,2025-12-11 14:20:52,2
Intel,nt9ouc7,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",hardware,2025-12-10 10:37:37,5
Intel,nvdnrp5,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,hardware,2025-12-22 15:05:12,1
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,-3
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,-3
Intel,ntd4133,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,hardware,2025-12-10 21:59:01,7
Intel,ntduxlv,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",hardware,2025-12-11 00:27:43,6
Intel,ntwk3p9,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",hardware,2025-12-14 00:54:40,2
Intel,ntmv22s,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",hardware,2025-12-12 12:29:41,1
Intel,nt8x38v,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",hardware,2025-12-10 06:09:56,15
Intel,nt9ufxf,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,hardware,2025-12-10 11:28:32,2
Intel,nt8wp7g,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",hardware,2025-12-10 06:06:35,8
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,9
Intel,ntgnaao,That looks like an AI post to me,hardware,2025-12-11 13:22:24,2
Intel,nt9re78,It has been going hayway since SME just like GB5 had issues with AES Skewing results,hardware,2025-12-10 11:01:14,1
Intel,nt92449,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,hardware,2025-12-10 06:54:12,-2
Intel,nvfvk61,Incredible hardware news. Thanks for the share.,hardware,2025-12-22 21:50:44,19
Intel,nvl1r29,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",hardware,2025-12-23 18:20:13,13
Intel,nu6hk69,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",hardware,2025-12-15 16:37:33,46
Intel,nu6rlfl,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,hardware,2025-12-15 17:26:31,12
Intel,nue205x,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",hardware,2025-12-16 20:01:26,4
Intel,nu7ksue,Not in the same system the 1080 ti was in.,hardware,2025-12-15 19:47:27,8
Intel,nu6oprg,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",hardware,2025-12-15 17:12:19,2
Intel,nu6jvho,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),hardware,2025-12-15 16:48:43,16
Intel,nuacl0f,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",hardware,2025-12-16 05:26:38,14
Intel,nugufgj,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,hardware,2025-12-17 05:53:53,2
Intel,nu8m2qq,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",hardware,2025-12-15 22:57:03,3
Intel,nwpbxq3,Iceberg's intention always been to show realistic performance and average user would get with prices imaginable lol. Otherwise for highest possible performance people would prefer gn or hub.,hardware,2025-12-30 09:19:38,1
Intel,nu75g90,"it's the retailers, they don't sell well and need higher margins",hardware,2025-12-15 18:32:51,13
Intel,nwpc21t,"Yeah retailer margin and taxes made arc GPUs very less desirable, in my country b580 is close to rtx 5060 in price",hardware,2025-12-30 09:20:46,2
Intel,nuitvat,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),hardware,2025-12-17 15:13:42,5
Intel,nuc491o,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",hardware,2025-12-16 14:18:10,9
Intel,nuj4t5k,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",hardware,2025-12-17 16:07:30,5
Intel,nuzrua9,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,hardware,2025-12-20 06:16:26,1
Intel,nuf0jlu,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for Â£150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",hardware,2025-12-16 22:58:15,3
Intel,nskpbhm,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",hardware,2025-12-06 10:33:27,74
Intel,nskzrhs,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",hardware,2025-12-06 12:12:49,136
Intel,nsl4bqs,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",hardware,2025-12-06 12:50:37,44
Intel,nspebmq,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,hardware,2025-12-07 03:40:56,6
Intel,nsm5bxh,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",hardware,2025-12-06 16:32:46,8
Intel,nszc9eq,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",hardware,2025-12-08 18:55:08,1
Intel,nt0zpd3,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,hardware,2025-12-09 00:04:12,1
Intel,nsnsg7n,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,hardware,2025-12-06 21:49:06,-4
Intel,nskuu1l,Wait isn't xess a lower resolution per quality setting?,hardware,2025-12-06 11:27:34,35
Intel,nsuy6fa,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",hardware,2025-12-08 00:59:57,6
Intel,nslxyj3,Maybe for the low end but high end I canâ€™t even buy a card at msrp outside of America.,hardware,2025-12-06 15:53:05,33
Intel,nspes5s,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",hardware,2025-12-07 03:43:50,7
Intel,nsl2hua,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,hardware,2025-12-06 12:36:00,10
Intel,nt09q01,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,hardware,2025-12-08 21:40:56,1
Intel,nslaf1q,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",hardware,2025-12-06 13:33:32,-9
Intel,nsl9m9r,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",hardware,2025-12-06 13:28:13,-8
Intel,nsmb10n,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",hardware,2025-12-06 17:02:45,-6
Intel,nslqts2,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,hardware,2025-12-06 15:13:39,27
Intel,nspdyuf,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",hardware,2025-12-07 03:38:38,1
Intel,nsn70s5,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",hardware,2025-12-06 19:49:40,12
Intel,nsmn51w,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",hardware,2025-12-06 18:06:54,9
Intel,nsl1d2d,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",hardware,2025-12-06 12:26:39,39
Intel,nskx602,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",hardware,2025-12-06 11:49:24,1
Intel,nsxjyy3,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,hardware,2025-12-08 13:22:59,1
Intel,nsxk5xm,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,hardware,2025-12-08 13:24:15,2
Intel,nt0aiwr,"They are all selling below MSRP in the UK. Â£979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",hardware,2025-12-08 21:44:54,1
Intel,nsl44tr,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",hardware,2025-12-06 12:49:07,77
Intel,nslgt6n,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",hardware,2025-12-06 14:14:46,17
Intel,nssphno,Gpus are way more expensive to produce,hardware,2025-12-07 18:04:53,1
Intel,nsxk9lk,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",hardware,2025-12-08 13:24:53,1
Intel,nslrykc,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,hardware,2025-12-06 15:20:04,0
Intel,nsridpx,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",hardware,2025-12-07 14:17:24,0
Intel,nsmyw1o,"It does, but also, inflation has been extremely bad for 5 years.",hardware,2025-12-06 19:06:53,-1
Intel,nslef0x,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",hardware,2025-12-06 13:59:32,36
Intel,nsmzdz9,TSMC inflation is FAR higher than CPI. You are half right,hardware,2025-12-06 19:09:24,12
Intel,nsxnqe7,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",hardware,2025-12-08 13:46:23,2
Intel,nslsb0f,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,hardware,2025-12-06 15:22:04,13
Intel,nsuys01,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",hardware,2025-12-08 01:03:40,1
Intel,nsmjt4j,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,hardware,2025-12-06 17:50:02,0
Intel,nsokkai,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  Iâ€™d say itâ€™s getting better and that Iâ€™d recommend it over a 5050 since the overhead is now fixed/negligible.,hardware,2025-12-07 00:31:34,10
Intel,nsxo5ua,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,hardware,2025-12-08 13:48:58,1
Intel,nsq49gv,And that only in terms of native resolution and does not mean equal final image quality.,hardware,2025-12-07 06:54:10,5
Intel,nsljs69,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,hardware,2025-12-06 14:32:33,21
Intel,nt3tjxg,Iâ€™m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. Thatâ€™s not even close at all to the msrpâ€¦,hardware,2025-12-09 13:05:09,1
Intel,nsq2h5v,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",hardware,2025-12-07 06:37:50,8
Intel,nslou4e,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",hardware,2025-12-06 15:02:28,-10
Intel,nsm6adc,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",hardware,2025-12-06 16:37:49,-11
Intel,nstfa7l,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",hardware,2025-12-07 20:08:33,-5
Intel,nsm3fs2,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",hardware,2025-12-06 16:22:46,18
Intel,nslu7ja,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,hardware,2025-12-06 15:32:39,-8
Intel,nsluf98,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,hardware,2025-12-06 15:33:51,20
Intel,nslugvd,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,hardware,2025-12-06 15:34:05,3
Intel,nsnx0y4,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,hardware,2025-12-06 22:14:23,5
Intel,nspdbpk,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",hardware,2025-12-07 03:34:19,3
Intel,nsxjt6i,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,hardware,2025-12-08 13:21:58,5
Intel,nsqehxx,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",hardware,2025-12-07 08:33:16,11
Intel,nsm3au4,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",hardware,2025-12-06 16:22:03,40
Intel,nsp8t2z,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,hardware,2025-12-07 03:04:36,3
Intel,nsm6g3n,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",hardware,2025-12-06 16:38:39,4
Intel,nsmzr6o,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",hardware,2025-12-06 19:11:17,3
Intel,nsn5d71,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",hardware,2025-12-06 19:40:49,6
Intel,nsmgtnv,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",hardware,2025-12-06 17:34:05,6
Intel,nsmsf6b,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",hardware,2025-12-06 18:33:48,1
Intel,nsxkhp8,it does not matter how close to the top GPU is. its a completely useless comparison.,hardware,2025-12-08 13:26:19,2
Intel,nsm74h5,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,hardware,2025-12-06 16:42:14,1
Intel,nslzd1j,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",hardware,2025-12-06 16:00:40,3
Intel,nsxodfi,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",hardware,2025-12-08 13:50:12,1
Intel,nsm6ry8,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,hardware,2025-12-06 16:40:22,3
Intel,nsxkob8,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",hardware,2025-12-08 13:27:28,1
Intel,nsmnevw,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",hardware,2025-12-06 18:08:19,-10
Intel,nsxl3k9,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",hardware,2025-12-08 13:30:09,2
Intel,nsxl8jt,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,hardware,2025-12-08 13:31:02,1
Intel,nsmwsat,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",hardware,2025-12-06 18:56:07,-5
Intel,nsoze8r,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",hardware,2025-12-07 02:04:41,3
Intel,nsxmt3k,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",hardware,2025-12-08 13:40:46,1
Intel,nsxn5gg,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",hardware,2025-12-08 13:42:53,1
Intel,nsm8g0h,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,hardware,2025-12-06 16:49:09,5
Intel,nslzlxy,"Fair point, but isn't lower and competitive prices good for us?",hardware,2025-12-06 16:02:01,1
Intel,nsmqlcg,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",hardware,2025-12-06 18:24:26,17
Intel,nspgewd,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",hardware,2025-12-07 03:54:39,6
Intel,nsxlsss,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,hardware,2025-12-08 13:34:33,1
Intel,nsxop63,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",hardware,2025-12-08 13:52:10,1
Intel,nsmvfxi,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",hardware,2025-12-06 18:49:17,0
Intel,nsxm00p,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",hardware,2025-12-08 13:35:48,1
Intel,nsrkwkw,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",hardware,2025-12-07 14:32:28,1
Intel,nsncxlc,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",hardware,2025-12-06 20:22:05,5
Intel,nsyfiin,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",hardware,2025-12-08 16:15:19,1
Intel,nss9lt6,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",hardware,2025-12-07 16:44:57,2
Intel,nt9j82j,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",hardware,2025-12-10 09:42:20,1
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,54
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,70
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,9
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,5
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,npgcjdt,This would still no where be close to M4/M5 in single core and GPU,hardware,2025-11-18 06:45:37,1
Intel,nvtf2fd,I sincerely hope Samsung will work on their camera and make it better in this version.,hardware,2025-12-25 03:06:03,1
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-4
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose itâ€™s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,43
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,13
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,30
Intel,notfij9,I mean the early Iris pro onboard GPUâ€™s traded blows with gtx 650m at a lower power draw. Itâ€™s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,10
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,7
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,0
Intel,nvy72ph,"I swear, the only issue i have with the book 5 is the camera, this shit doesn't deserve the book 5 hardware",hardware,2025-12-26 00:34:46,1
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,17
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,10
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,3
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,7
Intel,nouozrl,Iâ€™m assuming the caveat to posts like this is â€œrunning a version of windows/linuxâ€,hardware,2025-11-14 18:41:25,12
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,23
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,2
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,10
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-6
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,13
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,6
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,12
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,6
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,22
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,18
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,12
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,70
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,45
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,8
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-8
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-2
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse itâ€™s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS â€” thatâ€™s where itâ€™s at. And Iâ€™ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasnâ€™t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesnâ€™t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-6
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,51
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,17
Intel,nnntnit,"You gotta remember these CPUâ€™s donâ€™t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultraâ€™s you get one core and thatâ€™s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything itâ€™s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,4
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,21
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,9
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,10
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> [â€¦] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W â€“ In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-2
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-5
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,4
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,20
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,11
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,10
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,5
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,2
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-8
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,26
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Arenâ€˜t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they arenâ€˜t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they arenâ€˜t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,7
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,12
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,6
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,13
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I donâ€™t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but Iâ€™ve been keeping my eye out for a replacement. Iâ€™m not alone in that need, but I do get this is a minority group. Iâ€™m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,19
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,19
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of â€¦ 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-8
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,10
Intel,nnh18hs,">Â BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.Â    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.Â    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,11
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,5
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,10
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,7
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,7
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,7
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.Â    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.Â    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,15
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,6
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-3
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, Iâ€™d definitely be tempted to play around with it, and yes, I donâ€™t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-5
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,6
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,6
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,3
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,12
Intel,nngrskq,"They are definitely more than usable if itâ€™s anything like LNL, which basically defaults to them.  If itâ€™s more like ARL or MTL, itâ€™s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,12
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,10
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,8
Intel,nnhryps,>Arenâ€˜t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,16
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,13
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,5
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,12
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,8
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,12
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasnâ€™t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,10
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,17
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,4
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.Â    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.Â    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.Â    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.Â    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.Â    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,16
Intel,nnguuaj,"> 18A has always been *Xyz* â€¦  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient â€¦  > Wait for the desktop/server chips before you call it.  â€¦ and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,7
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile â€¦  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology â€“ Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-7
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,6
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then â€¦,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,8
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,3
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,9
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,-1
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,8
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,8
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so â€¦,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,4
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,8
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,7
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,14
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,7
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,3
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,6
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What delâ€” *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1â€“2 years later. That's called *Â»delayÂ«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,0
Intel,nnhohos,>Â Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.Â    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,8
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.Â    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm,Â but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,8
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,5
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,3
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,5
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.Â    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.Â    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.Â    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,12
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,6
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,6
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,6
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,2
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,4
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that â€”  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design â€¦  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 â€“ I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it â€¦  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* â€¦  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory [â€¦]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake â€¦",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-4
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,9
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,1
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,7
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term Â»differenceÂ«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,6
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,8
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,5
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.Â    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45â€“65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) â€¦ Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them â€“ So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,7
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually â€¦,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent â€“ CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced â€“ CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo â€“ CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true â€¦  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about â€¦  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP â€“ The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017â€“2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not â€” A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already â€¦  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,30
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,57
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,16
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-21
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,12
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,26
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,5
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? â€¦Â but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,31
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,12
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,4
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,22
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,14
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,3
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes XeÂ³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   XeÂ³ is a XeÂ³ based architecture like XeÂ³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. XeÂ³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,3
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,6
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,30
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,4
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entryâ€‘level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-3
Intel,nis0ezd,[AMD RDNA 3.5â€™s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,13
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,16
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,4
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,38
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,27
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,14
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,31
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,10
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,9
Intel,nii5519,Hahaha I can hear Tom saying â€œfucking Tom Peterson has been *lying* to youâ€ as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,10
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,4
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,17
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,5
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,8
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,4
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,5
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,7
Intel,nyxkxhx,"Intel GPUs have come a long way, and part of that journey was from A to B. The current lineup are th B series and are regarded as significantly better than the A series. I recommend B580 GPUs for gamers on a budget. It's a good card, and most of the a series issues are gone. Drivers work, upscaling happens, compatability is good.  tldr: Current Intel GPUs are fine, just avoid the A-series",buildapc,2026-01-11 06:49:31,8
Intel,nyxlbjn,"A580? Nah not that much, the RX 6600 and RTX 3050 are the same price and generally either have better better performance or better tech, both have better optimization, the Arc B580 though, absolutely as it has good drivers and is on the same level as a 4060 for a lower price",buildapc,2026-01-11 06:52:49,8
Intel,nyxou31,Look up the model on Gamer's Nexus. They seem to like them for the price point and pointed out that updates have fixed a lot of initial issues.,buildapc,2026-01-11 07:23:19,2
Intel,nyz9r5o,so will b570 be fine?,buildapc,2026-01-11 14:53:47,1
Intel,nyyoj98,"I wonder why no one ever recommends the B570. Sure, it might be less powerful than the B580, but I suppose that has to work better for certain CPUs that aren't that powerful",buildapc,2026-01-11 12:43:16,2
Intel,nz0htle,"Sure. It's about 10-15% slower than the B580, so take that into account when doing ""fps/$"" calculations in your head.  https://www.techpowerup.com/review/asrock-arc-b570-challenger/32.html",buildapc,2026-01-11 18:24:42,2
Intel,nz1l8yi,oh i deadass forgot that gpu existed ngl,buildapc,2026-01-11 21:22:02,1
Intel,nx5hhax,It's a good gpu for its price. The driver issues that ruined A-series seem to be gone. Just be sure to enable ReBAR: BattleMage needs to to perform best,buildapc,2026-01-01 22:20:13,5
Intel,nx5gj70,[https://www.youtube.com/watch?v=00GmwHIJuJY](https://www.youtube.com/watch?v=00GmwHIJuJY)   [https://www.youtube.com/watch?v=npIpWFSfmv4](https://www.youtube.com/watch?v=npIpWFSfmv4)  Not sure what kind of options you have where you are. 3060 is pretty long in the tooth unless your getting a smoking deal. No low priced 9060s or or anything around where you are?,buildapc,2026-01-01 22:15:11,3
Intel,nx6wr9i,At 250 its a crazy good deal. Its not top of the line but its winning competition is the fact that it beats out every other card in its weight class at its price point.,buildapc,2026-01-02 03:18:01,2
Intel,nx7n3le,Itâ€™s great,buildapc,2026-01-02 06:21:14,2
Intel,nxbtc77,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance?",buildapc,2026-01-02 21:45:50,1
Intel,nxbtinv,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance? Especially if the driver issues are no longer present with the more recent updates",buildapc,2026-01-02 21:46:42,1
Intel,nxbt4xl,"A bit out of my price range, even for a used one. Iâ€™m only 19 and Iâ€™m building this by myself with my own money as my first ever build. From where Iâ€™m from, the 3060 12 gb is a bit manageable price-wise, I just wanted to check out other options.",buildapc,2026-01-02 21:44:51,1
Intel,nxo75v2,"i remember hearing that the overhead issues are less of/not an issue now, due to driver updates",buildapc,2026-01-04 18:49:23,1
Intel,nxbtrn7,[similar to a 3060ti](https://www.techpowerup.com/review/intel-arc-b580/32.html) but with newer features,buildapc,2026-01-02 21:47:56,2
Intel,nxbzpsr,Fair. If your on a tight budget. Consider some of the RDNA3 AMD cards as well. I mean if your looking at a 3 generation old 3060 anyway. Something like the 7600 XT 16gb might be on the market where you are at a good price. Its probably 20% or so faster then a 3060.,buildapc,2026-01-02 22:17:35,1
Intel,nyk2zwg,">30c ish at idle to 60-80c under load.  That sounds normal my guy intel i9s are well known power hogs, the 200-300w that thing consumes will generate that kind of heat. Most people with that cpu run it with a 360mm aio to keep the temps in check.  Id suggest getting a 360mm rad.  [https://www.pcgamer.com/intel-core-i9-12900k-review-benchmarks-performance/](https://www.pcgamer.com/intel-core-i9-12900k-review-benchmarks-performance/)",buildapc,2026-01-09 07:27:00,8
Intel,nyk4qvy,"Those temperatures could be totally normal for the 12900kf / 240mm, depending on your room ambient temp and your case air flow. TjMax for that CPU is 100c as well, so running at 80c is totally safe and won't even throttle.",buildapc,2026-01-09 07:42:23,3
Intel,nyk22lk,"If the AIO screw has springs, you don't need to force it.. Check if paste covers whole surface and if you got the stock cooler try using it if not get a cheap cooler",buildapc,2026-01-09 07:18:55,1
Intel,nyk23cs,The pump is set at a certain speed. Can you go to bios and bump up the pump speed to check if that resolves anything?,buildapc,2026-01-09 07:19:06,1
Intel,nyk46lk,"Start with setting up your PL1/2 for the capacity of your AIO. If it is set to standard, 4096W, you are going to have problems cooling that thing. Start with 253/253 standard Intel would be 125/253/304A and keep lowering that until there is no more thermal throttling. After that you can fine tune with undervolt etc.  And ofcourse check AIO mounting, pump, fluid.",buildapc,2026-01-09 07:37:26,1
Intel,nyk88dk,For both 12th and 13th gen you can do the following:  - turn hyperthreading off - enable only 8 ecores - Set a negative voltage offset of - 0.050  Play games for a couple of hours if no crashes then decrease offset by another 0.025 (I.e. 0.075). And keep going till you find the lowest voltage without crashes. This will increase 1% lows tremendously in games too.   Once you're stable and if temps are still too high (thermal throttle) then you can reduce clock speed by 100hz and keep doing it till you're happy. Reducing by 200mhz doesn't hurt gaming performance.  Benchmark a CPU limited game (or make it CPU limited by lowering the resolution) before you start so you can see the gains.,buildapc,2026-01-09 08:13:22,1
Intel,nykdz7n,not a single mention of voltages?,buildapc,2026-01-09 09:05:28,1
Intel,nyk4j60,"I ordered a new case and a 360mm AIO which is supposed to be here tomorrow, since my current case doesn't support a 360 on the top, I plan to move everything over sometime this weekend. I was just wondering if there could be a different issue because the temps were rising instantly even with the AIO supposedly running.",buildapc,2026-01-09 07:40:30,0
Intel,nyk6ox6,"My ambient temperature is about 23c. This might be a dumb question, but do different CPUs have different safe temperatures? Everything I was reading was saying, generally, anything above 80-85c is risking permanent damage to the CPU.",buildapc,2026-01-09 07:59:35,0
Intel,nyk2hvo,Try mounting your AIO higher too.,buildapc,2026-01-09 07:22:36,1
Intel,nyk3yu1,"My pump control doesn't show up in the bios, saying it's at 0 rpm, but using corsair's software, it says it's running at about 2800 rpm with the extreme setting.",buildapc,2026-01-09 07:35:34,1
Intel,nyk84k2,"I'm not sure what PL1/2, 253/253 or 125/253/304A mean. I'll look up each of those unless you're willing to explain them in more detail.",buildapc,2026-01-09 08:12:24,1
Intel,nyka6tl,Okay thanks I'll give this a shot.,buildapc,2026-01-09 08:30:58,1
Intel,nyk68kd,"All cpus do that, remember electrical signals move at the speed of light but heat does not, so cpus can start doing work and heat up/heat down in nanoseconds.",buildapc,2026-01-09 07:55:36,2
Intel,nyk89gv,"They sure do, it used to change quite a bit with different CPU generations in the past. [Here is the official intel page for the 12900kf](https://www.intel.com/content/www/us/en/products/sku/134600/intel-core-i912900kf-processor-30m-cache-up-to-5-20-ghz/specifications.html), max temp of 100c is specified under ""package specification"" (TJUNCTION). You are definitely not risking any damage at 85c.",buildapc,2026-01-09 08:13:39,2
Intel,nyk5218,"I don't think the mount has springs, just a backplate for the motherboard and the standoffs that screw down the pump head. What do you mean mounting it higher? Higher in the case? I currently have it mounted at the top of my case.",buildapc,2026-01-09 07:45:08,1
Intel,nyk8ii4,"Do not know your mainboard, so canâ€™t help you. But Pl1/2 are the and short duration of max powerdraw so 253short 125W long with a max current draw of 304A. Basically the values you would need to cool.",buildapc,2026-01-09 08:15:55,1
Intel,nykgrws,Let us know results.,buildapc,2026-01-09 09:31:31,1
Intel,nyk7lpd,"Right, but I was expecting the AIO to keep the temperatures from jumping instantly to 90 or 100 at around 60% load.",buildapc,2026-01-09 08:07:43,1
Intel,nyk97ex,"Okay thanks, I'll do some research.  I have a MSI Z790 Gaming Pro WiFi motherboard.",buildapc,2026-01-09 08:22:05,1
Intel,nyqm5ak,"Thanks for the suggestion, I was able to get ARC Raiders running today for a couple of hours mostly below 80c (68-78ish) with a -0.100 offset with no stability problems so far. I haven't found the point where it becomes unstable, so I probably could decrease it more to get the temps even lower if needed.",buildapc,2026-01-10 05:29:36,1
Intel,nyqncbg,"My 14900k can do - 0.125 and stable. I'm sure yours can get there too, possibly beyond.",buildapc,2026-01-10 05:38:18,1
Intel,nza5qxr,"Do you happen to remember the windows stop code you were getting when yours  was unstable? I thought everything was running fine at -0.100, but I've had 3 CLOCK\_WATCHDOG\_TIMEOUT crashes when loading into a match of Arc Raiders and have since reduced the offset from -0.100, -0.090, -0.085, to now -0.080. Just concerned that I might be damaging my cpu.",buildapc,2026-01-13 02:35:38,1
Intel,nw5kwmv,"If the B580 has good performance in the game that you are interested in then it can be a good option. However, in some games there are driver overhead issues. So you cant trust every benchmark with a 7800x3d/9800x3d to estimate the GPU performance. In some games performance is reduced with a Ryzen 5 5600 for example. If this has been fixed in your games (or you have a stronger CPU) then the b580 is a good choice.",buildapc,2025-12-27 07:01:10,2
Intel,nw5kpu1,The b580 is just the overall better card. Intel may be new to the dedicated graphics market but they have been making their own GPUs for almost 20 years at this point and have the experience of supporting them through driver updates for many years,buildapc,2025-12-27 06:59:28,1
Intel,nw5kzt8,b580 cause vram,buildapc,2025-12-27 07:01:58,1
Intel,nw5na11,"How much is the 8GB 9060 in your country?  But regardless, the B580 is better for simple gaming if you can pair it with a strong CPU.  The RTX 4060 is better outside of gaming, but also works better with a weaker resolution. DLSS also helps with higher resolutions.",buildapc,2025-12-27 07:23:15,1
Intel,nwgazqm,"Thanks everyone for the help, I really appreciate it!",buildapc,2025-12-28 23:44:04,1
Intel,nw5o3fl,> B580 is better for simple gaming if you can pair it with a strong CPU  i pair the b580 with the 225f which is an entry level cpu and they work well. gpu 100% and cpu 33% usually when gaming,buildapc,2025-12-27 07:31:05,1
Intel,nwgaunz,Sadly it costs $350 here. I considered that too but it's out of the budget. Btw thanks!,buildapc,2025-12-28 23:43:18,1
Intel,nw5obdk,"The 225F is a very current CPU, there are many people who will try to pair this with something like an i3-6100 or Ryzen 1400.",buildapc,2025-12-27 07:33:12,1
Intel,nz7be98,Used rtx 3060ti for 250 or less. Fantastic value card now a days.,buildapc,2026-01-12 18:09:33,3
Intel,nz798wq,"So the 9th gen is a bit of a weird CPU right now, you do have a couple of decent upgrade options, but also you cannot go past the 9th gen without a new motherboard. Also the 9th gen is quickly coming up on 6 years old now as well.   My affordable advice would be to grab a 9700k and a better CPU cooler, then you can start to worry about the GPU which is very PSU dependent anyways. Without knowing the PSU I cant really make a suggestion anyways.   But the truth is, I would almost start fresh. New mobo, cpu, PSU and GPU. It would be more pricy, but also a far better use of your money than dumping it into a system that will be too slow to run new games in just a year or two.",buildapc,2026-01-12 17:59:50,2
Intel,nz78fo5,"I mean, really both kinda. 6500 XT is a notorious shite GPU but the games you play are CPU bound so a new CPU would do wonders. Budget is needed here.",buildapc,2026-01-12 17:56:11,1
Intel,nz7kn04,"I have the ASRock H310CM-HDV with an i5-9400. The beta BIOS will add Smart Access Memory (a.k.a., resizable bar). This may improve GPU performance...depending on game.  An i9-9900 is in the $200+ range. An i7-9700 is in the $100+ range. The i9-9900's \~24% performance gain is questionable for \~100% increase in price. There are other options between your i3-9100f and the i7-9700...  Before dropping Benjamins on a new GPU, consider an overclock and undervolt of your 6500 XT:  [https://www.youtube.com/watch?v=jxSLrvoejhg](https://www.youtube.com/watch?v=jxSLrvoejhg)  If the OC/UV yields ""good enough"" improvements, then you might be able to bypass the premiums being demanded in the current PC component market.",buildapc,2026-01-12 18:50:55,1
Intel,nz7avun,"All the games you play avoid the problem of having a weak but modern gpu very well. Assuming you dont suddenly decide to play something more graphically intensive i reccomend upgrading your cpu seeing how it feels, then upgrading your gpu. The immediate option that comes to mind is the i5 9400f. It should be an approximate 25%-30% improvement in most of the games like cs2 due to the added cores, threads and clock speed compared to that fairly weak i3 cpu. Then you could do something like an RX 6600 for $120 if findable, if not I usually reccomend the 3060 12gb as a braindead midrange option still able to play anything, it's about 200.",buildapc,2026-01-12 18:07:14,1
Intel,nz7ayh1,Get a 12th or 13th Intel i5 CPU and a decent GPU like the 5060 Ti or the 9060XT,buildapc,2026-01-12 18:07:35,1
Intel,nz78jpw,"What is your budget for upgrades?  Your GPU is currently being held back by your platform. The 6500XT only has PCIe x4, and LGA 1151 only has PCIe 3.0. 3.0x4 bottlenecks the card pretty badly, going from on par with an RX 580 to worse than an RX 570. Getting onto a newer platform with PCIe 4.0 would improve both your CPU and GPU.",buildapc,2026-01-12 17:56:42,0
Intel,nz7k4tc,"What you could/should upgrade depends entirely on budget. The CPU is nearly the slowest 9th gen Intel chip, so an upgrade there would help a lot. The ""Best In Slot"" CPU is the i9-9900K but those go for nearly $200 on eBay. You can find an i7-9700 for half that and while it's 50% faster multicore, it's only 10% faster single core and most games just use one or two cores, but there are exceptions: research which games you play to check. Fortnite and Arc Raiders can be CPU bound, so the i7-9700/K might help you out a lot with those games, or really any open world, multi-player type game. The GPU is also the slowest of the 6000 series. As far as new GPUs go, the Intel Arc B580 12GB at $250-$300 or the Radeon RX 9060XT 16GB at $380-$400 are best bang for the buck on the cheaper side, your options really open up on the used market. Go to the [Techpowerup specs page](https://www.techpowerup.com/gpu-specs/radeon-rx-6500-xt.c3850) and check out the Relative Performance of your card with faster ones. I would upgrade to something at least 200% relative performance or higher if you can afford it. Between the GPU and CPU, I'd say the GPU upgrade should take priority over CPU. While a much newer GPU might be bottlenecked by the 9100F, its vastly superior performance will leave the 6500XT in the dust. GTX 1080ti (\~$150) RTX 2080ti (\~240) RTX 3060ti (\~$220-250) RX 6750XT ($280-350) RX 6800 (\~$300) RTX 3080 (\~$350) all examples of decent used options that are considerably more powerful than the 6500.",buildapc,2026-01-12 18:48:41,0
Intel,nz7aten,Adding a 9700K isnâ€™t gonna fix the fact the GPUs being bottlenecked by PCIe 3.0.  An R5 3600 and a cheap B550 board would probably cost not much more than an 9700K and improve the GPUs performance quite a bit. It would still work with the 16GB DDR4 they already have.,buildapc,2026-01-12 18:06:55,0
Intel,nz78qkh,I have seen many videos on how it is a bad card I havenâ€™t had no problems so far but definitely on my list,buildapc,2026-01-12 17:57:33,1
Intel,nz7da6d,LGA 1700 isnâ€™t worth recommending at all. DDR4 performance has aged terribly because newer games are more memory demanding. DDR4 boards have also gone up in price a lot.Â   HUB revisited the Ryzen 5000 series last week and included a 12400F on DDR4 and DDR5 for comparison (https://youtu.be/RijAyVshtok?si=NbuCRlSV4uhHOzlv) - the 12400F on DDR4 performs closer to an R5 5500 nowadays than the R5 5600 it used to compete with.,buildapc,2026-01-12 18:18:05,0
Intel,nz7etoy,"I have not seen anywhere near that level of bottleneck from even this level of pcie mismatch. Cool you have pet issues, but this is about the cheapest possible options for the platform (and use case) at hand. Not your personal advice segment with little but misremembered content to advise from.",buildapc,2026-01-12 18:25:03,0
Intel,nz7dvey,Dude have you seen the DDR5 prices lately or have you been living under the rock?,buildapc,2026-01-12 18:20:45,1
Intel,nz7id4q,https://www.tomshardware.com/news/rx-6500-xt-pcie-gen3-gen4-tested,buildapc,2026-01-12 18:40:55,2
Intel,nz7ia34,"What? AÂ lot of the review coverage for the 6500XT was about how bad the PCIe bottleneck is. The 6400 and 6500XT are unique in that theyâ€™re PCIe x4, even current lower-end cards like the Arc B570 and RTX 5050 are PCIe x8  https://www.techpowerup.com/review/amd-radeon-rx-6500-xt-pci-express-scaling/28.html TPU saw a 24% drop from 4.0 to 3.0 at 1080p  https://www.techspot.com/review/2398-amd-radeon-6500-xt/ Techspot saw a 22% drop in averages and 25% drop in lows from 4.0 to 3.0 at 1080pÂ   I wouldnâ€™t be surprised if itâ€™s even worse nowadays, games demand more VRAM now than they did 4 years ago. More VRAM the card doesnâ€™t have = higher PCIe bandwidth need = more bottleneck when you donâ€™t have bandwidth  A cheap upgrade like an R5 3600 + B550 board would not only improve CPU performance substantially coming from an old 4c4t CPU, but give the GPU the PCIe 4.0 it needs.  If you donâ€™t know what youâ€™re talking about, donâ€™t comment lol",buildapc,2026-01-12 18:40:32,1
Intel,nz7jl7h,"Yes, Iâ€™m not stupid.  You are far better off going for Zen 3 over anything on LGA 1700.Â   The R5 5600(X) performs as well with DDR4 as the 12400(F) does with DDR5.  Used they both go for ~$100-120, new both are ~$170 right now. AM4 boards tend to be cheaper and the RAM is the exact same.  Why would you spend more on LGA 1700 to get worse performance?",buildapc,2026-01-12 18:46:18,1
Intel,nz7iqr0,"My real-world experience with low-end cpu power and this mismatch on those titles is 10%. It isnt worse than having that i3 does to cpu-bound situations. Upgrading a bad platform is probabaly not his best price to dollar, but it is the nuanced situation he's in. if he can sell what he has for the price it goes for, going ryzen is always correct. However his cpu and all the others here are bad. If he can simply get a better part price on a single chip, he can get equivalent value from the upgrade compared to going ryzen.",buildapc,2026-01-12 18:42:36,1
Intel,nz7kl1z,You can get an i5 13600k with better performance than a 5800x. Also it is difficult to find the AM4 X3D CPUs these days so the intel route makes sense. He can later upgrade to a 14900k even which no AM4 CPU can match in terms of performance,buildapc,2026-01-12 18:50:40,0
Intel,nz7kgsl,"Werenâ€™t you recommending upgrading to a 9400F?  Sure, the 9100F might be the limiting factor right now, but youâ€™re still gonna hit the same PCIe bottleneck. Itâ€™s just gonna be more pronounced.",buildapc,2026-01-12 18:50:09,1
Intel,nz7nzri,"13th gen pricing is crap so Iâ€™ll use 14th gen for comparison, itâ€™s basically the same  14600KF is $245 new and $180-200 used. It has a 181W PL2 so you need a board with good power delivery to not throttle it, especially if you want an upgrade path. The cheapest DDR4 boards you can get right now that fit the bill are $140.  5800XT is $219 new and $150-170 used. AM4 boards are still cheap, thereâ€™s plenty of quality B550s for $100.  Going Intel over AMD would cost at least $60 extra and most of the performance gains you would see using DDR5 on the 14600KF would be wiped out using DDR4",buildapc,2026-01-12 19:05:55,0
Intel,nz7kpgs,"It'll then axtually be the bottleneck. When you get something with a normal pcie layout, possible even the same value as his current card, that utterly disappears. A 6600 would be a great upgrade from that position...",buildapc,2026-01-12 18:51:13,1
Intel,nz7pbw9,"There is a high-end bias with this platform, like any. It helps a lot to understand that budget systems fall within their spec, it is only the highest-end and lowest-end options of this era that stretch the pcie issue... there are cycles of alarmist narritives in all media. That one is old news that likely shouldnt have been printed imo.",buildapc,2026-01-12 19:12:06,0
Intel,nz7r3bk,Youâ€™re going to hit the large PCIe bottleneck the moment the CPU isnâ€™t terrible. Itâ€™s not gonna take much of a CPU upgrade for a GPU worse than an RX 570 to become the issue.,buildapc,2026-01-12 19:20:11,0
Intel,nz7r90m,Im pretty sure a 9400f isnt gonna cause an issue here man... ive used this platform before. Then his pc is worth something...,buildapc,2026-01-12 19:20:54,0
Intel,nz7rnwn,Youre giving me nothing against the clear narritives youre following over substance of experience with this shitty level of cpu performance.,buildapc,2026-01-12 19:22:49,0
Intel,nz7tdgt,"Do you respect at all the starting position he is in or are u just an advice salesman? Both of our ideas coexist. selling his current pc for more than its worth and just buying a dirt cheap 2nd gen ryzen with a sane gpu for the same money, then upgrading from there is also perfectly correct. That is a similar level of effort to a platform swap, or whatever you would prefer, which is also correct. The only bad advice here wastes more time and money than his original choice of platform already sealed. Also ur just wrong about pcie bottlenecks im pretty sure",buildapc,2026-01-12 19:30:40,0
Intel,nz7whko,"Iâ€™m making the reasonable assumption that the person running a 9100F + 6500XT in 2026 doesnâ€™t have a ton of money to spend on upgrades lol  Upgrading the current platform makes no sense without also upgrading the GPU. You canâ€™t get very far with CPU upgrades on an H310 board, anything higher than an i5 is gonna be throttled.Â   Swapping in a cheap PCIe 4.0 CPU + board like an R5 3600 + B550 will improve both the CPU and GPU performance while not spending a ton of money. It also opens the door to future upgrades to something like an R7 5800XT which will hold up far better with a GPU upgrade.Â   I gave you multiple reviews showing 20+% losses on PCIe 3.0. Thatâ€™s not opinion, thatâ€™s fact. That loss is going to be the same regardless of how good or bad your CPU is as long as the CPU isnâ€™t the bottleneck.",buildapc,2026-01-12 19:45:08,1
Intel,nz84eos,I genuinely wonder what u get living in the idealized world where pcie 3.0 sucks ass below the highest-end 4.0 products and visa-versa. My dad was rly into the ideas of mearsheimer and I luckily biocontained his ass before that fkers gone and made the meme rounds again. You listen to Armand Hammer?,buildapc,2026-01-12 20:21:52,1
Intel,nz7xfno,"Yea. Your advice is fine and wasnt the target of my statements, I added the last comment as proof I agree a platform swap is a totally good option. He doesnt even need to change his cooler for a 9400f and can then get a normal gpu for a similar value to his current one. This process is much simpler and has the potential to grant equal value if the chip is recieved for a low enough bargain, increasing the chances of finding a dreal given limited time. Do we disagree that we are both good at selling advice? I am still completely debating the validity of your sicituational understanding of pcie bottlenecks in relation to low cpu power circumstances, partially because of ur choice of sources, and ur disintrest in confronting that statement. His pc can gain value whilst spending little, my option is simply the lowest I coukd imagine.",buildapc,2026-01-12 19:49:28,0
Intel,nvursni,r/homelab,buildapc,2025-12-25 10:48:55,1
Intel,nysw1sj,yeah thats fine,buildapc,2026-01-10 15:48:54,4
Intel,nyt3nnp,"I would probably pick a weaker CPU and stronger GPU, something like Ryzen 7600x / 7600 / 7500f and RTX 5060Ti 16GB or RX 9060xt 16GB.",buildapc,2026-01-10 16:25:28,2
Intel,nysvyfx,"Pretty much the only scenario that combo makes sense his high-fps gaming at 1080p. It will get probably 60-70fps for [AAA single player games](https://www.techspot.com/review/2935-intel-arc-b580/#Average_1080p) w/ high/ultra settings, but much worse with those that require [ray-tracing](https://www.techspot.com/review/2935-intel-arc-b580/#Ray_Tracing_Performance_Summary)  If you want to play both games like fortnite and Ghost of Tsushima I would recommend an AM4 setup like 5700X and RX 9060 XT 16gb.",buildapc,2026-01-10 15:48:27,2
Intel,nytq7z3,I found good deals on 7600X and RX 9060 XT 16GB. Thanks alot!,buildapc,2026-01-10 18:12:15,2
Intel,nyt4rsv,"Hm intereseting, I'll see benchmarks and if I can find good deals. I'll think about it but thx",buildapc,2026-01-10 16:30:42,1
Intel,nyt339q,Tbh I don't mind 60fps at high settings. Thanks for explaning ðŸ‘Also for AAA games i could use XeSS and Frame Gen ngl,buildapc,2026-01-10 16:22:47,2
Intel,nyt4fxb,Happy to help!,buildapc,2026-01-10 16:29:10,2
Intel,nzdu3xk,"If you are upgrading CPU (presumably to a 5000 series?) then B580 is probably the best option, but you need to make sure that your motherboard supports Resizable Bar otherwise performance will be bad.  Otherwise I'd probably take the 5050.",buildapc,2026-01-13 17:25:13,2
Intel,nzdubvy,At those prices arc b580 is the only thing that really makes sense. Any more options for used cards like a 7600/ 6600xt etc?,buildapc,2026-01-13 17:26:16,2
Intel,nzdup5i,"Get the Intel B580 12GB, it's drivers are more stable now as compared to before. Nowadays you don't want an 8GB GPU. Bad idea in 2026.",buildapc,2026-01-13 17:27:58,2
Intel,nzdux9c,"Have had my B580 since mid-Dec 2024, and have had no issues because of the games played, and it is also paired with a Ryzen 5 5600. Aside from the RX 6600, what are other AMD options available to you that maybe at the 400$ mark since that is your budget for the 5050...?",buildapc,2026-01-13 17:29:01,2
Intel,nzf7hvg,Get some RTX 3080 10/12 gb and don't fuck my brain.,buildapc,2026-01-13 21:10:34,1
Intel,nzduddl,Might be a dumb question but what is a resizable bar? and also how do i check if my motherboard has it,buildapc,2026-01-13 17:26:27,1
Intel,nzdusxl,i dont really want to get used cards but i found used 7600 for 230$ (8gb) and used 6600 for 250$,buildapc,2026-01-13 17:28:28,1
Intel,nzf641i,good to hear,buildapc,2026-01-13 21:03:59,1
Intel,nzdv92r,the AMD 500 series gpus has their support ended and the other decent ones are above my budget if i get a new one and i dont wanna buy used ones,buildapc,2026-01-13 17:30:34,1
Intel,nzf7s4c,shit is 650$ in my country way out of my budget,buildapc,2026-01-13 21:11:56,1
Intel,nzdvdyq,"It's a setting in your motherboard BIOS. [Here is Intel's support article](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html) about it. It is basically required to get the expected performance from Intel GPUs, but having it available as an option was something introduced around 2019, not all older motherboards have it.  Your board might also not have it right now, but if you update the BIOS (which you need to do for a new CPU anyway) that might add support for it.",buildapc,2026-01-13 17:31:12,1
Intel,nze4838,Yeah neither of those is really worth it over the 580 imo unless your board doesn't support rebar as other commenters have pointed out. If that ends up being the case i really think the 7600 used might be the best option.,buildapc,2026-01-13 18:11:19,1
Intel,nzdvqv6,So nothing newer than the RX 6600?  And what is your mobo and PSU?,buildapc,2026-01-13 17:32:55,1
Intel,nzdwas3,GIGABYTE GA-A320M-S2H motherboard and 500w psu,buildapc,2026-01-13 17:35:29,1
Intel,nzdyq5a,"There are multiple revs of that mobo model. If resizeable bar (or whatever Gigabyte calls it) is currently not available in your BIOS version, you may need to incrementally update BIOS (and you may need to do it all the way to the most recent). Check after each update; the Gigabyte site is showing that incremental updates are necessary...read the notes.  500W PSU will limit your GPU options. What is/are the PCIe connector(s) available from that PSU?",buildapc,2026-01-13 17:46:44,2
Intel,nzf6200,"I replied a bit late but i just updated my BIOS to the latest version, i will check if theres a resizeable bar in my motherboard. My PICe connector is 2 + 6 connector i think",buildapc,2026-01-13 21:03:43,1
Intel,nzfbzne,"If rebar exists, then you have 50 watts to spare and the connector for the ARC B580 aside from the overhead that may be there with the 1600...",buildapc,2026-01-13 21:31:16,2
Intel,nzfcoli,so should i go for it? even if i dont have rebar (i still didnt check it its 1 am) i will upgrade my cpu in a month,buildapc,2026-01-13 21:34:27,1
Intel,nzfd90j,ok i just checked it and it does have rebar,buildapc,2026-01-13 21:37:01,1
Intel,nzfevx5,Then the overhead issue with the Ryzen 5 1600 is the remaining concern...and that may be game dependent.,buildapc,2026-01-13 21:44:30,2
Intel,nz1ncrd,DDU ?,buildapc,2026-01-11 21:31:58,2
Intel,nz1ulf3,You shouldn't be getting worse performance at all. Did you run DDU to uninstall your old Nvidia drivers? Often switching from Nvidia to AMD and vice versa can cause issues like this.,buildapc,2026-01-11 22:05:53,2
Intel,nz2n2ta,most shooter games relyÂ on cpu and your running a 2018 cpu with a 2025 gpu not really a big problem just prob being bottlenecked you also may be running arc raiders with nvidia settings while using a amd card,buildapc,2026-01-12 00:29:14,1
Intel,nz1mt2f,"You're cpu bottlenecked. You can increase graphics settings. This won't increase your fps, but it might improve stutters and it will give you free graphics improvements.",buildapc,2026-01-11 21:29:22,1
Intel,nz1s0t7,Is the GPU at 100% utilization?  Are you vram limited?,buildapc,2026-01-11 21:53:46,0
Intel,nz1noqv,"yes, nothing change after DDU or Windows Reinstall",buildapc,2026-01-11 21:33:32,1
Intel,nz1n3nj,"i already trying this too, same FPS on LOW and EPIC",buildapc,2026-01-11 21:30:45,1
Intel,nz1o00e,"You can also set a fps cap, lower it a bit until you no longer see stutters.",buildapc,2026-01-11 21:35:00,1
Intel,nz1od1m,fps cap can't help because FPS are going to 45 to 70 to fast,buildapc,2026-01-11 21:36:42,1
Intel,nz1oyty,Then you just need to set it low enough. Otherwise you'll keep facing stutters.,buildapc,2026-01-11 21:39:33,1
Intel,nz3u612,There is no way this setup runs battlefield 6 on more than 60 fps,buildapc,2026-01-12 04:19:39,6
Intel,nz3uey9,All them cards will work fine with your motherboard. The pcie generations are not that much of a issue in actual performance. The issue I do seeing u having is CPU not really keeping up. But you can go up too 5000 AMD just flash your bios before hand. 3080 GPUs are good at 1080 and 1440p. But you will have to upgrade PSU to 850w. You can upgrade GPU first and see if your happy or not and than upgrade CPU later. The 5060 will use less power than a 3080,buildapc,2026-01-12 04:21:10,3
Intel,nz46roy,"You may gotta think also that your CPU seems to be a possible bottleneck cause.  I researched, and as long as your motherboard has a **rebar feature**, your above PCIe gen GPU should work fine. For this, you gotta update your BIOS.  So said that, I'd advice you to upgrade your CPU to a 5600/X and to look for an used GPU, like a RTX 2600 12GB! And maybe to play in 1440p.",buildapc,2026-01-12 05:47:44,2
Intel,nz3txwf,Pls dont forget iam talking about 2k gaming. ðŸ˜Š,buildapc,2026-01-12 04:18:16,1
Intel,nz3u8r4,Whats the price of a 9060XT 16GB there?,buildapc,2026-01-12 04:20:07,1
Intel,nz3ubxq,"9060xt 16gb?  If not maybe 7700xt used, 7800XT used, 4070 used, 6700xt used or 6800XT used. Those are some good options that are above 8gb of ram I believe",buildapc,2026-01-12 04:20:39,1
Intel,nz3wdpa,"tough spot to be in, the PSU is holding you back from the better value GPUs like the 3080. Not many GPUs you can actually upgrade to because of the PSU with only 1 8pin(daisy chain doesn't count). I think upgrade the CPU, then when you can upgrade your psu, then get a 3080.",buildapc,2026-01-12 04:33:42,1
Intel,nz48oac,Iâ€™m running a Ryzen 5 5500 with an RTX 3060 eagle and itâ€™s been pretty much able to handle anything Iâ€™ve tried playing. Considering I got the PC pre built for like $700 then replaced the graphics card and slapped in a second SSD it wasnâ€™t too bad.,buildapc,2026-01-12 06:02:53,1
Intel,nz4tfth,You will benefit a lot in esports titles by getting a secondhand 5600(x) for your CPU!  The thing with newer NVIDIA cards in the 60-tier is that they use x8 lane configuration and therefore PCIe 3.0 will be somewhat of a bottleneck.  [https://www.youtube.com/watch?v=V8KkRibhp7A](https://www.youtube.com/watch?v=V8KkRibhp7A) the 4060 Ti is a very small upgrade over the 3060 Ti in an ideal configuration. I couldn't find a PCIe 3.0 1440p benchmark but the 3060 Ti should be able to perform closer to its max due to its x16 lane configuration.,buildapc,2026-01-12 09:09:27,1
Intel,nz7mwqg,"How much do 6800(xt) 6900xt, 7700xt or 7800xtâ€™s cost? Or a 3070.",buildapc,2026-01-12 19:00:59,1
Intel,nz3vd8k,Smart guy,buildapc,2026-01-12 04:27:09,0
Intel,nz3wfin,"U sure if i get 5060 8gb it will be fine ? Its the cheapest option of them anyway and i like it so much cuz its the newest , if i upgrade to ryzen 5 5600x no problem then?",buildapc,2026-01-12 04:34:02,1
Intel,nz3vs19,"144 fps at 2K is insane even if you max out every slot that your motherboard and the cooling system can handle. Without the X3D CPUs, you are nowhere near avg 144 fps. However, consistent 60-75 fps is possible.",buildapc,2026-01-12 04:29:48,3
Intel,nz3vymn,420 usd for 8gb 510 usd for 16 gb,buildapc,2026-01-12 04:30:58,1
Intel,nz3wx4j,"Thats actually a new approach never thought that way , but what will i gain from upgrading cpu now?",buildapc,2026-01-12 04:37:14,1
Intel,nz40l7s,"Daisy chaining is normally fine but yes that psu is holding him back from older higher end cards and we don't even know if it's a good 600 or some junk. What I do see is 600 is enough for the 5060 GPU,",buildapc,2026-01-12 05:02:07,1
Intel,nz49e8e,3060 is just goated bro have fun â™¥,buildapc,2026-01-12 06:08:42,2
Intel,nzg5gf0,So what did you decide to get?,buildapc,2026-01-13 23:58:48,2
Intel,nz3zusp,"Yes, that's a good combo. It's what I would suggest for a good budget build if your wanting to buy new parts.",buildapc,2026-01-12 04:57:00,2
Intel,nz3wp5u,"Iam already there with the 1660 super in fps games , doesnt have to be 144 in AAA games and i can play on medium settings too ,i prefer fps",buildapc,2026-01-12 04:35:46,1
Intel,nz411sz,"and thats why iam very positive with the rtx 5060, because I intend to upgrade MB and CPU later",buildapc,2026-01-12 05:05:20,1
Intel,nz3wg8t,"If 510 works, that could be tempting. What about 5060Ti 16GB? Any other GPUs around that price?",buildapc,2026-01-12 04:34:09,1
Intel,nz3xilo,"maybe in more CPU bottlenecked games, but you will not see a large improvement. GPU also holding you back. If possible, maybe do the upgrades altogether, you will see a good improvement that way.   My recommendation is to wait for a price drop/deal for CPU and PSU, then look for the GPU.",buildapc,2026-01-12 04:41:10,1
Intel,nzgnmdv,"Not 100% yet, but it looks like a 9060xt 16gb + upgrade cpu later  Also maybe 5060 ti 16gb",buildapc,2026-01-14 01:39:42,1
Intel,nz40bms,"I teuat u man but u r the first one who say this , everyone else said its a big no and 5060 is a disaster on gen 3, i hope u right though",buildapc,2026-01-12 05:00:16,1
Intel,nz4fbji,"Actually, in FPS games, your prior is the CPU instead of GPU. Due to the nature of those games' engines, you will benefit most for stronger CPU. You just need an average GPU to render competitive settings.",buildapc,2026-01-12 06:58:35,2
Intel,nz3xkz6,"Thats 560, high for me , i mean can get both cpu and gpu upgraded with this",buildapc,2026-01-12 04:41:37,1
Intel,nz3xrvq,"GPU pricese are going up thats why i think this way , thanks a lot ðŸ˜Š",buildapc,2026-01-12 04:42:53,2
Intel,nzgwmen,Glad your getting 16vram. I have a 3080 that is handy capped by its 10 vram. I demoted it to my living room PC.,buildapc,2026-01-14 02:30:27,1
Intel,nz41557,I'll look into it,buildapc,2026-01-12 05:05:59,1
Intel,nz42r16,Oohh damn the 5060 are 8x buss not 16x so that's weird with pcie 3. Maybe you shouldn't get one for that motherboard I dint know they cheaped out with the buss lanes on them. You need to get a card that's 16x buss lanes for pcie 3 not to be a issue,buildapc,2026-01-12 05:17:29,1
Intel,nz44o92,"i dont think i understand that totally but yeah thats what i was told xd , so i just stick with the 4060 ti?",buildapc,2026-01-12 05:31:42,1
Intel,nz47ond,"So think of lanes if traffic, I'm sad to say even 4060ti is 8 lanes too but has much more vram so it'll be less of a issue than the 5060 because your less likely to run out of vram. I saw that you said you do plan on upgrading motherboard? The 5060 runs into issues with 3.0 pcie boards when the vram runs out meaning it needs to overflow into ram more. The 16gig on the 4060ti less likely run out. You can upgrade in steps.",buildapc,2026-01-12 05:54:59,1
Intel,nz49b47,"Mmmmmmm I don't know how to tell you , but iam talking about the 8 GB version of 4060 ti too :D",buildapc,2026-01-12 06:07:58,1
Intel,nywu4pt,With your budget don't settle for a 3060. Go to Andromeda insights and checkout their 5070ti or 5080 builds with the 9800x3d. You can get them for $2100 - 2500.,buildapc,2026-01-11 03:47:10,5
Intel,nywut4s,"1. The NH-D15 is a massive waste of money. Something like a Thermalright PS120SE will cost about 1/4th as much and perform about as well, plenty good for a 9800X3D  2. Iâ€™d consider a cheaper board depending on your needs, you probably donâ€™t need any of the features of a B850 over a cheaper B650  3. RAM pricing is awful. Get a 2x16 6000c36 Crucial Pro kit for $273, the slight timing difference isnâ€™t gonna be noticeable on a 9800X3D - the additional cache substantially reduces the impact of RAM on performance  4. SSD pricing is bad. Get a 2TB GameStop SSD for $170. Sounds weird, but itâ€™s just a rebadged Lexar NM790. Solid drive and plenty good if this system is primarily for games  5. You donâ€™t need that insane 1300W PSU. An 850W Montech Century for $90 or 1050W for $110 is more than enough for this build even with a substantially better GPU  6. You donâ€™t need to buy a Windows license at retail price, there are ways to get around that pretty easily  Dump the large amount of savings into an actually good GPU",buildapc,2026-01-11 03:51:06,5
Intel,nywsr66,"Specifically for the license, you could always get it online for way cheaper haha.",buildapc,2026-01-11 03:39:19,2
Intel,nywug7c,"If I could make a swap between 2 components, it would be the PSU and 3060TI FE.     But even if GPU performance doesn't matter, it's too overkill where system draws around 400watt even in peak..",buildapc,2026-01-11 03:49:00,2
Intel,nywvnp8,"I really like the build but u are definitely overspending in a few places here that PSU is extreme overkill but I do like when people get more wattage than they need, if u are looking for quiet then u should check out the ID-cooling A720 its half the price for 99% of the cooling and just as quiet  for a 3.5K budget u could get a 5070ti no problem and its ever so slightly slower but there is cheaper RAM out there. other than that I like what u got here  [https://pcpartpicker.com/list/VJDMGJ](https://pcpartpicker.com/list/VJDMGJ)",buildapc,2026-01-11 03:55:53,2
Intel,nywt585,I personally won't choose 'FE' cards unless temps/performance being expectedly worse is fine with you.,buildapc,2026-01-11 03:41:33,1
Intel,nyww4f2,Get a cheaper cooler cheaper ram cheaper PSU cheaper SSD and get a 5070ti. Spending 3.5k and getting a two gen old budget card is insane. This  [list](https://pcpartpicker.com/user/Photonman1/saved/#view=v87WBm) is better,buildapc,2026-01-11 03:58:30,1
Intel,nywx2lk,"the cooler is a waste of money and the 3060 ti 8gb is a card that should be $250, not $465, u can probably find a 9060 xt 16gb or 5060 ti 16gb for the same price, and if u get a cheaper cooler and psu you can allocate even more budget to the gpu. 1300w is a lot for your build, even an 850w will do perfectly fine.",buildapc,2026-01-11 04:04:16,1
Intel,nyx2rvf,https://pcpartpicker.com/list/gZTNkf Trimmed some fat to fit in a graphics card that will actually keep up with your monitor,buildapc,2026-01-11 04:38:32,1
Intel,nywuvp4,https://pcpartpicker.com/list/xdPY6Q  This is assuming you can find this card through Nvidia.,buildapc,2026-01-11 03:51:31,1
Intel,nywt4vg,Buy 3k in video cards to make a ai rig for rendering and make 6-30$ a day with your purchase,buildapc,2026-01-11 03:41:30,-1
Intel,nywt6j0,"whoa - good call, thanks! Weird to see prices lower on Amazon even, than Microsoft's own site!",buildapc,2026-01-11 03:41:46,1
Intel,nywtws7,Thanks for the input! Something you'd recommend in its place?,buildapc,2026-01-11 03:45:58,1
Intel,nywyclm,"The SSDs you recommended are 2230, not 2280, and wonâ€™t even mount on the motherboard you recommended.Â A single 4TB GameStop SSD is $320 and solves that issue  Spending $65 extra on 6000c30 over 6000c36 (2x16 Crucial Pro is $274) also seems like a waste of money for a 9800X3D where youâ€™d never notice the difference",buildapc,2026-01-11 04:12:01,1
Intel,nywylyr,I wouldnâ€™t recommend 6400c32 as out of the box compatibility beyond 6000 isnâ€™t 100%  Get a GameStop 2TB (rebadged Lexar NM790) over the parts bin UD90 for $20 less,buildapc,2026-01-11 04:13:38,1
Intel,nywtcbf,"In my country (Singapore) it very big haha, the official sell around 200 iirc but Chinese sellers for even 10~20",buildapc,2026-01-11 03:42:42,1
Intel,nywymak,Just spend 2 bucks on a 2230 to 2280 adapter.,buildapc,2026-01-11 04:13:42,0
Intel,nywyrv6,Fair point.,buildapc,2026-01-11 04:14:38,1
Intel,nywz8qx,"Or you could also just get a drive that actually fits instead of using adapters, is cheaper price/TB, and offers better performance - high-capacity 2230 drives all use lower-power controllers and QLC NAND",buildapc,2026-01-11 04:17:13,1
Intel,nwmhbvr,"The 9060 xt should be 30-35% faster than the b580 depending on games and resolution. To me, that's definitely worth a 14% increase in cost.  8 GB does mean the card won't age as well, but it doesn't mean the card is suddenly worthless. There are very few games where 1440p simply won't run on 8 GB of RAM. With the RAM crisis only going to get worse, my hope is that developers will put a little bit of effort into optimizing vRAM usage.  (edit: and based off your pricing, the 16 GB should only be about $50 USD more. That is probably worth it because not only will it perform better now, but you'll get a couple years longer out of it, and it will have higher resale later.)",buildapc,2025-12-29 22:17:02,12
Intel,nwmjhz4,"VRAM usage is based on your resolution, mostly, in conjunction with game settings at that resolution.  If you plan on gaming at all at 1440p, do **not** get a card with 8GB of VRAM. This isn't a future issue, this is a now issue. It currently does not have enough vram for 1440p gaming in nearly any new heavy titles, including battlefield 6 (just as a recent example).   Hell, at 1080p my 8gb card was over capacity on battlefield 6. There's an argument to be made that it isn't enough for future 1080p gaming as well.  Can you do plenty of gaming on an 8gb card? Yes. Can you run every game at max settings? No, not even on 1080p. You'll have to turn settings down. Which frankly I'm in the boat of ""not usually a big deal"", but it's not good when you pay 300+ bucks for a new piece of hardware that can't run your games at full tilt.   It is more powerful than the B580, yes. The B580 will last longer because of VRAM, also yes.   My suggestion is really think about how long it will be until your next upgrade and make your decision based upon that. Longevity, go with intel. More power now for just a couple of years (2 or 3), consider the 9060XT/5060 (at 1080p only, skip entirely for 1440p). Or look at their 16gb variants.   You could also look at used options. You can find a 6750XT 12GB for 200-250 USD typically, as an example.",buildapc,2025-12-29 22:28:10,2
Intel,nwmnkd8,"in that price range I think you can find good used deals. also maybe take a look at the rx 6800 xt (or generally gpus from 1 or 2 generations ago). I have one since 3 years or so and works like a charm (you will lose fsr 4 compared to the 9060 xt, but it has 16gb vram and better performance) in my country (in central europe) I saw some sold for about $250",buildapc,2025-12-29 22:49:17,2
Intel,nwmiajn,"It depends on what games you play and refresh rate not just resolution.  As you mentioned, it's not just a matter of the amount of VRAM, otherwise a 1070 or 1080 Ti would still be used -I bring it up because I just upgraded from a 1070 :)  I would spend the extra $40. I think the RX 9060 XT 8GB would also have more resale value.",buildapc,2025-12-29 22:21:58,1
Intel,nwmjwp3,16gb,buildapc,2025-12-29 22:30:14,1
Intel,nwmu4vj,"9060xt 16gb, is the one you should get if you are planning for 1440p in the future. Get that one that future you will thank and appreciate in the long run.",buildapc,2025-12-29 23:24:40,1
Intel,nwmv68o,"definitley wait and save up for the 16gb of the 9060xt, it will be worth it",buildapc,2025-12-29 23:30:20,1
Intel,nwn4giv,If you're on PCIe 3 the 9060 is the only choice here.,buildapc,2025-12-30 00:20:57,1
Intel,nwn5ndl,"So recently I bought a new prebuilt that came with a 5060 TI 8GB. For my old system, shortly before, I'd purchased a 9060 XT 16GB.  I haven't had an AMD card in many a year, and I was inclined to be generous to NVIDIA, thinking ""Hey, they've got DLSS, they've got multi-frame gen, yadda yadda yadda...""  So I'd been playing Borderlands 4 on my old rig with the 9060 XT when the 5060 TI system came in a few days ago. I tried playing Borderlands 4 on the new PC with DDR5 RAM and a much better processor, and for every game I tried (Witchfire, Borderlands 4), it was a much worse experience.  DLSS frame gen treated me well on my old 4060 so I thought  multi-frame gen would be rad and close the 8GB-16GB gap, but holy shit the lag was like improperly configured Lossless Scaling. It was awful, and I am NOT generally sensitive to latency. I really learned that without enough native frames to feed frame gen, it's not even a feature worth using.  I suspect had my new PC had the 16gb version of the 5060 TI it probably would have run everything as well as, if not better than, the 9060 XT.  As an example, on my new PC with the 5060 TI, Borderlands 4 was running like less than between 35-50 fps native on medium settings. With the same processor, same RAM, but a 9060 XT 16GB, it runs natively in the 70s and 80s on high settings.  Now, I do only game at 1080p, mind you, so that factors in as well, I suppose.  I was a true believer that 8GB was still enough in late 2025, especially with AI assistance, until this past weekend. I would urge you to go with more VRAM. I know, personally, I'll never buy another GPU with 8GB VRAM again after I see how impactful it is.",buildapc,2025-12-30 00:27:21,1
Intel,nwn9c6w,"The whole vram issue is valid. But, ultimately, all you can do is look at the games you want to play. For the vast majority of current games, the 9060xt performs better and not by a tiny amount. I don't get why you'd take 30% worse performance now for the hope that in 3 years' time you might get better performance.",buildapc,2025-12-30 00:47:14,1
Intel,nwntzzr,Especially if 1080p is still your target then I'd go with the 9060 no doubt about it.,buildapc,2025-12-30 02:41:24,1
Intel,nyzgjj6,"I'm currently researching the exact same thing, except I have a 1440p monitor so am targetting that. Price difference is about the same, too - can get B580 for AUD 400 vs RX 9060 at AUD 480 (though it's a MAXUN brand which I have mixed feelings about, and the Intel is an original Intel ""brand"" and looks beautiful). I'd like to point out two main considerations that I haven't seen others mention:      1. Upscaling quality and performance. As these are low-end cards, we will almost certainly have to use upscaling to get any decent performance for modern games (when you move to 1440p or even if you want to have higher qualities/details at 1080p, depending on your tastes), and XeSS seems superior to FSR. XeSS does work on other cards, but is hardware-accelerated on Intel cards and is apparently almost as good as DLSS. And for games that have upscaling but not XeSS, we can use OptiScaler to replace FSR or DLSS with injected XeSS.       2. Intel cards apparently have hit-or-miss compatibility/performance for older, non-DX12 games, since the hardware wasn't designed to natively support older API's and relies on a translation layer in the drivers. It is being improved over time with driver updates, though, and we also have the option of injecting DXVK (third-party DirectX to Vulkan layer) into games as another option. DXVK is used by the Steam Deck and other Linux gaming, so it should have some pretty decent coverage.  With all this said, even though I do play older games and want to keep that option open, I'm still leaning towards the B580. It still sounds like a solid upgrade over my aging RTX 2060.",buildapc,2026-01-11 15:28:44,1
Intel,nwmhqtw,"Idk about these specific cards but a few years ago I got a 1650 4gb and my buddy got a 1060 3gb  When he played games the fidelity was great but had some stutters here and there, just cruising at 70ish percent  Mine would be screaming at 99% usage with lower fidelity but boy oh boy was it significantly smoother  I'd go with more vram",buildapc,2025-12-29 22:19:09,1
Intel,nwn3tz7,"Neither, get the 9060xt 16gb version. Stay away from 8gb.",buildapc,2025-12-30 00:17:36,0
Intel,nwmr1mk,Tell OP they can get the 9060Xt in 16gb,buildapc,2025-12-29 23:07:52,3
Intel,nyzcgfq,"I assume you're speaking strictly without any upscaling, which IMO is essential for low-end cards like these. I'm researching the exact same thing as the OP, I am currently on an RTX 2060 6GB yet this card can handle Battlefield 6 at 1440p with Balanced DLSS and all Medium qualities extremely well (never dips below 60) - seems a well optimized game. Still, I'm leaning towards the Intel card, since I can find it for $80 AUD less and XeSS is apparently better than FSR (and we have OptiScaler for games that have upscaling but don't specifically support XeSS). EDIT: Well, XeSS will work on AMD cards too, but apparently it has better quality on Intel cards - I'm yet to look into the facts here. EDIT2: Apparently BF6 is quite CPU heavy, too, so might be worth mentioning I have a 3900X.",buildapc,2026-01-11 15:08:01,1
Intel,nzey633,I get 80-100 fps at 1440p in BF6 stock graphics settings (Auto-Balanced) with a 8gb 9060XT.,buildapc,2026-01-13 20:26:41,1
Intel,nxf1w8v,1080p yes on medium settings. 1440p probably if you turn down all the settings to low. 4K if you like slideshow.,buildapc,2026-01-03 10:37:04,2
Intel,nxf3qbn,"of course it will, runs comparable to a 3060. You can expect 70fps on AAA titles at 1080p mid-high settings.",buildapc,2026-01-03 10:52:35,1
Intel,nxf92eu,Why a 14600k with ddr5? At that point you might as well just get onto am5 with a 7600x,buildapc,2026-01-03 11:37:26,1
Intel,nxfiey9,It will be a great setup for 1080p,buildapc,2026-01-03 12:49:52,1
Intel,nxfxfre,Listen if I can run new games with a i5 4690k with ddr3 this is a non starter. Lol,buildapc,2026-01-03 14:22:12,1
Intel,nxezd8p,50/50 chance,buildapc,2026-01-03 10:15:34,0
Intel,nxf63rb,just give you a reference point. i'm using 225f (a cpu even weaker than yours) + b580. i play games at 4k medium quality. [here](https://www.reddit.com/r/IntelArc/comments/1q0diio/effects_of_driver_and_firmware_update/)'s one of them,buildapc,2026-01-03 11:12:36,0
Intel,nxf4fn1,Great. thank you :3,buildapc,2026-01-03 10:58:27,1
Intel,nxflrgz,Most comparisons seem to put 14600K on top though?,buildapc,2026-01-03 13:12:28,1
Intel,nxf0hlk,what does that mean xD,buildapc,2026-01-03 10:24:59,0
Intel,nxkv2gf,"40fps, worse than a console.",buildapc,2026-01-04 05:47:14,1
Intel,nxg25bc,"they trade blows, mainly depends on the game being played",buildapc,2026-01-03 14:48:19,1
Intel,nxf1hve,Hope and pray they optimize their game ðŸ˜†,buildapc,2026-01-03 10:33:38,1
Intel,nxlrk9m,which console,buildapc,2026-01-04 10:29:33,1
Intel,nxm9lvg,"Series X, PS5 pro.",buildapc,2026-01-04 12:59:14,1
Intel,nxmzk7n,"how many fps do they get at 4k medium quality for acs?  you have to compare them with same resolution, same image setting for the same game",buildapc,2026-01-04 15:30:08,1
Intel,nxnnp61,"60fps, they have 60fps modes with equal to or better settings than that.",buildapc,2026-01-04 17:22:41,1
Intel,nxntih2,prove it,buildapc,2026-01-04 17:49:43,1
Intel,nxnu9h3,Digital foundry video on it,buildapc,2026-01-04 17:53:08,1
Intel,nxnw0p3,"as said, you have to compare them with same resolution, same image setting for the same game  a lazy google search [result](https://www.google.com/search?q=ps5+pro+assassin%27s+creed+shadows+4k+medium+quality+how+many+fps)  The game offers several graphics modes on the PS5 Pro:   * Performance Mode: Targets 60 FPS at an upscaled 2160p (4K) resolution, and includes standard ray-traced global illumination throughout the world. * Balanced Mode: Targets 40 FPS at an upscaled 2160p (4K) resolution (requires a 120Hz display with HDMI 2.1 support) and includes extended ray tracing features, such as ray-traced reflections. * Fidelity Mode: Targets 30 FPS at an upscaled 2160p (4K) resolution with the full suite of extended ray tracing features for the highest visual quality.  it looks like you're (mistakenly) read the ""Performance Mode""  here's my game screen. my upscaling mode is ""quality""  https://i.ibb.co/7xCzKnF3/acs.jpg",buildapc,2026-01-04 18:01:04,1
Intel,nxnw9i9,And the performance mode is higher fidelity than you're playing at.,buildapc,2026-01-04 18:02:11,1
Intel,nyfdn4x,"5800x3d is selling for like $400+ on eBay right now. If you got that sweet microcenter deal, then selling your old parts might lead you to break even on the purchase.   It's a no brainer. Upgrade away",buildapc,2026-01-08 16:49:56,57
Intel,nyfm9q8,Brother you have a 5800x3D there's a lot of idiots paying 450 bucks for it on eBay. Just list it for 380. it will sell on the same day. Sell that ram and mobo for 100 bucks  You get an upgrade for 100 bucks.  Don't buy X870E if you don't need it,buildapc,2026-01-08 17:27:36,31
Intel,nyf8q0i,Arc Raiders is already pretty well optimized (new patches seem to introduce hiccups) but Tarkov [heavily benefits from CPU upgrades](https://www.youtube.com/watch?v=Pi3amneDRrk). [7800x3d](https://www.youtube.com/watch?v=96g2ibUvkSE) sits between them. If you're willing to spend an extra $100 you definitely want to skip the X870E and use it to step up to the 9800x3d instead.,buildapc,2026-01-08 16:28:42,4
Intel,nyf4vr8,"dang, that is a sweet deal these days.",buildapc,2026-01-08 16:11:47,3
Intel,nyf4zvr,Arch raiders,buildapc,2026-01-08 16:12:19,3
Intel,nygbipx,"Just my $0.02 but go for the 9800X3D. I originally went from a 5800X3D to a 7800X3D then went back to exchange it for a 9800X3D like 2-3 days later. Remember, you actually get a boost for non gaming stuff too with a 9800X3D over a 7800X3D.",buildapc,2026-01-08 19:16:30,3
Intel,nyfmm03,"In this current market that is an absolute steal. I would do it, even though you donâ€™t really NEED the upgrade.   You can sell your old combo and come close to breaking evenâ€¦ the 5800x3d alone sells for $300",buildapc,2026-01-08 17:29:06,2
Intel,nyjtqpw,Reading this post has given me a lot to think about as I'm in a similar situation with a 5800x3d except only with an RX6800. I haven't had any major performance issues and I was able to run the Doom Dark Ages recently without issue which was really the game I had been forward to playing albeit with FSR trickery.  I play at 5220x1440. I too have been reading the FUD articles and had resigned myself to putting off an upgrade for at least a couple more years. But I'm also close to an MC and the deal is tempting. Interested to see what you decide to do.,buildapc,2026-01-09 06:10:39,2
Intel,nyfgd74,The bundle may be as good as it gets for a long time. Go for it and sell your current components to offset the cost.,buildapc,2026-01-08 17:01:36,1
Intel,nyfoz3u,It will be a bit of an upgrade but nothing crazy. Iâ€™d say get the base bundle and sell your old parts. Youâ€™ll get good money for the 5800x3d.,buildapc,2026-01-08 17:39:31,1
Intel,nygc9jn,"That's amazing value, why even ask the question lol, buy it immediately",buildapc,2026-01-08 19:19:48,1
Intel,nygl3ou,"At 3440x1440p it wonâ€™t really make much of a difference. I havenâ€™t noticed a difference when I moved from my 5600x3d except higher temps and longer boot times of AM5. Iâ€™m sure if I turned on a counter Iâ€™d see the difference, but I canâ€™t visually tell.",buildapc,2026-01-08 19:58:42,1
Intel,nyh7srb,I wish I had a microcenter 25 minutes away.. 3 hours for me. Once lived 2 hours from the one in Atlanta and made the trip in 2021 for my first and only PC and haven't upgraded since.,buildapc,2026-01-08 21:39:24,1
Intel,nyhsh3s,"As a person who plays both those games, it's worth it",buildapc,2026-01-08 23:14:39,1
Intel,nyilxdp,"$580 is a steal! The 5800x3d, ram, and mobo could be sold together on fb marketplace for around $450 or so! So depending on taxes less than $200 for a nice upgrade! Probably could get $550 or so on eBay but after fees and shipping you'd probably get about $450, but Id be worried about scammers on eBay, at least in person after you make the sale its finished.",buildapc,2026-01-09 01:47:56,1
Intel,nyodz0a,"For Escape from Tarkov you will notice it a lot, you will probably be able to sell your AM4 Combo for about 400â‚¬ if not more, so take that deal of 580â‚¬, and unless you want to have a lot of Ports, Drives or doing Heavy OC, keep the B650, no need to go for the X870.",buildapc,2026-01-09 21:58:32,1
Intel,nzgjf2m,So against what I said to other commenters I went ahead and purchased the 9800x3D bundle. Within 12 hours I sold my 5800x3D and my x570 motherboard. As long as I sell the 32gb of DDR4 for asking price I will have fully paid for the upgrade.  Iâ€™m really happy I did this as it just cost me a 50 minute drive and a fun 3 hours of setting up the new build and installing a fresh copy of windows.  To all those debating doing something similar Iâ€™d just pull the trigger. Was worth it!,buildapc,2026-01-14 01:15:31,1
Intel,nyfrm2i,"You wont see much gain going to a 7800x3d due to both already using a 5800x3d but also because you are at ultra wide resolution.  However: the 5800x3d are going for a very decent premium right now so if you factor in selling the old platform you may get very close to covering all the cost of an am5, so if it costs you very little overall then sure, why not.",buildapc,2026-01-08 17:51:00,1
Intel,nyfnbw4,"Does your current CPU meet your needs? Is $580 important to you?  I made the jump from 5800X3D to 9800X3D before prices ballooned. It was a worthwhile upgrade for my usage. At 1440p UW, I don't know that you'd see a ton of gains in those 2 games specifically.",buildapc,2026-01-08 17:32:17,0
Intel,nyf509x,What's wrong with your current setup?,buildapc,2026-01-08 16:12:22,0
Intel,nyfzico,"You will have to upgrade the motherboard. 5800X3D works on the AM4 platform and 7800X3D requires the AM5 platform. Might as well build a whole new rig. However, you really don't need to upgrade right now. I am running a 5900X and a 4090 and playing games on ultra settings with a 4K monitor. I'm not upgrading until at least AM6.",buildapc,2026-01-08 18:24:53,0
Intel,nyf56mh,No,buildapc,2026-01-08 16:13:09,-1
Intel,nyf5ezq,"You won't notice any difference except from eSports games like CS, valorant etc. and maybe bf6 but on AAA titles you won't see any major improvements!",buildapc,2026-01-08 16:14:11,-1
Intel,nygauyt,"I have both, the only difference is the GPU, honestly.  At that resolution you won't notice much of a difference, unless you want to have a newer CPU later on once they release the 16c single CCD beast in the 109800x3D or whatever they'll call the thing.",buildapc,2026-01-08 19:13:37,0
Intel,nygbat0,"Second one this  The 5800x3D is legendary but the 7800x3D blows it out of the water. I would just sell the 5800x3D for 400$ on eBay, get the combo and call it a day.",buildapc,2026-01-08 19:15:33,15
Intel,nygbrg6,"Or list it for $430 and sell it that same day, why take a 20% cut for nothing lol?",buildapc,2026-01-08 19:17:35,4
Intel,nyh0psi,"its cuss they dont have the money for the ram, so for them its iethere stay with a 3000amd cpu or the 50003ds",buildapc,2026-01-08 21:08:27,1
Intel,nyg4naq,Great advice right here OP,buildapc,2026-01-08 18:46:51,2
Intel,nyf5dji,Fixed. My bad. Thanks,buildapc,2026-01-08 16:14:00,1
Intel,nyhap9f,"I was about to say that my 7800x3d roasts all the autocad application I am running cleanly, but then I remembered I got the 7950X3d so I could use it as a home workstation, and my GF has the 7800x3d rig I built.",buildapc,2026-01-08 21:52:09,-1
Intel,nym42yv,"Iâ€™m leaning towards not doingâ€¦tell me if you agree or disagree.  1.) itâ€™s not guaranteed I sell everything for the price I want.  2.) if I do, Iâ€™m gaining roughly 8% performance for a $60-$100 upgrade at best? Why go through all that?  3.) Iâ€™ll have more of a reason to upgrade when AM6 comes out. And I can just drive there for the deal when they inevitably have it then.  4.) Iâ€™m happy with what I have? ðŸ¤·",buildapc,2026-01-09 15:47:41,1
Intel,nyfe2n9,"Resale value on the 5800x3d is insane right now, like $500.  It honestly makes a lot of sense to upgrade now when after selling the old Mobo, RAM, and CPU, the money will nearly completely pay for the entire upgrade",buildapc,2026-01-08 16:51:46,4
Intel,nyf59sd,"Nothing, Iâ€™m just trying to plan for the future and if parts are gonna be through the roof going forward I figured maybe this is a better option to hunker down with?",buildapc,2026-01-08 16:13:32,1
Intel,nyg7rhh,"I appreciate your comment! However, if you re-read the post I said that Iâ€™d be buying the cpu, motherboard and ram combo from micro center. Totally understand that a cpu upgrade would require mobo and ram",buildapc,2026-01-08 19:00:07,3
Intel,nygbji6,Wait you wouldn't sell it?,buildapc,2026-01-08 19:16:36,2
Intel,nyhucd6,Legendary in what sens if itâ€™s crushed by a $100 7500F?,buildapc,2026-01-08 23:24:06,0
Intel,nygcnm7,"Yeah, my bad. I came up with that number just to make a point that there are some AM4 suckers out there that will spend 9800x3D money on a 5 year old CPU  380 usd sells in probably 1 hour.",buildapc,2026-01-08 19:21:30,3
Intel,nz2mtic,"I'm mostly in agreement as there's risks involved in selling.  Maybe the price comes in lower than expected or the buyer disputes.   Looks like the ground is still moving beneath our feet with people posing that prices are still jumping. At least, we are in good situation in that we have a decent setup either way. Happy gaming!",buildapc,2026-01-12 00:27:55,1
Intel,nyf6b6a,"So, you're basically trying to ""future proof"" because you're worried that when your current setup fails you in some way, the hardware you would've to buy would be more expensive than right now.  That's a bit faulty logic because:  1. Your current setup can easily last you a couple of years without problems. 2. By the time you truly feel a need to upgrade, the current shortage would've ended already and prices would be back to normal. 3. At that time, something else better than 7800X3D would be around and at the same price, 4. 7800X3D, despite how good it is, it's not the future proofing CPU you think it is. If you want something that you wouldn't feel a need to change for a good 7-9 years, then 9800X3Dis where it's at, and that's probably more expensive than what you're comfortable with spending.",buildapc,2026-01-08 16:18:11,1
Intel,nyf5mf2,"Theyâ€™ll drop eventually, and long before your current setup is obsolete. The AI bubble will collapse, and theyâ€™ll dump all of theirs chips back into tue consumer market.",buildapc,2026-01-08 16:15:06,1
Intel,nygbvsy,"I meant I â€œwouldâ€, it was a typo my friend",buildapc,2026-01-08 19:18:07,1
Intel,nyi3ncb,"Is it though? Depending on the game, the 7500F does better, but in anything that likes that extra cache the 5800X3D is still amazing. https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more for example, literally *every* game they tested, the 5800X3D matched or significantly beat the 7600X.",buildapc,2026-01-09 00:12:10,1
Intel,nyidbpi,"I love me a great troll  But yes, the 7500f is a fantastic value AM5 cpu and yes trades blows with the 5800x3D. However the 7500f did not start the x3D legacy, the 5800x3D did and for that itâ€™s legendary. Sort of like how a 9060xt 16gb absolutely beats a 1080 Ti, but that still doesnâ€™t make it a legend. It just makes it a good value product for the timeðŸ‘Œ",buildapc,2026-01-09 01:02:06,1
Intel,nygd0jw,"I'm sure it can make sense in some scenarios, with the current cost of RAM, but I agree it seems silly to pay that much",buildapc,2026-01-08 19:23:05,1
Intel,nyfh3m3,"Check eBay sold listings, homie",buildapc,2026-01-08 17:04:52,4
Intel,nyfoq7n,He might even make money on this upgrade after selling the old parts lmao,buildapc,2026-01-08 17:38:28,3
Intel,nyfnpvr,5800X3D is definitely selling for $400+ right now. I sold mine for ~$100 more than I paid brand new in 2022.,buildapc,2026-01-08 17:34:00,2
Intel,nyf6wsy,"Gotcha. The 9800x3D bundle is $680. So right what the 7800x3D bundle would be with the board upgrade. So, I guess not out of the question.  But I see what youâ€™re saying. Probably should just wait then. I also donâ€™t see a need for a 9800x3D if Iâ€™m just gaming as well",buildapc,2026-01-08 16:20:48,2
Intel,nygcafr,Oh thank God.,buildapc,2026-01-08 19:19:54,1
Intel,nyidx9p,I think you mean 5050? I have a lot of cpus the 7500F with the right configuration is on average 30% slower than a stock out of the box 9800x3d.,buildapc,2026-01-09 01:05:18,0
Intel,nygbvqn,The computer market is so weird right now,buildapc,2026-01-08 19:18:07,2
Intel,nyf86gl,"At the resolution you're playing on, your CPU is highly unlikely to be bottlenecking your GPU. The only scenario where I would consider a CPU upgrade (with your setup) is if my normal PC usage is feeling (noticeably) sluggish or if I'm playing some heavily CPU reliant games where I'm unhappy with the FPS I'm getting. If in your case neither is happening, then there's zero reason to upgrade IMO.",buildapc,2026-01-08 16:26:20,4
Intel,nyif4sn,"Yes a 5050 is faster than a 1080 Ti (as it should be and I would hope it would be).  Yep, a 7500f should run about that on average with 1% lows being being even lower than a 9800x3D  The 7500f is a fantastic chip, a lot of performance for the price. Frankly more than enough for most people. Whatâ€™s not to like and whatâ€™s the point youâ€™re trying to make here my friend?",buildapc,2026-01-09 01:11:48,1
Intel,nyjjn58,Honestly it wasnâ€™t that far off before. This bundle used to be $450-$500 and the 5800x3d used to sell for $300ish.,buildapc,2026-01-09 04:57:55,1
Intel,nyf8qgg,Appreciate the input! I likely wonâ€™t. Thatâ€™s for the good points here sir! ðŸ«¡,buildapc,2026-01-08 16:28:45,1
Intel,nyiqor2,"In fortnite in particular,   max overclocked 7500f performed as follows: 542avg/289 lows 1%/ 74w power  Stock out of box 9800x3d: 721avg/ 373 lows 1%/ 92W power  Overclocked 9800x3d ( just an undervolt with serious ram oc ) 723avg/ 404 lows 1%/ 64W power.  Thatâ€™s a 33% gain in avg and 29% gain in lows.   Overclocking the 9800x3d saw virtually no gain in average (33.02->33.3%) but 1% lows jumped from from 33.3% to 39.79% greater than max overclock 7500F   When you consider the hefty premium X3d chips call for, Iâ€™d say 7500f is winner all day. 33% difference in fos might sound huge but when talking 542 vs 721fps sure itâ€™s 33% but who cares when both chips already run so fast.",buildapc,2026-01-09 02:13:13,1
Intel,nyf9y9y,"You're welcome. May I add that right now it's a good time to tune-out of tech news. All you're going to see is doomsaying articles/posts from people trying to generate either sales or clicks.  It's pushing an extreme FOMO narrative because that gets people's attention, but it's not logical or wise.  Good luck with whatever you decide to do.",buildapc,2026-01-08 16:34:03,2
Intel,nygc7at,Don't wait. Sell your 5800x3d for $500 on eBay and buy the bundle. You'll never get an opportunity for a nearly free upgrade like this again,buildapc,2026-01-08 19:19:31,2
Intel,nz9lefm,you must have some huge hands judging from the apparent size of that box,pcmasterrace,2026-01-13 00:45:35,2
Intel,nzakbj4,"Holy shit my hands look like a giant Hold on ill get you a pic with me actually holding the BOX, not the gpu.  https://preview.redd.it/kkndv19xh1dg1.jpeg?width=4000&format=pjpg&auto=webp&s=924d6b75377cecc6a98ac1bae1a765b4349dc025",pcmasterrace,2026-01-13 03:55:11,3
Intel,nx0v4hv,"CPUs don't support resizable BAR, devices do. Arc cards don't require resizable BAR it's just expected to be used.",pcmasterrace,2026-01-01 03:06:37,2
Intel,nx0ze2r,"Rebar isn't needed for Arc gpus to function, but it is needed for them to function as intended. I believe Arc does need a motherboard with a UEFI bios, but could be wrong on that.",pcmasterrace,2026-01-01 03:35:19,2
Intel,nwywe0i,You can look up compatibility on the PC builder site.   I got a Z790 for 125$   I got the I5-14600Kf for 150$   It holds up really nice.  BE REALLY CAREFUL the Intel GPUs need REBAR so keep your eyes peeled for that.,pcmasterrace,2025-12-31 20:02:48,2
Intel,nwyvx1g,"Must be nice to have a MC, my closest is a few thousand miles away.",pcmasterrace,2025-12-31 20:00:16,1
Intel,nwz9s45,intel 13th and 14th gen have cpu failures so only buy if you are willing to risk that,pcmasterrace,2025-12-31 21:15:39,0
Intel,nwyxczb,Totally forgot about the rebar. Thanks for the reminder. My current cpu and mobo support rebar and I have it activated. Definitely something Iâ€™ll have to consider when I upgrade to ddr5,pcmasterrace,2025-12-31 20:08:03,1
Intel,nwyxm93,Would 12600kf for 100 be good?,pcmasterrace,2025-12-31 20:09:27,1
Intel,nwyxsvs,A few thousand miles? Sorry that sucks. Their bundles/combos on parts are better than sex sometimes.,pcmasterrace,2025-12-31 20:10:26,2
Intel,nwyxsdq,I mean.. CPUs are pretty far ahead of GPUs at least in terms of gaming.   Unless you planned on running 1440 with everything on ultra the B580 is great on most games. Youâ€™ll never feel like you are forced to run a game on low graphics.,pcmasterrace,2025-12-31 20:10:22,1
Intel,nwyy97j,If you plan on investing in Intel GPUs I would press you to get the newest gen.   Intel ARC B580 which is an entry Intel GPU didnâ€™t play nice with older Intel Chips   12600 would hold up just fine.,pcmasterrace,2025-12-31 20:12:54,1
Intel,nwyyd72,Thanks thereâ€™s also the brand new core ultra 5 225f with bf6 for 150 on amazon. Would ultra core be a better fit?,pcmasterrace,2025-12-31 20:13:30,1
Intel,nwyyobw,If you donâ€™t plan on upgrading itâ€™s fine but if Intel releases a newer card that more powerful it might get bottlenecked.,pcmasterrace,2025-12-31 20:15:12,1
Intel,nwyyr3l,Dang maybe am5 cpu is better,pcmasterrace,2025-12-31 20:15:37,1
Intel,nwz119d,Iâ€™d say to go AMD.   Intel royally fucked up in that race but since they did have a major price drop because of it I scooped one up cheap.,pcmasterrace,2025-12-31 20:27:59,1
Intel,nysacsv,lol old skool,pcmasterrace,2026-01-10 13:50:14,3
Intel,nyzx9wu,"Personally I think the fps jump is worth it, I prefer higher fps over better looking games, the rtx 2060 will make the games look way better for sure, but you will feel a jump in responsiveness from 144 fps, specially based on the fact that you play cs and Fortnite, since shooters are games that feel the best with high refresh rate.",pcmasterrace,2026-01-11 16:48:49,5
Intel,nyzqu8f,how much does a gpu cost for you? I think 144hz IPS monitor would be much cheaper,pcmasterrace,2026-01-11 16:18:17,2
Intel,nz1rpun,Fps doesn't matter if your monitor doesn't show it.  Have your dad buy the more expensive one and you save money for the cheaper one.,pcmasterrace,2026-01-11 21:52:18,2
Intel,nyzyn6p,Monitor would be better value,pcmasterrace,2026-01-11 16:55:20,2
Intel,nyzstoh,"i think the monitor would be smth like 400PLN (94EUR/$110) and the GPU i could be looking for would be 600-700PLN, but that doesnt matter that much, im just thinking what would be better value, an old gpu with 144hz, which would be useful only in some esport games (bc its too weak to play SP on good settings on >60 frames) or a new gpu with a old 60hz monitor so the extra frames wouldnt change anything",pcmasterrace,2026-01-11 16:27:49,1
Intel,nz4vkgt,"i told him to buy a monitor cuz its gonna be at least few months till i could buy a gpu, so i could use higher refresh rate meanwhile and the price difference wasnt that bad (600pln for the gpu arc a750 and 400 for 144hz ips monitor)",pcmasterrace,2026-01-12 09:30:26,2
Intel,nyzzzox,"yea, i think im gonna buy that for now",pcmasterrace,2026-01-11 17:01:43,1
Intel,nz5drh0,"I see that you are from Poland, you can find good deals on monitors here:Â https://www.pepper.pl/grupa/monitory   And other things like GPU or anything really, Pepper is a neat site, kind of like reddit but you upvote or downvoteÂ deals other users find.",pcmasterrace,2026-01-12 12:09:55,2
Intel,nyzyams,"I think monitor would be better, if you already hit 60 fps in SP games you would get no smoothness improvement from a new gpu     gpu should be your next upgrade though",pcmasterrace,2026-01-11 16:53:41,1
Intel,nyzzsg4,"yea, i was thinking just like you, so im gonna buy a monitor for now (some budget Philips E-Line 24E2N1110/00 for 279pln) and look for some intel arc a750 in the near future, you think its a good idea or i should look for smth else?",pcmasterrace,2026-01-11 17:00:45,1
Intel,nz045zv,"Not sure about budget monitor market tbh, might wanna make a separate post on this (or other) sub, idk if its allowed to ask about this here  include your country btw when asking for recommendations  just look for 144hz ips 27'' or 24'', chinese is fine",pcmasterrace,2026-01-11 17:21:33,1
Intel,nyq1n9b,Maybe the psu is a little light for the system? If the gpu can draw 225w and 65w for the cpu. Thatâ€™s already around 300w. With fans and MB added to the wattage it could be too much for the power supply to provide.,pcmasterrace,2026-01-10 03:16:53,3
Intel,nyr02bd,Are your graphic drivers up to date?  If so the next thing I would check is your power draw.,pcmasterrace,2026-01-10 07:21:39,2
Intel,nyq1q1m,change your hdd or ssd,pcmasterrace,2026-01-10 03:17:21,-1
Intel,nyr7wxs,"I set it up so the GPU draws 180w from the PSU, and the PSU is probably not the problem because the game ran smoothly at 100+ FPS on the Legacy version",pcmasterrace,2026-01-10 08:33:16,1
Intel,nyr7r2v,"Yeah they are, i did a clean install this time wich means every old driver i didnt delete got deleted, still didnt fix it, i think the PSU isnt the problem because GTA online on the legacy version ran at 100+ FPS in high settings",pcmasterrace,2026-01-10 08:31:44,1
Intel,nyqzxtr,No,pcmasterrace,2026-01-10 07:20:33,1
Intel,nyt28e6,Why not buy a cheaper gpu?,pcmasterrace,2026-01-10 16:18:42,1
Intel,nz0a11q,How would that make anything better ?,pcmasterrace,2026-01-11 17:49:15,1
Intel,nz2aytt,Youâ€™re not allowing the GPU to run at its full engineered potential (waste of money/starving the GPU of power it needs to run enhanced version). Then complain about graphical issues and studders while using an underpowered PSU. Thatâ€™s the problem.,pcmasterrace,2026-01-11 23:27:24,1
Intel,nz671bd,"Ik my psu is bad i just dont have the money for it, i was looking for temporary fixes",pcmasterrace,2026-01-12 15:02:35,1
Intel,nz6v13z,When you get a stronger PSU change ALL power supply cables too.  Even the same brand of PSU can be pinned differently and it can potentially fry part/all of your system. At least legacy works good so thatâ€™s better than nothing. ðŸ˜‹,pcmasterrace,2026-01-12 16:54:57,1
Intel,nywynr9,mmmm if your Dell doesnt support Resizeable BAR I wouldn't bother with Intel ARC. It's a requirement to get useable performance out of them.,pcmasterrace,2026-01-11 04:13:57,1
Intel,nyx0qhq,"Yeah that was the original intent before it got fried. Pulled the cmos out to see if I can reset the bios and get it moving. Otherwise Iâ€™ll try those methods, thanks!",pcmasterrace,2026-01-11 04:25:57,3
Intel,nyx0kkz,"Itâ€™s got resizable bar support, but apparently I did something wrong along the way because this thing boots straight to that by the time my monitor is up and running",pcmasterrace,2026-01-11 04:25:01,3
Intel,nyx0pkx,Which model is it?,pcmasterrace,2026-01-11 04:25:48,1
Intel,nyx0wg2,"5680, cpu should support it as should the bios if my prior research came up right.",pcmasterrace,2026-01-11 04:27:00,1
Intel,nyx1gdi,I really dont think a PC from 2018 supports Rebar. I could be wrong though.  Maybe you could try booting with the integrated GPU and go from there.,pcmasterrace,2026-01-11 04:30:34,-1
Intel,nyx1xz5,"I tried that option, but the only integrated option was a display port. Wouldnâ€™t have thought thatâ€™d be an issue, but my monitor hits me with a â€œdisplay disabled, reset computerâ€ screen",pcmasterrace,2026-01-11 04:33:47,1
Intel,nyx5ns1,"Update, unplugged the arc and went for the display port. Itâ€™s not as fucked but still hardly booting",pcmasterrace,2026-01-11 04:55:40,1
Intel,nyxa4s1,"Just a FYI resizable base address register has been a thing since PCIe 3.0 was released in 2010.  The real question is if the BIOS has the settings or not.  RebarUEFI can add support for pretty much any UEFI based system but it isn't easy to get working properly. Back when I was looking into using RebarUEFI for an el cheepo system I discovered that AMD's ""Smart access memory"" is based on REBAR but with improvements.",pcmasterrace,2026-01-11 05:25:35,1
Intel,nyyb392,My z370+8600k supports reBAR no problem,pcmasterrace,2026-01-11 10:48:26,1
Intel,nyvo2bv,"I've recently tried Bazzite and went back to Windows 11 because Battlefield 6 doesn't work due to the anti-cheat problem on Linux (not just Bazzite, all distros).   Apart from this, it looks fine, runs fine. It takes a bit to get used because the GUI is different from Windows, but it's not an issue.  \-------------  The following I haven't tested, just read from others: Linux favors a lot AMD because of the open source nature of their drivers compared to nvidia which don't work as well: not optimized I think.   \-------------  The anti-cheat issue, constant AMD push, not all games and apps (for example DaVinci: it's not enough to just run, but it needs to run the same way as in Windows, or you search for an alternative and learn that) run on Linux vs Windows, the philosophy that you can run only the apps the Linux world or cult allow you to run have made me return to Windows 11, which is not perfect, but I can use whatever HW I want and run what games I want. It's my PC and I can use it how I want it.   Best thing to do is try it for yourself and see how it goes.",pcmasterrace,2026-01-10 23:59:45,6
Intel,nywtnbm,"Make sure the games u want to play are supported. Otherwise, stick to Windows.  [https://www.protondb.com/](https://www.protondb.com/)",pcmasterrace,2026-01-11 03:44:30,4
Intel,nyvm10z,I have and I succeeded. I use cachyOS and NixOS,pcmasterrace,2026-01-10 23:49:06,3
Intel,nyvoors,CachyOS ðŸ˜‰,pcmasterrace,2026-01-11 00:02:56,2
Intel,nyxz5d1,"As far as I saw, Nvidia does definitely better on windows. I would like to have at least the same performance. I knew AMD does better, so I moved my rog ally to steamOS and it's great and even the desktop mode is easy to figure for day to day tasks. It's a good trend, it's progressing so hopefully at a point it will be a viable solution for all hardware.",pcmasterrace,2026-01-11 08:57:19,2
Intel,nyvonaq,Since you are installing os from scratch this is the perfect opportunity to try Linux. I'm not sure steam os plays with nvidia. I would recommend Mint.   Keep in mind some games will not work ie) BF6. Non of these games are worth keeping windows imho but they may be important games for you.,pcmasterrace,2026-01-11 00:02:44,3
Intel,nyvlafm,"Bazzite works but it depends on having supported hardware. I tried it, liked it, and reinstalled windows due to horrible intel ARC support.   TLDR try it, it's nice, but may not work. I've heard that AMD hardware works great.",pcmasterrace,2026-01-10 23:45:15,2
Intel,nyvlxql,New dlss isnt for the 20 and 30 series.  Its a drastic performance cut.,pcmasterrace,2026-01-10 23:48:38,2
Intel,nyvu2l9,"SteamOS requires a full AMD build (CPU and GPU). There are other, non-Valve, Linux distros that can provide a similar experience on a wider range of hardware.",pcmasterrace,2026-01-11 00:31:15,2
Intel,nz6yb3k,Iâ€™ve done both SteamOS and Bazzite. Itâ€™s why I only get AMD GPUs. I feel like theyâ€™re about 90% there. I went back to Windows because itâ€™s just a little more compatible with the software I use (or Iâ€™m not interested in troubleshooting).,pcmasterrace,2026-01-12 17:10:07,1
Intel,nyvzlue,"linux is essentially useless for gaming and unless you are cs major you will have horrible time, 95% of stuff is completely undocumented and there are no guides for anything",pcmasterrace,2026-01-11 00:59:13,-8
Intel,nyw8ghu,"Maybe Iâ€™ll do more research on how to minimize Windows 11. After a factory reset.    Iâ€™m actually so spoiled by my current Apple M4 Pro Mac mini because it has factory reset options built in, and privacy stuff all over the place. Itâ€™s a great OS lol.   Iâ€™m dreading the return to Windows 11 because of bloat. I just want a clean new install of Windows, install of my motherboard drivers and GPU drivers, and then Steam + OBS.    Guess I will also need to research what YouTubers are using for their video editor nowadays too. Everything else is in browser (Instagram, Reddit, YouTube, Twitter etc.). Just want a super lean super fast PC for gaming and social media.",pcmasterrace,2026-01-11 01:47:58,1
Intel,nyvp3lp,Why are you comfortable with giving kernel access to games like this?,pcmasterrace,2026-01-11 00:05:05,-4
Intel,nyvz1uw,Thank you. That addresses all I needed to know.  Anti cheat is the bane.   If I want to play B6 and Arc Raiders then this is a 100% nonstarter.   How tf is Steam Machine going to succeed if all the main multiplayer games are not compatible? Seems dead in the water.,pcmasterrace,2026-01-11 00:56:16,-3
Intel,nyvysd1,"Nice. Thats awesome.   Question tho, can you use the GPU drivers? Can you stream with OBS on twitch and YouTube?",pcmasterrace,2026-01-11 00:54:54,1
Intel,nywyij4,"Can confirm, AMD hardware is great. I have a half TB SSD with a Windows install I boot into when needed.",pcmasterrace,2026-01-11 04:13:03,2
Intel,nyvvtt0,Ya AMD is much better supported but heard bazzite does a better job with Nvidia than most,pcmasterrace,2026-01-11 00:40:02,2
Intel,nyvmcap,"What about an older version?  Or can I simply go to Nvidiaâ€™s website and download the 3080 Ti drivers and let it do the full nvidia experience and I will get the best experience that it recommends for me?  Maybe going in blind is better, as Nvidia would be directing me to what they think is best",pcmasterrace,2026-01-10 23:50:44,1
Intel,nyw166u,No anticheat means their SteamOS and Steam machine is dead in the water anyway. Just learned this from the replies.   Thanks. Iâ€™ll stick with Windows 11,pcmasterrace,2026-01-11 01:07:39,-5
Intel,nz7mz4x,"Iâ€™m curious to try Bazzite but did the windows 11 factory reset last night.   If I do Bazzite, thereâ€™s no guarantee that I can livestream on OBS to Twitch or YouTube. And game availability might be limited. certainly the anticheat games will be.   So I could take a chance and experiment for a week, or just stick to windows. I have an RTX 3080 Ti right now so Iâ€™ve heard itâ€™s not as good an experience as with AMD. But Iâ€™d benefit from at least getting to experience it",pcmasterrace,2026-01-12 19:01:18,2
Intel,nyw0w6z,Steam Machine is fucking dead in the water then. I heard Bazzite and SteamOS donâ€™t support anticheat  Iâ€™ll factory reset Windows 11 tomorrow (somehow),pcmasterrace,2026-01-11 01:06:09,-6
Intel,nyylviu,"You'll definitely need to do your own research, but at one point you'll need to install one distro and play with it: see what works for you and what doesn't and if you can find workarounds or alternatives.   I've tried Bazzite and was happy with it. Most likely I'll like CatchyOS and Mint.   For me, the only thing Microsoft I use now is the OS, for everything else I've transitioned to something else: Brave for browser, Proton Mail and Drive, Standard Notes.",pcmasterrace,2026-01-11 12:22:35,1
Intel,nyvpvtc,"Because I like to play BF6, or the campaign in COD Modern Warfare 2022, last AAA I got during a sale.",pcmasterrace,2026-01-11 00:09:03,5
Intel,nyxphwi,"Because you either accept that no matter how much you hate kernel access, you only got two choices you either accept it in order to enjoy playing your favorite games or you choose not to allow it preventing you from playing the games you love",pcmasterrace,2026-01-11 07:29:10,2
Intel,nyw4hpe,"Some games with anti-cheat work, some don't. As Linux gaming becomes more popular, game devs will adapt.",pcmasterrace,2026-01-11 01:26:17,3
Intel,nywswhp,Arc Raiders works fine,pcmasterrace,2026-01-11 03:40:10,3
Intel,nyw4w9b,"Yes, I have had both nvidia and amd graphics cards running on Linux, streaming to both platforms. Also yes I can run the latest drivers if they are available",pcmasterrace,2026-01-11 01:28:36,3
Intel,nyvo7p8,Just dont use dlss 4.5 and you're good.,pcmasterrace,2026-01-11 00:00:31,3
Intel,nyxzb6u,Say that to the steam deck and most handhelds that support/run steamOS ðŸ˜‚,pcmasterrace,2026-01-11 08:58:49,2
Intel,nyz9muj,I wouldn't say dead in the water. Linux is becoming more popular and all it takes is the game developers seeing there's enough market to get them to support anti cheat on Linux.  I think it's a current problem but I'm sure it'll become a non issue in time.,pcmasterrace,2026-01-11 14:53:08,1
Intel,nz7rzs4,I would definitely look into and give it a shot. I had some weird issues using Steam OS where the WiFi would turn off when I started downloading things but that was the biggest problem I had.,pcmasterrace,2026-01-12 19:24:18,1
Intel,nywc17f,Easiest way is to have another windows computer and use the microsoft windows 11 creation tool.,pcmasterrace,2026-01-11 02:07:11,2
Intel,nyvq8v3,I get that you enjoy those games but why do you feel comfortable handing out kernel access to your OS?   Do you feel this is a good habit to get into where you hand over this type of access to any program?,pcmasterrace,2026-01-11 00:10:58,-13
Intel,nz0szra,I am just very lucky because I don't like any of these aaa games that are pushing this anti cheat. So at this point its been very easy. Cod and BF are trash now IMHO. If something comes along that actually is something I want to play it will be harder but I will still just pass on it.   None of these games are worth keeping windows for. Not even close.,pcmasterrace,2026-01-11 19:13:12,1
Intel,nyw7kqd,Iâ€™ll do more research then and not assume. Thanks.,pcmasterrace,2026-01-11 01:43:22,2
Intel,nyw7vpf,"Interesting, thank you. Probably a simple Google Reddit search about my 3080 Ti + (OS) + (OBS Twitch/youtube streaming) should do the trick.   Someone said some games with anticheat work on Linux also. Guess it depends on the devs.",pcmasterrace,2026-01-11 01:45:02,2
Intel,nyyb3nx,"I meant purely as a desktop box for people looking to play competitive games at their desk with their existing mechanical keyboard and gaming mice, and for the guys that typically only buy call of duty and want to play on their couch in the living room.   For that use case, the Steam machine becomes immediately less desirable unless they can workout something related to anticheat with the specific devs.   For Steam deck **this does not apply at all, nor does my comment**.",pcmasterrace,2026-01-11 10:48:33,1
Intel,nz2ftp2,"if you have flash drive with usb-c, you can just download windows iso on your phone and extract it to that flash drive, you don't even need creation tool or rufus... I mean it's good to have, it lets you adjust the installation process, but it isn't necessary",pcmasterrace,2026-01-11 23:52:34,1
Intel,nyvtb93,I am not the guy but i am speaking for millions of people i think:  - They donâ€˜t care  - They want to play the game,pcmasterrace,2026-01-11 00:27:20,12
Intel,nyvt3k7,"I simply don't care. My life doesn't revolve around this (Windows vs Linux). These things are so unimportant to me that I don't think about them.   I think if you've asked me when I was in my 20s, I think I would have given you a different answer.   I just moved on.",pcmasterrace,2026-01-11 00:26:11,3
Intel,nyvz966,"The biggest anticheat in the world does not have kernel security concerns. Relax buddy. Unless youâ€™re doing some heinous illegal shit on your own computer, most normal people here arenâ€™t paranoid about this stuff.",pcmasterrace,2026-01-11 00:57:21,-2
Intel,nyvu4vg,So I suppose it is just ignorance on what it means when you grant kernel access to a game?Â    Or do you think they still wouldn't care if they knew?   I personally won't buy any game requiring this. That's just a hard nope.,pcmasterrace,2026-01-11 00:31:35,-11
Intel,nyvtewa,No this isn't a Linux vs Windows thing. Stay on whatever OS you want.   I'm just wondering why you are ok giving kernel access to a game? Are you just saying that system security is unimportant for you? Or maybe you run a separate OS install just for games?,pcmasterrace,2026-01-11 00:27:52,-6
Intel,nyw19kz,"You don't need to do illegal things to be smart about security, what a strange thing to say.",pcmasterrace,2026-01-11 01:08:09,1
Intel,nyw3hdq,"I think tons of people know and just donâ€˜t care. If you care and lock yourself out of potentially very fun games, good for you",pcmasterrace,2026-01-11 01:20:38,6
Intel,nyynsj1,"System security is important up to a point until it gets in a way of actually enjoying life. I believe there should be a balance between security/privacy and usability.   I know Linux people think they have the most secure and most private systems out there, but I seriously doubt it: not talking about the MS telemetry, but what traffic goes out of the system, through your ISP, then what's seen my your own country security and surveillance systems.   I know what I'm going to do: I'll browse with incognito mode :))   I run one OS for everything, one 2TB SSD. I was thinking of going the dual boot route, but I've discovered I need to enable and re-enable Secure Boot anyway so I abandoned that idea.   I'll switch full time to a Linux when I'll be able to run whatever HW I want (no way I'm going back to AMD) and the games I play, run on that OS. For the same reason I'm not considering a Mac. Or I could switch full time on Linux, get nvidia gfn ultimate and play my games that way. But what's the point? I have a 12700KF, 32GB, 3070Ti and soon a 5070. Of course I'm going to play my games locally.   What's the point of locking myself into something that I don't want to use. I know it's popular to switch to Linux these days but let's be real here: it's not always feasible. I know, I know: Cyberpunk and Baldur's Gate 3 run on Linux, that's all I hear from ytbers but other games exists.   I'll agree with you about the AAA vs indies: it's a complete mess. In the last 5 years I can count the AAA games I bought on the fingers from one hand vs indies which I buy monthly.",pcmasterrace,2026-01-11 12:37:40,1
Intel,nyym2qi,That's the case for me too: I know very well and moved on.,pcmasterrace,2026-01-11 12:24:10,1
Intel,nywdyuj,"So far none of these games have felt like any sort of a sacrifice to not play. I did get interested in BF6 so I tried it but got bored right away. The indie developers are on fire lately, AAA has gotten really bad.",pcmasterrace,2026-01-11 02:17:21,-4
Intel,nz0sa5b,"I switched to amd this time but I was having good luck with nvidia on Linux mint.   I don't understand pigeon holeing into one brand or the other, it's not a marriage.",pcmasterrace,2026-01-11 19:09:59,1
Intel,nz106dq,"Because I donâ€™t want to waste more money. If I switch to AMD and I have problems again, like many 9800X3D users and like myself since 2019 till today, nobody will answer: no techtuber, no redditor, nobody else. Itâ€™s money down the drain. For what? To be AMD beta tester again on my time and with my money? No, thank you.",pcmasterrace,2026-01-11 19:45:25,1
Intel,nyg1xcu,"Id get a 5800x, i heard its good",pcmasterrace,2026-01-08 18:35:11,1
Intel,nyg1xvj,"My $0.02: get a 5000-aeries chip, upgrade your bios and pop it in. Get a gpu and hope ramageddon ends when you're ready to go ddr5.  It might be worth paying the scalper for an X3d if you think that'll hold you that much longer. Even a 5700x3d is an amazing cpu, but then we're getting close to ddr5 money",pcmasterrace,2026-01-08 18:35:15,1
Intel,nyg3lj7,5700x3D if you can find it around â‚¬250. 14600/14700k are best all aroundÂ CPU with DDR4 support,pcmasterrace,2026-01-08 18:42:19,1
Intel,nygmhr1,"If a microcetner is close to you, you can get a 7800x3d, ram, and mobo for 580 which is unbeatable     If not, I just upgraded from a 3700x to a 5800xt, CPU heavy games still crank away at utilization (like battlefield 6) but in battlefield 6, I saw a 20-30fps increase, and generally a much smoother experience, and am running a 2070 super for reference.  The 5800x is a 'under' (barely) clocked 5800xt, but I only grabbed the xt because it was easy to find and on sale for 160 at the time.",pcmasterrace,2026-01-08 20:04:57,1
Intel,nv5otsy,Going to be a downgrade obviously. Intel has Been getting to the point those cards are actually good but itâ€™s still going from high mid tier to low tier,pcmasterrace,2025-12-21 06:00:51,1
Intel,nv6e0b8,you got a free game from intel,pcmasterrace,2025-12-21 10:05:37,1
Intel,nv6e403,as a low tier card it's quite decent. i use it to play games at 4k,pcmasterrace,2025-12-21 10:06:36,1
Intel,nv6hh4w,Yep went in expecting a downgrade but I don't game too much lately haven't found the latest games all that good and mainly playing older games at the moment so the trade off for now isn't huge for me.,pcmasterrace,2025-12-21 10:39:59,1
Intel,nv6hlv9,Yep Battlefield 6 key.,pcmasterrace,2025-12-21 10:41:18,1
Intel,nw0ptay,How?,pcmasterrace,2025-12-26 13:17:50,1
Intel,nydkpg0,"Were in ""ITS SO OVER"" Phase of memory calipse, I'm pretty sure arc  580 is weaker than ur ded gpu. So u better get some money somwhere fast. Ssd situation is the same its about to get expensive. In short its so over. Im sad for gamers :(",pcmasterrace,2026-01-08 10:49:46,2
Intel,nydmdpn,I'd say either get a 9060XT 16GB or get a used 2080ti and call or a day,pcmasterrace,2026-01-08 11:03:44,1
Intel,nydovhe,"Arc B580 is a very decent GPU and if you can buy it at that price you should snap it up.  As for storage, buy it if you can afford it but if you're ok deleting stuff you're not playing or have completed you shouldn't realistically need to.  A second hand 2.5"" SATA drive for additional storage can be added at any time you actually need it.  EDIT: also sell that 1080 ti or repair it. It might only need the solder reflowed and new paste applied (which you can try yourself with a heat-gun and a laser thermometer (there are tutorials on YouTube).",pcmasterrace,2026-01-08 11:24:20,1
Intel,nyk4vlm,The problem with Intel Arc GPUs is that they require newer hardware to work at its full potential   Afaik you need to have a PCIe 4.0 slot and ReBar support   Also you need a modern CPU at least a Ryzen 3000 series on AMD side iirc for best performance   Don't know what CPU gen is needed on Intel side,pcmasterrace,2026-01-09 07:43:32,1
Intel,nykvu23,"just found a video(1y old) saying the B580 runs bad on older pc's.  i have a Z370n mobo with and i7-8700 CPU, would i work on mine ? has there been any driver update in the last year that fixes this ?",pcmasterrace,2026-01-09 11:42:04,1
Intel,nyfizy4,"b580 is around 10% faster than the gtx 1080 ti, has 1 gb more vram and is much newer so better support and it has raytracing cores unlike the gtx 1080 ti. (still wouldn't do such a small upgrade personally.)",pcmasterrace,2026-01-08 17:13:12,0
Intel,nydqzcw,"i tried the oven trick 3 times, 10min on 200Â°, 15min on 220Â°, 2hours on 125Â°, everytime it works again for a couple hours / days, then black screen again.  When im gaming i randomly get screen freezes with red artifacts for a couple minutes, then black screen and it 'works' again. and after a couple days i just get black screen when booting my pc, monitor turns on, gets signal but its just a black screen. Chatgpt says its the VRAM",pcmasterrace,2026-01-08 11:40:59,1
Intel,nydy2n5,"silly idea, do the thing again, but mount it upside down so the heatsink pushes the chips towards the PCB?     (no idea if this will actually help)",pcmasterrace,2026-01-08 12:31:29,1
Intel,nye6ibn,"If baking it fixes it it's something on the board where the solder has cracked.   Pouring flux around and using a heatgun and temperature gun (or baking it) still has a chance to fully melt and reflow the solder if you get it hot enough (but not too hot).  You have a repairable card, at least for someone if not you, so all I'm saying is sell it on based on it being fixable but needing reballed or reflowed at minimum.  Someone will be able to get use out of it once it's fixed, and in the meantime you have some small amount ($?) to put towards the B580, which would be my likely choice in your price range.  There's a guy on YouTube you might have seen fixing GPU's, but he does charge as much as you'd expect to pay for the same card on the second hand market to do the work.  Someone like that would probably snap it up on eBay and fix it for selling on, but I totally understand the work he does that we can't do is pretty serious knowledge and experience at work and you or I probably aren't up to tackling it.",pcmasterrace,2026-01-08 13:23:16,1
Intel,nyqwby3,"You are using an F-series CPU, those don't have integrated graphics which is why you can't get a signal from the motherboard. You need to have your GPU plugged in and you need to have your display cables plugged into the GPU to get a signal.",pcmasterrace,2026-01-10 06:49:20,5
Intel,nz2dk38,"Turns out worst case scenario: my Graphics card took a crap. Took it to a local PC shop that builds/repairs them.  It's still under warranty, so fingers crossed that ASUS comes through for me.  Don't come at me for the ASUS parts, it was a prebuilt and a gift (I desperately told the gifter NOT to get me lmao).",pcmasterrace,2026-01-11 23:40:50,1
Intel,nzdkwlp,"have you properly clean up the heatsink fins of your laptop, not just the fans as sometimes lint and dust accumulate and gets trapped overtime that even a decent powerful air compressor wasn't even able to blow it out. Like the one on this image where you have to toally disassemble the cooler, remove the fan, then now have access to the heatsink fins.  ![img](r74g010585dg1)  make sure you are using the right thermal paste for direct die/laptop applications, those pastes that are viscous, good conductivity and not prone from pumping-out or deterioration when exposed from high concentrated heat.  like for CPU+GPU:  * the best will be Phase Change Thermal Interface Materials (PCTIMs) like: Thermal Grizzly PhaseSheet, or Thermalright Heilos. These provide excellent conductivity and long-term stability, making them ideal for bare-die and high-performance applications. (also not electrically conductive so safer than liquid metal and often the go to for the best). * If these arenâ€™t available, go with thick, viscous thermal pastes that resist pump-out and handle repeated thermal cycling well. Good alternatives Iâ€™ve tested include Kold-01, Maxtor CTG10, DOWSIL TC-5888, Maxtor CTG8, Arctic MX-6 Rev.4 (2025 batch), Arctic MX7, Thermal Grizzly Duronaut and Cooler Master MasterGel Maker. These perform much better over time compared to less reliable compounds such as the Arctic MX-4, MX-5, MX-6 (2023/2024 batches), which are prone to displacement, or Thermal Grizzly Kryonaut, which tends to degrade under sustained high heat.  and if your laptop use thermal putty, you could reuse them as long as they are clean and malleable (same if they are pads). But do not replace thermal putty with thermal paste or thermal pads, as that's no good.  Also ensure you are using your laptop elevated and not on a soft surface that could obstruct the bottom air intakes/vents of the laptopIif your laptop CPU is pulling more than 100-120+ watts these temps make sense duing a CPU benchmark like Cinebench",pcmasterrace,2026-01-13 16:31:15,1
Intel,nzeia1z,Make sure you're elevating it when using to allow for airflow. Also get a good cooling fan/pad. Those things NEED them.,pcmasterrace,2026-01-13 19:13:31,1
Intel,nzeoch4,"Okay, what's the problem? Crashes? Freezing? Games don't load?",pcmasterrace,2026-01-13 19:41:06,1
Intel,nzdhd4k,"Welcome to gaming laptops lol, make sure you didn't fuck up when putting it back together",pcmasterrace,2026-01-13 16:15:11,1
Intel,nzdjv13,That's how laptop gaming for the most part is. You can do is try feeding it some air with those laptop stands that have fans.  Before setting out to buy anything you could unscrew the back cover off and try to play like that to see where that gets you.  I drilled holes into the back cover on mine. It's an older laptop with gtx 850m but it did improve the temps some.,pcmasterrace,2026-01-13 16:26:32,0
Intel,nzdsgwc,"The paste box did not say its batch but it is rev.4 and box says imported in nov 2025 and yes I reused all my thermal putty and thermal pads, didnâ€™t look like very bad condition.",pcmasterrace,2026-01-13 17:17:14,1
Intel,nzdkxg3,"So is it normal? Ik my last laptop had similar temps but I only started playing games on it when I was gonna replace it and realised it had a Dgpu then only, so it was pretty old laptop and I didnâ€™t care.",pcmasterrace,2026-01-13 16:31:21,1
Intel,nzdo6ze,I donâ€™t like how those cooling pads look so I checked jarrodâ€™s tech and it showed elevation stand and cooling pads donâ€™t have a huge difference. My laptop isnâ€™t even a year old so I donâ€™t think I can make holes in it lol,pcmasterrace,2026-01-13 16:46:11,1
Intel,nyuuud1,Nice! How's 4k on the B580?,pcmasterrace,2026-01-10 21:30:10,1
Intel,nyvluph,"I would rate it 8/10. That looks like too much thermal paste and the space is a little cramped, but your build is unique!  I just did a new build and I might go with a green color scheme this time around, last battle station had a red scheme.",pcmasterrace,2026-01-10 23:48:12,1
Intel,nyvshta,"This is great for a first setup!Â    (A little too much thermal paste, but oh well)",pcmasterrace,2026-01-11 00:23:00,1
Intel,nyuswxm,This is a really bad setup. Table and chair way too small...,pcmasterrace,2026-01-10 21:20:33,1
Intel,nyuvmui,"amazing! only 1 notable game ive tryed playing that runs like shit and thats marvel rivals, but i think thats just the lack of optimization, but generally im getting 60fps on high/max settings in almost every game ive been playing",pcmasterrace,2026-01-10 21:34:05,1
Intel,nyvpuhm,"thank you! im pretty small so i dont mind, plus i dont have a lot of room tbh, i used to have a much smaller desk",pcmasterrace,2026-01-11 00:08:51,2
Intel,nyvv1q2,thank you! yeah im gonna bite the bullet when/if i get a better cpu cooler,pcmasterrace,2026-01-11 00:36:04,1
Intel,nyutgn8,"im small..., and even tho i looks like i have space, i dont, i just moved shit to take a better picture",pcmasterrace,2026-01-10 21:23:19,1
Intel,nyysjaf,https://preview.redd.it/w3c8v3zezpcg1.jpeg?width=4000&format=pjpg&auto=webp&s=679258d6def2d56136130e36a0f556df49fbc98f,pcmasterrace,2026-01-11 13:11:58,1
Intel,nyvxv72,Tbh it reminds me of my first hand me down pc crammed in the corner of my room growing up. It's cozy :],pcmasterrace,2026-01-11 00:50:15,1
Intel,nyuts92,"Yes, but you are to close to the screen and your legs are cramped under the desk. Also, no armrest. You will have problems in the future...",pcmasterrace,2026-01-10 21:24:55,1
Intel,nyvykz1,tbh that is kinda my living situation lmao,pcmasterrace,2026-01-11 00:53:51,2
Intel,nyuv6jy,"i sit on my legs... they generally get cold, also i dont really have any issues with my arms",pcmasterrace,2026-01-10 21:31:51,1
Intel,nzbyuov,this is 1000% a clankerâ™ˆâ™ˆâ™ˆ,pcmasterrace,2026-01-13 10:52:29,14
Intel,nzc6jyt,Very nice :D,pcmasterrace,2026-01-13 11:57:37,6
Intel,nzeabpt,No RGB = 11/10  Ive wanted a dark black build for years. I hate RGB.,pcmasterrace,2026-01-13 18:37:59,2
Intel,nzcfcd4,Solid 8,pcmasterrace,2026-01-13 12:59:02,1
Intel,nzcisb5,Sexy,pcmasterrace,2026-01-13 13:20:26,1
Intel,nzeeskz,"I'm jealous of that back panel cable management. Overall, very nice ðŸ‘Œ",pcmasterrace,2026-01-13 18:57:48,1
Intel,nzfmqau,Love the all black look. Did that myself this most recent build in a Lian Li Lancool 217.,pcmasterrace,2026-01-13 22:21:19,1
Intel,nzc5rl7,Why canâ€™t people make simple posts these days without ai? Like seriously why was it necessary lmao,pcmasterrace,2026-01-13 11:51:22,7
Intel,nzbz4sb,Clanks efficiently,pcmasterrace,2026-01-13 10:54:54,-1
Intel,nzcr7x6,"fr clean build bro, love the stealthy look and airflow setup is ðŸ‘Œ",pcmasterrace,2026-01-13 14:07:17,1
Intel,nzcz8dy,Yup I build a similar no rgb setup. I was actually looking at this case as well but went with a Lian li 217 because the included fans,pcmasterrace,2026-01-13 14:48:49,1
Intel,nyiyx49,I'll believe them (either Intel or AMD) when I see the benchmarks.  Until then this is all just pointless noise.,AMD,2026-01-09 02:57:02,203
Intel,nyj1h7b,well yeah you can't compare them because strix halo is on a signficantly larger die wheras panther lake is more comparable to something like the hx370.   If amd is able to get strix halo at a competitive price then sure it will compete but the issue is that with such a large die I don't think it is possible for them to compete in price with panther lake,AMD,2026-01-09 03:11:08,58
Intel,nyj2ydj,Iâ€™ll never understand why AMD is not committing to design RDNA4 based APUs and at this point I just take RDNA 3.5 as a joke because they canâ€™t even support FSR4 on it officially nor the RX 7000 cards.  Itâ€™s like they are losing on purpose,AMD,2026-01-09 03:19:12,59
Intel,nyja8ig,"AMD has this â€œitâ€™s good enough for a while and weâ€™ll release something great that people will forget this happenedâ€  Vega lasted in mobile for nearly 5 years and got RDNA2 designs. Now, itâ€™s RDNA3.5 being built for mobile platform and betting on that to be good enough until RDNA5/UDNA bridge die designs releases (unverified rumor)  AMD also has this weird obsession with competitor naming. Sure, itâ€™s meant to confuse buyers but itâ€™s hurting them than helping, maybe it does help in terms of inventory.  Theyâ€™re not intel-like of stagnation. Theyâ€™re competing but not for us in the consumer market and weâ€™re just getting scraps until enterprise trend die down (currently AI trend/bubble).  Well, itâ€™s understandable as Zen designs are really focused in Epyc and scale down to Ryzen SKUs.  And the 400 series is a bad refresh when Ryzen 6000 mobile is the definitive refresh they have done, Zen 3+ and move to RDNA2. AMD couldâ€™ve done similar commitment but itâ€™s not currently.  Also, AMD forgor Strix Halo laptops are still nowhere to be found aside from 1 or 2",AMD,2026-01-09 04:00:20,12
Intel,nyitizl,"AMD is going to f--- around and let Intel catch up, in CPUs and GPUs. They're slacking off on that stuff trying to focus on data center. Hope that works out for them.",AMD,2026-01-09 02:28:24,77
Intel,nyj02vs,Amd really doesnâ€™t gaf about anything other than data centre these days,AMD,2026-01-09 03:03:22,11
Intel,nyjy5yc,Idgaf when Strix Halo products are nowhere to be seen (notebooks),AMD,2026-01-09 06:46:12,4
Intel,nykgnvm,"Except that the B390 will be far more common as it will be seen in far more laptops. Yes, the 8060S & 8050s can be found in some laptops, but for the laptops you'll find in places like Currys, Best Buy or Mediamarkt, the B390 will be the most powerful iGPU you'll likely find & it'll happily outdo a Radeon 890M",AMD,2026-01-09 09:30:29,5
Intel,nyj9p6p,"AMD is at the point where Intel was before they went down the route and are recovering, history repeats before it's too late.  Not going to believe either until we get actual benchmarks and results.",AMD,2026-01-09 03:57:09,8
Intel,nz0u8e4,Meanwhile AMD keeps putting out new chips with years old GPUs.,AMD,2026-01-11 19:18:48,3
Intel,nyjg6co,Core Ultra 2 is already a better mobile soc.  I don't know why AMD thinks 12-16 cores is more important than battery life when it comes to laptops.,AMD,2026-01-09 04:35:45,8
Intel,nyjsbo1,It does sound complacent but ultimately the proof is in the pudding.,AMD,2026-01-09 05:59:29,2
Intel,nyku0xe,"If it is not even fair to compare (because Strix Halo is WAY more watts) then why is AMD comparing them? B390 will exist, Strix Halo virtually does not in laptops.",AMD,2026-01-09 11:27:40,2
Intel,nynggco,"AMD is playing the same intel book a few years ago. Except now instead of 14nm+++++++, it is RDNA 3.5555555.",AMD,2026-01-09 19:23:52,2
Intel,nykofoz,Lot of markets and Intel did bribe the oems for decades and still do,AMD,2026-01-09 10:40:44,3
Intel,nykbcoh,The Intel igpu has a better upscaler by far. FSR3.1 is a third class competitor in comparison. People have been crying out for AMD to release FSR4 for RDNA3.5 but AMD has some seriously stupid execs in charge.,AMD,2026-01-09 08:41:31,2
Intel,nyjsl7o,Benchmarks first,AMD,2026-01-09 06:01:32,1
Intel,nyn2qhh,Beware hubris.,AMD,2026-01-09 18:22:54,1
Intel,nynbvp5,"There are already benchmarks, look them up.",AMD,2026-01-09 19:03:00,1
Intel,nynoj5f,Don't they already have an integrated GPU that's on par with an RTX 4060? According to Framework?,AMD,2026-01-09 20:00:51,1
Intel,nyorjcc,"strix halo is nice and all, but too prohibitively expensive to be considered for many people  arc b390/b370 will be available in much cheaper products for which amd doesn't have a proper answer to atm. amd's next lineup can't be lazy if they want to stay competitive",AMD,2026-01-09 23:05:05,1
Intel,nyqdhsx,"The GPU doesn't matter if you don't have proper drivers and they are so far behind still Intel.   Great progress, but the drivers are still going to be the thing that makes people say no.   If intel keeps on chugging away and they work with all the DirectX games backwards and going forward.   I'm talking past DirectX games, you can't just worry about the new games there's games that are older that don't run well.   It took AMD many many years to get decent drivers, Intel I don't know if they're just focusing on hardware and not the drivers, but that so far is what's been holding it back.   Hopefully they can release a true dedicated GPU back in rival something that's out there at a much better price that will bring at least some competition back until the AI scam is over.",AMD,2026-01-10 04:30:09,1
Intel,nzbi19w,Panther Lake is using a superior process technology. So they are right. But it doesn't matter as customer will choose what's better. But until AMD has something out that uses 2nm then yes they will be behind probably,AMD,2026-01-13 08:12:42,1
Intel,nzgbs70,I hope Intel stays competitive and AMD also brings its best to the table.,AMD,2026-01-14 00:33:01,1
Intel,nyj94g8,They should be worried about DLSS 4.5 though. Fix stuttering on FSR 4 and improve image quality,AMD,2026-01-09 03:53:50,2
Intel,nyj4pf7,I just wish Intel would make a very cut down panther lake offering to be the successor to the N1xx/N3xx line of efficient chips that have found their way into mini PCs.,AMD,2026-01-09 03:28:57,1
Intel,nyoak0m,amd unfazed? I bet they are talking big shit again then will get absolutely demolished (as it happened with vega too),AMD,2026-01-09 21:42:52,1
Intel,nyjappo,"Yeah it's old architechture, that's the point, Intel moved to the lastest node and barely manages to eek out a win. A win is a win nonetheless, but AMD still have plenty to dials to turn up.",AMD,2026-01-09 04:03:08,-6
Intel,nyju3k9,"I don't understand the fuss about iGPUs? Like why do they assume the average Joe would care about an IGPU at all? That's maybe 5% of the market and even then...most of them would get a dGPU anyways.  And apart from the GPU, what's special about the CPU? Combining (Lunar Lake) efficiency with (Arrow Lake) power? Sorry but my Ryzen AI 7 350 does that already. The top of the line x9 388h is about ~10% faster in single core aka the only thing that matters and will probably be in 2500â‚¬+ laptops whereas my 7 350 is in 500â‚¬ laptops.  I tried a 285h laptop besides the AI 7 350 and not only did it run hotter and less efficient, it also felt less snappier.    And the AI 7 350 was designed as a Lunar Lake competitor anyways so it was never worse in efficiency and ahead of Arrow Lakes like the 255h in that regard.  So I don't see why anything should really change...?",AMD,2026-01-09 06:13:30,-11
Intel,nyj661m,"AMD doesn't want to bring RDNA 4 to APUs, so as not to give FSR4 to users other than those with dedicated GPUs.",AMD,2026-01-09 03:37:14,-7
Intel,nyjyeyc,a brand new product on a newer node is better than an older product on an older node?! who knew?,AMD,2026-01-09 06:48:14,-2
Intel,nyj7q2f,"And the price. I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand. Hope Intel gets back into the game and if Intel can slash their prices it might end-up being the right choice.  At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.",AMD,2026-01-09 03:45:51,82
Intel,nyjgluo,Plenty of folks tested it at CES.  Intel was confident enough to let reporters run benchmarks and it's basically around 4050 level.  You should be able to run most games at 1080p at medium-high settings in an Ultrabook form factor.,AMD,2026-01-09 04:38:25,20
Intel,nyj2b71,"Well lunar lake is a monster and competes directly with the z2e both on performance and efficiency, so no reason to think panther lake will be worse.  Even if it falls short of Intels claims it will still be the leader until next year.",AMD,2026-01-09 03:15:43,24
Intel,nylsap7,"Both are right/wrong.  Intel made their comparisons to Strix point because theyâ€™re in the same power class. Panther lake is much faster than Strix Point at the same power level (according to Intel, AMD doesnâ€™t deny that) at 45W.  AMD says it doesnâ€™t matter because their Strix halo (up to 120W) is faster which is pretty obvious.   Itâ€™s not technically lying, AMD is just referencing an entirely different class of product.",AMD,2026-01-09 14:52:54,3
Intel,nyjcak9,The noise is doing a great job advertising for them. A war between them with fighting words will get them tons of free advertising.,AMD,2026-01-09 04:12:29,1
Intel,nylt6fh,And a much higher power budget.  AMD says they win because their 120W chip is faster than Intels 45W chip.   No surprise to anyone.,AMD,2026-01-09 14:57:05,22
Intel,nyjcmeq,"I generally agree but i suspect that the 388h is using a much larger gpu than people suspect. I think its probably ~165mm2 in size, not 55mm2. i suspect the 55mm2 die varient is for the 4xe version, and the 12xe version is 3x that size.  I also dont understand why strix halo is so expensive. It would be interesting to see bom and packaging costs.",AMD,2026-01-09 04:14:25,8
Intel,nyje36x,"It sounds like RDNA4 just doesn't scale at all. All the rumors point to them going straight from RDNA3.5 to RDNA5 in APUs, just skipping RDNA4 all together.",AMD,2026-01-09 04:23:13,36
Intel,nyjuufi,RDNA 3.5 was the only reason I didnâ€™t invest in a STRIX HALO mini PC. The price is too much for outdated unsupported tech.,AMD,2026-01-09 06:19:22,16
Intel,nyji62l,"Might be the same reason they stuck with Vega for so long in APUs.  At the current available desktop memory (DDR4 at the time) an architecture change wouldn't have made a huge difference.    Once DDR5 came out for laptops, we finally saw RDNA 2+ APUs (Ryzen 6000 APUs).  I'd bet once DDR6 starts appearing on laptops we'll get a similar iGPU architecture leap.",AMD,2026-01-09 04:48:17,9
Intel,nynizjw,"AMD probably just didn't bother making a new APU design when they didn't have new CPU core to go with it. Medusa Halo is rumored for 2027 with Zen 6 and UDNA/RDNA5, so the Point version will likely release then too.",AMD,2026-01-09 19:35:27,1
Intel,nyiu18v,AMD has and always will be their own worst enemy,AMD,2026-01-09 02:31:09,77
Intel,nyiyjxo,"It's not like they aren't developing something this whole time, releases are planned many years in advanced. Intel will have some rope and then will get inevitably leap frogged",AMD,2026-01-09 02:55:05,8
Intel,nyj83ic,It's not like Intel isn't doing the same. Panther Lake and ARC are holdovers of things developed under Gelsinger.,AMD,2026-01-09 03:47:56,8
Intel,nyj28hs,"> AMD is going to f--- around and let Intel catch up, in CPUs and GPUs.  this is what we actually need: competition. AMD kicked intels butt, now intel is kicking back. it's a win for us either way.",AMD,2026-01-09 03:15:19,12
Intel,nyiwa7d,I hope Intel will catch up and encourage AMD to compete. Having cleat leader in CPUs or GPUs is bad for consumers.,AMD,2026-01-09 02:43:07,9
Intel,nyjspa3,"I mean, we know that AMD is innovating. They literally showed Zen 6 at CES. Its just not ready yet for mobile, and Intel caught up. Same thing happened with Alder Lake, where intel released that before Zen 4 was ready.",AMD,2026-01-09 06:02:25,6
Intel,nykmhz9,"So AMD having much faster iGPUs for decade or more did not do much.  But now Intel rolling out something at unknown price/power package will absolutely decimate AMD.  Regardless of what will happen, ""AMD's fault"" indeed. (amazing silicon designers and experts at everything posting for free on reddit have convinced me)",AMD,2026-01-09 10:23:35,4
Intel,nynjyg2,"Laptops haven't really been AMD's focus, and apart from Zen1, AMD's focus has been mostly on data center, with desktop being the natural offshoot.",AMD,2026-01-09 19:39:58,1
Intel,nyjozjz,Yeah they ain't immune to being complacent.  And bad press doesn't make Intel stay bad.,AMD,2026-01-09 05:34:41,0
Intel,nyj8pzu,"Yes, their leaders aside from Ms. Su are all holdovers from AMD almost bankrupt times. Software sucks, graphics sucks, and now they are taking CPU down the drain too with high prices. Shitty leaders, lead to shitty outcomes.   They post high useless benchmark numbers no one cares about. They cannot compete with Apple in terms of battery life, and get creamed at low power levels.",AMD,2026-01-09 03:51:30,-9
Intel,nzb2682,"News at 10: ""Companies prioritise profits""",AMD,2026-01-13 05:54:06,1
Intel,nynkzx7,"so either AMD doesn't have the capacity to produce them, or OEMs aren't interested, neither option is a compelling reason for AMD to focus on mobile",AMD,2026-01-09 19:44:43,1
Intel,nyjj3pe,"Weâ€™d wish they were, but theyâ€™re not. Intel was struggling on all fronts due to their fabs. Amd is actually moving super fast in data centre so both epyc and instinct which is where they believe their money will be. They just donâ€™t care to do anything in the consumer market.",AMD,2026-01-09 04:54:22,7
Intel,nynm27a,"Halo is so much faster that the upscaler difference doesn't matter at all. Of course, it is probably also bigger.",AMD,2026-01-09 19:49:33,0
Intel,nyoktyw,"Yes, Strix Halo",AMD,2026-01-09 22:31:24,1
Intel,nyjieva,And having fsr4 supported in mobile at all,AMD,2026-01-09 04:49:53,4
Intel,nzat1bf,FSR 4+ may be great but game support (number of titles + GPUs supported) is embarrassingly low,AMD,2026-01-13 04:48:21,1
Intel,nyjm8fi,Dlss 4.5 not that great in my opinion. It fixes some ghosting but creates more shimmering because it has so much sharpening. I had to dial back to 4.0.,AMD,2026-01-09 05:15:20,0
Intel,nyj9k89,wildcat lake.  only issue it seems to be using 2 P cores and 4 LPE cores instead of E + LPE,AMD,2026-01-09 03:56:23,4
Intel,nyjicwx,70% faster being â€œbarely eke out a winâ€? Go ask why amd is stuck with 18 month old architecture despite intel managing to replace arrow lake after 12?,AMD,2026-01-09 04:49:30,10
Intel,nyjvty1,"Youâ€™ve got it backwards. The majority of laptops use IGPUs. IGPUs being as powerful as integrated graphics allows for cheaper thinner devices that are more power efficient. The entire intel CPU/IGPU performs on par with a 4050 at 60w at only 45w. When you factor in the 10-15w the CPU takes with the 4050 and youâ€™re looking at similar performance at like half the power.   Being power efficient opens up a lot of form factors to be able to game with such as thin and light laptops, tablets, or gaming handhelds",AMD,2026-01-09 06:27:13,9
Intel,nyksz11,"Absolutely consumers win when competition is hot, AMD has a bit too much of a lead ATM so they are cashing in and getting lazy. That said I am glad they are having their day, only because a few years ago they were on the brink of bankruptcy and I really want to see them on a fairly level playing field with Intel... If we end up with 2 juggernauts training blows, having big resesrch budgets, etc we'll get lots of innovation and competitive pricing.",AMD,2026-01-09 11:19:09,8
Intel,nyl2kgg,"> I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand.   That's why it's silly to be a ""fan"" or ""supporter"" of one company or the other.  They don't care about you, they care about making money and when they have a dominant position they will exploit it.  > At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.  Outside of that one device (Ayaneo maybe?), you're right.  But now AMD is also releasing an 8-core version of Strix Halo with the full 40 GPU CUs, which should be cheaper.  I expect that we'll see that in more handhelds at the high end.  Realistically speaking, it's easy to make the case that on a 7""-9"" screen the 40 CUs is way overkill.  There's still room for a middle ground that Intel could easily fill.",AMD,2026-01-09 12:30:34,27
Intel,nykkoyj,"You had to shovel $1k for a 8 core CPU for about a decade, before AMD came.  So ""it just hiked the price"" is BS.  AMD cannot keep prices low while TSMC, effective monopolist, keeps posting record profits quarter after quarter.",AMD,2026-01-09 10:07:29,37
Intel,nyjf6qu,AMD is basically just waiting for their chance to do the bad things. They are a corporation after all.,AMD,2026-01-09 04:29:52,36
Intel,nyo271t,"Help us Cyrix, you are our only hope...",AMD,2026-01-09 21:04:11,3
Intel,nykfx0k,AMD has always been like this. The OG Athlon FX line from ~22 years ago were $1000 CPUs.,AMD,2026-01-09 09:23:31,-3
Intel,nyjjlvs,Wasn't it equivalent more or less to the 4050m as it was power limited to 30 watts?,AMD,2026-01-09 04:57:40,12
Intel,nykm4lu,"It's the price of the final product that will matter.  And given that Intel has lion's share of the mobile market, I don't see why the would not ask outrageous $$$ for it.  It is ""impressive"" only in the ""for iGPU"" context.   Based on the benches shown, laptops were consuming around 60W.  While AMD""s 370 HX has been shown to be able to game at below 20W, so uh.  Let's bait for wenchmarks in any case.",AMD,2026-01-09 10:20:17,4
Intel,nyl1xad,"Sure, sure.  I'm going to wait for proper benchmarks done under lab conditions and documented by more than ""Intel let me run this game with the FPS counter on.""  I mean, the general impressions for Intel are quite positive and if they're true then I hope it spurs AMD to do more.  I'm just not going to blindly accept ""first impressions"" as a replacement for proper testing.",AMD,2026-01-09 12:26:10,1
Intel,nym6wt0,*With 64gigs of ram at 9600mhz,AMD,2026-01-09 16:00:23,0
Intel,nyj7zfr,"It was a Z1E competitor, but AMD gravely disappointed with the Z2E zero performance increase over Z1E and coming at a major price premium.",AMD,2026-01-09 03:47:18,-9
Intel,nynhnmj,"12 Xe cores is 60% of the 20 cores in the B580 and that's 272mmÂ², but of course that also has GDDR memory controllers, and such that aren't needed on a GPU chiplet, but it's likely that the die for the top SKU is quite a bit bigger than 55mmÂ², I'd say between 90 and 130 mmÂ².",AMD,2026-01-09 19:29:20,3
Intel,nyoht8c,"https://x.com/jaykihn0/status/1812898063502938260/photo/1  ""PTL-H 12Xe pictured."" so according to that the 55mm2 die variant is xe12",AMD,2026-01-09 22:16:37,3
Intel,nykps9b,">I think its probably ~165mm2 in size, not 55mm2.   Which might open an ""dGPU sized iGPUs"" race.  NV could be the main victim here Surely AMD can oversize its iGPUs too.  I actually thought that AMD was forced to do so, by Filthy Green's GPP effectively banning AMD dGPUs. Typing this from G15 AMD Advantage Edition TUF.",AMD,2026-01-09 10:52:22,1
Intel,nyjhzne,What matters is the intel chip regardless of actual die size runs on quad channel LPDDR memory instead of the octa channel of strix halo and is fitting into mid and small size laptops 15-45w.,AMD,2026-01-09 04:47:09,-8
Intel,nykcsfk,Then how is the exynos 2600 using rdna4 fron samsung if it doesn't scale?,AMD,2026-01-09 08:54:32,9
Intel,nyktb13,RDNA5 doesn't exist. The actual name for the next generation architecture is UDNA1.,AMD,2026-01-09 11:21:54,-2
Intel,nzasd0f,"Same here. No new FSR tech and ROCm was just as poor. It works now but Vulkan is often better.. Very disappointing. For AI, NVidia is so far ahead.",AMD,2026-01-13 04:43:51,1
Intel,nym1e1u,If intel can extract more out of LPDDR5x with B390 then I don't see how AMD can't. Just too stingy to give more die area to cache?,AMD,2026-01-09 15:35:23,3
Intel,nyjxf3k,So basically RDNA4 is just another RDNA1,AMD,2026-01-09 06:40:05,1
Intel,nytzgu6,Kepler said in another subreddit that Medusa Premium and Halo is launching in 2028. You're only getting the crappy RDNA3.5 iGPUs for the third time.,AMD,2026-01-10 18:54:53,1
Intel,nyj1177,"Intel: but the enemy of my enemy, is my friend.  Intel ðŸ¤ AMD  we're cooked guys /s",AMD,2026-01-09 03:08:41,24
Intel,nyleac5,"> But now Intel rolling out something at unknown price/power package will absolutely decimate AMD.  Are you finished, or you wanna make up some more shit I didn't say?",AMD,2026-01-09 13:41:18,2
Intel,nykbk7y,"Yeah the people saying AMD is stagnating are just wrong. AMD is kicking all kinds of ass... They just don't care much for the consumer market currently.    The other issue is that there's no point releasing a new line of products when no one can afford anything because nand flash is so expensive.    Companies CAN afford this because they need to ride the ai wave, but consumers can't because the average PC cost almost doubled.",AMD,2026-01-09 08:43:25,3
Intel,nyjeicz,"Wow! Excellent. Hopefully we'll see the products coming to market soon, and hopefully the 2 P cores won't matter as much since we're seeing a major lithography improvement. Intel is really impressing me lately.",AMD,2026-01-09 04:25:42,2
Intel,nyltrkg,This. Plus even lunar lake outperformed Strix Point in many scenarios already. Intel is at least one generation ahead here,AMD,2026-01-09 14:59:54,2
Intel,nyk0tw7,"It's not the 4050 at 60W. The laptop they compared only allows for 30W to the 4050. Nvidia's specs for the 4050 is 35W minimum, so I don't know how Dell even got to 30W. Below a certain wattage, gpu performance decreases exponentially because a minimum level of power is required to even have the gpu turned on.   Panther Lake is built on Intel 18A, which is supposed to be much better than the 'ancient' TSMC 5nm the 4050 is built on. The 4050's cpu is also Arrow Lake, which is less efficient than Lunar Lake. Again, that skews the agenda.   You can already game on thin and light devices with discrete graphics. Laptops like Asus's G14 is only 3.3lb, but sports a 4060 which is like twice as fast as intel's new igpu. The dgpu turns itself off when on battery, and the integrated graphics takes over. Anything more intensive should be used with a charger plugged in.   In short, paying for a big igpu doesn't make much sense for anyone interested in performance. And gaming handhelds? Does anyone really care about those useless bricks for investment into integrated graphics? It's not like the cost of Panther Lake is going to be cheap when its laptops start at $1300. With that kind of money, you can get 2025 Asus Zephyrus G14 with a 5060 and blow its shit out the water. Or for those on a budget, 5050 laptops have been seen for $600.   Integrated graphics have come so far, pairing Intel's newest 18A Panther Lake with an RTX 4050 could still make a lot of sense.",AMD,2026-01-09 07:08:24,-1
Intel,nyjxwil,"Ehh not the mayority but *all* computers use iGPUs. The thing is, for the average Joe aka 95% of the market, there won't be a difference in the usage between an Intel Iris or RTX 5090 dGPU. And the efficiency would only come into place if they would game on battery (who does that anyways) or create/edit videos (again, virtually no one would do that without a dGPU). And even then, having your laptop drained in 2 hours 15 minutes instead of 2 hours is not ""gamechanging""   So it does not affect the efficiency at all during webbrowsing, watching videos, creating documents etc.  Thats also the reason why Intel has the non X 5,7,9 which will properly be by far the more demanded version as, again, the average Joe does not care the slightest about iGPU.  And apart from the iGPU, PTL is just a tiny step up from the Ultra 200 series...",AMD,2026-01-09 06:44:02,-2
Intel,nykyb9y,"What no AMD deserves much worse. How can a company fuck up so much and still survive. I would rather good competition rather than competing for the sake of competing. Marketing is bad, products are bad and they keep shooting themselves in the foot. Id rather Qualcomm or some other ARM company compete with X86. Its ARM or RISCV time to shine.",AMD,2026-01-09 12:00:44,-15
Intel,nyp5e4a,They need to release a strix Halo with less ram. 24GB is good enough for portable devices. I don't want to pay for useless ram because of bullshit AI or bullshit comparisons with Apple M5.,AMD,2026-01-10 00:18:51,6
Intel,nyqzsy8,"It's so weird that people treat their computer parts with a cultish following. Most of the people I know don't think about their cards at all and are just happy to play whatever games.   Super weird to be ""team red"" or ""team green"".   I can't imagine describing myself as a ""my computer chip manufacturer fan"". Cringe lmao.",AMD,2026-01-10 07:19:21,1
Intel,nylsh5z,Ever head a look at their profit margins?,AMD,2026-01-09 14:53:45,2
Intel,nyp4i2m,"Nice try AMD employee #1234. Check the price hikes for devices with Z1E vs. HX370 vs 395 AI ++. Steep short term price increase, and AMD is increasing their profit margin.",AMD,2026-01-10 00:14:12,2
Intel,nyp51ta,Bullshit Z1E extreme devices were all together sold for less than a $1000.,AMD,2026-01-10 00:17:05,1
Intel,nykktsq,"Bullshit.  For starters, pricing is not a ""bad thing"".  Bad thing is, pick any piece from blue/filthy green's arsenals:  1) Strongarming OEMs 2) Strongarming Journalists 3) Proprietary standards",AMD,2026-01-09 10:08:42,15
Intel,nylsknw,Waiting? Theyâ€™ve been busy doing that for years now,AMD,2026-01-09 14:54:13,0
Intel,nyosqz5,"I WISH they were still around. I think their IP got sold to Via, who's not doing anything with it.  At this point, we might only get competition in the desktop x86 space if the government forces Intel and AMD to license x86 and x86-64 to some other chip designer (like Qualcomm or Mediatek) or Windows on Arm and Linux on Arm start getting wide application support, including office software and games.",AMD,2026-01-09 23:11:26,2
Intel,nyp60wf,Lol. Blast from the past. It only took AMD 2 generations of Zen to get the lead. Intel can get it done if they get rid of all the useless VPs and get back the good engineers.,AMD,2026-01-10 00:22:09,1
Intel,nykpg6r,"""Being good"" was never about price.",AMD,2026-01-09 10:49:30,1
Intel,nyp4qvh,"Nah, Z1E prices were reasonable recently.",AMD,2026-01-10 00:15:29,0
Intel,nyjpruz,"They said it rivals a 4050 at 60W. The 4050 maxes out at 100W on paper but it's actually at 80W that it hits its peak performance. a 60W 4050 is about 85% of it's max performance.  So performance wise panther lake should be about on par with a full powered 3050Ti laptop.  That plus more advanced ray-tracing cores, it's running doom dark ages really well, AMD is still stuck at RDNA 3.5 and ray-traced games suck on the 890M.  I wish intel released a 24 Xe Core Variant with a 256-bit bus, double the cores and bandwidth. That would compete with the 5060/5070 laptop GPUs.",AMD,2026-01-09 05:40:22,16
Intel,nyl1feq,"Not synthetic benchmarks, we want to see benchmarks in games.",AMD,2026-01-09 12:22:47,4
Intel,nyld8l0,"You're either intentionally mis-stating this, or truthfully aren't aware, BUT, you can game sub 20w on any igpu. What actually matters is the performance scaling.  Also, just to clarify, while the 890m CAN game between 6-20w its performance is essentially identical to the 780m, z1e, etc. It only gets impressive at power draws 30+ (signed, a very happy 7840u handheld owner)  So, what we need to know is how well the new Panther Lake chips scale",AMD,2026-01-09 13:35:33,1
Intel,nyjhpru,Lunar lake came out after strix point. It was squarely a competitor to the 890m. Amd just officially released the cut down strix point as Z2E later.,AMD,2026-01-09 04:45:20,-2
Intel,nyojqy6,"Yeah, i am gonna contend that either that is wrong, and is the 4xe version, or that intel basically lied on their benchmarks.  If none of those two things are true, Intel's new graphics architecture will absolutely dominate in the next round of discrete graphics GPUs.  with B580 intel needed \~80% more silicon to match nvidia performance. Now they need \~10-20% less die area. meaning their performance per transistor basically doubled gen/gen. which is unheard of. Even maxwell (largest architectural uplift in the history of GPUs in the last 10 years) did not achieve anything close to that. And it was a massive overhaul with huge changes.  So . . . there is something big i am missing . . . or intel is going to dominate in all things graphics going forward.",AMD,2026-01-09 22:26:03,1
Intel,nyjiq2t,"Let's not perpetuate this ""octa channel"" DDR 5 nonsense; it's a quad channel chip, and the Intel one is a dual channel",AMD,2026-01-09 04:51:54,15
Intel,nyk3wyi,"If you're going to be pedantic about it at least be correct please, they both use 16b LPDDR channels so the actual counts are 8 channels for Pantherlake and 16 for Strix Halo.  You're fighting a losing battle either way, the industry has long since settled on 64b as the standard channel width for marketing, independent of the actual number of address/command buses.",AMD,2026-01-09 07:35:06,2
Intel,nyki3az,Where did you find information of it being rdna4?,AMD,2026-01-09 09:43:42,6
Intel,nykhvc9,It's a custom implementation. IIRC it's not even RDNA4 but some Samsung derivative that probably has a ton of changes in silicon design to drive power down.  The short story is that AMD didn't bother to do low power optimizations in the architecture and silicon design. RDNA5 should change that.,AMD,2026-01-09 09:41:40,5
Intel,nynjaao,"Mark Cerny talked about RDNA5, AMD's leaked documents have talked about both UDNA and RDNA5",AMD,2026-01-09 19:36:51,7
Intel,nykyia5,This is nonsense RDNA5 does exist. I used to work there.,AMD,2026-01-09 12:02:08,-4
Intel,nyu9kyt,"Kepler's track record with AMD stuff isn't great, but everything is possible",AMD,2026-01-10 19:43:39,1
Intel,nyj1emc,">we're cooked guys /s  No sarcasm there lol  All tech companies are colluding right now, seeing as american business laws don't matter anymore",AMD,2026-01-09 03:10:44,14
Intel,nyj8sm2,"Yes, a frienemy.",AMD,2026-01-09 03:51:55,0
Intel,nyleo84,"Oh, you didn't say AMD was ""slacking off"" and letting intel ""catch up"". Figures.",AMD,2026-01-09 13:43:25,4
Intel,nylti7r,But thatâ€™s also more because of the increased demand from AI than anything else.,AMD,2026-01-09 14:58:39,1
Intel,nyjxwti,"It's 6C/6T and 2x Xe3, so don't expect a whole lot of performance. This is Intel's Mendocino.",AMD,2026-01-09 06:44:06,1
Intel,nylwgde,The slide specifically says 60w sustained for the 4050. I couldnâ€™t find your claimed 30w anywhere. If Iâ€™m wrong Iâ€™d be interested to see where you got the 30w number from because that would be shady by Intel,AMD,2026-01-09 15:12:36,2
Intel,nyp8y9d,"I posted elsewhere about the Framework Desktop with the Ryzen AI 385 and 32GB of RAM.  That's a pretty sensible config for a small gaming device, though it has 32 CUs instead of the full 40.  Still, that puts it ahead of anything in it's class other than the 395+.",AMD,2026-01-10 00:37:45,5
Intel,nym6sbh,"Yes, have you compared it to that of the competitors?  In general, pricing is not the issue to me.   Dirty play like blackmailing OEMs, proprietary standards and other misuse of the dominant market position is. (on top of being illegal)",AMD,2026-01-09 15:59:50,14
Intel,nytz6q1,"Let me guess, you're too young to recognize he's talking about before your time.",AMD,2026-01-10 18:53:34,4
Intel,nylxgon,"Yeah while AMD can't even announce their product AT A CONSUMER ELECTRONICS SHOW! and instead ONLY TALK about AI and government work......   AMD sucks just as bad as Nvidia just as bad as Intel, it's just a constant moving circle jerk as to whom is the least evil.",AMD,2026-01-09 15:17:16,10
Intel,nyr90di,How about this one: strongarming game developers into NOT including DLSS?,AMD,2026-01-10 08:43:32,0
Intel,nytewb4,"Nobody is interested in x86, otherwise Via would have been bought up.  We are in the age of the cloud and all software is custom made. Hence RISC+",AMD,2026-01-10 17:18:43,2
Intel,nyk5oi5,"Also Intel has XeSS which is very helpful for handhelds since they can't manage higher wattages, although not all games provide XeSS as an option",AMD,2026-01-09 07:50:42,4
Intel,nykdl7b,Is there really a 4050 or are you referring to the 4050m even when you don't add the m?,AMD,2026-01-09 09:01:51,2
Intel,nyobdr7,"they said 60W, but if you look at the laptop they used, it's 30W, probably 60W whole system",AMD,2026-01-09 21:46:41,1
Intel,nysouji,Nvidia paid intel 5B dollars ....Read whatever u can ... But it was to stop intel giving high power gpu to mainstream... .,AMD,2026-01-10 15:11:53,1
Intel,nylhxw8,"Perf + price + (to a lesser extent, but it still matters) power consumption together is what matter.   None of the 3 is decisive on its own.",AMD,2026-01-09 14:00:38,3
Intel,nylu1zs,"I think youâ€™re getting downvoted because it was sold by reviewers as a Z1E competitor as thatâ€™s what was available in regular devices.  Hx370 was only used in niche manufacturers, like GPD only when it launched.   Youâ€™re right though, it was supposed to be a competitor for hx370, but was held back by drivers and other things until mid to late 2025, which corresponded to Z2E (cut down hx370) release.  Fast forward to now, and it competes/beats both",AMD,2026-01-09 15:01:17,1
Intel,nyplsu3,"i mean you can legit put it over the intel provided slides and its pretty much a dead on match. the PCH is smaller in the presentation photos so they can make it look pretty, but the real chip is an exact match to that leak https://cdn.videocardz.com/1/2025/05/INTEL-PANTHER-LAKE-DEMO-1200x675.jpg  ~~this generation is seeing quite a significant leap forward in manufacturing technology (Gate All-Around/RibobnFET & Backside power delivery/PowerVia) these usually do result in big gains and that does make it a bit harder to compare to prior nodes. not to mention~~ **[GPU is TSMC N3E still quite a bit denser than N4 class tho]** its not the exact same uarch as B580, while still a derivative of battlemage, there does seem to be some (rather significant) improvements between xe2 and xe3 https://gamersnexus.net/gpus/intels-new-gpu-xe3-architecture-changes-handheld-gaming-cpus-xess3   but where a lot of the fps gain will be from is N-Frame and Pixel generation intel want to promote those numbers over native performance. AMD cant do with RDNA 3.5. id expect 8060s to be a much more powerful igpu but it is lacking what is essentially lossy compression for realtime graphics. that is a pretty big deal and i do think it will be what causes Strix halo to be a product that just ages poorly, costs too much for what it is really (308mm^2 io/igpu chiplet cant be cheap on 4nm) its really amds pipe cleaner for future packing tech.  people said they same when the zen chiplets were rumored to be the size they are. you save a lot of area not needing memory controller on that chip would be my guess. d2d bonding is very space efficient compared  for some perspective strix halo igpu block + media engine block is about 120mm^2 on N4P(143.7216MTr/mm^2) of the iod, the rest is i/o and the npu.  the 12 Xe3 chip is 55mm^2 on N3E(216MTr/mm^2) (so we could napkin approximate about 80mm^2 if it was on N4P)",AMD,2026-01-10 01:48:30,4
Intel,nyjjq7p,"Technicality is technicality, if the channel width is cut down by half but channel number is doubled then they still doubled the memory channel. People just need to know memory channels are not all equally wide just because thatâ€™s all they know being PCMR enthusiasts.",AMD,2026-01-09 04:58:28,-2
Intel,nykilu3,"https://www.thelec.kr/news/articleView.html?idxno=50232. Seems your out of the loop, you think amd can't scale rdna4 but samsung can?",AMD,2026-01-09 09:48:29,8
Intel,nykicmf,"Rdna4 is objectively faster than rdna3/rdna3.5 at the same clock, power' cu count and bandwidth  something 100% desirable for apus. Stop making excuses for amd and their bad decisions. Everything b your claiming samsung did for rdna4 to scale is something amd could have done aswell and has done as amd has made changes to rdna3(rdna3.5) for apu specifically the same can be done for rdna4.",AMD,2026-01-09 09:46:05,1
Intel,nyui8sr,Sure. weâ€™ll see,AMD,2026-01-10 20:27:04,1
Intel,nyj73k6,Have the business laws mattered since the dawn of post-dialup internet?  I don't think they have. Where's our fucking bell-style breakup? 41 years ago was the last *real* monopoly breakup... and they let it come right back.  EU does half measures and they don't come to the rest of the world. It's a travesty that we don't have nationwide GDPR or force allow sideloading on ios.,AMD,2026-01-09 03:42:31,4
Intel,nyjduwz,An enerend of sorts,AMD,2026-01-09 04:21:49,0
Intel,nylh400,"Where do you get this ""absolutely decimate AMD"" nonsense from? I did not say that.",AMD,2026-01-09 13:56:20,2
Intel,nylwn40,"Sure. But they're still not stagnating. Since December 2023, when Mi300X and Mi300A were released, they released Mi325x, Mi350x, Mi355x and soon, Mi400x.    The latest gen is running HBM3e and 3nm CDNA4. Those are some immensely advanced products.    On the Epyc side, they've got the 9965, a 192 core 384 thread monster that Intel can't even attempt to compete with.    Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.    AMD has gone from 32 cores first gen in 2017 to 6 times that in 2024.    It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.",AMD,2026-01-09 15:13:28,3
Intel,nylaft6,"The modern Atom is fine by me, the N100 had more performance than a 6500t so this one should have more than enough compute for many different use cases whilst retaining low load efficiency. This product could potentially obliterate even the newest and best SBCs for home lab use cases, even regarding efficiency.",AMD,2026-01-09 13:19:41,1
Intel,nymqm53,"[Intel Performance Index](https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-core-ultra-processors-series-3_1/) Search 4050. [Dell 14 Premium is this laptop, with a TGP of 30W](https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor)  [In PCWorld's test, they got 48 fps for Cyberpunk](https://www.youtube.com/watch?v=NdLYuQQPo5c). My 4060 gets 73 fps using 60W using high settings and 2880x1800 DLSS instead of XeSS. That's a game where Intel gpus performs well above average. A 4060 optimus laptop uses around 3.5W an hour at idle without the screen turned on. With the screen and igpu powering it, it's about 8W. Having discrete graphics in modern systems doesn't really impact battery life anymore.   So yeah, Intel was intentionally being misleading, hoping people wouldn't actually bother to check their figures. Panther Lake's massive igpu still doesn't make sense for anyone who cares about performance. Maybe a little bit for battery life, if it's more efficient to drive high resolution displays, despite its large size being wasteful. Most igpus go into office pcs. In terms of gamers, Steam's hardware survey suggest that desktops and gaming laptops with dgpu are the biggest share.",AMD,2026-01-09 17:28:27,2
Intel,nypiewn,Interesting. What is the price of that compared to Z1E?,AMD,2026-01-10 01:29:46,2
Intel,nym76rq,"Yes, I did. AMD could hold prices low, they choose not to.",AMD,2026-01-09 16:01:38,-5
Intel,nymy6xm,9850X3D lol gottem,AMD,2026-01-09 18:02:35,6
Intel,nyplvwk,It is getting worse. Was better when Herkleman was there. The new guy Hyounh is a joke.,AMD,2026-01-10 01:48:58,3
Intel,nyp6syu,you didn't just try to suggest that CES is for....consumers..... did you.... seriously?,AMD,2026-01-10 00:26:18,0
Intel,nyrhvdt,"Ahaha, lovely lie. And even if true, how would that change a lit of ""bad things"" lol.",AMD,2026-01-10 10:06:50,1
Intel,nytjk6m,"Seems like things are going that way. I guess all we can do is wait and see if the Arm takeover gets so complete that Intel and AMD have to join in, and then suddenly have to compete with Qualcomm and Mediatek.  If that ever happens, hopefully we'll see more competition.",AMD,2026-01-10 17:41:05,1
Intel,nykh31e,linux and optiscaler is the way,AMD,2026-01-09 09:34:24,3
Intel,nylckas,"4050m, although it really wouldn't matter either was as the 4060 and 4060m are functionally identical in regards to performance (+5-7% for desktop) so if there were a full size 4050 we'd expect it to be the same or even less of a difference",AMD,2026-01-09 13:31:44,1
Intel,nyljjoq,"That's all fair, I'm looking at it from a handheld perspective. Performance at power draws that are actually feasible in handhelds has been stagnant since handhelds have really gotten popular. That is, it has if you want more than an hour of battery life",AMD,2026-01-09 14:08:59,0
Intel,nylvmjs,"Actually, the supposed driver issue was only a MSI Claw specific issue and not a general lunar lake issue.  https://www.notebookcheck.net/Intel-Lunar-Lake-iGPU-analysis-Arc-Graphics-140V-is-faster-and-more-efficient-than-Radeon-890M.894167.0.html  Here's a review from September 2024 using a LNL Zenbook S14 with 28w TDP. It had no issues generally outperforming the HX370 in the Zenbook S16. As usual the PCMR-esque dominated crowd on here paid no attention to laptops (which is the real life volume) and only looked at some handheld (which is a niche irl) so they thought that supposed ""lunar lake issue"" was widespread.",AMD,2026-01-09 15:08:44,3
Intel,nyjl1zy,"Each DIMM of DDR5 has 64 bits total of bus width, same as DDR4, 3, 2, and 1. And I do understand what you're talking about (not to mention that Strix Halo can't even take SODIMMs), but nobody else talks like that. When you call it an ""octa-channel"" chip, what people read is that it has as much bandwidth as a Threadripper Pro, because that is how [AMD is marketing those chips themselves](https://www.amd.com/en/products/processors/workstations/ryzen-threadripper.html).",AMD,2026-01-09 05:07:19,9
Intel,nyko7q2,"Yeah, and AMD says Strix Halo has four channels in their customer facing spec as well, because they have 128b and 256b buses respectively. They use the 64b channel convention as it is a customer facing spec, that doesn't meant they actually have that many channels in hardware.   The Pantherlake datasheet isn't public yet, but you can see plainly in the [actual spec sheet](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/memory-controller-mc/) for Arrowlake H that it supports 8 channels of LPDDR5X (additional [spec](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/supported-memory-modules-and-devices/) for channel width). Pantherlake will be the same.   The equivalent AMD doc is not available for Strix Halo but you can see the 16x16b spec quoted by Chips and Cheese [here](https://chipsandcheese.com/p/evaluating-the-infinity-cache-in#:~:text=Strix%20Halo%20has%2016%20memory%20controllers%20and%20CS%20instances%2C%20each%20handling%20a%2016%2Dbit%20LPDDR5X%20channel).  You cannot gang these channels into a dual channel mode, that is not how modern memory works, and there is no allowance in the LPDDR5 spec for 64b channels. The 16b channels have separate command/address buses and burst for a sufficient length (32n) to fill a cache line with each access.  To be clear I think standardising on 64b ""channels"" for marketing specifications is a good thing, it allows quick mental calculation of memory bandwidth without having to get into the nitty gritty. But if you're going to be pedantic and use the actual channel count, it's best to be correct.",AMD,2026-01-09 10:38:46,3
Intel,nykiuny,"Im not original guy you responded to I just wanted to know, becouse I couldnt find it on google. thx",AMD,2026-01-09 09:50:43,8
Intel,nykjg5k,"I'm not making excuses just explaining the rationale, which I don't agree with BTW.       Yes I know AMD are some lazy mofos. RDNA 3.5 till 2029 for iGPU is cheapo strategy as usual.",AMD,2026-01-09 09:56:09,6
Intel,nyli3fq,https://en.wikipedia.org/wiki/Hyperbole,AMD,2026-01-09 14:01:27,4
Intel,nym2zdk,">It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.  >Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.  Since we're talking about the server side now, Haswell-EP went from 18 cores maximum to 22 core Broadwell-EP to 28 cores on Skylake-SP. Cascade Lake-AP (rare bespoke sku) went up to 56 cores per socket. ""Haven't moved an inch"" is inaccurate.",AMD,2026-01-09 15:42:41,3
Intel,nym7k8z,"In CPUs theyâ€™re losing market share to arm, the Datacenter GPUs are mostly bought by companies who canâ€™t afford NVIDIA",AMD,2026-01-09 16:03:20,0
Intel,nymvxz5,"I think you were looking at the old core ultra series 1 testing not the current CES testing. For their claim they used the following settings:   Intel B390: Processor: Intel Core Ultra X9 388H (Panther Lake) PL1=45W; tested in Intel reference platform; Memory: 32GB LPDDR5 9600; Storage: Samsung PM9A1 512GB; Display Resolution: 2880x1800; OS: Windows 11 26200.6725; Graphics Driver: Intel Arc Graphics Pre-Production driver; NPU Driver: Pre-Production driver; BIOS: Pre-Production BIOS; Power Plan set to Balanced, Power Mode set to ""Best Performance"".  NVIDIA RTX 4050: Processor: Intel Core Ultra 7 255H (Arrow Lake); tested in Dell 14 Premium with Nvidia GeForce RTX 4050; Memory: 32GB LPDDR5 8400; Storage: Samsung 9100 Pro 1 TB; Display Resolution: 2k IPS; OS: Windows 11 26200.7171; Graphics Driver(s): dGPU: 32.0.15.8180 (GeForce 581.80) & iGPU: 32.0.101.8250; NPU Driver: 32.0.100.4404; BIOS: v1.4.0; Power Plan set to Balanced, Power Mode set to ""Best Performance""; Dell Optimized = Ultra Performance. Battery Size: 68Whr",AMD,2026-01-09 17:52:34,2
Intel,nyma4vw,"Why would AMD ""hold prices low""?  Gross margins are below 50% (48, as in 2022), while NV has it at 70%.  We know they are worse in PC/GPU market and better in datacenter.",AMD,2026-01-09 16:14:51,9
Intel,nypf46n,What did you think the acronym CES stands for?,AMD,2026-01-10 01:11:10,5
Intel,nyrmvf6,It would further validate what HisDivineOrder said which is that AMD is just another corporation.  Which they are.,AMD,2026-01-10 10:52:27,0
Intel,nyl953n,"Really wish Optiscaler had a better installer, something akin to Reshade. The whole manual process for each game makes it annoying to use.",AMD,2026-01-09 13:12:04,3
Intel,nyks16k,Intel still takes a heavy penalty on Linux in graphics vs. AMD. Hopefully that improves as well.,AMD,2026-01-09 11:11:24,3
Intel,nyjobcx,Well thereâ€™s more than one type of memory ðŸ¤·â€â™‚ï¸ PCMR crowd just defaults to DDR DIMMs but the world of mobile is mostly LPDDR from phones tablets to handhelds and most small laptops,AMD,2026-01-09 05:29:47,0
Intel,nylj87u,"So you can exaggerate what I said and make it mean something completely different. Got it.  Welp, I got better shit to do this morning. See ya. Feel free to have the last word.",AMD,2026-01-09 14:07:21,3
Intel,nynuctv,"Nope, I was looking right at the current testing. I do have to make a correction though: Panther Lake's cpu is built on Intel 18A, and the gpu is built on TSMC N3E  Let's summarize. In a head to head battle, Intel claims the 45W Panther Lake Core 388H with its ""massive graphics"" is 10% faster than a 30W 4050 paired with a 30W Arrow Lake 255H. Panther Lake's cpu is built on the most advanced silicon process node 18A, designed to compete against TSMC's N2 (2nm) which is set to release in products in the second half of 2026. Panther Lake's gpu is built on TSMC N3E, a significantly more efficient N3. The 4050 is built on a custom 2020 TSMC 5nm variant, and Arrow Lake is built on TSMC N3+6nm. Arrow Lake is designed for specifically for high power use vs the low Lunar Lake and Panther Lake.   The future of integrated graphics is truly bright. I can see it being exactly where it is now. Vital for battery life in office laptops and actual gaming laptops with discrete graphics. Big igpus? Mostly irrelevant and a waste of money.",AMD,2026-01-09 20:27:44,2
Intel,nypiqmg,Yes you are confirming that AMD is as scummy as Nvidia. Not surprised when I see the weasel new leaders they have.,AMD,2026-01-10 01:31:37,2
Intel,nyqxy0s,"Yes acronym has ""consumers"" in the name, but it's not directed or intended for consumers, it's intended for the big industry, the maker's manufacturers, the ones creating services, and the subtle parts of the distributors and such, it was and has NEVER been intended for the end users, the broad consumers.  Maybe bloody well look up what CES is and what it's for before asking a silly question.",AMD,2026-01-10 07:03:08,1
Intel,nys7tqn,"No, it would not. There is a difference between a shoplifter and a serial killer, even though both are criminals.  Filthy Green plays in a league of pieces of shit of its own.",AMD,2026-01-10 13:35:08,0
Intel,nynpovt,True but you put command once in your game and you forget about it,AMD,2026-01-09 20:06:13,0
Intel,nysa4r8,"This is so delusional  You're talking about AMD that made Int8 version of FSR4, which is THE hardest part, and then keeps it away from users to sell more RDNA4 cards.",AMD,2026-01-10 13:48:55,-1
Intel,nyov5va,"Missing the point. It's about accessibility and ease of use not how often you need to do it. If a tool to bring similar functionality as Nvidia isn't at a similar level of accessible and easy to use as the manufacturer apps, then it's relegated to enthusiasts only.   Reshade is one of the most popular modding tools for post-processing shaders because it's so easy to install, use, and manage for multiple games on the same system.",AMD,2026-01-09 23:24:12,1
Intel,nytis7z,"AMD had no reasons for such lock-in, it makes sense only for companies dominating the market, to push people to refresh.  I have not seen palatable proof that FSR4 could be ""easily backported"" but isn't.",AMD,2026-01-10 17:37:25,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,4
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100Â°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104Â°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,5
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nzdyg7l,"Is PowerColor a good brand of GPUs?  Iâ€™m planning on upgrading my gpu from my almost 7 year old nvidia rtx 2060 to an PowerColor rx 7800xt Red Devil, and am bit worried if theyâ€™re a reputable brand.   Was holding off the upgrade due to not wanting to chase percentages, and now that I fully embraced Linux (Fedora 43 KDE) I wanted to get something that has better compatibility with the OS as I did encounter some issues due to NVidia drivers.  Edit: forgot to mention that I have a compatible system with 600w power supply",AMD,2026-01-13 17:45:26,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  â€¢ MB Gigabyte|GA-970A-UD3P AM3+R   â€¢ VGA Sapphire|100365BF4L R9 270 2GD5   â€¢ PSU Roswell|RX850-S-B 850W RT (has been replaced)   â€¢ CPU AMD|8-Core FX-8350 4.0G 8M R   â€¢ SSD 256G|Samsung MZ-7PD256BW R   â€¢ MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencÃ© Ã  rendre l'Ã¢me fin 2025 et je dois donc me dÃ©pÃªcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prÃ©montÃ© Ã©tant donnÃ© que je suis peu douÃ© lÃ  dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose Ã  part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxu74dg,what are the best settings for my rx 9070 xt steel legend on adrenalin? should I prioritize lower temps or higher performance? and will the performance between settings be negligible playing in 3440x1440p? I'm currently running the default option under Performance>Tuning,AMD,2026-01-05 16:15:52,1
Intel,nxurns3,"When I'm playing a game, my screen suddenly goes black and I have no way to shut down my PC; I have to restart the power supply. Does anyone have any solutions, please?",AMD,2026-01-05 17:50:56,1
Intel,nxv130z,"Are there plans for chipset refresh for Zen 6 or 7 or there will be only firmware and BIOS  updates  for existing ones? I heard Zen 6 should have better memory controller , with higher 1:1 RAM speed support (perhaps 8000MT/s + ) etc. , but of course still same AM5 socket.",AMD,2026-01-05 18:33:10,1
Intel,nxv5q21,"Hey guys.  Whats the best way to get a smooth 60fps lock on a 120hz display?  I use MSI Afterburner and the adrenaline app, neither felt as smooth as native 60hz.  On nvidia i used the half vsync feature and that worked for me but AMD doesn't have an equivalent option.",AMD,2026-01-05 18:53:49,1
Intel,nxzp3f0,"weird issue as off 2 days ago: RX 7700 XT with 25.12.1 driver on W10 - when powering on the system, the secondary screen (HDMI) is not receiving any signal until the HDMI cable is unplugged and plugged back in. No recent updates installed.",AMD,2026-01-06 11:41:16,1
Intel,ny1f10g,"7800x3d SUSPICIOUSLY LOW TEMPERATURES   I just finished building my computer and tested it in two games, at 2k resolution and the highest settings: The Last of Us Part Two and Battlefield 6. My 7800x3d is showing temperatures below 50 degrees Celsius, even though I'd read on forums that it can get hot. I checked it on the cooling display, HWMonitor and in MSI Afterburner. Is it possible for air cooling to be this efficient, or do I need to configure something in the BIOS to get the processor to run at full performance? Bf6 runs with 180fps and TLOU have around 100fps.  I have rtx 5070 and 32gb ddr5. Cooler: Phantom spirit Evo vision with stock paste.",AMD,2026-01-06 17:15:25,1
Intel,ny4j9i2,"New to AMD and plan to keep the same cpu cooler, I have a NH-D15. I bought this cooler back in 2021-22. Would I need a new mounting bracket to accommodate this change?   I have upgraded to 7 9800X3D, Mobo is a Tuf gaming B650E-E if this information is needed. Any help appreciated!",AMD,2026-01-07 02:15:55,1
Intel,ny70dtx,"Hi there, hope everyone is doing fine and started new year on a good note :)      Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically (https://www.powercolor.com/product-detail214.htm)  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?     Thank you !",AMD,2026-01-07 13:16:10,1
Intel,nyc3nmx,"looking for help understanding core parking on the 9950X3D, does it outright disable the other cores while gaming? or do other applications running use the non X3D cores?",AMD,2026-01-08 03:51:56,1
Intel,nyci0og,"I'm building my first ever AMD PC, and my second ever PC (My old one had a 2080 super, 10th gen i9 and sadly died a few months ago). I did not know that you were supposed to buy certain ram depending on what CPU/motherboard you used. I'm either going to be buying a 9850X3D or a 9800X3D, and the motherboard I have currently is the MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard. My ram is the G.Skill Trident Z5 RGB 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory. Should I return the ram and get the AMD EXPO equivalent? Will I lose performance if I keep it? Will I lose stability if I keep it? Will it even work properly?  Some extra info:   I can afford to return it and buy the equivalent for an extra $100 or so. My GPU is the Sapphire Pulse 9070 XT, I'll be gaming at 1440p, my I have an WD\_BLACK SN8100 2 TB SSD, and a Corsair RM850x (2024) 850 W Fully Modular ATX Power Supply.",AMD,2026-01-08 05:22:26,1
Intel,nygdcim,"Is a reasonable upgrade for my system possible?    Hello there,  i would like to know if there is any reasonable upgrade possible on my AM4 System...   I would like to play Call of Duty Warzone on a 1080p Monitor with 180 fps but ALSO use ~ 500 tabs at the same time.  Currently my System runs the 500 tabs but only gets ~ 120 fps in cod.   Since AM5 is very expensive currently due to RAM prices, i do not see any reasonable chances for a Upgrade and therefore am looking for advice :)    My current System:  AMD Ryzen 9 5900X - 12x3.7GHz  => OVERLOCKED at 4.7GHz with 1.304v (undervolted for that speed -> Temps below 80Â°C)   32GB DDR4 3600MHz Team Group T-Force Vulcan Z - DDR4 (2x16GB) => UPGRADED to 64GB (4x16GB)   AMD Radeon RX 7900 GRE 16GB => slight OC possible BUT Temps tend to go above 80Â°C, at higher OC even above 90Â°C... (possibly i could add more cooling to the Tower?)   * Systemtreff Gaming Mid Tower AirForce GT1   * Systemtreff ITS-Raven - Prozessor - LuftkÃ¼hler  * Gigabyte B550 Gaming X V2 - AM4  * 850W MSI MAG A850GL PCIE5 80+ Gold  => UPGRADE MSI MPG A850G  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0   In the future i might want to play COD2026, which could receive a huge engine upgrade... and i also will run ~500 tabs at the same time.",AMD,2026-01-08 19:24:33,1
Intel,nyj771y,"Hi there, hope everyone is doing fine and started new year on a good note :)  Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically ([https://www.powercolor.com/product-detail214.htm](https://www.powercolor.com/product-detail214.htm))  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?  Thank you !",AMD,2026-01-09 03:43:02,1
Intel,nykrihq,"im running a 5600x currently, what would be the best cpu to upgrade to on am4 for gaming except for the 5800x3d?",AMD,2026-01-09 11:07:02,1
Intel,nyld64a,"What does the 18th byte do?  On my system it changes on a daily basis. Display port radeon software rx 580  Also Current Link Settings - 2.7 Gbps x 4. Seems I have a bandwith issue, should be more as i have a DP 1.2 standard gpu port cable etc  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,89,ff,ff,00,00,00,00,00,00,00,00  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,80,ff,ff,00,00,00,00,00,00,00,00",AMD,2026-01-09 13:35:10,1
Intel,nylqaba,"AM4 CPU Compatibility question.     I currently have an HP system with a Ryzen 2700.   I'm thinking about picking up an AM4 motherboard for a ""new"" build. To be precise, an ASRock B550M-ITX/AC.  I can get a Ryzen 2600 on the cheap and swap out the 2700 into the new mobo. That way I can hand the HP system to my wife as an upgrade.  But. According to the ASRock lists. These older CPU's are not compatible. Just the 3000 series and up.  My question is, what makes these older CPU's incompatible on the same socket? I see some Chinese boards that support the 1000 to 5000 series Ryzens.  Right now, I can get the ASRock new for a decent price. Given the DDR5 debacle, I still have enough DDR4 sticks laying around that makes sticking to AM4 an easy , affordable choice.",AMD,2026-01-09 14:43:10,1
Intel,nymut0x,"HELP - GPU not detected after Ubuntu boot repair and CSM toggle  SYSTEM SPECS CPU: AMD Ryzen 5 9600X GPU: AMD Radeon RX 9070 XT Mobo: AsRock B650M PG Riptide Main Storage: Crucial T500 M.2 NVMe (Windows 11) Secondary Storage: ADATA SATA SSD (Old Ubuntu install)  THE ISSUE My PC was working fine until I tried booting into an old Ubuntu installation on my secondary ADATA SATA SSD. Now my RX 9070 XT is not detected at all in BIOS or Windows Device Manager, and I only get display output from the motherboard.  WHAT HAPPENED To see the old SATA SSD in the boot menu I had to enable CSM in the BIOS. Booting into that drive resulted in a black screen. I then used a Live USB to run the boot-repair utility with the recommended repair settings. This seems to have installed the GRUB partition onto my Crucial T500 M.2 drive instead of the SATA drive. Now I can boot into both OSs, but the GPU is completely invisible to the system.  CURRENT STATUS Windows Device Manager only shows the Integrated Graphics. When I try to install AMD drivers, the installer fails because it cannot detect the GPU. I was briefly able to get graphics output from my GPU by unplugging the PC, flipping the PSU switch to off, and holding the power button to empty the capacitors. I then booted it up with the HDMI cable plugged into my GPU and I saw the asrock logo but it was stuck there for 2 minutes and I impatiently turned it off. I'm also considering trying this again and letting it run it's course  PLAN AND QUESTIONS I am planning to disconnect the SATA SSD and try to wipe the Ubuntu boot entries from the M.2 drive to see if the GPU reappears. Has anyone experienced a Linux bootloader repair or CSM toggle hiding a GPU from the BIOS? Specifically, could the Crucial T500 and ADATA drive conflict be causing PCIe initialization issues after the CSM change? Should I try clearing the CMOS first? Any help would be greatly appreciated. Thanks!",AMD,2026-01-09 17:47:28,1
Intel,nyp2whf,"I can buy a Ryzen 7 5700 (without X, the one that is 5700g without a built-in graphics chip) for 120 euros (\~$140) or 5800x for 188 euros (\~$218). Is it worth the extra? My GPU has 16GB, so PCIe shouldn't have too much of an impact.",AMD,2026-01-10 00:05:44,1
Intel,nz62itx,"So I have a question guess I want a 2nd opinion, with the new information of AMD potentially bringing back some old AM4 chips.   I recently bought a ryzen 5800x for my little brother to upgrade his 3600 thing is my brother is currently at the military until May so his upgrade isn't super urgent. Should I return it and potentially hold out on the chance that AMD brings back the 5800x3d?!",AMD,2026-01-12 14:39:27,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89Â°C. If you're seeing temperatures over 100Â°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 Â°C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nxtun1i,"check if memory and/or fabric clocks spike at the same time (max values basically), if they do it is a sensor bug.",AMD,2026-01-05 15:16:47,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nyidpo3,"The ram by itself is fine, though you'll probably need to manually set it to something like 6000 CL32. You can get the expo sticks, but the only thing that'd really change for you is the preprogrammed profile to allow you to get it with just a single setting.  The 7200 should run out of the box, but the CPU will switch into a different memory mode where it runs its memory controller at half clocks that lowers performance until about DDR5 8000, so that's why you'd go to the 6000 instead",AMD,2026-01-09 01:04:11,2
Intel,nyv1n08,good point,AMD,2026-01-10 22:03:25,1
Intel,nxn91rb,"it's not constantly hitting 100 Â°C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxrzsmj,"Thanks bud. It's the actual AMD card branded 6950xt, some people mis name it as a founders edition. I'll.try reach out to AMD today.",AMD,2026-01-05 06:52:53,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,152
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. Iâ€™m loving it,AMD,2025-12-22 10:10:46,40
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,4
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,3
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,2
Intel,nvahjmg,Only compares 8GB cards from teams red and green since itâ€™s only considering <$300.,AMD,2025-12-22 00:43:27,8
Intel,nw2aruf,ðŸ˜®ðŸ«³ðŸ¿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-13
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-14
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-8
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-13
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-32
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,9
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,51
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,10
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,20
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,-1
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-27
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,4
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,-2
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,6
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,27
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. Itâ€™s also half the vram.,AMD,2025-12-23 20:25:01,8
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-4
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,8
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because thereâ€™s people like me to whom prices in dollars means nothing (or who donâ€™t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).ðŸ™‚",AMD,2025-12-22 12:11:20,-8
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-8
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,9
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,4
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-12
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMDâ€™s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. Youâ€™re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,12
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,8
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  Iâ€™m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,107
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,305
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,87
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,49
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,25
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,20
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,83
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,36
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,13
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,48
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,32
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,9
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,27
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,19
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,8
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,9
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,14
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,8
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,5
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,12
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,24
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,21
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,61
Intel,ntb9myj,Please add the broken noise suppression to â€œKnown Issuesâ€.,AMD,2025-12-10 16:33:11,5
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,6
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,5
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,5
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,6
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,5
Intel,ntb4cu0,Whereâ€˜s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,13
Intel,ntaofpr,Iâ€™m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,19
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,3
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,4
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,3
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,8
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,7
Intel,ntawnis,Still no fsr 4 support for rdna3 ðŸ™„,AMD,2025-12-10 15:29:24,9
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,12
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.Â   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me ðŸ¤”  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,5
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,5
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelmingâ€¦ We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile againâ€¦   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,10
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,14
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,3
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry Iâ€™m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I canâ€™t play Warzone because I canâ€™t update my bios, there doesnâ€™t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeonâ„¢ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ny008s3,"AMD sucks: FSR 4 is locked to RDNA 4, while NVIDIAâ€™s DLSS 4.5 runs even on RTX 20-series GPUs. My next GPU will be NVIDIA only, and I advise everyone against buying AMD. Itâ€™s a greedy company with no respect for customers â€” you buy a graphics card last year, and the next year itâ€™s already outdated",AMD,2026-01-06 13:00:32,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,3
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,3
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,3
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? ðŸ˜‚ðŸ˜‚,AMD,2025-12-10 18:46:48,3
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,2
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,3
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,3
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"Â«#AmdNeverAgainâ€ Whereâ€™s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les sÃ©ries 7000 ? Câ€™est mort. Pour ma part, je nâ€™achÃ¨terai plus de cartes AMD. Si Nvidia continue Ã  proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,2
Intel,ntaitkk,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well itâ€™ll be worth it. Not enough games yet but hereâ€™s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"Iâ€™m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still presentâ€¦ :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   Iâ€™ve tried four different older drivers and all of them give me error 182 â€“ GPU is not supported (RX 9070 XT) during install.   Iâ€™ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvf8wml,"ParabÃ©ns, fiz a atualizaÃ§Ã£o para 25.12.1Â e agora nÃ£o consigo jogar nada sem travamentos, alÃ©m do google estar lento",AMD,2025-12-22 19:52:06,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeonâ„¢ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,nymudet,"czeÅ›Ä‡, jest sen instalacji jak uÅ¼ywam 7900xtx i nie uÅ¼ywam Å¼adnych wspomagaczy ?",AMD,2026-01-09 17:45:31,1
Intel,nzbmbw2,"Hi devs!   I would like to bring again to your attenction this thread: [AMD Software: Adrenalin Edition 25.9.2 Release Notes : r/Amd](https://www.reddit.com/r/Amd/comments/1nk9qgo/comment/nfb2o2j/)  Is there any chance you can bring us 60Hz mode for ex WMR drivers, now working directly in steamVR with Oasis Drivers?",AMD,2026-01-13 08:53:49,1
Intel,ntakvhf,FSR Redstone support? Will my minecraft machine run faster now?,AMD,2025-12-10 14:26:09,1
Intel,ntap40e,Gonna be able to play modern titles on my HD5750 thanks to redstone !,AMD,2025-12-10 14:49:52,1
Intel,ntdi8fj,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,AMD,2025-12-10 23:14:14,1
Intel,ntclu0p,Windows just installed the June   update from AMD. The fck is this,AMD,2025-12-10 20:29:51,1
Intel,ntcrpsp,I'm not seeing the new update in AMD Install Manager,AMD,2025-12-10 20:58:55,1
Intel,ntd2bv7,so in 2028 10 games will have it like FSR4 XD,AMD,2025-12-10 21:50:50,1
Intel,nte3gul,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,AMD,2025-12-11 01:18:47,1
Intel,ntf7sz1,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",AMD,2025-12-11 05:46:09,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,49
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,11
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,8
Intel,ntbyc9u,There are well over 100 warhammer 40k games. Did they not specify?,AMD,2025-12-10 18:33:41,1
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,57
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,20
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,17
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,22
Intel,ntb2rjk,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",AMD,2025-12-10 15:59:33,-3
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,132
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,7
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,26
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,3
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,19
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,8
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,13
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,7
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,9
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,3
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,28
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,37
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,16
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,4
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntb6txh,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,AMD,2025-12-10 16:19:26,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,7
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,27
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,17
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,58
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,15
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,20
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,9
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,14
Intel,ntcro7s,"Iâ€™d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,3
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,2
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develerâ€™s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develerâ€™s VKD3D-Proton  - Develerâ€™s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesnâ€™t officially enable it. - Global override toggles in AMDâ€™s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,27
Intel,ntakqc7,This has been announced for months.,AMD,2025-12-10 14:25:21,24
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,9
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,8
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntaoydb,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",AMD,2025-12-10 14:49:01,0
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,7
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,16
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,5
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,23
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,wonâ€™t happen,AMD,2025-12-10 16:18:18,3
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,4
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,4
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,ntb6g84,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,AMD,2025-12-10 16:17:35,1
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,2
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,4
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,8
Intel,nteeogf,la mÃªme. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,5
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,1
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,2
Intel,ntbhx4e,I still have my 5670,AMD,2025-12-10 17:14:01,2
Intel,ntfsndh,"No problems with RX 9070 xt in ARC raiders, i have win10",AMD,2025-12-11 09:01:20,1
Intel,ntazsxe,Itâ€™s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. itâ€™s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,10
Intel,ntfnlow,"It's for Darktide. But it's not even ready for launch there, either.",AMD,2025-12-11 08:10:07,1
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,19
Intel,ntbjons,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",AMD,2025-12-10 17:22:43,14
Intel,nteh1sb,Found the install manager dev lol,AMD,2025-12-11 02:41:05,3
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,17
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,21
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,19
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,29
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,8
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,5
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,2
Intel,nv8ptlv,pÅ™esnÄ› zustÃ¡vÃ¡m na 25.9.1 vÅ¡echno jinÃ© crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,4
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,8
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performanceâ€¦ so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,6
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when thereâ€™s already community workarounds,AMD,2025-12-10 15:19:15,9
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that somethingâ€™s terribly wrong with something in your software thatâ€™s breaking vsync. thatâ€™s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntb77ho,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,AMD,2025-12-10 16:21:15,3
Intel,nte5171,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",AMD,2025-12-11 01:28:24,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,4
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,21
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti Iâ€™d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever itâ€™s convenient but theyâ€™ll just as quickly throw us under the bus and fuck us raw once theyâ€™ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshitâ€™s right out front where you can get a good strong whiff of it. You know what youâ€™re in for.",AMD,2025-12-10 22:41:15,4
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,24
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,16
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,7
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,6
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting Iâ€™m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,4
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,11
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,15
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,4
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntar1pe,"Not really, they teased the possibility of including other architectures.",AMD,2025-12-10 15:00:06,0
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,ntaqnxp,"It's also related to getting new features in generations other than just the latest one, ""bro"".",AMD,2025-12-10 14:58:07,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,ntb6pmb,"Yeah im sorry for all of us, already shopping for a 5080 rnâ€¦",AMD,2025-12-10 16:18:51,2
Intel,ntpptsg,Yes thatâ€™s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, Iâ€™ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,3
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,nylsudn,"Good morning, when will the new AMD Software driver be available?",AMD,2026-01-09 14:55:31,1
Intel,ntg8mgs,6970 here,AMD,2025-12-11 11:36:12,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,9
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,3
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,15
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,5
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,6
Intel,ntctlcs,ðŸŒðŸ‘¨â€ðŸš€ðŸ”«ðŸ‘¨â€ðŸš€,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,12
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,14
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,9
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,3
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,3
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,5
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,nte56dv,Enhanced sync makes games super stuttery even in  25.9.2.,AMD,2025-12-11 01:29:17,2
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well ðŸ˜­ðŸ™. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,5
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,34
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,24
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,4
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,4
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,8
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,9
Intel,ntarc4r,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",AMD,2025-12-10 15:01:37,3
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,2
Intel,nym5qu9,"I think our SVP noted in a recent interview it'll be later in Jan, the date they provided was the 21st, though I'd treat this as a tentative timeline just in case anything crops up",AMD,2026-01-09 15:55:10,4
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,7
Intel,ntcauqa,Hell yeah ðŸ‘ðŸ»   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,4
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,3
Intel,nthi3lk,I might be an ass but Iâ€™m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If youâ€™re a **consumer** and want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,8
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,2
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) ðŸ‘,AMD,2025-12-10 19:55:01,8
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,6
Intel,ntaufrk,Oh great will also test after work itâ€™s been headache since last driver update,AMD,2025-12-10 15:17:58,5
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,2
Intel,nwtxl54,yeah somethingâ€™s definitely wrong. iâ€™m assuming youâ€™ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,7
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,7
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problÃ¨me avec EAC fonctionne pour la 7900XT. Merci d avoir rÃ©glÃ© le problÃ¨me. Bonne fÃªtes de fin d'annÃ©e.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,6
Intel,ntu98tq,"Before the Black Ops 7 (which I donâ€™t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,2
Intel,nym5z84,OK thanks.,AMD,2026-01-09 15:56:13,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,4
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,7
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,5
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,4
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,ntciae3,Thank you for letting us know ðŸ‘,AMD,2025-12-10 20:12:10,7
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,4
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,3
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,5
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,2
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,2
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,3
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,3
Intel,nwvgoe4,"i donâ€™t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, Iâ€™ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,2
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,5
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,2
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,3
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,2
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,5
Intel,nx0meck,"iâ€™m using a 1440p freesync monitor, i basically always have fps counter on in all of my games so i can verify that vsync always works. even works without the freesync monitor. frame rate only ever goes uncapped when i disable vsync. is it only an issue at 4k?",AMD,2026-01-01 02:07:41,1
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,nuyojnk,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,AMD,2025-12-20 01:45:48,1
Intel,nwyqsn1,it's tagged as a milestone for feb,AMD,2025-12-31 19:33:08,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.Â   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,82
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,124
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,77
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,17
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,13
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too ðŸ˜¿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,17
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? Iâ€™ve spent 1 entire afternoon try every solutions given by Google but today the problem is still thereâ€¦,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,8
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,12
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,24
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,5
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,7
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,9
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,5
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,19
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,5
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,3
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,7
Intel,nonmrak,"Brooooo, they didnâ€˜t fix the flickering in BF6 when recordingâ€¦",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.Â  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if itâ€™s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update ðŸ‘ðŸ‘ðŸ‘ðŸ‘, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just canâ€™t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video Iâ€™m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 ðŸ˜…   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,Ð£ Ð¼ÐµÐ½Ñ ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼ÐµÐ´Ð¸Ð° ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð»ÐµÑ€ Ð²Ñ‹Ð´Ð°ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ñ‹. (ÐšÐ¾Ð´ 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalaciÃ³n del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me ðŸ™",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly ðŸ˜ž.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,2
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,3
Intel,nonqjy0,FSR AI frame gen??? Didnâ€™t they say thatâ€™d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? Iâ€™m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-3
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stÃ¼rzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen auÃŸer XMP war aktiviert, dann stÃ¼rzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das Ã¼bernehmen mÃ¼sst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team GrÃ¼n nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch ðŸ˜°,AMD,2025-11-13 16:52:03,20
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesnâ€™t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,6
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,11
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but itâ€™s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,27
Intel,noo9nj4,"V25.10.2  hereâ€¦ I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,3
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,5
Intel,nonkdfa,combined again it looks like ðŸ¤·â€â™‚ï¸,AMD,2025-11-13 16:25:10,2
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,98
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,4
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,6
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens ðŸ˜¡ and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,9
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,7
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,6
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, IÂ just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDUâ€™d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,13
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,5
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,10
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as theyâ€™ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since theyâ€™re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it ðŸ¤“",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesnâ€™t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, thatâ€™s when you get driver updates, and theyâ€™re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesnâ€™t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,Iâ€™m hoping Valveâ€™s new steam machine will push them on that since itâ€™s RDNA3 based.,AMD,2025-11-13 16:28:30,19
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,4
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,5
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,6
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,2
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,13
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,7
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,4
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,Iâ€™d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,2
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,3
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,3
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,9
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,3
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,12
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,15
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,19
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,16
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,23
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,14
Intel,nononki,Unfortunately happens to me too. So for me itâ€™s a big issue as I canâ€™t update to this driver until it is fixed ðŸ˜°,AMD,2025-11-13 16:46:06,4
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.Â  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.Â  Issue goes away using a non 4k 240hz display.Â Â    I believe this system crash is deeply related to DSC on Windows.Â  I only got these two PC bsods when I bought a 4k 240hz display.Â  Returned a monitor (bad oled) and the issue went away.Â  Got a new oled a few weeks ago and now I have these bsods again.Â Â    Never had a bsod before I got these 4k 240hz displays.Â  Fresh Windows 11 installs too between both PCs and between my first and second oled.Â  Systems are both solid and stable.Â Â    Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20Â  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.Â  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,5
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,5
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why itâ€™s failing. Would be cool to see the technical details if thatâ€™s possible. (Iâ€™m actually more interested now on why itâ€™s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,12
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,7
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. Iâ€™m on lg c5 oled 42inch and it only has hdmiâ€¦,AMD,2025-11-13 16:47:11,3
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,6
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,4
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,5
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? ðŸ¤”,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,7
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,7
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game ðŸ‘€?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,9
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,-2
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didnâ€™t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic cardðŸ¤¡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,4
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,12
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, Iâ€™m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select â€œGPUâ€ you get a file that has a different dimension from the one you download if you choose â€œCPUâ€. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with â€œminimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,3
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,2
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,47
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,14
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,7
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,26
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,2
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so youâ€™re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what itâ€™s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,3
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,7
Intel,nonozwd,Could be grounds for lawsuitâ€¦ Thatâ€™s funny!,AMD,2025-11-13 16:47:47,3
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,4
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,3
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,2
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and donâ€™t use the latest drivers. At least AMD owned up to it so I canâ€™t be too upset but hopefully they really do fix this soon as new users may not understand whatâ€™s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly itâ€™s stable for them and they donâ€™t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs donâ€™t always have a DP connector at all.,AMD,2025-11-13 19:43:06,3
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named â€œminimal installâ€). Obviously Iâ€™m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,21
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,7
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah ðŸ™‚ amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,6
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,8
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,4
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience ðŸ˜–,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man itâ€™s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for meâ€¦ and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,3
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,5
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,3
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,Â    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,2
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,158
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,81
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,46
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,5
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,4
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,Youâ€™re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asfðŸ˜­,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,45
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,15
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here:Â https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D:Â https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md ðŸ––,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (Iâ€™m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,3
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,7
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,17
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,8
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,5
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,3
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  â€‹â€‹â€‹  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"ðŸ‘   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,21
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,5
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,33
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-7
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,8
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didnâ€™t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,76
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,12
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,24
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,17
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-15
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-14
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"Iâ€™m pretty sure HUB doesnâ€™t like Nvidia *or* AMD. Theyâ€™re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,8
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,60
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,21
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-15
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,44
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,1
Intel,lgze3vw,"It depends on what your goals are for a laptop.Â  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!Â Â  I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.Â  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-8
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,16
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,4
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,4
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,5
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-2
Intel,lfkw8g2,throw it in the next steamdeck and Iâ€™ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,6
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if youâ€™re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,12
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, itâ€™s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,10
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,10
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.Â  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.Â  The system should have an actual TRUE quadcore memory setup.Â  Many of these systems have currently (and will absolutely continue to have)Â dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.Â  This matches my system that runs a 780m with 7500mhz lpddr5x.Â  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.Â  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"â€œAbsolute monsterâ€? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.Â   Â AMD has a long way to go before claiming â€œMonsterâ€ class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet arenâ€™t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Maxâ€¦ let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. Thatâ€™s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,5
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming â€œMonsterâ€ class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMDâ€™s biggest markets. You are kidding if you think they donâ€™t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they donâ€™t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games arenâ€™t the only thing APUs are used for so PS5 isnâ€™t wholey in the conversation. PS5 also costs monthly to play online and their games arenâ€™t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP â€” PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,30
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,9
Intel,lf385p0,"This was a graphics card, not a â€˜GPUâ€™ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,6
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-2
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,8
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,4
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, thereâ€™s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,23
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,13
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,10
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,0
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,5
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I donâ€™t mind free markets, Iâ€™m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,11
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,25
Intel,kxiush3,"""The ability to â€œturn it off and on againâ€ should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,112
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,18
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,119
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,67
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,7
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,34
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,39
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,59
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,12
Intel,kxiah6c,"Iâ€™ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, itâ€™s usually a day late and a dollar short. Blender on Linux still canâ€™t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that thereâ€™s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but weâ€™ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*â„¢ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,21
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,3
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,24
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,Iâ€™ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,5
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100Âºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,12
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still canâ€™t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,8
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,17
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,4
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,3
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,4
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,3
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,-1
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-7
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-24
Intel,kxk9iir,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,48
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  ðŸ˜¤ðŸ˜­,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,30
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,16
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,33
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,12
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,8
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,3
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,7
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,3
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:Â Â  Â    ""Speaking of VRAM, The drivers use VRAM less efficiently. Look atÂ any side-by-side comparison between games on YouTubeÂ between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.Â    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?Â    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.Â    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?Â    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,29
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,8
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,7
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-5
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-4
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,23
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocksÂ EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. Iâ€™ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I havenâ€™t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems ðŸ˜‚ but I guess some do idk ðŸ¤·. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,2
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me Â£540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent Â£200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,7
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,3
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-14
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,10
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,12
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,1
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-3
Intel,kyy38w2,"Hey OP â€” Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,29
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,44
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,26
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,4
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,12
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,1
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,7
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,9
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,2
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,2
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,1
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,9
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,19
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,5
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,5
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,4
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-13
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,-1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,32
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,27
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,0
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,7
Intel,kxj49ms,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,7
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,6
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,14
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,11
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,-3
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,12
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,10
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,6
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting ðŸ˜‚ðŸ¤£,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,8
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,9
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,8
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-2
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,7
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-1
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,0
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-5
Intel,kxih401,Oh then just ignore my comment ðŸ˜…,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,8
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-11
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-13
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,5
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,0
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,7
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,11
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,4
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pcâ€™s are better and donâ€™t really complain about upscaling and 30fps.. youâ€™re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldnâ€™t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,7
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned offÂ   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,5
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,42
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,11
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,22
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,17
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,15
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,16
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete.Â And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,7
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers werenâ€™t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,4
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,0
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?Â    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,2
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-9
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-8
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-1
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-3
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-6
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,3
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,7
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,18
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,5
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,6
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-3
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,1
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-2
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,11
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,2
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,6
Intel,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,11
Intel,nz80svg,I would love an ITX motherboard Strix Halo Competitor,Intel,2026-01-12 20:04:58,2
Intel,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,1
Intel,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,1
Intel,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,4
Intel,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,2
Intel,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,11
Intel,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
Intel,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,14
Intel,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,1
Intel,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,1
Intel,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
Intel,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
Intel,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,5
Intel,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
Intel,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,9
Intel,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
Intel,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,3
Intel,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,1
Intel,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
Intel,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,1
Intel,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,1
Intel,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
Intel,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
Intel,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
Intel,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,1
Intel,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
Intel,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
Intel,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
Intel,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,1
Intel,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,4
Intel,nyet7we,"I wonder if Valve would be considering Panther Lake for a Steam Deck 2. It sounds like a generational leap from RDNA2 (which is what they were looking for), and can be cheap enough for valve to slightly subsidize the cost.   I guess the main thing is the power envelope. I think Valve is only interested in making handhelds that deal with a 15W TDP or lower",Intel,2026-01-08 15:18:52,30
Intel,nyf9pbk,"the handhelds with these chips are going to slap. Also people gotta remember that using upscaling on smaller screens especially 7-9 inch screens is a lot more tolerable.  and these benchmarks are of triple A titles, monster hunter wilds is a dogshit unoptimized game, and people will be playing a mixture of game from older titles, indies, emulation etc.",Intel,2026-01-08 16:32:58,10
Intel,nyepcic,"Series 3 seems like the biggest Intel W in a long while.   Plays AAA games at 45-60fps with upscaling, and all other games at native at 60+. Not to mention this is 50% faster than AMD's equivalent HX370 while being massively more power efficient.   There's also already laptops listed on sites like Best Buy for reasonable prices (sub $1300). Compare this to the AI 395 from AMD that can't be found for less than $2500 while being significantly less power efficient. Granted, that APU isn't really comparable.",Intel,2026-01-08 15:00:39,32
Intel,nyhyhe2,"So remind me, the ARC B390 is not a discrete GPU?  although the ARC B580, and B570 are discrete GPUs?  and the ARC A380 is a discrete GPU?",Intel,2026-01-08 23:45:30,1
Intel,nyejwbi,"That should say ""playable at 540p""",Intel,2026-01-08 14:33:53,-7
Intel,nyf0j6v,"Linux driver support is the only problem at the moment. Lunar lake is already competitive with Z2 extreme in gaming in windows, but not even close in Linux. Hopefully this changes by the time handhelds with panther lake come around",Intel,2026-01-08 15:52:10,21
Intel,nz26s22,There's an Xbox project within the coming years apparently.,Intel,2026-01-11 23:05:33,1
Intel,nygbkt8,I think Valve is looking for an ARM chip. Intel could dip their toes in this market before Qualcomm catches up to Apple.,Intel,2026-01-08 19:16:45,-6
Intel,nyg3cne,AMD should have made an igpu from rdna4...  but nope... the engineers are too busy making AI gpus.,Intel,2026-01-08 18:41:17,4
Intel,nyf1d4m,"Yeah, amd went to extremely greedy!",Intel,2026-01-08 15:55:52,7
Intel,nyg345w,AMD should have made a RDNA4 igpu... RDNA5 igpu is gonna be a huge boost.,Intel,2026-01-08 18:40:16,7
Intel,nyfhqqg,"In a year or two,when they can get panther lake for cheap, this could be resolved. They could even get more efficiency with a refined 18a for the CPU and whatever node is available for the GPU. With the current RAM prices I wouldnâ€™t expect any major console style updates until 2027 anyways",Intel,2026-01-08 17:07:42,4
Intel,nygdjk9,Do you think there's any particular reason why they would want to go with ARM? Quite sure Intel proved here that efficiency is essentially equal between both ARM and x86 here,Intel,2026-01-08 19:25:24,6
Intel,nyhii5z,It's hilarious to think Steam would work with Qualcomm.,Intel,2026-01-08 22:26:42,4
Intel,nyhzkyv,"Very unlikely. Proton and DXVK work alright with x86 but adding ARM conversion on top is, uh, a **very** poor experience. In fact that first generation of Snapdragon laptops had among the highest return rate of laptops I have ever seen. Amazon [literally warns potential customers](https://www.tomshardware.com/laptops/snapdragon-x-powered-surface-laptop-7-gets-frequently-returned-item-warning-on-amazon) about it.  Valve definitely wants a wider adoption rate of SteamOS and ditching x86 is not going to help that. They are **not** Nintendo that can ask devs to target their architecture. It's going well so far because for most games you can just make a standard Windows version, slap Proton and it works within 10% of native performance under optimal circumstances. Anything that can increase incompatibility rate (and it VERY well can, ARM **does not** support AVX2 natively for instance meaning [a lot of games that might have issues](https://www.reddit.com/r/macgaming/comments/1dekmtz/avx2_game_list/) even starting or underperform).",Intel,2026-01-08 23:51:14,3
Intel,nyjz9hz,i'm so tired of people throwing ARM around for everything. superficial much?,Intel,2026-01-09 06:55:17,3
Intel,nyi9wg9,Yep AMD ces presentation dry as a bone until that rack announcement when ceo turned giddy.,Intel,2026-01-09 00:44:04,2
Intel,nyfnu8h,"Intel and AMD especially have this habit honestly, they hit the lead and stagnate *bad* and the catch up for whatever company stagnated takes a hot minute, though hopefully this level of pushing boundaries in the handheld PC space keeps both of them moving at a steadier pace for a while rather than one team moving way the fuck forward",Intel,2026-01-08 17:34:32,4
Intel,nyincie,why didn't they do one? Why use tech that is so old?,Intel,2026-01-09 01:55:33,3
Intel,nygxlyp,"Even more efficiency that doesn't exist yet. Snapdragon elite can do some heavy workloads for several hours at a time, but it's not powerful enough for heavy gaming. Valve says they know what they want, it's just not ready yet. I think as great as Panther Lake benchmarks are, the biggest complaint of the Steam Deck is still the 2ish hours of battery life in more demanding titles.  With the existence of other handhelds already, I don't think Valve is trying to aim for the beefier spec department here, if it means the same battery life.",Intel,2026-01-08 20:54:40,-3
Intel,nyidygm,"I'm not sure if this is sarcastic or if you missed the memo, but their new VR headset coming later this year is powered by a Snapdragon 8 gen 3.  [https://store.steampowered.com/sale/steamframe](https://store.steampowered.com/sale/steamframe)  A Qualcomm based steam deck isn't out of the question.",Intel,2026-01-09 01:05:28,1
Intel,nynct2x,"Valve's stand-alone VR headset uses Snapdragon, and x86 emulation, so will be interesting to see how well it performs",Intel,2026-01-09 19:07:13,2
Intel,nylpddj,"It depends. For a PC, yes. For a handheld, I don't think so. For the future of enterprise notebooks, probably, especially since Apple has been doing it for a while.",Intel,2026-01-09 14:38:38,1
Intel,nykkj1q,Save cost. AMD's assessment is that no one in mobile cares about gaming and if they do they should just get Strix Halo or build a PC.   cheapskate AMD as always.,Intel,2026-01-09 10:06:00,4
Intel,nyndytz,"probably didn't want to bother redesigning the APU without also having a CPU upgrade, it's expensive after all, and takes resources from other projects",Intel,2026-01-09 19:12:31,2
Intel,nyoo6vt,"""Even more efficiency that doesn't exist yet.""??????????????",Intel,2026-01-09 22:48:02,1
Intel,nyk1lrj,"But Qualcomm drivers on Linux are even worse than Intel lol. The custom AMD APU on the Steam Deck had the advantage of great Linux driver support for gaming(not just being able to support the GPU hardware and benchmarks, but run games at good performance which Intel still can't match). No other company has both high performance GPU and good Linux graphics drivers",Intel,2026-01-09 07:14:57,3
Intel,nyo6aao,isn't the APU a modular unit where they could just put the new one into the same die space?,Intel,2026-01-09 21:23:15,2
Intel,nyp4s4e,"Not my words. I'm taking Valve's. They said they know what they want, and if it was Panther Lake, we'd know already.",Intel,2026-01-10 00:15:40,1
Intel,nyo9nvh,"The Strix Halo is that basically, but the normal Strix Point APUs (e.g. HX 370) are not. The Strix Halo follow-up, Medusa Halo, is slated for 2027, and to use Zen 6 and RDNA5. While the Strix Halo could benefit from FSR4 if it got an RDNA4 update, it's still way stronger than the B390, even at similar power.",Intel,2026-01-09 21:38:47,1
Intel,nytzm0l,Medusa Halo = 2028,Intel,2026-01-10 18:55:32,1
Intel,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,31
Intel,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,36
Intel,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,32
Intel,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,3
Intel,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,11
Intel,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,2
Intel,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,2
Intel,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
Intel,ny3a7m6,"Iâ€™ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, Iâ€™ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
Intel,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
Intel,ny5mck0,"Can anyone say real life performance diff, and how much increase in battery life in real laptops, Because I feel many times those ppt numbers don't nearly match real usage (especially when ppt numbers are huge).   Can I get Mac like battery(or atleast 7hrs with no performance drop) and how much does it compare to Mac m1/a18 air performance with them on a $600 laptop.(Assuming fedora/mint as OS)",Intel,2026-01-07 06:22:50,1
Intel,ny68bvw,"I don't understand, we need to see the price of this thing, because otherwise we have to compare it to the AMD 8060s which will be more powerful, but even the cheapest machine with that costs â‚¬1500/â‚¬2000. We just need to see what price point this chip will be offered at.",Intel,2026-01-07 09:39:36,1
Intel,ny87s84,"On just 2 channel / 128 bit RAM, well done Intel!",Intel,2026-01-07 16:51:19,1
Intel,nyaqr0z,An ancient Chinese proverb (roughly) states: *'Talk...* does not make rice...' ðŸ¤”,Intel,2026-01-07 23:35:58,1
Intel,nybohxz,Steam deck 2??? Take my money gaben.,Intel,2026-01-08 02:30:03,1
Intel,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-19
Intel,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,13
Intel,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,4
Intel,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,21
Intel,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,4
Intel,nylh0bq,"A quick Google says 9 TFLOPS or the equivalent to an RTX 1080, 2070, 3060, 4050 give or take.",Intel,2026-01-09 13:55:48,1
Intel,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,6
Intel,ny0hpkm,82% faster than 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,3
Intel,ny7zxvb,>77% faster while using 80% more power.  Are you following CES at all?  The top feature of that architecture so far has been power reduction,Intel,2026-01-07 16:16:03,1
Intel,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,4
Intel,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,2
Intel,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,1
Intel,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,13
Intel,ny6mhyg,Mfg on or off? The article wasnâ€™t clear on that.,Intel,2026-01-07 11:42:22,1
Intel,ny5jwr8,their Zen 5 desktop iGPU still use RDNA2; a 5 yr old architecture let that sink in...,Intel,2026-01-07 06:03:30,5
Intel,ny89039,"AMD didn't have money when they were using Vega iGPUs, and they were still the best iGPUs around",Intel,2026-01-07 16:56:45,2
Intel,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,2
Intel,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,0
Intel,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,5
Intel,ny8apbn,native rendering,Intel,2026-01-07 17:04:31,1
Intel,nz2dezh,Those weren't good though.  They didn't get close to the 1050ish equivalent that's a decent min spec card until the steam deck.,Intel,2026-01-11 23:40:06,1
Intel,nya3a7m,"High with XESS maybe at â€˜okayâ€™ frame rates. Still, crazy.",Intel,2026-01-07 21:45:51,1
Intel,nxxv1o4,And Intel steps back into the game. The next generation of handhelds is upon us.,Intel,2026-01-06 02:58:42,25
Intel,nxxytpz,"despite me using amd, please let intel succeed",Intel,2026-01-06 03:20:08,25
Intel,nxy0xsg,"7200 DDR5 support out of the box slaps. the IMC should be bonkers, like the RAM prices(I know I know.,..)",Intel,2026-01-06 03:32:13,9
Intel,nxxlswe,This is insane for a ultrabook without a dedicated dGPU. Intel cooked! AMD brought us the same thing refreshed.,Intel,2026-01-06 02:08:08,27
Intel,nxxbn9v,"intel cooked, amd socked. I cant wait to replace my old aging i7-9750h laptop",Intel,2026-01-06 01:13:24,14
Intel,nxxy441,can't wait to get my hand on this in handheld PC.,Intel,2026-01-06 03:16:02,4
Intel,nxy6trb,Wonder why they haven't announced Wildcat Lake yet,Intel,2026-01-06 04:07:06,3
Intel,nxxbuuu,Is this meant for the Strix Point or Strix Halo price segment ?,Intel,2026-01-06 01:14:32,4
Intel,nxxslar,"I'm curious to see if framework will have Panther Lake options, but I'm not sure if these are compatible with SODIMM or not.",Intel,2026-01-06 02:45:15,2
Intel,nxzl8ju,I want this in my nuc.,Intel,2026-01-06 11:09:30,2
Intel,nxz6yui,"So it's available at Jan 27th, but is that only for laptops or is it for desktops as well?",Intel,2026-01-06 08:57:51,1
Intel,ny3n1s5,ok but can we talk about how the ultra 5 332 is worse than the ultra 5 325? i mean wtf did they do to the names?,Intel,2026-01-06 23:25:18,1
Intel,nyqhrdf,I am neither intel or amd biased...hope competition stays alive..and reduce prices..which i doubt,Intel,2026-01-10 04:58:44,1
Intel,nxys920,"Iâ€™m confused, is cooked good or bad? ELI5",Intel,2026-01-06 06:42:48,4
Intel,nxxce1j,"Strix Point, but we'll see what happens with the RAM situation.",Intel,2026-01-06 01:17:25,10
Intel,nxxf85s,"Isnâ€™t much point buying strix point if you can get this instead. Better chips and a faster iGPU, and better battery. Issue is whether you want to buy any laptop this year given the price of componentsâ€¦",Intel,2026-01-06 01:32:44,14
Intel,nxz1cdn,The Arc B390 parts are LP-DDR5x only.,Intel,2026-01-06 08:04:24,1
Intel,ny3gnxb,"No Sodimm on the existing X series parts, LPCAMM2 for DDR5 seems dead still so I only expect to see good use of it with DDR6.",Intel,2026-01-06 22:52:59,1
Intel,nxzf4k0,Its a BGA mobile part - you might see some desktops using it (particularly in SFF NUC type designs or All-in-Ones) but its not a socketed desktop part.,Intel,2026-01-06 10:15:33,3
Intel,nxzmu72,Desktop version drops next year like usual with intel. First laptop then pc.,Intel,2026-01-06 11:23:01,1
Intel,nxytdvk,"Usually when someone cooks its good, but if they are cooked its bad.  Intel/AMD cooked/ is cooking = good Intel/AMD is cooked/ they are cooked = bad  At least that's how i differentiate it.",Intel,2026-01-06 06:52:29,25
Intel,nxytih9,"Cooking is good.  Getting cooked is bad.  One has you as the victim, the other as the victor",Intel,2026-01-06 06:53:35,14
Intel,nyvb37j,SÃ¤g mig du Ã¤r pensionÃ¤r utan att sÃ¤ga att du Ã¤r pensionÃ¤r.,Intel,2026-01-10 22:51:08,1
Intel,nxxfmz4,"There could be if I can get it far cheaper.  This kind of performance should melt good part of premiums off Strix Halo, too.",Intel,2026-01-06 01:34:55,5
Intel,nxzmr6n,Its probably LPCAMM memory.,Intel,2026-01-06 11:22:20,1
Intel,ny3iqio,"Yeah I certainly don't expect a smaller company like FW to adopt a niche standard like LPCAMM2 for DDR5, probably DDR6 given I doubt SODIMM can support the speeds that DDR6 will have.",Intel,2026-01-06 23:03:18,1
Intel,nxzyhhz,"desktop version will be novo lake, later this year or early next year.  It will a significant upgrade.",Intel,2026-01-06 12:49:16,3
Intel,nxzn7ws,So in 2027? It takes them a whole year to release the desktop versions?,Intel,2026-01-06 11:26:10,2
Intel,nxylinl,Strix halo is irrelevant because that shit isnâ€™t and wonâ€™t be available in mainstream laptops regardless of its pricing. Even in amd marketing material just now itâ€™s the same asus tablet and hp Zbook nothing else.,Intel,2026-01-06 05:48:06,2
Intel,ny3g4rp,"Nah the love of Strix Halo is it's 256 bit bus and 128GB of memory, which this doesn't address at all. It's good groundwork though, you could imagine an update that grows the current design with two Xe4 12 core chiplets connected to the I/O die that then goes to DDR6 192 bit bus being good enough to compete with an RTX5060.",Intel,2026-01-06 22:50:22,1
Intel,nxzez6p,"I get downvoted, but it literally shows on the chart that the 388H, 368H, 358H and 338H are LP-DDR5x only: [INTEL-PANTHER-LAKE-1.jpg (2629Ã—1341)](https://cdn.videocardz.com/1/2026/01/INTEL-PANTHER-LAKE-1.jpg)  You won't see Intel supporting consumer designs which use DDR5 with these parts, and if you do make a product running it you'll lose the Arc branding - same as if you do an Arrow Lake-H design with single channel memory, or a Lunar Lake with a sub-17W PL1.",Intel,2026-01-06 10:14:13,1
Intel,nxznayq,LPCAMM2 is still LP-DDR5x.,Intel,2026-01-06 11:26:51,1
Intel,ny3jgvd,"I mean they tried for Strix Halo, but AMD did not validation or design work in mind to support it sadly so framework couldn't risk it. Intel tends to be better, but still.",Intel,2026-01-06 23:07:01,1
Intel,ny2jemc,"No, I have no idea what u/kazuviking is talking about.   Panther Lake won't have desktop chips. The mobile chips might get used in some mini PCs or whatever, but PTL is a mobile architecture.   What is coming late 2026 though, near the end of the year, is Nova Lake, a completely different arch than Panther Lake. And then we will likely see Nova Lake for laptops at CES next year.",Intel,2026-01-06 20:17:53,2
Intel,nxznspb,"It was always like this, release new gen on laptop then a year later on pc.",Intel,2026-01-06 11:30:53,1
Intel,ny33fhy,It is going to be in the Asus Tuff A14 this year.,Intel,2026-01-06 21:49:56,1
Intel,nxznqea,Yeah but completely different standard.,Intel,2026-01-06 11:30:22,1
Intel,ny3kun0,"That's a bit of a different case, because they are designing a new product either way they might as well look into supporting something like LPCAMM2. They already have a mainboard for the FW13 that can support these newer CPUs, so there has to be a very compelling reason to spend money to redesign that, which I think a single Intel SKU is not unfortunately.",Intel,2026-01-06 23:14:05,1
Intel,ny3gxyu,Do we know if we are getting real Nova Lake mobile chips or just a BGA version of desktop parts with a panther lake refresh?,Intel,2026-01-06 22:54:23,1
Intel,nxzp8ts,"The DRAM standard between LPCAMM2 and LP-DDR5x is the same - that's the point. LPCAMM2 is a replaceable implementation of LP-DDR5x memory.  But LPCAMM2 is completely different from SO-DIMM. SO-DIMM is DDR5. LPCAMM2 is LP-DDR5x.  You will see Arc B390 designs with LP-DDR5x memory down on the mainboard, and you'll see (some) LPCAMM2 designs, but they'll be far less common.  But you won't see DDR5 SO-DIMM designs - if they claim to be DDR5 SO-DIMM and Arc B390 then its wrong.",Intel,2026-01-06 11:42:26,1
Intel,ny4seb7,"It certainly sounds like we will get dedicated NVL-H, and that's what the rumors claim as well. Here's an excerpt from an Intel executive at an Intel BoA conference:   >Yeah, so maybe just baseline everybody on Panther Lake, so Panther Lake is a product thatâ€™s going to launch in the second half of this year, and it is all built on Intel 18A. Really, Panther Lake is an all mobile stack. When you get to the next generation Nova Lake it is both a mobile stack and a desktop stack",Intel,2026-01-07 03:05:35,1
Intel,ny4w3p0,"Looks like some folks are saying Nova Lake replaces panther lake stack exactly while also having a really big H chip at top, missed that cause I thought it was just the big 8+16+4 design",Intel,2026-01-07 03:26:25,1
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,3
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nxsj613,100Â°C,Intel,2026-01-05 09:52:28,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nyktz5u,"surprisingly fast, it has its own suite on windows, but i want to use it in linux, trying to figure out how as im not that good ta linux.",Intel,2026-01-09 11:27:16,1
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,6
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,5
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,2
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,5
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxvcb13,"It's the Flex ATX form factor. I think the most powerful one that is also reputable is the Enhance ENP-7660L, which is 600w.",Intel,2026-01-05 19:23:31,1
Intel,nzf8l4a,Around 65c-75c under load,Intel,2026-01-13 21:15:43,1
Intel,nxs9peb,"a lot of commercial software also just does not give a shit about improving, and I don't mean that as a defense for apple.  after effects is just ass for a 2025 product. basic filters are still using legacy code and memory management is horrible. like you're not going to clean 64gb of memory until I hit 96gb utilization, then you're going to slow to a crawl and maybe crash because of threadlock? why even bring back MT rendering? 3rd party scripts people wrote in their basements outperform this stupid thing. spoofing multiple instances and then stitching the results works better than just running the software, its baffling.  anyway yeah, there's a lot of good to x86 and not having to reinvent the wheel, but god damn if so many companies are using it as an excuse to resell garbage.",Intel,2026-01-05 08:22:17,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nyqxyvg,"> Memory - The Core Ultra X9 will feature soldered Dual Channel LPDDR5x 9600 MT/s memory up to 96GB   96GB of RAM? So it's $20,000?",Intel,2026-01-10 07:03:20,11
Intel,nyodemu,"Great, want one!",Intel,2026-01-09 21:55:54,2
Intel,nywvp1h,any plan for Wildcat Lake variant for very cost effictive mini PC solution?,Intel,2026-01-11 03:56:05,2
Intel,nyovlyi,It will be very interesting to compare it with the Nuc 15 pro. I am currently reviewing this model with a U5 225H processor. And I know what this processor is capable of in combination with ARC 130T.,Intel,2026-01-09 23:26:34,1
Intel,nyr50we,any word on availability?,Intel,2026-01-10 08:06:45,1
Intel,nyxxa7v,I hope it costs under 1600.,Intel,2026-01-11 08:40:19,1
Intel,nz7zsaj,5x4 nopeâ€¦ stick with the 4x4 thatâ€™s been around for a decade at this point.  Any increase in size just dilutes the meaning of NUC,Intel,2026-01-12 20:00:12,1
Intel,nyryfbb,"Ok atleast this doesn't have co pilot button,plus one to that",Intel,2026-01-10 12:30:39,1
Intel,nzad5zk,"Nothing to share about that. I see some news about that platform, but nothing has been shared with me internally to say one way or the other. For these ASUS NUC models, they are all listed from the Intel Core Ultra 5 and higher.",Intel,2026-01-13 03:15:32,3
Intel,nyudelu,"It's mentioned in there, but late Q1 - early Q2 is our current target.",Intel,2026-01-10 20:02:42,5
Intel,nyv67be,"If I were to buy one, are these like just hardware or do they include an OS with all the drivers installed out of the box?",Intel,2026-01-10 22:26:12,1
Intel,nyx7821,"Thanks, is that for global availability or just US? Also any word on what the lowest spec SKU will start at?",Intel,2026-01-11 05:05:43,1
Intel,nyvi99v,"They will be available in both types of models. I don't have a full breakdown on which hardware will be included with the complete Mini PC (e.g. with memory, storage and OS), or the Barebone kit (no memory, storage, or OS), but you'll be able to purchase it in either configuration.",Intel,2026-01-10 23:28:53,5
Intel,nzacphp,"I would expect US availability to be around global availability, but that different barebone kits sometimes are configured a bit later.   To your second question, pricing information isn't available at this time, but if you're just asking the specs, I would follow what I posted above.   However, if you go to our global product page, you'll usually find a download link for our spec datasheet (sometimes this doesn't show on mobile + plus the document isn't available yet). This will be a better way to see this. I'll ask our team when the datasheet will be available.",Intel,2026-01-13 03:13:08,2
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,48
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,12
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400â‚¬) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, Iâ€™m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesnâ€™t sound easy at all. Memory is a huge part of the BOM, and weâ€™ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), Iâ€™m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,5
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,19
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,14
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,10
Intel,nw63y3k,CXL killed octane itâ€™s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,4
Intel,nvbtvc9,If the b770 is 5060ti levels even â‚¬500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it wouldâ€™ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,5
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
Intel,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
Intel,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
Intel,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
Intel,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
Intel,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
Intel,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
Intel,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
Intel,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
Intel,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
Intel,ntkfg69,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Intel,2025-12-12 01:18:37,20
Intel,ntmjjev,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Intel,2025-12-12 10:55:28,4
Intel,ntlssa2,Wish theyâ€™d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Intel,2025-12-12 06:40:14,4
Intel,ntxw9ob,"How many concurrent users will this serve, 30 devs would be nice",Intel,2025-12-14 06:36:27,1
Intel,ntsaqxn,:),Intel,2025-12-13 08:27:35,2
Intel,nvplqob,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Intel,2025-12-24 13:00:31,6
Intel,nvpsi8z,Mac Pro Trashcan 2.0 is crazy,Intel,2025-12-24 13:45:33,6
Intel,nvt0j8h,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Intel,2025-12-25 01:15:41,3
Intel,nvte7q0,No one knows. Synthetics seem to put it roughly at a 3050m.,Intel,2025-12-25 02:59:28,2
Intel,nwfd6hr,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Intel,2025-12-28 20:51:38,1
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,55
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,10
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,3
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,6
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,3
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If itâ€™s just â€œ16% faster than 890mâ€ itâ€™s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,6
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.Â      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.Â  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.Â  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,2
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.Â  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nv5fgk8,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Intel,2025-12-21 04:48:32,9
Intel,nv5nfm2,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and itâ€™s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Intel,2025-12-21 05:49:09,3
Intel,nv5h74y,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Intel,2025-12-21 05:00:55,3
Intel,nv5hr1e,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality itâ€™s overall better and better IMC as well. Itâ€™s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Intel,2025-12-21 05:04:58,2
Intel,nv5yk6v,If it doesnâ€™t cost you extra then get the 14900ks and lock all the cores to 5.6 and power limit 256w,Intel,2025-12-21 07:30:16,2
Intel,nv70zt0,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Intel,2025-12-21 13:28:34,2
Intel,nvehvk5,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30Â°c cooler in game!!! I would average 80-85, now itâ€™s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and Iâ€™m extremely happy with the outcome!",Intel,2025-12-22 17:37:19,2
Intel,nv6um6s,I also PL1/2 at 150. My temps stay under 60c when gaming.,Intel,2025-12-21 12:41:07,1
Intel,nv7icoh,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Intel,2025-12-21 15:16:34,1
Intel,nvb3hok,Im using a duel air tower for my 14900k game temps are at 60 to 70,Intel,2025-12-22 02:56:21,1
Intel,nvkr6z0,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Intel,2025-12-23 17:28:35,1
Intel,nv62yz6,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Intel,2025-12-21 08:13:54,-3
Intel,nv6s3tx,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Intel,2025-12-21 12:20:08,6
Intel,nvx118g,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Intel,2025-12-25 20:10:57,2
Intel,nv6848y,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Intel,2025-12-21 09:06:11,0
Intel,nwbwttq,"Dropped the voltage further down by -0.10000 and now Iâ€™m getting 58Â°c core temp and max 65Â°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings Iâ€™m able to still maintain a framerate that matches my screen refresh rate.",Intel,2025-12-28 07:43:31,1
Intel,nv7zbxm,This post is about the K and KS. Both have the same iGPU,Intel,2025-12-21 16:46:01,3
Intel,nv7imu3,even better. i take it they extended tbe warranty period for those products?,Intel,2025-12-21 15:18:10,1
Intel,nvgy84t,For 5.5 39k in CB23 is a little low,Intel,2025-12-23 01:34:28,0
Intel,nvx2yr5,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Intel,2025-12-25 20:22:49,1
Intel,nyq5yj3,"Yeah your method is better, itâ€™s what I do.",Intel,2026-01-10 03:42:43,1
Intel,nv7bdbq,The ks should run cooler because of the better bin. Less voltage being required to hit certain frequency points.,Intel,2025-12-21 14:35:05,2
Intel,nv84jg4,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Intel,2025-12-21 17:12:59,1
Intel,nv7jm5d,Yes because of the degrading issue.,Intel,2025-12-21 15:23:39,2
Intel,nvhfrt3,Lol stock is 5.8ghz lol and most stock after the update get 35k,Intel,2025-12-23 03:22:04,1
Intel,nvhi35n,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Intel,2025-12-23 03:36:52,1
Intel,nxyvuyn,"That is a little on the lower end, my 12900K gets 30K. Also in single core a little below 285K, multicore just below 9900X, top of 13700K. Back to it :)",Intel,2026-01-06 07:14:02,1
Intel,nvhitar,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Intel,2025-12-23 03:41:34,2
Intel,nvhk1pd,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Intel,2025-12-23 03:49:33,2
Intel,nvkpd9m,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Intel,2025-12-23 17:19:30,2
Intel,nvkq3xk,Talk to him not me... The guy said 35k is the score ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚,Intel,2025-12-23 17:23:13,2
Intel,nsktadm,Will be a interresting CES,Intel,2025-12-06 11:12:47,22
Intel,nso11hn,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Intel,2025-12-06 22:37:18,7
Intel,nswpbo1,Merry Christmas everyone,Intel,2025-12-08 08:52:05,2
Intel,nswyceh,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Intel,2025-12-08 10:25:38,2
Intel,nsofcmj,They can't even ship B60's.,Intel,2025-12-07 00:01:12,3
Intel,nsp4pld,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Intel,2025-12-07 02:38:20,1
Intel,nsm5mtm,Aren't they always,Intel,2025-12-06 16:34:23,3
Intel,nt8d8jl,give it some time...,Intel,2025-12-10 03:42:11,1
Intel,nt8d10w,Merry Bitmas,Intel,2025-12-10 03:40:49,1
Intel,nt9tjuy,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Intel,2025-12-10 11:20:47,0
Intel,nsol1s5,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Intel,2025-12-07 00:34:27,6
Intel,nsq0noc,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Intel,2025-12-07 06:21:43,7
Intel,nswuidx,What is taking Nividia so long with the super cards?,Intel,2025-12-08 09:46:34,2
Intel,nt0mfun,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Intel,2025-12-08 22:47:54,1
Intel,nsn1l8j,There's definitely been lame ones.,Intel,2025-12-06 19:20:53,3
Intel,ntco711,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Intel,2025-12-10 20:41:40,0
Intel,nt0k5mq,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Intel,2025-12-08 22:35:24,2
Intel,nt0ni97,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Intel,2025-12-08 22:53:46,1
Intel,ntafqck,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Intel,2025-12-10 13:56:29,1
Intel,nqdrca0,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It wonâ€™t be like the 4050 but 3050M isnâ€™t bad for an iGPU that fits in a normal socket",Intel,2025-11-23 16:52:41,13
Intel,nqe5eoy,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Intel,2025-11-23 18:04:37,13
Intel,nr20ozq,"I donâ€™t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7â€“2 times the performance of the desktop 3050",Intel,2025-11-27 14:11:52,1
Intel,nu0mh30,and also an iGPU won't be stuck with 6GB of vRAM ðŸ˜…,Intel,2025-12-14 18:17:52,1
Intel,nqga537,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Intel,2025-11-24 00:43:14,5
Intel,nqg407n,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Intel,2025-11-24 00:08:08,3
Intel,nqfqmb2,I see it personally as not really being at 20W if youâ€™re asking much from the GPU. Itâ€™ll increase the juice dynamically if it gets demanding enough. So itâ€™ll be hard to say unless you force the power limits way down manually.,Intel,2025-11-23 22:49:52,2
Intel,nnbivzp,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Intel,2025-11-05 22:28:54,38
Intel,nnbkl4o,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Intel,2025-11-05 22:37:53,15
Intel,nnbr1vs,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Intel,2025-11-05 23:13:11,7
Intel,nncz7bz,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Intel,2025-11-06 03:36:19,5
Intel,nnftti4,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Intel,2025-11-06 16:14:24,2
Intel,nndm2ga,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Intel,2025-11-06 06:31:30,2
Intel,nnc0ph2,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Intel,2025-11-06 00:08:27,-4
Intel,nnjsaw2,Will the 10 core Xe be better than radeon 890m or worse?,Intel,2025-11-07 05:14:22,0
Intel,nnbhb9s,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Intel,2025-11-05 22:20:34,-7
Intel,nnbj579,Still weaker than x3d,Intel,2025-11-05 22:30:15,-16
Intel,nnm29u0,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Intel,2025-11-07 15:46:06,3
Intel,npu5d43,Doesn't the B already serve that purpose?,Intel,2025-11-20 13:03:06,1
Intel,nnbmjlf,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Intel,2025-11-05 22:48:16,15
Intel,nnd5hpv,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Intel,2025-11-06 04:19:26,9
Intel,nnclctl,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Intel,2025-11-06 02:10:09,6
Intel,nnde8rz,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Intel,2025-11-06 05:25:14,8
Intel,nngpesf,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Intel,2025-11-06 18:45:02,0
Intel,nnklixi,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Intel,2025-11-07 09:57:31,2
Intel,nnlg7cm,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Intel,2025-11-07 13:50:18,1
Intel,nnbigvq,What're you doing on a laptop?,Intel,2025-11-05 22:26:37,20
Intel,nnbiwg3,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Intel,2025-11-05 22:28:59,8
Intel,nnbowvu,It's for handhelds and office laptops not hyper enthusiast shit.,Intel,2025-11-05 23:01:09,2
Intel,nncdk44,Nobody buys AMD laptops,Intel,2025-11-06 01:23:02,12
Intel,nnbk5wm,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Intel,2025-11-05 22:35:40,7
Intel,nnco8fw,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Intel,2025-11-06 02:27:36,1
Intel,nnz3axo,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Intel,2025-11-09 18:46:41,2
Intel,nnbn2as,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Intel,2025-11-05 22:51:02,9
Intel,nndnorc,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Intel,2025-11-06 06:46:30,9
Intel,nndrn8v,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Intel,2025-11-06 07:23:49,3
Intel,nnbivk1,Highly immersive porn on the go,Intel,2025-11-05 22:28:50,14
Intel,nnc0bgd,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Intel,2025-11-06 00:06:14,5
Intel,nnbkp8h,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Intel,2025-11-05 22:38:29,7
Intel,nnbkxvp,PTL extends up to the -H series too.,Intel,2025-11-05 22:39:46,3
Intel,nnbkts2,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Intel,2025-11-05 22:39:10,3
Intel,nnbl91n,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Intel,2025-11-05 22:41:25,4
Intel,nnc0woc,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Intel,2025-11-06 00:09:35,2
Intel,nnc2335,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Intel,2025-11-06 00:16:15,10
Intel,nnblgop,Just get a Vision Pro?,Intel,2025-11-05 22:42:32,6
Intel,nnbyre8,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Intel,2025-11-05 23:57:18,1
Intel,nnbmp20,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Intel,2025-11-05 22:49:05,4
Intel,nnbrc9f,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Intel,2025-11-05 23:14:48,0
Intel,nnc5j69,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Intel,2025-11-06 00:35:47,6
Intel,nnd2wbt,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Intel,2025-11-06 04:01:09,4
Intel,nnc1auf,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.Â    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Intel,2025-11-06 00:11:48,1
Intel,nncm4mg,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Intel,2025-11-06 02:14:47,9
Intel,nnj3sul,"Two options seems right, either you care about it or you don't.",Intel,2025-11-07 02:28:49,1
Intel,nnh5cgt,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Intel,2025-11-06 20:02:16,1
Intel,nncnlt8,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Intel,2025-11-06 02:23:44,3
Intel,nncrc61,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Intel,2025-11-06 02:46:29,0
Intel,nnd4ak4,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Intel,2025-11-06 04:10:51,2
Intel,nndr8zp,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Intel,2025-11-06 07:19:59,4
AMD,nzcnd0i,just run an 9800x3d at 5.6ghz and u know how this cpu performs...,hardware,2026-01-13 13:46:30,82
AMD,nzcl9w0,"Who even needs that? Cool, a slightly faster X3D CPU.    Everyone will say:   \-It might be 1-7% faster in some scenarios (testing in 720p/1080p)   \-There is no reason to upgrade from the 9800X3D   \-Something something intel, remember intel has done similar things.   \-It's too expensive right now because the msrp is higher than current market prices for the 9800X3D.   Long story short: It's a refresh, once it's abut the same price/cheaper than the 9800X3D, it's the better opinion/ That's all you need to know.    But influencers will milk it, like they always do. Expect clickbait titles for weeks/months.",hardware,2026-01-13 13:34:51,36
AMD,nzct8af,Aren't we all excited to see if it's 2% or 3% faster?,hardware,2026-01-13 14:17:52,7
AMD,nzctlg8,i don't get a review embargo of a product that's essentially a refrest of an existing one.,hardware,2026-01-13 14:19:47,8
AMD,nzcpoe3,Steve's going to be standing up on this one.,hardware,2026-01-13 13:59:01,8
AMD,nzdtm07,What has changed? Why not explain it?,hardware,2026-01-13 17:22:49,2
AMD,nze3o81,This will be the ultimate CPU for 480i low settings with an OCed RTX 5090.   Look at that whole 6% uplift over the slow 9800x3D. Complete domination.,hardware,2026-01-13 18:08:52,3
AMD,nzcrenm,I already have a 9850x3D  Itâ€™s my PBOd 9800x3d  Thing is a beast,hardware,2026-01-13 14:08:16,3
AMD,nzci13p,"Hello Jumpinghoops46! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-13 13:15:55,1
AMD,nzdszu6,I'm even more excited for this than I was for the 5800XT,hardware,2026-01-13 17:19:51,1
AMD,nzg1vsm,Is it going to blow up on motherboards? Is voltage sensitivity still an issue ???,hardware,2026-01-13 23:39:40,1
AMD,nzgjkbl,"You know what man, fuck it why not",hardware,2026-01-14 01:16:21,1
AMD,nzgtjmw,"Yaawwwwn. AMD, I know you're not listening, but if you were to, no one cares. Just release the damn thing and get your money.   9950X3D-2? Now that would be something to get some attention. 6 months ago.",hardware,2026-01-14 02:13:08,1
AMD,nzdct62,"Got a 13700k and very much looking forward to this thing, assuming the price is reasonable. Otherwise, will just get the 9800x3D.",hardware,2026-01-13 15:54:20,1
AMD,nzdzywr,"*""yawn....""* Wake me when ZEN 6 is announced...",hardware,2026-01-13 17:52:22,1
AMD,nzcof7w,Exactly  And most 9800X3D are capable of doing that without problems,hardware,2026-01-13 13:52:15,21
AMD,nze98e7,Yes it's equivalent to the Intel KZ line. Just run a K processor at KZ clocks to know how the KZ performs. Problem is not all K chips can reliably hit those clocks.,hardware,2026-01-13 18:33:07,2
AMD,nzcnrwq,"In ye olden days the answer would be, it's an extra push for people on older platforms to upgrade. But with current RAM prices even that is not really a market.  On the other hand, it's not like there's any significant R&D in this thing, it's just a better bin. Releasing it might be a ""why not?"" type decision. As is the decision for influencers to cover it",hardware,2026-01-13 13:48:45,16
AMD,nzcwne5,"It's not a refresh, it's just better binning.  AMD has likely been stockpiling these better dies for a while.",hardware,2026-01-13 14:35:35,10
AMD,nzecx5o,"> -There is no reason to upgrade from the 9800X3D  Not to upgrade, but if you're building a new PC, might as well get a better one.",hardware,2026-01-13 18:49:26,1
AMD,nzgtyzy,"Right, the Intel ""KS"" SKUs. Or Extreme, in the past.   Almost never a reason to buy them other than ""Oh no, I can't hold all these bundles of $100 bills!"" kind of money and bragging rights.",hardware,2026-01-14 02:15:31,1
AMD,nzcn07g,"I see your point.   I've been holding out for a refreshed 9800, so this will hopefully fit the bill if the performance matches the price point. I may just go for a 9800 since it's price will drop (hopefully).",hardware,2026-01-13 13:44:32,0
AMD,nzcrjzb,This this this. 100% this.   All the YouTubers just got new content for a few weeks. Sooooo happy for them.,hardware,2026-01-13 14:09:04,0
AMD,nze9t4f,"It'll be the fastest gaming CPU (faster than base 9800X3D) on the planet, who ever wants that will need that.",hardware,2026-01-13 18:35:40,0
AMD,nzdptpi,More like Zen 5%,hardware,2026-01-13 17:01:34,7
AMD,nzeyk8d,At 1080p*,hardware,2026-01-13 20:28:32,2
AMD,nze27kv,You must be new to this hobby,hardware,2026-01-13 18:02:19,1
AMD,nzcwrl1,"Oh no, I'm sure AMD is shaking in their seats RN!",hardware,2026-01-13 14:36:11,7
AMD,nzd6nk4,Why?,hardware,2026-01-13 15:25:31,1
AMD,nze6snz,"benchmarking nerds will love it though (as long as the memory controller is also binned), then they will ditch it immediately when the next big thing is released, like the hypothetical 9950X3D v2.",hardware,2026-01-13 18:22:31,2
AMD,nzd6nth,You're running max 5.45ghz on that. The 9850x3d will do 5.6ghz out of the box and probably 5.8ghz with an oc. Not that that would change that much,hardware,2026-01-13 15:25:33,5
AMD,nzctxoa,9850x3d is a double pbo basically yeah,hardware,2026-01-13 14:21:34,4
AMD,nzd6dju,"Thats not exactly true, you need a motherboard with eclk support. Running static 5.6ghz needs a golden sample",hardware,2026-01-13 15:24:11,13
AMD,nzcrs4g,I'm not against it or anything.    If anything I'm generally supportive of releasing refreshes even though most of them were perceived poorly by at least the tech bubbles.   I just think that how they're presented by influencers hurts more than anything.,hardware,2026-01-13 14:10:16,2
AMD,nze65bf,is the memory controller also a better binning ?? Imagine selling your 9800X3D capable of doing 6400 1:1 C28 FCLK 2133-2200 with tight timings and getting a 9850X3D stuck at 6000 CL30.,hardware,2026-01-13 18:19:43,1
AMD,nzgulns,Probably cause it's going to cost 20% more and be 2% faster.,hardware,2026-01-14 02:19:03,1
AMD,nzd8h5k,Yeah Iâ€™m good. But Iâ€™m sure there will be plenty of degens with stupid money that go spend $500 to upgrade their 9800,hardware,2026-01-13 15:34:15,1
AMD,nzdf2dk,"what.. not at all with 9800x3d it is easy peasy. u just set the multiplier and it will be locked, that is if u are have unlocked all the powerlimits and u dont have lost the silicon lottery.  have had 3 9800x3d now. all of them ran at 5.6 locked.",hardware,2026-01-13 16:04:39,2
AMD,nzduxwc,>Running static 5.6ghz needs a golden sample  Nope,hardware,2026-01-13 17:29:06,1
AMD,nzcss86,Fair enough,hardware,2026-01-13 14:15:30,2
AMD,nzeo6im,"> 6400 1:1   Are the 9800x3d better at this? I have a 7800x3d and didn't want the headache of testing this again. Mind you, I don't care about IF above 2000 nor lower CAS (28).",hardware,2026-01-13 19:40:20,1
AMD,nzdgygt,> golden sample  > u dont have lost the silicon lottery   Youâ€™re reaffirming the comment youâ€™re replying to.,hardware,2026-01-13 16:13:20,14
AMD,nzepc43,"it's still silicon lottery, no 9800X3D is guaranteed to do better than 6000 CL30 1:1 FCLK 2000, but some (many ?) can do 6400 1:1 with tight timings, here's my setup: [https://i.imgur.com/UTA1CHL.png](https://i.imgur.com/UTA1CHL.png)  now imagine I sell my 9800X3D with its great memory controller just because I want the ""latest and shiniest"" 9850X3D and then I end up stuck at 6000 1:1 (1% low fps would be impacted, but also helped by the increased clock speed... so they will cancel each other), that's why I was wondering if they're also binning the memory controller for the 9850X3D or they're just binning the cores.",hardware,2026-01-13 19:45:36,1
AMD,nzh0aog,How is a refresh and better binning any different?,hardware,2026-01-14 02:51:04,1
AMD,nzduw1y,"Pretty sure most 9800X3Ds csn hit 5.6 without any problems (unless cooling is limited, but if cooling is a limitation then that will also be for the 9850X3D)  Don't need a golden sample  Don't need to win the silicon lottery. You can have below average silicon and still reach it easily.  They are the same CPU... The 9850X3D OC is just done out of the box",hardware,2026-01-13 17:28:52,3
AMD,nzdts9g,how am I? I did not even touch the voltages. if not that is a pretty good indication that many can hit those speeds.,hardware,2026-01-13 17:23:41,1
AMD,nzeqnrz,"Knowing corporations, just the core. Binning the memory cont. would be too much work lol",hardware,2026-01-13 19:51:38,2
AMD,nzfury3,How is your memory clock voltage (CPU VDDIO) only 0.87V? That makes no sense.,hardware,2026-01-13 23:01:44,1
AMD,nzgmmlr,"*some* percentage of 9800X3D's can't hit 5.6 stable, otherwise it would have shipped at that speed",hardware,2026-01-14 01:33:59,3
AMD,nzfv5h4,"It isn't, it's a misread of Zen Timing, it's 1.350",hardware,2026-01-13 23:03:42,2
AMD,nzglfxh,"Got it, makes way more sense. My Zen Timings doesn't misread it, so I was concerned.",hardware,2026-01-14 01:27:03,1
AMD,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,112
AMD,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,40
AMD,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,88
AMD,nysrktd,Iâ€™d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  Whatâ€™s the point of hardware if you canâ€™t apply it to what you need.,hardware,2026-01-10 15:26:18,11
AMD,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,19
AMD,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,27
AMD,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,27
AMD,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,7
AMD,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,13
AMD,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,9
AMD,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,6
AMD,nyrhl2f,Website doesnâ€™t load with adblocker,hardware,2026-01-10 10:04:14,5
AMD,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,5
AMD,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
AMD,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
AMD,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
AMD,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
AMD,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
AMD,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
AMD,nys0jf3,Needs a conroe vs fx62 moment. It doesn't look promising.,hardware,2026-01-10 12:46:32,0
AMD,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
AMD,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-7
AMD,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-10
AMD,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,33
AMD,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,1
AMD,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
AMD,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,15
AMD,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
AMD,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,29
AMD,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,37
AMD,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,23
AMD,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,10
AMD,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,14
AMD,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,12
AMD,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,8
AMD,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
AMD,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,13
AMD,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
AMD,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,15
AMD,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,17
AMD,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
AMD,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,23
AMD,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,10
AMD,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,6
AMD,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,2
AMD,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
AMD,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,7
AMD,nyt7vgu,"It seems the revived Dell XPS 16 will have the â€œB390â€ and no dGPU, as another option. LTTâ€™s video on it said Dell is quoting 27 hours of battery life in â€œgeneral tasksâ€ and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but hereâ€™s hoping.",hardware,2026-01-10 16:45:16,4
AMD,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,39
AMD,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 â€œLow Powerâ€ efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobileâ€™s 285H (not HX!), trading 2 performance cores for 2 â€œLow Powerâ€ efficiency cores.  Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intelâ€™s normal lineup.",hardware,2026-01-10 16:28:50,14
AMD,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
AMD,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
AMD,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,43
AMD,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,10
AMD,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
AMD,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
AMD,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
AMD,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,17
AMD,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,0
AMD,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,8
AMD,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,5
AMD,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,8
AMD,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,17
AMD,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
AMD,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,12
AMD,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-4
AMD,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,6
AMD,nytp8ri,"Huge step forward, I just wish the didnâ€™t struggle with older and brand new games. Itâ€™s a great card if you are willing to do troubleshooting and know computers but I wonâ€™t recommend them to family yet.",hardware,2026-01-10 18:07:45,6
AMD,nz0sex5,I hope they start supporting dx11 stuff. Thatâ€™s a ton of games.,hardware,2026-01-11 19:10:35,1
AMD,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,6
AMD,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
AMD,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,13
AMD,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,11
AMD,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,3
AMD,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,3
AMD,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,16
AMD,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
AMD,nyvvmm4,"> Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,15
AMD,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,5
AMD,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
AMD,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,2
AMD,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,1
AMD,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,8
AMD,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
AMD,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,2
AMD,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
AMD,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,8
AMD,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
AMD,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
AMD,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,11
AMD,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,3
AMD,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,29
AMD,nyuw1ry,"To be frank, high end mobile gaming is also pretty niche.   There are edge cases for AI.   However, the pricing for strix halo was just awkward. At the price levels their SKUs were coming, you might as well go with a nice M4 max and call it a day.",hardware,2026-01-10 21:36:09,6
AMD,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,4
AMD,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,7
AMD,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,10
AMD,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,7
AMD,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,23
AMD,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45Â°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
AMD,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
AMD,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,7
AMD,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,35
AMD,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,20
AMD,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,12
AMD,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,2
AMD,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-1
AMD,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,6
AMD,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,9
AMD,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,2
AMD,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,5
AMD,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
AMD,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
AMD,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
AMD,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
AMD,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,5
AMD,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
AMD,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,11
AMD,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
AMD,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
AMD,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,7
AMD,nywmg0a,"Depends on what you consider to be â€œhigh end mobile gamingâ€, the laptop 4060 is currently the 2nd most popular gpu on steam, and thatâ€™s the level strix halo targeted",hardware,2026-01-11 03:03:40,0
AMD,nyxpn7n,"What youâ€™re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. Thatâ€™s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,0
AMD,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-4
AMD,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,4
AMD,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,8
AMD,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
AMD,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
AMD,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,8
AMD,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,12
AMD,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
AMD,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,9
AMD,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,8
AMD,nywgma9,"QC last year had a bad product.Â  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200â‚¬   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,2
AMD,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,2
AMD,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
AMD,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.Â   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,5
AMD,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.Â  They have 5 total. AMD, Nvidia, ARM, Qualcomm,Â  Apple",hardware,2026-01-11 13:19:34,1
AMD,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,13
AMD,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,5
AMD,nysq6pk,"Not true. All you have to do is use iGPU and disable dGPU. On linux this is a non issue, on Windows I have no clue",hardware,2026-01-10 15:19:02,4
AMD,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,5
AMD,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
AMD,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
AMD,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,9
AMD,nz565l5,"Donâ€™t even get me started! My current HP laptop straight up doesnâ€™t support any type of fan control on Linuxâ€¦  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
AMD,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,4
AMD,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,1
AMD,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
AMD,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
AMD,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
AMD,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
AMD,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,2
AMD,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,7
AMD,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
AMD,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,2
AMD,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-3
AMD,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under Â£2k then maybe.",hardware,2026-01-10 16:20:10,3
AMD,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
AMD,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,2
AMD,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
AMD,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
AMD,nytd8yk,"On Linux you can use tools like supergfxctl to completely disable dGPU. It doesn't require a reboot but a logout. Pretty happy with G14 2023, dGPU is simply off and doesn't consume any power at all.  On Windows, it looks like on Asus Laptops you can do the same with G-Helper.",hardware,2026-01-10 17:10:47,2
AMD,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
AMD,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
AMD,nzbx72p,Medusa halo isnâ€™t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
AMD,nxxyy8i,Does this pair well with Nvidiaâ€™s newly release of the 3060?,hardware,2026-01-06 03:20:50,38
AMD,nxxmhi8,"So exactly the same specs as the 9800X3D but clocked slightly higher? Seems incredibly pointless, but I suppose it's a convenient way for them to make extra money off the chips that win the silicon lottery.",hardware,2026-01-06 02:11:50,82
AMD,nxxkel3,Would be killer value if this replaces 9800X3D at the same msrp. But it's AMD so they will price it at $549 and sell it alongside 9800X3D.,hardware,2026-01-06 02:00:35,52
AMD,nxyyl7u,"So, no 9950X3D2 announcement? I wonder if it was just a rumor after all.",hardware,2026-01-06 07:39:03,11
AMD,nxxnb08,"it just exists to keep the prices high and higher when it replaces 9800x3d which is already too expensive frankly but what you gonna do, buy intel?",hardware,2026-01-06 02:16:19,24
AMD,nxzdpug,"I'd rather it have a better memory controller so you could do 6400 6600 1:1, not 200 extra mhz but we get what we get",hardware,2026-01-06 10:02:41,4
AMD,nxxp3jo,Release ðŸ‘ more ðŸ‘ AM4 ðŸ‘ X3D ðŸ‘ chips ðŸ‘,hardware,2026-01-06 02:26:02,23
AMD,ny0hjfh,"Intel Nova Lake-S with bLLC can't come fast enough, AMD just slapping X3D to everything and not doing anything interesting at all. At least Intel's APO+ will be special sauce along with their bLLC technology.",hardware,2026-01-06 14:38:00,3
AMD,nxywuqr,Hopefully the 9950x3d2 too,hardware,2026-01-06 07:23:00,2
AMD,ny08w0s,cant wait to push this baby to 5.9 ghz or higher <3,hardware,2026-01-06 13:51:01,1
AMD,ny2l8d6,I just got the 9950x3d and had no clue about this until todayðŸ˜‚ðŸ˜‚ not mad but I mightâ€™ve waited,hardware,2026-01-06 20:26:26,1
AMD,ny2w378,"""4% more performance, 20% more cost."" All jokes aside, halo products do be like that. I'm assuming it'll come in at an MSRP of $549.99, but I hope it's more like $529.99 so at least it makes sense for the price to performance gain if you just want the absolute best CPU for gaming.",hardware,2026-01-06 21:16:16,1
AMD,ny3gx28,"I was not expecting amd to release a new cpu, am so closing to finishing my pc too just need cpu I might wait a little longer to save up and buy this cpu instead of the r7 9800x3d, I read itâ€™s going to have a tdp of 120w I hope my psu is enough for it considering I have an rx 9070 xt.",hardware,2026-01-06 22:54:15,1
AMD,ny6geho,Think I'm just gonna wait for the 9875X3D at the 2027 CES,hardware,2026-01-07 10:52:17,1
AMD,ny7779f,"Is there no new CPU generation coming ?  The 9800x3d is over a year old now , usually this is when the new chip comes out",hardware,2026-01-07 13:54:34,1
AMD,nxyagl1,"No, need more CPU for that if you want to play CS2 at 640x480p with below minimum graphics settings.",hardware,2026-01-06 04:30:14,12
AMD,ny00uek,if you plat MMOs then yes.,hardware,2026-01-06 13:04:22,8
AMD,ny397o2,"It doesn't have AI in the name, so clearly no. Zero future proofing, dead on arrival, no place in the market with zero useful performance metrics.  Once again, AMD never misses an opportunity to miss a opportunity.Â   /s",hardware,2026-01-06 22:17:00,5
AMD,nxxo2dm,If it translates to 7% better peak performance it could be worth it. The 9800X3D is already pretty awesomely fast.,hardware,2026-01-06 02:20:25,49
AMD,nxyire2,"Yes, pretty common for a CPU sku. This should at least be more manageable than the Intel version (KZ CPUs).",hardware,2026-01-06 05:27:19,5
AMD,ny1hdpn,Everything always pointed to it just being the equivalent to a -KS version of a 9800X3D.,hardware,2026-01-06 17:26:07,3
AMD,nxyeem4,it has a new step which allows memory OC to 9800,hardware,2026-01-06 04:56:30,1
AMD,nxz6guz,"Can't tell without the pricing, but if it'll be a price bump as expected, why exactly would one get this over a 9950x3d that already has a higher clocking cache ccd while not being an overpriced 8 core cpu?",hardware,2026-01-06 08:53:08,7
AMD,ny07opx,I'm betting this will be $499 and the 9800X3D will go to $449. AMD confirmed they will continue selling the 9800X3D.,hardware,2026-01-06 13:44:21,1
AMD,ny1fihl,"CES rumors has the MSRP at $499, slight bump from the 9800X3D at $479 I believe. That chip is still at $469 right now.",hardware,2026-01-06 17:17:38,1
AMD,nxxqv30,"hey, 12/13/14 with DDR4 is gona be the way to go for a ton of people rofl  i would laugh if they started to make 5850X3D soon",hardware,2026-01-06 02:35:48,23
AMD,nxypfeh,"It's not replacing the 9800x3d, both will exist side by side.",hardware,2026-01-06 06:19:09,3
AMD,nxxnx1n,Intel with ram,hardware,2026-01-06 02:19:37,4
AMD,nxyu9ss,delusional,hardware,2026-01-06 07:00:04,-1
AMD,ny11elk,"With that much cache, does the additional memory bandwidth even improve performance? I'm genuinely asking.",hardware,2026-01-06 16:13:26,3
AMD,nxy9rjp,Ryzen 5500X3D with PCIe3.0 speeds /s,hardware,2026-01-06 04:25:43,6
AMD,ny00xl8,never going to happen.,hardware,2026-01-06 13:04:56,2
AMD,nxy1dz6,AMD is saying a 2-3% uplift over the 9800X3D.  https://www.digitalfoundry.net/news/2026/01/amd-unveils-ryzen-7-9850x3d-fast-incremental,hardware,2026-01-06 03:34:49,47
AMD,nxxrngk,"7% higher boost at stock, but the 9800X3D can already hit 5.6-5.7 via ECLK and I doubt the 9850X3D will reliably do 6.0-6.1.  Serious overclockers will probably still want it for the better binning. Hard to say it will be a better value for anyone else yet.",hardware,2026-01-06 02:40:07,34
AMD,nxymq6e,Is this confirmed? Or is it still just that one screenshot and MLID's source?,hardware,2026-01-06 05:57:30,8
AMD,nxz68tb,a new strap means absolutely nothing except to 5 OC people when the iod is the same,hardware,2026-01-06 08:51:00,6
AMD,nxyiu8v,Woah,hardware,2026-01-06 05:27:54,0
AMD,nxz92km,"9950x3d are more usefull for multi-threading while 9800x3d bench show better perf at single-thread gaming  Which was my hope for 9850x3d, but 2-3% for probably 150-200$ more isn't worth it, better to just OC the 9800",hardware,2026-01-06 09:18:24,6
AMD,nxz7od0,Same reason people get 9800X3D over 9950X3D to be honest. Gaming vs AI,hardware,2026-01-06 09:04:41,1
AMD,nxy8w8y,"If they made a 5850X3D, I would be first in line to buy one.  I doubt they would since it seems chip packaging is a big bottleneck at the moment and it has to be more profitable to make 9000 series CPUs.",hardware,2026-01-06 04:20:08,12
AMD,ny1r5tm,[It seems to at 4K.](https://www.youtube.com/watch?v=EBF6B-f5me0),hardware,2026-01-06 18:09:59,2
AMD,nxyadpc,PCIE3.0 speeds and cut down lanes.,hardware,2026-01-06 04:29:43,1
AMD,ny1ehnc,You must be fun at parties,hardware,2026-01-06 17:12:59,-4
AMD,nxyt3ba,atleast they are honest,hardware,2026-01-06 06:49:59,22
AMD,nxyq5cx,Then I'd say that would warrant maybe a 5-10% price hike at most!,hardware,2026-01-06 06:25:04,8
AMD,ny07azf,AMD said last night that gaming uplift will be 7%. Which just about the same uplift as the 9800 was over the 7800.,hardware,2026-01-06 13:42:13,9
AMD,nxznkht,"Which you'd need an ECLK board for in the first place.  Meanwhile with 9850X3D 5.8 will just be enabling PBO, seems pretty sweet to me.",hardware,2026-01-06 11:29:01,7
AMD,nxzp2sm,Was seen with a valid benchmark.,hardware,2026-01-06 11:41:08,-2
AMD,ny3hp8u,"My understanding is that thereâ€™s virtually no difference for gaming. Digital Foundry had the 9950x3d performing within 1% of the 9800x3d in most games.  If youâ€™re only gaming obviously the 9800x3d has much better price:performance, but you arenâ€™t losing gaming performance by getting a 9950x3d, just gaining productivity performance.",hardware,2026-01-06 22:58:09,2
AMD,ny15xzk,"AI? The 9950X3D is more useful for classic ""HPC"" tasks, at least for the subset that sees the benefits of the additional cache",hardware,2026-01-06 16:34:12,7
AMD,ny39k7o,"Or for all the countless stuff that actually uses more cores, rather than AI.  Plenty of current popular ""AI"" stuff are heavily GPU bound and notoriously don't care at all what cpu you use, 4 core or 40 core processor. 9950X3D is probably one of the worst choices you should select when balancing out building a machine dedicated to ""AI.""",hardware,2026-01-06 22:18:40,2
AMD,nxy9vmv,"yeah, I mean, it would be a value play and it would make no sense unless 78 and 98 demand craters hard or something  it was a joke that I think will never happen, or rather, I pray never happens because it means that DDR5 got ratfucked",hardware,2026-01-06 04:26:28,2
AMD,ny0otr1,I know you're trying to make a joke but it's a dumb one.  The 5500 has 20 PCIe 3.0 lanes available.  It's up to the motherboard to determine how those lanes are allocated.,hardware,2026-01-06 15:14:47,3
AMD,ny6kvod,Just by the fact that i do not use emojis i am automatically more fun.,hardware,2026-01-07 11:29:43,1
AMD,ny028ex,"Yes, I hope +200 at least is still nearly guaranteed. That 2-3% figure in the DF article worries me a bit.",hardware,2026-01-06 13:13:01,3
AMD,ny1p6b2,"""AI"" is the new know-it-all buzzword.",hardware,2026-01-06 18:01:09,4
AMD,ny3m9fu,"Ddr5 is ratfucked. By the time prices come back down to even half of what they are, drr6 will be moving in. And you know ddr6 is going to start at the price ddr5 is at.    It's GG for home computing unless the americans wake up and do something about their technocracy problem. So it's GG for home computing.",hardware,2026-01-06 23:21:17,0
AMD,ny03vxc,For games I'd imagine it could hit that quiet well since they aren't really heavy workloads but I'm really interested to see aswell how good these chips manage.,hardware,2026-01-06 13:22:47,2
AMD,ny3a68x,"Haha so true.  I'll buy a fancy 5000 dollar computer case made with exotic materials and has too much RGB fans!  Why?  Because AI needs it!  It also needs this 3000 dollar HDMI cable, I can practically hear the AI singing in high fidelity from the digital bits!",hardware,2026-01-06 22:21:34,2
AMD,ny7zsh2,"It's not like ddr4 is getting any cheaper. But at least you can reuse your old RAM. I'd hope it be the x3D chips making a return, to make up for slower RAM speed.",hardware,2026-01-07 16:15:22,172
AMD,ny8plqm,Just re-release some x3d they'll make a killing,hardware,2026-01-07 18:10:44,21
AMD,ny7zah1,"eh, would feel better about this if they weren't winding down DDR4 production",hardware,2026-01-07 16:13:06,13
AMD,ny87car,"5950x3d please, to end am4 with a bang",hardware,2026-01-07 16:49:21,52
AMD,ny8abpl,"Unless they come out with a 5900/5950x3d theres nothing really for me to upgrade my 5800x to. Sure aint buying a used 5800x3d for 400 eur, much less a 5700.",hardware,2026-01-07 17:02:47,38
AMD,ny7tyjs,"I still have an upgrade path from my 3900X to 5000 series, but I don't need to as I have plenty of power for now. AM4 is the GOAT platform. So upgradable.",hardware,2026-01-07 15:48:52,67
AMD,ny81asb,"I would love for the 5800x3D to become affordable - I have a 5600x that Iâ€™d love to upgrade (not that I need it just yet), and hand down to my server running on a 2600 non-x.",hardware,2026-01-07 16:22:13,14
AMD,ny8j7i4,Iâ€™ll shed tears of happiness if the 5800X3D comes back,hardware,2026-01-07 17:42:53,4
AMD,ny9w3du,"Just ordered a 5700X to upgrade from my 3600.   The plan was to upgrade to AM5 later in 2025 but I waited too long. Decided to improve my AM4 system, and I really really really REALLY wanted a 5700X3D, but they are +300â‚¬ used.   So I ordered the 5700X before they went out of stock as well.",hardware,2026-01-07 21:15:04,4
AMD,ny9z4pa,"Come on, put zen5 on am4.  Do it!",hardware,2026-01-07 21:27:56,4
AMD,ny8gu2y,One fucking billionaire setting the world back a fucking decade,hardware,2026-01-07 17:32:14,19
AMD,nya2rbu,If they reissued the 5800x3d theyâ€™d sell a ton of them. Iâ€™d buy one,hardware,2026-01-07 21:43:35,3
AMD,nyahkrt,"I bet they could retrofit zen 4 into am4 if they wanted to. It may not be the easiest and it might not be the best, but it would be godsend in these trying times.",hardware,2026-01-07 22:50:29,3
AMD,nybvq7h,If they release the 5800x3d for cheap I'd buy it.,hardware,2026-01-08 03:07:55,3
AMD,ny8qvb1,I am using a 5800x3D as my server at home with a ton of RAM. Undervolted.  I am finding it difficult to load it up and use all of the horsepower it offers. AM4 is just absolutely slaughtering still.,hardware,2026-01-07 18:16:13,6
AMD,ny7xi9p,I think this kind of stuff is the best thing we could have realistically hoped for. Hopefully game developers pay attention and work on targeting more average builds.,hardware,2026-01-07 16:04:59,5
AMD,ny7s0m6,"Hello krumpfwylg! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-07 15:39:55,2
AMD,ny82lfd,"That'd be nice. I'd give my kid my 5600x3d possibly if I can get a 5800 one, or maybe get her the same/5700x3d",hardware,2026-01-07 16:28:07,2
AMD,ny9bdpa,"Ditch that, go with Zen 3+ for the desktop instead. Bring the efficiency improvements from the mobile Zen 3+ CPU's to the desktop, and eek out a little more performance.   6800X3D LET'S GO!",hardware,2026-01-07 19:45:30,2
AMD,nyd7ful,Hmm if true then it's quite nice. Have two AM4 builds with R5 5600 and I could see myself buying better models this year.,hardware,2026-01-08 08:50:06,2
AMD,nydxwnh,Zen 4 on AM4 and I'm sold,hardware,2026-01-08 12:30:22,2
AMD,ny7wbsd,With betterÂ  Zen 3 cpus availability comes greater demand for memory necessary to build new AM4 platforms. Higher demand = higher DDR4 prices.,hardware,2026-01-07 15:59:35,6
AMD,ny85ci3,"Its a 6 years old generation, yes its great for certain scenarios depending on what you already own, or if you can get all parts new for a very low price (not that likely). I hope people aren't buying it just on desperation and FOMO and not taking newer platforms into account. If its a good deal for the performance, then good. However, lower end parts of a new platform could also beat it easily in most scenarios, even if you might have to settle for for example less RAM or cores for now.",hardware,2026-01-07 16:40:31,3
AMD,ny8d8jx,"Would be great if they could backport newer Zen generations for AM4, but I guess that would only work easily, if they had put the memory controllers on the IO die.",hardware,2026-01-07 17:16:03,3
AMD,ny98ncg,"Good Lord, we really are going backwards.    And scum bags like Elon Musk promise the age of abundance and no more poverty.  What a bunch of lies.",hardware,2026-01-07 19:33:39,4
AMD,ny8cs5c,u/Strazdas1 what emoji should I use,hardware,2026-01-07 17:13:57,2
AMD,ny82gpt,"I guess some AIB would increase MB availability if this is actually true.  But would they be cheap MBs, mid-range, or top-end?",hardware,2026-01-07 16:27:31,1
AMD,ny8jt8b,"Hmmm, I have a 3800X, I wonder if I should upgrade it to get more life out of it? I do suspect it will be like in the past Iâ€™ll spend a few hundred and maybe buy another two years of use.",hardware,2026-01-07 17:45:33,1
AMD,ny8ox8j,Sure if they're priced appropriately.,hardware,2026-01-07 18:07:48,1
AMD,ny8sgwl,Dumb question but is there any reason they couldn't potentially release am5 chips that could function with ddr4 memory?,hardware,2026-01-07 18:23:11,1
AMD,ny9zlfa,I mean it's infinity fabric they could just add zen4 chiplets to the old controller,hardware,2026-01-07 21:29:52,1
AMD,nzefa43,A slow CES certainly brings out the clickbait articles. At least this is more plausible than NVIDIA adding more tensor cores to old hardware.,hardware,2026-01-13 18:59:58,1
AMD,ny9az4i,"At least package them with X3D and clock them back to 5800X3D if you are going to relaunch them.   Nothing wrong with 5600/5700/5800, but not having X3D added is like selling a bike with single speed cranks. It works well at slower speeds (mountain bike /gravel), but you can get much more top speed with it.",hardware,2026-01-07 19:43:43,1
AMD,ny8r41t,Hopefully they donâ€™t pull an Nvidia and give us just the 5600x or some other weird option and actually go with something the gaming community actually wants.,hardware,2026-01-07 18:17:16,-2
AMD,ny8hvm2,You canâ€™t tell me the industry canâ€™t put together a ram fab for 2-3 gens behind,hardware,2026-01-07 17:36:57,-2
AMD,ny8fy1c,True. Anyone with retired DDR4 can get some of their money back on it. If not what they paid for it. I just hope not too much of it has ended up in recycling/trash.,hardware,2026-01-07 17:28:14,24
AMD,nyaubtk,"The used DDR4 market hasnâ€™t gone up that much, itâ€™s reasonable. And also helps reduce e waste.",hardware,2026-01-07 23:54:16,5
AMD,ny9ctbw,"You'll get a 1700x and you'll say thank you, buddy",hardware,2026-01-07 19:51:44,4
AMD,nyehn37,At the settings and resolutions people actually game at a 5600X is just as GPU limited as a 5800X3D .,hardware,2026-01-08 14:22:30,2
AMD,nydif90,"Since they're aiming for upgrades of AM4 platforms, it would make sense to produce mainly top tier models. E.g. I have been eying 5950X/5900XT variants for a few years, and it would be nice if they came back in stock at a decent price, possibly at new/better clock speeds.",hardware,2026-01-08 10:30:02,1
AMD,ny9blng,5950X3D2 please.,hardware,2026-01-07 19:46:28,4
AMD,ny9ytml,Better hope there isn't a SoIC packaging shortage.,hardware,2026-01-07 21:26:38,1
AMD,ny814bw,Are you upgrading from ddr3?,hardware,2026-01-07 16:21:23,8
AMD,ny9n7po,I have one right now with 128gb of ram and the other has 64gb I gave 32GB to my coworker for 80 bucks a few weeks ago as a flex,hardware,2026-01-07 20:37:17,3
AMD,nya1i6s,Samsung or another company announced they were postponing their DDR4 production shutdown... only because an unnamed customer bought the entire year's worth of production run at a fixed price.,hardware,2026-01-07 21:38:05,2
AMD,nya1tcr,Or a backported Zen 4. It's not like they're going to sell many more DDR5 motherboard chipsets at this rate.,hardware,2026-01-07 21:39:25,13
AMD,ny9ey9a,"Conversely, they could use all of their dead-core chips and sell as many 5100x3d as they could produce",hardware,2026-01-07 20:00:55,11
AMD,ny8ftbj,YES!!!,hardware,2026-01-07 17:27:38,2
AMD,nyehu5d,I don't think they had you in mind lol. Not every product is aimed at you.,hardware,2026-01-08 14:23:30,7
AMD,nyaxypl,Wait till AMD releases a Ryzen 5550 and Ryzen 5650,hardware,2026-01-08 00:12:39,6
AMD,ny7wq14,"5900xt is really good if you need a 32t processor, even a 5700 would be a pretty big upgrade from a 3900 though  the single core is what makes it feel snappy",hardware,2026-01-07 16:01:23,22
AMD,ny7vtom,I built a 3700X PC right after the release of those processors in late 2019.  I've never had the same. damn. computer. for. 6. years. (now 5800X CPU and went from 2080S -> 3080 12GB),hardware,2026-01-07 15:57:20,16
AMD,ny8eppz,Went from a 3700x to a 5950x... and from 16gb 3200mhz to 32gb 3600mhz. (and sold my used cpu and ram).  Love this platform.,hardware,2026-01-07 17:22:41,2
AMD,ny7wdkz,3xxx to 5xxx is only worth if youâ€™re going to 5800X3D,hardware,2026-01-07 15:59:49,2
AMD,nya3ekc,>AM4 is the GOAT platform  AM4 is the whitewashed platform.,hardware,2026-01-07 21:46:24,-3
AMD,ny8jiqz,"Same here, I really want to replace my 5600x with a 5800X3D",hardware,2026-01-07 17:44:15,5
AMD,nyc1kzf,I bought one last year and I saw the prices again on AE had went up a bit since I bought mine.,hardware,2026-01-08 03:40:09,2
AMD,ny93ko8,"Ohhh it was more than one of them, lol.",hardware,2026-01-07 19:11:19,28
AMD,nya265x,"Promise to make millions of white-collar jobs obsolete, and also make consumer electronics increasingly unaffordable to the masses.  Cyberpunk 2077's Datakrash increasingly looks like a blessing.",hardware,2026-01-07 21:41:00,6
AMD,nyarf7a,Indeed.,hardware,2026-01-07 23:39:25,1
AMD,nyeoghp,AAA devs have been targeting PS5/XSX this whole time.  Nothing will change here.,hardware,2026-01-08 14:56:26,1
AMD,ny83ktf,I think a decent chunk of people are sitting on a system that already has the amount of DDR4 RAM they want but isn't maxed out on CPU power for what Zen 3 can offer.  Tossing something like a 5700/5800X3D into your existing system looks fairly attractive at present for that sort of user if they can bring those back at a reasonable price point. (and those still sell on Ebay *used* for more than their last retail price).  I don't know that there's going to be a ton of people looking to do a full new build AM4 system at this point if they don't already have RAM for it.,hardware,2026-01-07 16:32:33,14
AMD,ny7zb5f,"For better or worse, there are plenty of fast DDR4 in the used market, and CXMT has been making DDR4 for a while now and is a source of DDR4 modules that will at least fulfill demand in China, and thus alleviating demand elsewhere even if they don't make it out.  On top of makers now stopping to retire their DDR4 lines due to the shortage, I think that it would at least be better than DDR5.",hardware,2026-01-07 16:13:11,6
AMD,nybw0ow,There is a ton of DDR4 on ebay,hardware,2026-01-08 03:09:29,1
AMD,nycv8a7,One that conveys a massive disappointment and fear for the future of hardware.,hardware,2026-01-08 07:02:23,1
AMD,ny9meu5,"AMD would need to redesign the memory controller to be compatible with DDR4 and motherboard makers would have to make DDR4 AM5 motherboards. Easy, technically speaking. But then that redesign, tape-out, validation, etc. all takes time and costs lots of money. DDR4 can cost just as much as DDR5 and it doesn't seem like DDR4 supply will go anywhere but down, so this effort would only make sense for the budget-oriented people reusing the RAM they already own. And while the RAM situation is horrible, it's not going to last forever. It may take several years for the situation to ease if things continue the way they are. Or the bubble may pop next week and take memory prices down with it.  So the biggest consideration is money and whether AMD thinks it's worthwhile to spend it chasing a pretty small market that may no longer exist by the time product is available.",hardware,2026-01-07 20:33:43,4
AMD,nya677g,"Would make more sense to go the other way around, design new I/O die and then put Zen5 on AM4. The way you said would need new chipset, new motherboards and a new I/O die. Putting it on AM4, in theory all you would need is a new I/O die. You'd still be stuck with DDR4 and PCIe Gen4 and the other platform limitations on AM4/x570/b550/etc, but that's the tradeoff there.",hardware,2026-01-07 21:58:37,1
AMD,ny8wgkc,"I've held on to every PC part and SIMM/DIMM since the early 90s, someday my stockpile of PC100 SDRAM will make me rich!!",hardware,2026-01-07 18:40:29,16
AMD,nyeltb7,Last year I had 8 sticks of Crucial ddr4 (64gb in total) and sold them all.  Kinda wished I kept onto them.,hardware,2026-01-08 14:43:34,1
AMD,nyiigld,"I just sold my 5800X3D for $410 cash, b550 board for $90 cash, and 32GB DDR4 for $150 cash  Then I bought the 9800X3D+B650+32GB DDR5 bundle from Microcenter for $730 after taxes  Less than $100 out of pocket.   We're in a small window where it's actually a good time to upgrade if you live near a Microcenter.",hardware,2026-01-09 01:29:31,1
AMD,nyawhbu,"But that's more due to lack of demand and retailers happy to let their stored inventory dwindle as they predict demand will only further reduce. If people actually start buying it again it'll shoot up as new supply would be near nonexistant.  This is more for the last people who are still sitting on an earlier ddr4-based chip (or platform in the Intel case) to reuse that memory and give up on waiting for an entire platform refresh to ddr5.  I'm more surprised they're thinking of this for what is pretty much the x3d boost - as there's still plenty of stock of zen3 parts lacking that, and those prices haven't increased *that* much to suggest they're nearly out of stock.",hardware,2026-01-08 00:05:12,3
AMD,nya27so,"You can get those for $40 on eBay probably. And they can't even fire up the old factories for that cost. If they said Zen 3, so I'd hope it actually will be.",hardware,2026-01-07 21:41:12,4
AMD,nygah78,i have 5600x (pbo 200+mhz -23co scalar 1x 4.85GHz temps max with gaming 60 AIO) running with 32gb cl16 and 9070xt and im very happy with it all in a dan li lain h20,hardware,2026-01-08 19:11:56,1
AMD,nye35ga,"At one point it was rumored that Zen4 would come to am4. That would have been nice, but I suppose that won't save them process cost that 7nm does.",hardware,2026-01-08 13:03:38,1
AMD,nyafd9j,"at 250$ or lower , one can only dream",hardware,2026-01-07 22:40:07,3
AMD,ny82frm,"No, but 8GB isn't enough for me anymore for some of my builds. DDR4 will start becoming scarce soon",hardware,2026-01-07 16:27:24,5
AMD,ny82prt,"Exactly, there's still tons of people on Intel D4 platforms and people using older AM4 CPUs.  The best upgrade you can do right now is one that doesn't need RAM at all.",hardware,2026-01-07 16:28:39,2
AMD,ny8wkbi,5900XT3D would be the AM4 endboss CPU,hardware,2026-01-07 18:40:56,11
AMD,nyavqga,"5700x you mean. 5700 is gimped in gaming because itâ€™s a cut down APU, so it has way less cache.",hardware,2026-01-08 00:01:23,1
AMD,ny7ywiy,"I got mine in 2017 with first gen ryzen. The 3900X was a great upgrade. Currently have a 6700XT GPU. I feel no need to upgrade yet, which is good given the current hardware prices.",hardware,2026-01-07 16:11:21,8
AMD,nya6ych,"I've been running this same machine since Zen 1 release in 2017. Started with a 1600, got a free upgrade to a 1700 which I'm still running. Went from 16GB to 32GB DDR4 3200, and ran an 8GB RX480 up until this year when a close friend gave me their old RX 5700XT.  Currently hoping to upgrade to a cheap 3700X in the next year when finances permit. 5800X3D would be the dream but a 3700X is so affordable and would still be a very nice jump.",hardware,2026-01-07 22:01:54,2
AMD,nyalj9i,"My rig started as 3600X + GTX 1080 w/ 2x8gb. Now it's on its 3rd CPU (5700X, now 5800X3D) and 3rd GPU (3080 that died in November, now 5070 Ti) and has 4x8gb of the same RAM. I'll probably keep it as-is for at least another 2-3 years, probably longer. Getting 10 years out of one platform is insane.",hardware,2026-01-07 23:09:39,1
AMD,ny8ddn8,"Zen 3 without X3D is still a big step up for gaming from Zen 2, due to the unified CCD.  About 20% on average, with many cases of 30% plus.",hardware,2026-01-07 17:16:40,22
AMD,ny8h39f,3700X to 5950X here. Extremely worth it if youâ€™re using your PC for something other than gaming,hardware,2026-01-07 17:33:23,7
AMD,ny8eymk,I went from 3700x to 5950x (i do a lot of photo and video work). The change was incredible.,hardware,2026-01-07 17:23:48,4
AMD,ny7xskn,Or the 5950X if you need more cores.,hardware,2026-01-07 16:06:17,3
AMD,ny7wy8n,or 5900xt i turned my 5600x into a dedicated linux server and i want a 16/32 to go in that mobo,hardware,2026-01-07 16:02:25,1
AMD,nycvmc1,"AM4 had many, many issues with its releases that are now ignored and everyone pretends it was wonderful.",hardware,2026-01-08 07:05:36,1
AMD,ny9bd60,The DDR shortage is specifically Sam Altman's doing.,hardware,2026-01-07 19:45:26,21
AMD,nyhed9y,"Uh, definitely not. Games have been performing like the developers only have the very latest high performance GPU in their lab targeting 1440p/60FPS with that hardware.",hardware,2026-01-08 22:07:55,1
AMD,ny919a2,"lol. I always try to resell anything I'm retiring. That said, during the 2000s and early 2010s, tech moved so fast that 3+ year tech didn't retain much value. As a result, I have a framed shadowbox with a Pentium 1, an Athlon T-bird, 2 Athlon XPs, 2 Athlon 64s, and two Phenoms, lol.",hardware,2026-01-07 19:01:10,3
AMD,ny9uwno,r/VintageComputing or r/RetroBattleStations is calling!,hardware,2026-01-07 21:10:02,1
AMD,nyikdz8,"That's a 5800X3D, though. And also, it depends on your area.  If you live somewhere techy, the resale value will be *lower.* Because there's just more of them. (San Jose, CA)  My 5800X3D only went for $300~. No one wanted to give me more than $50 for my X570 board, and RAM hadn't skyrocketed yet. So I just gifted the board/32GB RAM to my SO's brother who was still gaming on a 6th gen Intel and a Radeon 7800XT. He slapped a 5700X3D in it and off he went. The 5700X3D he got for $200 2nd hand.",hardware,2026-01-09 01:39:48,1
AMD,nyf8lsb,"Also would likely require more engineering. My assumption is that when they moved to AM5 they got rid of some baggage, changed the memory subsystem, power management, etc. It's probably not just plug-and-play.",hardware,2026-01-08 16:28:11,1
AMD,ny88dx3,Buy it now then lol,hardware,2026-01-07 16:54:00,6
AMD,ny87cuk,"Are you using 2x4gb?  Just add 2 more, its like $10... The price of 4gb modules hasn't changed at all and wont, nobody wants them. Even 8gb modules are the same price as before and unlikely to change much. Its not costly to have a 32gb ddr4 system...",hardware,2026-01-07 16:49:24,4
AMD,nygjqab,"8Gb wasn't enough a year ago when RAM was cheap, your problem is your own not the markets.",hardware,2026-01-08 19:52:40,1
AMD,ny9yozs,5950GEX3D,hardware,2026-01-07 21:26:06,8
AMD,nyb994a,yes the x,hardware,2026-01-08 01:09:34,1
AMD,ny867ud,Running a 3700X with 64Gb RAM (I upgraded from 32Gb when DDR4 3200 was cheap a few years back and thought I'd wait another Zen generation) and a Radeon 9060 XT with 16Gb of VRAM... plenty of CPU power even for running gentoo (software installs are mostly built from source) and a Windows 11 VM for work...  I suppose I could upgrade the m/b from a B450 to B550 model to get PCIe 4 and faster USB and 2.5GBe but I feel no need to jump to a 5700X ... yet...,hardware,2026-01-07 16:44:23,4
AMD,ny8ltn1,"Same here, got an X370 board, a Ryzen 1800X and 16 GB of DDR4-3000 back in 2017. Then I upgraded to a 3800X and then to a 5950X and 32 GB of DDR4-3600. Still on the same, now 9 years old motherboard.  If AMD were to put out a 5950X3D I guess I could use it for another 5+ years.",hardware,2026-01-07 17:54:20,1
AMD,ny8n7xy,"Regular Zen3 is still great for gaming, it just gets overshadowed by X3D (for good reason). Hardware Unboxed recently did a comparison of several different Zen3 CPU's.  Also keep in mind many CPU gaming benchmarks are done at 1080p or even 720p with something like a 4090. In reality, for a budget DDR4 build you're more likely to use a much lower tier GPU, while running games at 1440p (1440p monitors are nearly the same price as 1080p these days) so the difference won't be as exaggerated.",hardware,2026-01-07 18:00:21,10
AMD,nyemfw7,"That's not a 'big' step up.  That's a pretty mild step up.    Most people dont upgrade their CPU every new generation, and that's basically all this is effectively.  The only real consolation here is that Zen 3 CPU's pass muster in modern games better than Zen 2 CPU's do.  So for that I can understand it, but you're only doing it to get over a line, not because it's actually a big improvement.",hardware,2026-01-08 14:46:39,2
AMD,nya3vqu,The irony of AMD's best AM4 gaming CPUs having a single CCD is so thick you could cut it with a knife.,hardware,2026-01-07 21:48:29,1
AMD,ny879on,"How's the 5900XT feel compared to the 5600X?  I'm still sitting on my 5600X and don't generally need more cores, but would still like a performance boost. Kind of an ironic situation to be in as the 5600X was originally the better buy at the time because it had the highest single core/thread clocks.",hardware,2026-01-07 16:49:01,1
AMD,nyd31jd,"So in terms of value and upgradability, what's a better platform in your opinion?   I had issues sure (first gen was flaky with ram), but as an early adopter still using my motherboard from 2017, with a 3900X, and still having an upgrade path, I've gotten my moneys worth.",hardware,2026-01-08 08:10:07,2
AMD,nyakwy2,Sam Altman caused all of the hype bullshit.,hardware,2026-01-07 23:06:35,5
AMD,nyil60b,"> My 5800X3D only went for $300  Rip, you could get around $410-$420 after shipping and fees on ebay today. They're currently selling for around $470-500 + shipping used (filtered for sold listings)",hardware,2026-01-09 01:43:53,1
AMD,ny883xf,I need 16G dimms. Looking to build a VM and docker Host,hardware,2026-01-07 16:52:46,0
AMD,nygq20n,"Dude, my use case for the machine changed. Mind your damn business, jackass.",hardware,2026-01-08 20:20:56,1
AMD,nyc1asb,Is this chip tryna graduate high school?,hardware,2026-01-08 03:38:33,3
AMD,nyf6izx,35w? Id be totally into it,hardware,2026-01-08 16:19:07,2
AMD,nyb2ni0,I don't think I get your point.  That's only true because the only V-cache models released on AM4 had a single CCD.,hardware,2026-01-08 00:36:05,1
AMD,ny89jje,"The 5900XT has twice as much L3 cache and higher clocks, it's a little better. If you don't need more cores I wouldn't bother.  I also wonder about the 5600x (and 7600x/9600x). 6C/12T doesn't seem much but they seem to do well anyway. I wonder at what point of background tasks the 8 core chips start to become a noticable improvement.",hardware,2026-01-07 16:59:11,3
AMD,ny8cn67,"oh i meant to say my server still runs the 5600 and i want a 5900, but id assume beefy for multicore apps and a little better on single core.",hardware,2026-01-07 17:13:20,1
AMD,nyd7jld,How are your USB ports and wifi/bluetooth?,hardware,2026-01-08 08:51:01,1
AMD,nyenell,"First gen was flaky for a number of reasons.  Another is that many people who bought boards for Zen 1 were shit out of luck if they wanted to upgrade to Zen 3.  Zen 1 was also fairly lackluster in terms of single thread performance, and main selling point was really just being able to have a 6 or 8 core CPU at a more affordable overall price point.  Which was great dont get me wrong, but Intel CPU's were still pretty clearly faster in almost all cases at the time for gaming.  Zen 2 was AM4's real arrival in my opinion, helped a lot by Intel stumbling off a cliff at the time.  But if you got in with Zen 2, then your upgrade path was ok, but it wasn't half as miraculous as people make it out to be unless you went for Zen 3 Vcache parts specifically.    AM4 was definitely good and worthwhile, but there's certainly a bit of overselling the narrative here.",hardware,2026-01-08 14:51:20,1
AMD,nyd3x2b,Right now? AM5. Upgradability is really a blown out or proprtion thing. Less than 1% of ethusiasts (let alone average users) will ever replace a CPU on same platform. By the time people upgrade they will buy a new socket.,hardware,2026-01-08 08:17:58,1
AMD,nyivukl,"Sell on eBay? No thanks. Too easy to get scammed. Cash in hand only.   Also, and I swear this is unrelated to my distaste for selling personal items on eBay, the last time I tried to sell something on eBay they banned my account. No warning, no nothing. The account that was over 15 years old, but had only purchased items. An item I took my own pictures for, wrote all the descriptions for. Nothing copy/pasted into the item listing.   Never could get that account reopened, and I had to use a fake name and address to open a new account to even so much as purchase with. They blacklisted ME, the person.",hardware,2026-01-09 02:40:49,1
AMD,nygjw27,You're just making stuff up now. You really didn't want to do all that stuff a year ago when RAM was cheap?,hardware,2026-01-08 19:53:22,1
AMD,ny8cvmi,worth recalling that the 5600x  held crazy OC records for a while so if it gets sluggish you can OC the tits off it and theyll take it so long as your VRMs on the motherboard are good  its gotta be my favourite cpu from recent years.,hardware,2026-01-07 17:14:23,3
AMD,nyd9g2o,My usb ports are fine. I use ethernet so no wifi card is installed.,hardware,2026-01-08 09:08:31,2
AMD,nyd97yh,"AM5 is significantly more expensive, both motheboards and CPUs. Its more performant yes. But I'm not asking for what is the most performant platform today is. I'm asking what other platform in history provided better value over time than AM4. Upgradability is important to me.",hardware,2026-01-08 09:06:25,3
AMD,nyjab9s,"Ebay is definitely not ideal, I've had people try to scam me before but thankfully turned out okay. Jawa prices arent too much less than Ebay, not sure how the experience is on there though.",hardware,2026-01-09 04:00:47,1
AMD,nygq8ss,No because I was planning to build a ddr5 based system with,hardware,2026-01-08 20:21:46,1
AMD,nydafq1,You must have gotten lucky then.,hardware,2026-01-08 09:17:40,0
AMD,nyjqicl,"No, you asked what is the best value today. and AM5 is best value.   Upgradability of socket is important to 0.1% of users if that, so you are an exception.",hardware,2026-01-09 05:45:46,-2
AMD,nyk3t6r,No I did not. You're derailing the discussion.,hardware,2026-01-09 07:34:12,3
AMD,ny4yjkq,Seems like it would make one heck of a NAS CPU.,hardware,2026-01-07 03:40:31,9
AMD,ny5t4x1,Mega-sick raspberry pi?,hardware,2026-01-07 07:19:35,2
AMD,ny9p2k9,"Cool, now give it RDIMM support. Still havent seen a Epyc 4004/4005 (AM5) RDIMM board",hardware,2026-01-07 20:45:24,1
AMD,ny4ql8t,just in time for OpenAI to announce they're out of stock until 2030,hardware,2026-01-07 02:55:40,-5
AMD,ny54o6l,Synology will use it in their NAS in 2035,hardware,2026-01-07 04:17:45,20
AMD,nyeptr6,These have 2x 10gbe controllers on them too.  So maybe Synology will finally upgrade from 1gbe.,hardware,2026-01-08 15:02:56,1
AMD,nxxeocb,Great.  Does this mean AMD will finaly stop pricing Strix Point as if it was made out of gold ?,hardware,2026-01-06 01:29:45,313
AMD,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,107
AMD,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,35
AMD,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,10
AMD,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,20
AMD,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,51
AMD,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,18
AMD,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,18
AMD,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,21
AMD,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
AMD,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,4
AMD,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
AMD,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,3
AMD,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,4
AMD,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,2
AMD,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-26
AMD,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-12
AMD,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-12
AMD,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-11
AMD,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-16
AMD,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,60
AMD,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,85
AMD,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,7
AMD,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,22
AMD,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,3
AMD,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,3
AMD,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-12
AMD,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-15
AMD,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,68
AMD,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,23
AMD,nxz85mh,"With everything happening around NVIDIAÂ´s price increases and AMDÂ´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,10
AMD,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,1
AMD,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,12
AMD,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
AMD,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,55
AMD,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only â€œArc Graphicsâ€.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arcâ€™s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,10
AMD,ny7ef2l,It's a mid-gen refresh of battlemage.,hardware,2026-01-07 14:33:10,2
AMD,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,50
AMD,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,20
AMD,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,8
AMD,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,6
AMD,nxy6cgs,"At best, itâ€™s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,1
AMD,ny0ce80,What do you mean that a refreshed Strix Point canâ€™t compete with an updated architecture?,hardware,2026-01-06 14:10:24,6
AMD,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,9
AMD,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,8
AMD,nxzf0cx,I think theyâ€™re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,6
AMD,nxxj6sm,Intel charts for their gpus have been pretty on point   Msi claw with lunar lake is one of the best handhelds,hardware,2026-01-06 01:54:01,61
AMD,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,10
AMD,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,6
AMD,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,14
AMD,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,15
AMD,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
AMD,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,65
AMD,nxxh29b,Example?,hardware,2026-01-06 01:42:33,15
AMD,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,29
AMD,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,18
AMD,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,5
AMD,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,22
AMD,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,21
AMD,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,28
AMD,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,35
AMD,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,14
AMD,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,1
AMD,nxy0jsm,Eh? It's a standard 128 bit memory bus.â€‹,hardware,2026-01-06 03:29:57,33
AMD,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
AMD,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,6
AMD,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,4
AMD,nxz7zsg,"Reddit isnâ€™t indicative of anything really, most casual laptop buyers donâ€™t even know what AMD is.",hardware,2026-01-06 09:07:48,9
AMD,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,33
AMD,ny7dxyt,AMD is planning on again using RDNA 3.5 on their next mobile chips as well.,hardware,2026-01-07 14:30:42,4
AMD,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,14
AMD,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,8
AMD,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,7
AMD,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,3
AMD,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,20
AMD,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,11
AMD,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,3
AMD,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xeÂ³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
AMD,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,4
AMD,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,1
AMD,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,22
AMD,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,19
AMD,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,25
AMD,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,2
AMD,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,25
AMD,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,12
AMD,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,7
AMD,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that itâ€™s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,16
AMD,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,2
AMD,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
AMD,ny0na4n,"Itâ€™s a super strong generational gain though, itâ€™s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,10
AMD,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
AMD,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,2
AMD,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,49
AMD,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
AMD,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,2
AMD,nybz77q,"I don't think that will really be an issue, laptops can be configured with soldered 8 channel RAM like AMDs Z2 extreme, or they can still manage easily with regular DDR5 6400Mhz sodimms, which run at 102.4GB/s  Plus the CPUs that have intels new B390 iGPU are 4P/8E CPUs, so I doubt there will be much issues from low ram speeds. Something like the Radeon 890M have done fine with such speeds",hardware,2026-01-08 03:26:50,1
AMD,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,5
AMD,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
AMD,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-1
AMD,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-19
AMD,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-20
AMD,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
AMD,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-6
AMD,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,10
AMD,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,3
AMD,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,78
AMD,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,15
AMD,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,8
AMD,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,40
AMD,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
AMD,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,4
AMD,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,30
AMD,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
AMD,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isnâ€™t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
AMD,ny0wejw,"> most casual laptop buyers donâ€™t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
AMD,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,23
AMD,ny7wgeg,This is unfortunate news  (â•¥ï¹â•¥),hardware,2026-01-07 16:00:09,1
AMD,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,7
AMD,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-2
AMD,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.Â   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.Â   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.Â    > and better process node   Very much unproven.Â    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,3
AMD,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,8
AMD,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,3
AMD,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,3
AMD,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,0
AMD,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,14
AMD,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,24
AMD,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,28
AMD,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,3
AMD,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,3
AMD,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,7
AMD,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,6
AMD,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,5
AMD,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isnâ€™t more performance difference.,hardware,2026-01-06 04:18:43,2
AMD,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,0
AMD,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
AMD,ny7dl7u,"Nvidia's graphics have shown to be more efficient for space than both Intel and AMD, so whatever they use it will likely be better than what Intel can currently put out.",hardware,2026-01-07 14:28:52,2
AMD,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,4
AMD,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,9
AMD,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide thatâ€™s suspect.  Itâ€™s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,5
AMD,ny7raf2,I think they meant that the chip is very pricey which sucks because the handheld is already low-margin otherwisr and can't be priced too high else it got undercut by its competitors.,hardware,2026-01-07 15:36:28,1
AMD,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
AMD,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,10
AMD,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,8
AMD,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,9
AMD,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,18
AMD,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,16
AMD,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
AMD,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,10
AMD,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,4
AMD,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,17
AMD,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
AMD,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,6
AMD,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,4
AMD,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-11
AMD,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-5
AMD,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,17
AMD,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,18
AMD,ny6k0v5,Except with UDNA it might be the first time over a decade AMD isn't phoning it in.,hardware,2026-01-07 11:22:51,1
AMD,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,5
AMD,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,6
AMD,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,-1
AMD,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,5
AMD,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,6
AMD,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,11
AMD,nxysgiw,Mobile rtx 4050-70 maxes out at roughly 90-100 watts due to voltage limitation,hardware,2026-01-06 06:44:37,7
AMD,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
AMD,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
AMD,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
AMD,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,8
AMD,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
AMD,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,22
AMD,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,8
AMD,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
AMD,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,26
AMD,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,8
AMD,nxy622y,"It needs dedicated GPU hardware to run faster, but theyâ€™ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,6
AMD,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,8
AMD,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
AMD,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
AMD,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,10
AMD,ny6ka8x,"Yeah but basically unusable on pre 40 series. But at least NVIDIA gives users the choice.  AMD should just stop the BS pretending and just enable the full FP8 model across RDNA2-3 with FP16 emulation. But it prob runs so bad that they won't, far far worse than DLSS 4.5 on 20-30 series.",hardware,2026-01-07 11:24:58,2
AMD,ny6ob4j,"It would be good if that is true, but so far ive seen nothing that would inspire me confidence in AMD. And yes i remember the AMD patents you posted last year.",hardware,2026-01-07 11:55:47,2
AMD,ny7ebjk,"Yeah, it's DLSS4>FSR4>XESS (Intel)>=DLSS3>XESS (fallback)>FSR3      quality wise.",hardware,2026-01-07 14:32:39,1
AMD,nxzj5en,"First custom design OEM are fast, here 4400â‚¬ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAXâ€¦ one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
AMD,ny5eqws,"When I talk about ""design targets"", I'm not referring to an arbitrary TDP. There are very specific decisions each SoC made that have tradeoffs at different power envelopes.   Also, the context was LNL which is an entirely different beast from MTL.",hardware,2026-01-07 05:25:06,2
AMD,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,4
AMD,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,3
AMD,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
AMD,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,3
AMD,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,3
AMD,ny6po5w,"If you're referring to the April dump, heck even the August dump (analysis of Kepler\_L2 patents) then that's not close to the complete picture. A lot of new patents have surfaced since that expand upon the design in many ways, but I'm waiting for the last RDNA5 to be made public before making a potential follow up post.  But regardless even if they fix HW situation completely they'll prob fail spectacularly with SW stack as they've done so far with FSR Redstone and FSR4 game adoption. Even hear a lot of people complaining about having to use Optiscaler, even in newer games.   Also NVIDIA will no doubt move the needle a lot nextgen yet again. They already did with DLSS 4.5 and DFG and something tells me that DLSS5 is gonna be even worse for AMD. They better prepare for what's to come.  Worst case it's a complete massacre. I can see the following scenario happening:  **HW:** NVIDIA invests all their silicon budget into fixing 5090 scaling bottlenecks (16 GPCs instead of 12, revamped scheduling etc...), fixes other problems with 50 series (redesign cachemem mostly) + goes Brr on ML and to some extent RT. Raster goes up 35-40%, everything else goes up multiple times.   Worst case ML HW gets bumped to 4-8X NVFP4 rate, although 2-4X sounds more likely.  **SW:** NVIDIA uses this new insane ML HW to make new DLSS models. DLSS5 goes all in on NVFP4 and is faster than DLSS4.5. DLSS5 SR and RR for 50 series + 60 series which is lightweight and fast on new GPUs (high FPS), and a new DLSS ULTRA SR for 60 series (released across stack but painfully slow for anything pre 60 series) striving for maximum Image quality. The smaller model will be better than DLSS4.5 and the big model another tier entirely (DLSS3 -> 4 leap easily on top of DLSS4.5).   They also make DRS compatible with DLSS SR and RR so users get greater flexibility here similar to DFG for framegen.   FG will also release in two versions one light and heavy. Will also work with Reflex 2. It's possible only the big model will be frame extrapolation + limited to 60 series. Should make FG result in lower ms instead of higher + overall image quality far superior and basically all issues solved up to at least 4X.   Oh and a flood of MLPs and a demo showcasing the absurd visuals the 6090 can push. Moves goalpost past ReSTIR PT and will look borderline offline render quality. Very close to Blender renderers. IDK how they'll do it but MLPs are borderline magic, so prob doable.  Thinking about it more you're prob right and even if RDNA5 HW is amazing even beats 6090 in PT, a DLSS5 feature suite this impressive + moving goalpost to MLP based neural rendering will make RDNA5 irrelevant. As always SW and marketing will kill any momentum from HW side. Really hope I'm wrong but don't think so.  Sorry for the rambling.",hardware,2026-01-07 12:05:46,2
AMD,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
AMD,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.Â    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.Â    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,1
AMD,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
AMD,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
AMD,nycu2b3,"I enjoy reading your optimism. I hope it all comes true, but it sounds a bit too good to be true given the recent hardware developements. The 5090 scaling issue is that we stopped resolution scaling. If you go beyond 4k the 5090 scales a lot. VR resolutions report the 5090 being as much as twice the framerates of 4090.",hardware,2026-01-08 06:52:50,1
AMD,nydd2fq,I've rewritten prev reply to provide more info.  I'll also link the scheduling patent here in case anyone reading this thread is interested: [https://patents.google.com/patent/US12153957B2](https://patents.google.com/patent/US12153957B2)   It sounds like gains in workgraphs scenarios will be be even greater.,hardware,2026-01-08 09:42:10,2
AMD,nyd5bcu,"Yeah prob not realistic. I just tried to outline a nightmare scenario for AMD. As for the RDNA5 stuff we'll see how good it ends up being.  Agreed serious issues fs. RTX 5090 scheduling is brain dead. 16 SM GPCs, one central scheduler for 170 CUs. The smaller the internal res the harder it is to keep things going. Someone smarter than me could prob make a core scaling efficiency chart for different resolutions clearly showcasing how RT > raster and derive different formulas for 1080p, 1440p, 4K etc... . There's simply no reason why it has to be this bad moving forward.       But it's also interesting to entertain that RDNA5 could be a nightmare for NVIDIA. If NVIDIA doesnâ€™t fix scheduling AMD's nextgen could be a real nightmare scenario for them. The modular and decentralized scheduling will be a gamechanger and based on what patents have said scaling is almost perfect and can scale to [arbitrarily large configurations](https://patents.google.com/patent/US12153957B2), yes they used that wording. AT0 will function like 8 x AT4 instead of running into massive scaling issues. In fact based on what the patent has said it might be even better. Consider each scaling domain with a local cache independent of the L2, where the global command processor only acts as a distributor of work, not an orchestrator. Gains will be observed across the stack but expecting IPC gains to scale with number of CUs. Is this a big deal for RT and 4K native? Yeah but even more so for lower res gaming.    And assuming they reduce CPU overhead even further in new uarch AMD will easily take the max FPS crown although I suspect NVIDIA can finally address their driver overhead issue after booting Maxwell-Pascal. Weâ€™ll see who comes out on top in CPU overhead nextgen.  I thought most of that gain vs 4090 was due to extra BW? But yeah high end perf scaling falls apart at sub 4K internal res.",hardware,2026-01-08 08:30:40,1
AMD,nyjq3tx,"I dont think much can be done with overhead. AMDs overhead is already small, basically letting the API go directly to GPU as it is. While for Nvidia side, isnt most of the overhead related to how Nvidia handles DX12? in that case i dont see it going away for a long time.",hardware,2026-01-09 05:42:46,2
AMD,ny7nu04,Me plugging a keyboard into my keyboard because the stock keyboard is bad: ðŸ¤¡,hardware,2026-01-07 15:20:03,88
AMD,ny7jic2,"I think there could be customers for this. However, it fundamentally looks like a terrible keyboard, which kind of kills it as a product.",hardware,2026-01-07 14:59:05,28
AMD,ny85hnx,"Built in hand warmer on a keeb? Damn, someone gonna be warm nxt winter",hardware,2026-01-07 16:41:09,7
AMD,ny7414t,"This is amazing for office work, presentations and similar but I fear it's going to be expensive.",hardware,2026-01-07 13:37:07,23
AMD,ny7d81s,I think this is actually a great idea. Screens are everywhere. Might be the ultimate portable pc.,hardware,2026-01-07 14:26:57,10
AMD,ny7n6zp,"I miss the innovation of the tech industry in the late 90s and early 00s, because this isnâ€™t it.",hardware,2026-01-07 15:16:59,5
AMD,ny81uat,"makes sense for a raspberi pi, but if it's over 200 it's doa",hardware,2026-01-07 16:24:42,5
AMD,nyfzupg,They have a whole system inside yet they choose a limited layout to make it less ergonomic and have less space for components at the same time. Good job HP. Making products that others will do better and cheaper.,hardware,2026-01-08 18:26:20,1
AMD,nyh04e9,"Lol I thought they were going to say they were putting the new AI max chips in these, I  was going to fucking explode if they didn't put it in a laptop. P.S. they did it in the newer Asus Tuf A14.",hardware,2026-01-08 21:05:49,1
AMD,nyjsmw8,Up to 64GB ram? By the time this is released the minimum price will be $4k just for memory,hardware,2026-01-09 06:01:54,1
AMD,nyju5se,Pretty cool for an AR setup. Imagine just bringing this and ar glasses and you have the ultimate portable workstation,hardware,2026-01-09 06:13:59,1
AMD,nyycczj,It wouldâ€™ve been more useful with an integrated trackpad instead of the num keys.,hardware,2026-01-11 10:59:56,1
AMD,ny9mcap,"This would be great when I spill my coffee on the keyboard, now its the entire computer.  Obviously it isn't for regular users.",hardware,2026-01-07 20:33:24,0
AMD,ny729th,"Hello Touma_Kazusa! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-07 13:27:09,-1
AMD,ny98t2w,Keyboard inception,hardware,2026-01-07 19:34:20,9
AMD,ny9mxq8,It looks decent to me but I wonder if with it being user replaceable I wonder if it could be modded to have mechanical keys.,hardware,2026-01-07 20:36:04,5
AMD,nyaiutd,No different than plugging a keyboard to a laptop for the same reason,hardware,2026-01-07 22:56:34,2
AMD,ny91wku,"But it doesnâ€™t. In a workplace, people donâ€™t care, and even go out of their way to buy even worse keyboards that only fit an aesthetic.",hardware,2026-01-07 19:04:01,7
AMD,nz4lg7h,"I saw a reviewer trying the keyboard, and he mentioned that it felt marginally better than a regular laptop keyboard. It makes sense, laptops have to sacrifice some ergonomic features of keyboards. This might not be as bad as people say.",hardware,2026-01-12 07:53:48,1
AMD,ny7afk6,I just don't see it. Its a laptop except it has no built in screen. That's just a worse experience all around. For the use cases described by you and the article... basically everyone will just opt for a laptop.,hardware,2026-01-07 14:12:13,39
AMD,ny85ted,My company has a fleet of aging thin clients. These are quite literally drop in replacements since the screens are fine.,hardware,2026-01-07 16:42:36,7
AMD,ny7cqo5,"Would it really be that different compared to a 12/13"" laptop? Or even a tablet?",hardware,2026-01-07 14:24:25,2
AMD,ny7r46b,It could make for an amazing home entertainment center though.,hardware,2026-01-07 15:35:38,2
AMD,ny76j1a,"""500 bucks for a keyboard?!""",hardware,2026-01-07 13:50:52,1
AMD,nycjlrv,"Yup, especially with how good USB-C docks can be these days. One cable to plug in, completes power, display out, keyboard, mouse, microphone, etcâ€¦",hardware,2026-01-08 05:33:29,1
AMD,nyci1zv,"It would probably be easier to â€œdeconstructâ€ a mini PC and mod it onto your preferred mechanical keyboard base, than to mod mechanical keys on this based on the slimness.",hardware,2026-01-08 05:22:40,7
AMD,nz5j3x8,"If it's as good as a ThinkPad's, I would have no complaints tbh.",hardware,2026-01-12 12:47:57,1
AMD,ny7btsz,Yeah I think so too and it makes this product meaningless.,hardware,2026-01-07 14:19:39,4
AMD,ny9n6py,"Me who always plugs my laptop into my desktop setup without ever flipping it open to use the screen...  A surprising amount of people will enjoy not needing to carry that screen around between home and work, as they just need a portable databox, and a laptop is actually the worse experience.  There's a definite place for this product in the market.",hardware,2026-01-07 20:37:10,9
AMD,ny9b8sj,"This isn't meant to replace laptops but thin clients often used in business offices.Â  One downside I see there is that VESA mounted thin clients can be more easily locked in place to prevent theft or tampering.Â Â    I have seen a surprising number of business users who are given a laptop... and also monitors and a docking station, and the laptop is only ever used plugged in.Â  This keyboard pc to be useful to them and not waste money on an unused laptop screen, but they would first need to recognize the issue",hardware,2026-01-07 19:44:55,5
AMD,ny9t7v1,Could be really neat combined with next gen AR glasses if they have sufficient pixel density for text-based workspace applications.,hardware,2026-01-07 21:02:56,1
AMD,ny77ozb,500 bucks won't even cover the 64GB RAM in this. It wouldn't surprise me to see $1500 for this.,hardware,2026-01-07 13:57:17,11
AMD,ny78qlp,Can easily spend this on a custom mechanical keyboard,hardware,2026-01-07 14:03:04,3
AMD,ny8vakh,norbauer obliterates the chat,hardware,2026-01-07 18:35:26,2
AMD,nycjgn8,"Nah this density is very good and in a perfect shape, what will most likely happen is people 3D print a mechanical keyboard compatible case with it.",hardware,2026-01-08 05:32:29,-1
AMD,nycjusm,"Keep in mind the screen, and surrounding enclosure and hinges for durability add a lot of weight.",hardware,2026-01-08 05:35:15,2
AMD,ny9o5k1,Pretending that the screen somehow makes a laptop harder to carry is wild stuff.  I'm confident in saying this product will never see a second generation.,hardware,2026-01-07 20:41:26,3
AMD,ny9beqc,"This is definitely not meant to replace thin clients. Look at the specifications of it....  Also, the ""wasting money on a screen"" is really a very minimal expense that the moment a user does need to move their computer pays for itself instantly.",hardware,2026-01-07 19:45:38,6
AMD,nyb9p68,"It's a huge amount of people.  At work, we have so many of these little databoxes that we plug into a monitor and keyboard. Small, portable, gets the job done, easy to replace and fit in all situations.  These would be a very nice drop-in replacement for those. These aren't designed for individual buyers, though they certainly won't mind the sales. It's more targeting corporate deployments where a company will want to buy hundreds to thousands or more, have them easily stored in a warehouse that can quickly be deployed when needed. When a computer for work breaks, I let my project manager know and they'll contact logistics to organize a swap, happened a few times last year and recently just had to swap a computer out due to a broken power button. These are super convenient for such cases, especially with office work where they take way more abuse as they remove the fragile screen + hinge on laptops if they're gonna be used in a situation where nobody cares about them.  My main concern is the keyboard. In office, keyboard and mouse are replaced all the time, so having one tied to the rest of the hardware is problematic. Companies often favour buying the cheapest en-masse and replace when broken, as hiring someone to fix expensive stuff that nobody cares about is not cost efficient. Yes you can just plug a new keyboard in, but I do have a bit of e-waste concern if some companies simply decide to throw them out due to speed/efficiency, kinda like how mostly fine laptops that have some broken bits are sadly discarded all the time by companies.  Then again, throwing away this is less wasteful and cost efficient than throwing away a laptop, so maybe it's still net better.",hardware,2026-01-08 01:11:56,4
AMD,nyedpkl,The laptop screen is definitely useful once you have to go into a conference room. The cost savings might be pretty marginal given the economies of scale for laptops,hardware,2026-01-08 14:02:04,1
AMD,nyedy6f,"This is aimed at businesses. Nobody buying these new will be interested in modding to that extent.   Not to mention that if you're going to replace the only thing that makes this PC special, there's no reason to start with it. Just grab any laptop motherboard or a mini PC",hardware,2026-01-08 14:03:19,3
AMD,nycnrt3,Yee,hardware,2026-01-08 06:03:40,2
AMD,nyciqs6,"Itâ€™s 760 grams or something. Thatâ€™s quite significant to certain people. I mean itâ€™s not hard to carry, but any lighter weight in my backpack is always welcome.   I hear your skepticism though. Letâ€™s see if this finds a market. I hope it does, because one of the nice things about PCs is the wide range of form factors â€” laptops, convertibles, mini PCs, compute sticks, to full desktop builds â€” and itâ€™s nice to have options in the market.",hardware,2026-01-08 05:27:25,3
AMD,nycivz4,"The keyboard is replaceable according to the press release, but no details on how.",hardware,2026-01-08 05:28:25,3
AMD,nyk6npe,"Yes, definitely true for the people who might end up presenting or needing to work on the laptop during a meeting.Â  But offices also often have many people who will never need that and can use cheaper devices",hardware,2026-01-09 07:59:18,1
AMD,nyxyfy1,"Looking at some videos with light disassembly, it seems pretty modular/repairable!",hardware,2026-01-11 08:50:50,1
AMD,nxys8b4,I had the fastest gaming CPU in the world for 2 weeks,hardware,2026-01-06 06:42:37,37
AMD,nxyg77q,"Could be worse, you could be a threadripper customer.",hardware,2026-01-06 05:08:52,47
AMD,nxyibmy,"I donâ€™t quite understand HUBâ€™s hubbub here.  AMD stumbled upon a handful of higher-binned chips and decided to spin them off into a slightly higher-tier SKU.  Thatâ€™s not exactlyâ€¦ unorthodox.  The original Ryzen 7 1700 not only had the 1700X, but also an even more â€œpremiumâ€ 1800X variant, the latter of which (seemingly) had a very short production run (Iâ€™ve yet to see one in the wild).  But sure, letâ€™s write a whole song and dance about it.   Thatâ€™s what modern tech journalism is about, I imagine.",hardware,2026-01-06 05:24:08,68
AMD,nxz9oh5,"back in the days intel and amd released the same cpus but the higher ends usually had simply higher frequency so for avg joes it was easy peasy, but for enthusiasts it meant they could buy the lowest sku of that u-arch and just oc it.  So it is basically a return to the old, just oc your old 9800x3d and u are there.  mine is ticking at 5.6 all cores locked all the time in all gaming worklaods.",hardware,2026-01-06 09:24:16,4
AMD,nxyo3y8,I don't see how it's milking unless it's more expensive **and** they stop selling the original. As long as they keep selling it alongside then who cares,hardware,2026-01-06 06:08:30,20
AMD,ny0g0qc,"I don't understand how this is milking - at least compared to the amount of milking that goes on across the industry, where paying a premium for largely irrelevant performance gains is the standard, not the exception.  What I really wonder is what amount of overhead these chips will have - I'm guessing they won't be able to keep up with the PBO / CO capabilities of the average binned 9800X3D, making them even less appealing to enthusiasts.  Looking forward to seeing what they are capable of.",hardware,2026-01-06 14:29:53,4
AMD,nxykroo,HUB continues to milk AMD.,hardware,2026-01-06 05:42:26,20
AMD,ny187xs,What does this equate toâ€¦ +3 fps @4k max settings?,hardware,2026-01-06 16:44:33,2
AMD,nxyn5gh,If they would have simultaneously launched a 9750x3D for the lower binned parts I wouldn't be this disappointed.,hardware,2026-01-06 06:00:49,2
AMD,nxyit9q,Plenty of people will pay for +7%.,hardware,2026-01-06 05:27:42,2
AMD,ny13cu1,"I just bought a 9800x3D it's still in the box, should I return it? Is this even going to be a noticeable difference?",hardware,2026-01-06 16:22:22,1
AMD,ny1vaqd,It's a little refresh within the same gen and it's not like it's targeting 9800x3d users anyways so... Milking??,hardware,2026-01-06 18:28:14,1
AMD,nxyhlu7,"JSH said in his presentation that Moore's law has stopped delivering the goods on speed so Nvidia has to redesign every part of their whole system stack to get performance increases. Â That maps onto AMD's dilemma with these CPUs perfectly. Â They have run out of ways to make a single generalist part give big performance increases at the same price point. Â Nvidia's trick was to get you to buy a $400 GPU on year 1, a $800 GPU on year 3, a $1600 GPU on year 5, etc",hardware,2026-01-06 05:18:55,-5
AMD,nxzg285,Interested to see if it has the random combustion issues as the last one.,hardware,2026-01-06 10:23:59,0
AMD,nxykpaj,"Yea they did, this â€œmilking gamersâ€ thing is so odd. Itâ€™s like saying Nvidia is milking when releasing the super series.   Itâ€™s replacing the 9800X3D and making it better.",hardware,2026-01-06 05:41:56,67
AMD,nxykm5q,The internet is full of impatient people who don't understand how things work,hardware,2026-01-06 05:41:16,10
AMD,ny1v5cs,Also like it's normal to have multiple of the technically same configuration but at different clock speed for CPU. Intel and amd have done this for decades,hardware,2026-01-06 18:27:34,1
AMD,nxzyc9d,"There's a weird round of negative sentiment about AMD right now.   On the GPU end, people are pointing to the new DLSS 4.5 and saying, ""Look at this! Nvidia is supporting all of their GPUs with this while FSR4 only works on RDNA4!""   Mind you, RTX3000 GPUs that were new in 2022 have never seen any sort of frame-gen support from Nvidia. You had to rely on AMD's FSR3 or Lossless Scaling if you wanted that.   And multi-framegen (including the DLSS 4.5 flavor) are for the newest Nvidia cards only.",hardware,2026-01-06 12:48:19,-10
AMD,nzgnv3i,>3090ti buyers,hardware,2026-01-14 01:41:05,2
AMD,nxzlzvu,Never again.,hardware,2026-01-06 11:15:58,11
AMD,nykltd4,What was wrong with threadripper?,hardware,2026-01-09 10:17:32,1
AMD,nxysnln,"Honestly, I havenâ€™t been able to watch alot of the hardware YouTubers rage bait the last few weeks as all their videos are â€˜is gaming deadâ€™ â€˜is PC hardware overâ€™ â€˜RAM destroying PCâ€™ etc. I feel like itâ€™s the same content over and over from a hand-full of YouTubers saying the same thing.",hardware,2026-01-06 06:46:17,27
AMD,nxyt7aq,"Was the 1800X rare? I had one from around 2017-2023 in my main pc, replaced it with a 3950X. I did ball on basically an all in day 1 AM4 build because I was upgrading from like a 2nd or 3rd gen i5.",hardware,2026-01-06 06:50:55,10
AMD,nxyrhi4,"> The original Ryzen 7 1700 not only had the 1700X, but also an even more â€œpremiumâ€ 1800X variant, the latter of which (seemingly) had a very short production run (Iâ€™ve yet to see one in the wild).  I don't recall them being launched after a year. This seems more like milking with the 9800X3D likely getting discontinued and AMD retaining the same price bracket.",hardware,2026-01-06 06:36:18,6
AMD,ny1vcx7,2600k 2700k 4700k 4790k. 10900k 10850k. Etc,hardware,2026-01-06 18:28:30,2
AMD,nxylozh,"> I donâ€™t quite understand HUBâ€™s hubbub here  You see, they're afraid of people calling them the amd channel so they sometimes do the ""we have to shit on amd for consumers"" kinda thing. It's just a show to present their ""fairness"".",hardware,2026-01-06 05:49:26,16
AMD,nxyn4tl,"To put it as mildly as possible, HUB isn't that great. The 321URX burn-in test is helpful, though.",hardware,2026-01-06 06:00:40,7
AMD,nxytq8z,">AMD stumbled upon a handful of higher-binned chips and decided to spin them off into a slightly higher-tier SKU.  It's not even a new bin, it's just the VCache die bin that goes into the 9950X3D. You could technically already make a ""9850X3D"" by just disabling the other CCD.",hardware,2026-01-06 06:55:24,2
AMD,ny30toh,If this video was about Intel KS chips no one would have any problems with it.,hardware,2026-01-06 21:37:59,1
AMD,nxzvabn,"Itâ€™s a fan channel for fan bases, like many others",hardware,2026-01-06 12:27:32,0
AMD,nxytydo,"No money in just a normal preview, HUB since the VRAM outrage discovered that rage videos get more views than normal videos.",hardware,2026-01-06 06:57:19,-7
AMD,nxyqx8g,"They are gonna be more expensive. As to whether or not theyâ€™ll stop production on the 9800X3D remains to be seen, but Iâ€™m thinking theyâ€™ll slow down the production some.",hardware,2026-01-06 06:31:36,13
AMD,nxyuxh9,"> I don't see how it's milking unless it's more expensive and they stop selling the original.  Yes, it's more expensive.  Why wouldn't they stop producing the original? Or at least slow down the deliveries? Seems like a no brainer, you can sell the same CPU but slightly faster for higher MSRP again if you make the older cheaper model scarce.",hardware,2026-01-06 07:05:46,2
AMD,ny1h9to,At 4k max you probably will gain nothing,hardware,2026-01-06 17:25:38,2
AMD,nxynrkv,9849X2D,hardware,2026-01-06 06:05:43,4
AMD,nxyoblz,"It ain't 7%, more like 3-5.",hardware,2026-01-06 06:10:14,19
AMD,ny1h77z,Wouldnâ€™t even be worth the effort to return the one you have now.,hardware,2026-01-06 17:25:19,6
AMD,ny24urt,When it's 2-4% faster in certain cherrypicked scenarios there is probably a lot of good 9800x3ds that are better than bad 9850x3ds.  These are margin of error type percentages,hardware,2026-01-06 19:11:05,3
AMD,ny1yo8a,"I decided not to do this two weeks ago. Like yeah it will be better, but also probably hotter... and they will release AMD Zen 6 in Q4 2026 or Q1 2027... and to deal with return system (i ordered it from another country with a small discount). Nah.",hardware,2026-01-06 18:43:31,2
AMD,ny2uts2,Let the milking begin!,hardware,2026-01-06 21:10:36,2
AMD,nxykire,"What? They didnâ€™t run out of ways.  No itâ€™s because CPUs for AMD have ~2year release cadences and that AMD needed something to â€œkeep the crownâ€ from Intels 2026 offerings until they can get Zen6 parts available.   Same with laptop, refresh the lineup with better clocks, improved firmware etc until the next gen CPU Engine is ready.   For Nvidia, itâ€™s because they do yearly GPU releases, but their other components typically are kept and used for multiple years.  This just happens to be the year that â€œeverythingâ€ in their system is going new.  Next year Nvidia will keep everything but the GPUs.   AMD too is making all new chips for their Helios platform using MI450s, Venice Zen6, and Vulcano AI NICs along with new Scale-Up and Scale-Out switches from a switch vendor.",hardware,2026-01-06 05:40:32,13
AMD,nxyko09,"This has nothing to do with AMD (or perhaps TSMC) hitting a silicon barrier.  Itâ€™s simply silicon binning, where higher quality dies are allocated to higher-tier SKUs, a concept HUB seemingly isnâ€™t familiar with.  Besides, N2 is a major leap over the ongoing N4, and while itâ€™s true that Mooreâ€™s Law is no longer in effect, N2â€™s transistor density advantage over N4 is seemingly sufficient for AMD to not only improve IPC on the upcoming Zen 6 (which almost always costs transistors) but also increase the core count by four per CCD.",hardware,2026-01-06 05:41:39,13
AMD,nxykjow,Moores law was never about performance...,hardware,2026-01-06 05:40:44,-3
AMD,ny0r7ip,"The absolute highest outlier is 6% Even in games they cherrypicked to show benefit most are less than 3%. In some cases like battlefield 6 literally 0%  This is AMDâ€™s testing which I wouldnâ€™t call rigged, but itâ€™s going to be the absolute best results they can put forward.  I donâ€™t really care other than AMD constantly makes the naming scheme awful for uninformed consumers.",hardware,2026-01-06 15:26:14,5
AMD,nxyol8e,"The most positive reaction to a Super refresh was that ""this is closer to what they should have released in the first place"".  There has been calls of milking customers with Super releases before too.",hardware,2026-01-06 06:12:22,57
AMD,nxzk4di,"Super series *usually* gives us more than just a minor clock boost, at least.  And tend to provide a general value boost by slotting in at previous price points, rather than stacking on top.   Also, people never had any problem criticizing Intel for milking people with similar sorts of 'refresh' releases.  Dont see why AMD should be off-limits.    The pricing will be the main thing.  If this thing comes with a somewhat tall premium and costs $550+, then yes, it's pretty much the definition of milking consumers.",hardware,2026-01-06 10:59:52,27
AMD,nxys3t7,"Well super series have historically given meaningful improvements in value.   Like the 4070S was around 20% faster than a 4070 for the same MSRP. The 4070 TiS was 10-15% faster than a 4070 Ti at the same MSRP, and the 4080S was around 3% faster for $200 lower MSRP.   That's different than releasing an overclocked CPU with insignificant performance improvements at a higher MSRP.",hardware,2026-01-06 06:41:32,26
AMD,nxyqoec,Theyâ€™re also making it more expensive.,hardware,2026-01-06 06:29:31,13
AMD,nxyvqpg,"Audience loves the, ""greedy corpos are cheating you,"" angle though.",hardware,2026-01-06 07:12:59,26
AMD,nxzc81l,"Its replacing 9800X3D and it's resetting the price for minor clock improvements (maybe 2-3% faster), which means 9800X3D will be discontinued, soar in price - so either 499 for 9850x3D or around the same (maybe more for 9800X3D new/second hand.  It's milking at its finest.",hardware,2026-01-06 09:48:41,13
AMD,ny0z5ol,they slightly typoed the headline. Youtubers Milking gamers slightly faster.   Its like you said nothing its fluff but they need that ad revenue,hardware,2026-01-06 16:03:05,5
AMD,ny05n9u,>  Itâ€™s like saying Nvidia is milking when releasing the super series.   thats what this sub said when the 4000 series supers released...,hardware,2026-01-06 13:32:54,2
AMD,ny1j28j,The sad thing is now is that every 9800X3D will be obsolete /S,hardware,2026-01-06 17:33:51,2
AMD,nxz35kb,Replacing it outright at a higher price point would qualify.,hardware,2026-01-06 08:21:22,2
AMD,ny1da7h,"Is it replacing the 9800X3D? will it be the same price? According to Tom's hardware it's not:   [https://www.tomshardware.com/pc-components/cpus/amds-ryzen-7-9850x3d-promises-7-percent-uplift-over-ryzen-7-9800x3d-amd-fights-itself-with-new-fastest-gaming-processor](https://www.tomshardware.com/pc-components/cpus/amds-ryzen-7-9850x3d-promises-7-percent-uplift-over-ryzen-7-9800x3d-amd-fights-itself-with-new-fastest-gaming-processor)  I'm Not saying anything on the merit of the product, as it's not out yet for reviews and I don't know the price, but if it will be considerably more expensive (like the KS models from intel) then I can see why it will be ""milking gamers""  for the extra few % of performance.",hardware,2026-01-06 17:07:29,1
AMD,ny4panv,If the super series was 2% faster then yes?  What would you call milking a consumer?  When intel spent a decade releasing 4 core cpus with 5% increases was that not milking their consumers?,hardware,2026-01-07 02:48:40,1
AMD,nyedgbp,"so gawd damn true, especialy because i could have told ya  that its a meaningless difference but then again this is why im hanging with what TechDeals said that ppl cling too much onto Benchmarks vs Realworld Experience which is kinda lame now specialy the corcount is ABYSMAL, 550+ USD for a friggin 8 core, i think i cant see straight \^\_\^ 8c is deffinelty the new Quadcore stagnation Era for sure   good thing i went 9950X3D back in August 2024 and just be done with it :)",hardware,2026-01-08 14:00:44,1
AMD,ny00ic5,The upscaler part is what everyone is interested in.,hardware,2026-01-06 13:02:13,7
AMD,ny1buwn,"> There's a weird round of negative sentiment about AMD right now.   There are pretty global round of negative sentiment going on right now. It's a general economy thing. Consumer are feeling prices rising and ready to lash out at everyone. Just about every upvoted youtube video in this subreddit can be summed up by ""[company] is screwing you so hard"".   ""Milking"" gamers by releasing a new product is ridiculous. Don't want it? Don't buy it. It's not like they are taking away your old CPU if you don't agree to pay the new price.",hardware,2026-01-06 17:00:57,2
AMD,ny1omor,"Naturally you were downvoted for this. How depressing.  For what it's worth, this has been going on for many years now. It's just *constant* excuses.",hardware,2026-01-06 17:58:45,-2
AMD,nysqpgz,"They abandoned the first round of Theeadripper chipsets earlier than most people expected, for one.",hardware,2026-01-10 15:21:46,3
AMD,nxz4thi,"Plenty of reasons to not have the highest hopes for consumer PCs at the moment, but reddit and youtube somehow still manage to take it to a comical degree. Avid techtube watchers must be living in the end times right now",hardware,2026-01-06 08:37:20,15
AMD,ny23pf9,Do you disagree? DIY PC gaming as a hobby has been a mess for like 8 of the last 10 years.  Do you want them to say this is good?  Or to benchmark GPUs that came out a year ago? Or do custom build videos and just ignore that the RAM kit cost as much as the GPU?,hardware,2026-01-06 19:05:47,5
AMD,ny316pk,"People are eating it up. Itâ€™s rage bait. Just look at the comments on GeForce Now threads, which actually launched 10 years ago. People are mad about a decade old service they donâ€™t even use.",hardware,2026-01-06 21:39:39,5
AMD,nxz51sm,"Yeah it was pretty rare, you were just way better off getting a 1700X or later the 1700 for way less money and like 95-98% of the same performance. The only area where the 1800X shined was clock speed and even then it basically capped at like 4.0-4.1 GHz on golden samples. You were just better off buying a 1700X and OC'ing it to 1800X clock speed and pocketing the difference or spending it on more RAM or something else.  Notice how AMD basically doesn't allow for a slower speed variant with the same cores anymore on the high end like they used to? If you want a 9950X or 9950X3D you have to buy it. Only lower end SKUs like the 9600X will ever see a 9600 or 9500X with similar specs other than clock speeds. AMD might do a refresh of say the 5950X and make a '5950XT' but that's basically just a new stepping and it releases pretty much late into the lifecycle where it makes no sense to wait for it. You may as well wait another 3-6 months for the next generation stuff as your money goes further and it's a more substantial performance upgrade or has new platform features like PCI-E upgrades, DDR5/DDR6 etc.",hardware,2026-01-06 08:39:32,9
AMD,ny0bei2,No it wasn't that rare it was just more expensive,hardware,2026-01-06 14:04:55,3
AMD,ny4e9bm,"The 10850k was actually the reverse of this, it came after the 10900k. The 10900k was so aggressively binned the yields were terrible",hardware,2026-01-07 01:48:57,1
AMD,nxyo2l4,"They are at least consistent on this topic, having also reacted the same to AMD XT refresh CPUs that this is based on",hardware,2026-01-06 06:08:12,29
AMD,nxz96n8,I really hope more people start waking up to HUBâ€™s bias. Techyescity called them out for questionable benchmarking not long ago and itâ€™s always HUB with these crazy fine wine videos that nobody else can replicate.,hardware,2026-01-06 09:19:28,-10
AMD,ny1m0pm,Where did you see this? I'd love to learn more about it.,hardware,2026-01-06 17:47:21,1
AMD,nxz16wn,"Arrow Lake refresh? I think Intel said it's not a competitor to the x3d, meaning the 9850x3d is a CPU you don't need.",hardware,2026-01-06 08:02:59,1
AMD,nxylika,">Itâ€™s simply silicon binning, where higher quality dies are allocated to higher-tier SKUs, a concept HUB seemingly isnâ€™t familiar with.   What are you talking about?  They called this AMD's version of the KS weeks ago.  Looks like they were right.",hardware,2026-01-06 05:48:05,6
AMD,nxz9fs0,HUB isnâ€™t familiar as they are deathly afraid of overclocking.,hardware,2026-01-06 09:21:57,2
AMD,nxym0si,Performance and density go hand in hand ...,hardware,2026-01-06 05:51:56,11
AMD,nxzmauf,Exactly why should AMD get a free pass just because their AMD if any we should be harder on AMD than on Intel. They are absolutely milking us for higher performance we should much better pricing. Those should go in tandem not one or the other. Its not like AMD has very little time between generations if Apple can release a good chip every year. I dont see why AMD cannot give comparable performance uplift at decent prices.,hardware,2026-01-06 11:18:32,6
AMD,nxz4gd7,You forgot the VRAM increase on the 4070 Ti Super as well. That's a huge boon for a performance tier like that in terms of longevity and also just overall performance.,hardware,2026-01-06 08:33:48,12
AMD,nxz1mex,Some people also love gaslighting the audience.,hardware,2026-01-06 08:07:01,14
AMD,ny1v7fe,"Glad to see more people calling it out. It seems like doomposting and ""let's see how we can spin this into something negative that make people feel like victims"" is the norm these days and it sucks.",hardware,2026-01-06 18:27:50,3
AMD,nxzmehl,Its milking there is no question about it. If Apple can release CPUs every year with meaningful improvements I dont see why AMD can't.,hardware,2026-01-06 11:19:22,3
AMD,nyt318w,Changing the socket got me so upset. I bought in because of AMDâ€™s history of long term support.,hardware,2026-01-10 16:22:31,2
AMD,ny367tk,I agree but no need for 80% of content to be the same doom and gloom thing over and over. Itâ€™s rage bait and clearly working for them.,hardware,2026-01-06 22:02:47,4
AMD,ny36j66,Yeah itâ€™s just content strategy for the most part. People are loving it. Nothing like sticking it to the manâ€¦.!,hardware,2026-01-06 22:04:14,3
AMD,nxzb2cf,"I still watch hub, but I have accepted that they test hw like it was an prebuild system from hp, dell, and so on.   When I started building it was just to build a pc cheaper than an prebuilt, then the more I dug into the hobby then I discovered that one can tune the hw, u dont need to oc it to the brink of destruction but having a system that run optimally for your workload was a given.  I am surprised how hubs and gns crowd/audience defend running systems which are not optimally setup. It is like they dont build their own systems but buy systems built by the stores.  so many issues, with these hw techoutlets, like skewed results, obviously faulty settings in the bios, 5.1ghz 14900k in gaming? gear 2 ddr4 vs ddr5?  But what can they do, they are locked into this now, as this is their audience.",hardware,2026-01-06 09:37:42,1
AMD,ny31pa7,Nova lake,hardware,2026-01-06 21:42:02,1
AMD,nxytrlw,"Regardless of what they labeled it as, this is *still* just silicon binning, which is hardly a new practice.",hardware,2026-01-06 06:55:43,10
AMD,ny31v2q,And everyone complaining about this video would shit on the KS chips,hardware,2026-01-06 21:42:46,3
AMD,ny06ib8,Density does improve performance all else being equal but its not the only contributor.,hardware,2026-01-06 13:37:47,1
AMD,nxyminy,Kinda   Some performance improvements are from efficiency gains from design   Some performance gains come from density   You can cheat density to some regard by just making it bigger but that adds cost and lowers yields,hardware,2026-01-06 05:55:49,-1
AMD,ny36i60,"It's because they know your only real choice is between Intel and AMD, the two pillars of the x86 duopoly.   If you choose something else you may lose compatibility with your x86 software, which many people aren't willing to do.",hardware,2026-01-06 22:04:07,1
AMD,ny3a8ne,"> if any we should be harder on AMD than on Intel     Objectively yes, kinda.  The reason we tend to be more forgiving to AMD is that it's been in 2nd place for so long, and without 'em we end up on monopoly land where everything is so much worse than it is now.     AMD's been kicking this shit out of Intel for a while now and you figure that means the script should flip and now *they're* the ones that get the extra scrutiny.  I dunno if we're at that point yet...just the other day we saw posts that 'games leave intel in droves', leaving them at a piddly [55%](https://www.reddit.com/r/technology/comments/1q3uw7g/gamers_desert_intel_in_droves_as_steam_share/) steam share, aka *still* a freaking majority.     But given how shit things are atm, we should probably just be suspicious of everything...",hardware,2026-01-06 22:21:53,1
AMD,ny6gvw9,"Because Apple has a better larger team on it, and is able to shovel more money",hardware,2026-01-07 10:56:28,1
AMD,nxzcz4b,"On a side note amount of gamers that have no knowledge about PCs is staggering. I was doing for ""tech support"" in my old WoW guild way to often.  If a reviewer has an OEM system and tests it as such it's fine. HUB rather has test systems rather than OEM boxes. And you can see when GamersNexus reviews an OEM system that fails to work properly :) such systems would quickly skew the results tables.",hardware,2026-01-06 09:55:45,3
AMD,nxzbg7u,"Yeah, I completely agree with you.  There is a big difference between dangerous overclocking and tuning your system. I thought the hardware enthusiasts would understand this.   I bought up how much faster raptor lake is with tuned fast ram the other day and people countered by showing benchmarks at stock XMP.",hardware,2026-01-06 09:41:21,2
AMD,ny4lsvw,Did they imply it was new?,hardware,2026-01-07 02:29:41,1
AMD,nxyqx3u,Duh. Tick tock was all about either getting your performance gains from using the same design at a new node or gains from a new architecture.,hardware,2026-01-06 06:31:34,5
AMD,ny3tfls,Well compatibility layers exist and Microsoft is working hard on making WoA a first class solution. This X86 nonsense has to stop then only we will get progress in other architectures which would benefit consumers greatly of more companies can contribute.,hardware,2026-01-06 23:58:42,3
AMD,ny3tp9d,Then X86 is the issue. I dont see why ARM could potentially be cheaper and yet better than X86. Then these companies would be irrelevant to the needed scrutiny.,hardware,2026-01-07 00:00:05,1
AMD,nxzf00t,"I had a few low end mobos, and all of them were way overbuilt compared to oem/branded prebuilts. But in many countries butique prebuilts is the most common. Oem prebuilts were very popular during the crypto crisis though, remember it was basically cheaper to get one complete system instead of just the gpu :P",hardware,2026-01-06 10:14:25,2
AMD,ny5m3go,"Compatibility layers can't fix missing bios and drivers though, which is key if you don't want to be stuck with Windows.",hardware,2026-01-07 06:20:50,2
AMD,ny6wzu3,"I've used Linux/RISC at home since 2007 and now have more ARM (2017) and RISC-V (2025) hardware than x86, but it looks like most consumers are being held hostage by Windows/x86 software and aren't willing to use the best tool for the task at hand, but would rather have a ""one size fits all"" solution.   I may pick up a Qualcomm Snapdragon X device when those go on sale to clear stock over the next few months.",hardware,2026-01-07 12:55:29,2
AMD,ny5m9it,Not when there's no easy way to install an alternate os on an ARM device like with x86.,hardware,2026-01-07 06:22:10,1
AMD,ny696wb,I mean those are developed alongside the processor arch and its much easier to develop stuff for ARM compared to X86. Most drivers are processor agnostic anyways unless you optimise for specific CPU architectures.,hardware,2026-01-07 09:47:39,2
AMD,nzd1gfc,"that amt of hair in a clean room, steve should wear a hooded rain coat lol.",hardware,2026-01-13 15:00:01,126
AMD,nzcvzjv,I thought that was Denis lol,hardware,2026-01-13 14:32:09,46
AMD,nzfe7p7,"Pretty good video with a lot of detail I haven't seen in previous factory tours. I probably still won't use an Intel GPU for gaming, but for a media server it's probably sufficient.  Side note: It's funny how whenever a GN video gets posted, the same cast of characters comes out and writes essays criticizing the video, apparently without watching it. I guess that's what it means to ""make it"".",hardware,2026-01-13 21:41:25,6
AMD,nzd7t05,"**GN:** NVIDIA and AMD abandoned this segment!  #Reality:  **B580:** $249 USD  **9060 XT 8GB:** $299 USD  **9060 non-XT:** $259 USD  **5060:** $299 USD  **5050:** $249 USD  I wouldn't say this is abandoned. I will say though the actual factory tour is cool, great content. But a dumb headline/title for the video.",hardware,2026-01-13 15:31:02,96
AMD,nzd3sjp,"Yeah this is a really surprisingly open factory tour, at least compared to when Sapphire took Linus and Alex along. It feels unprecedented to see this much access and insight, but that's the goodwill nurtured by Steve paying off in spades. The lament about AMD and Nvidia ditching the sub-250 side is real, so having Arc actually be a usable option down here is going to be significant later on.",hardware,2026-01-13 15:11:40,24
AMD,nzf5m3z,"Amazing video, GN is so good with content like this",hardware,2026-01-13 21:01:37,5
AMD,nzgtt65,"Love the Sparkle heatsink aesthetic, just wish there were some higher end card offerings from them instead of just Intel's lower midrange GPUs.",hardware,2026-01-14 02:14:38,2
AMD,nzd8jmh,"rtx 5050 149 mmÂ² 128bit has similar performance to B580 272 mmÂ² 192bit. Selling for same msrp, winning by not ""trying"". Love how 5050 is actually at msrp now cheapest b580 is $349+  ""intel's gpu division seems like the only place in tech right now where the customers arent getting shafted these days""  People just want their Nvidia gpus cheaper  Nvidia crashouts making people hype an even worse product at current prices lol  [https://imgur.com/a/8iRI87Q](https://imgur.com/a/8iRI87Q)",hardware,2026-01-13 15:34:35,12
AMD,nzd585j,He probably just single handedly killed 6 wafers worth of intel GPUs.,hardware,2026-01-13 15:18:37,3
AMD,nzd6pvl,AKA 'NVIDIA and AMD are going where the real money is.',hardware,2026-01-13 15:25:50,0
AMD,nzcwvps,Finally a video that isnâ€™t â€œAI badâ€ or â€œcompany X badâ€,hardware,2026-01-13 14:36:46,-23
AMD,nzdfxes,That hat is so useful,hardware,2026-01-13 16:08:37,-4
AMD,nzdz3jo,Making them isn't important selling them is.  The reality is that this segment doesn't actually exist it has no buyers in it.  Seems intel is truly doomed trying to win segments that if they even exist aren't big enough to pay back their R&D even if the dominate them.,hardware,2026-01-13 17:48:25,-9
AMD,nzhjk4i,"Woah, a GN video that is actually interesting and informative for once instead of just ranting about the fact hardware companies exist to make money, not please gamers.",hardware,2026-01-14 04:51:57,1
AMD,nzdogu2,"To be fair this is not a clean room, just SMT assembly. Basically, soldering components. A lose hair might be a bit of a problem (kind of like in any factory), but his hair is tied and doesn't seem to be an issue.  At an actual clean room (where the actual silicon is processed and etched), the standards can become very crazy. Some parts of it aren't even accessible to humans, just automated lines to avoid contamination.",hardware,2026-01-13 16:47:26,69
AMD,nzd8zmt,"Gaming Jesus could walk on wafers without corrupting a single tile, His body is that pure.",hardware,2026-01-13 15:36:40,-15
AMD,nzd5ucm,"same thought, just feels disrespectful",hardware,2026-01-13 15:21:37,-22
AMD,nzdzd0h,"He desperately needs to learn how to take care of his hair. I have no idea why nerds think a dry, frizzy mess of hair is some kind of enviable quality.   I feel like I'm walking into friday night magic every time he pops up.",hardware,2026-01-13 17:49:38,-13
AMD,nzd0sl2,"who knew he can into GPU manufacturing? I thought he was a video editor and first real ""why are you employed"" guy on LTT.",hardware,2026-01-13 14:56:41,13
AMD,nzhet37,"Intel hired him on the spot when they saw the ""Live, Laugh, Liao"" sign",hardware,2026-01-14 04:19:24,1
AMD,nzd9rkp,"Wasn't that the B310? That was supposed to be the $100 bracket. To be fair that's an almost useless tier nowadays because iGPUs are capable of similar performance or even outperforming the cards in those brackets. Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.",hardware,2026-01-13 15:40:18,53
AMD,nzgaa45,He(Lucas) stated $100 was the market that was abandoned.  10:37  > Steve: So why still making A310?  > Lucas: Because what's the competitor have? Nvidia? like... GT710? GT1030? (laughs) No way. So literally Nvidia AMD already give up the segment of this like... $100 price card.,hardware,2026-01-14 00:24:50,7
AMD,nzdtxiv,"They're talking about the A310 and A380, of which Nvidia doesn't have anything made in this decade to compete with and AMD has the 6400 that came out 5 years ago.  The only cards with modern features in that price segment that consumers can directly buy are the A310 and A380.",hardware,2026-01-13 17:24:22,24
AMD,nzfp313,This guy was talking about the A310 which is a $100 GPU. Basically said that the only other options at that price bracket are either a GT 710 or a GT 1030. And from AMD you can still get an old RX 550. The A310 may be slow but it beats those two gpu's by a mile.,hardware,2026-01-13 22:32:49,6
AMD,nzejfd4,Now do SR-IOV and 16GB RAM for under $400.,hardware,2026-01-13 19:18:41,3
AMD,nze3uy1,> 9060 non-XT: $259 USD >  >   LOL. Good luck finding it. It's OEM exclusive,hardware,2026-01-13 18:09:42,8
AMD,nzdcsu3,The title is a literal quote from Lucas,hardware,2026-01-13 15:54:17,17
AMD,nzejmb9,Found the guy who didn't watch the video and is making assumptions based on headlines.,hardware,2026-01-13 19:19:33,6
AMD,nzdb9mw,The title is in quotes. That is from Sparkle.,hardware,2026-01-13 15:47:16,6
AMD,nzenlzf,Go to r/PCMasterrace and they will downvote you into oblivion for even MENTIONING the possibility of gaming on a 5060 let alone 5050 XD,hardware,2026-01-13 19:37:44,0
AMD,nzdm48p,The title is just playing the YouTube algorithm game. It's stupid but they have to do it.,hardware,2026-01-13 16:36:44,-4
AMD,nze22z8,"> Love how 5050 is actually at msrp now cheapest b580 is $349+  Just an FYI but B&H has the [Acer Nitro B580 for $249.99](https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html) and the [Intel Limited Edition model for $259.99](https://www.bhphotovideo.com/c/product/1869297-REG/intel_31p06hb0ba_arc_b580_limited_edition.html). So you can get B580 for MSRP, but for how long who knows.",hardware,2026-01-13 18:01:44,18
AMD,nzdnidn,"die area is not the only cost indicator. B580 actually uses N5 fab, which is likely cheaper than N4, used by 5050. In reality, B580 only has about ~15% more transistors and if we assume N5 is cheaper per transistor and N5 has higher yields (by being more mature) i'd say their die cost might be very similar.  While yes, it has wider memory, the chips are clocked lower, so they can buy slower bins, reducing per chip cost.   B580 has more power draw, which in turn costs more for power delivery and cooling.  All in all, AIB manufacturer likely has lower margin per card as it stands, so i wouldn't be surprised if intel is taking a lower margin on the gpu/gddr combo to get more market share.  Nvidia on the other hand optimized their cost REALLY well, as they have been doing GPUs for almost 30 years.",hardware,2026-01-13 16:43:04,8
AMD,nzhfs1m,"Nvidia is getting such good performance out of such a small die because they're the best. Simple as. They've been doing GPU's for decades. It's not unexpected that at this stage Intel needs to use a larger die to match the performance - it would be incredibly surprising if that wasn't the case.  But a small *part* of that die size advantage comes from that narrow 128bit bus, and *part* of B580's appeal is its wider bus and subsequently more VRAM.",hardware,2026-01-14 04:25:57,1
AMD,nzd4pls,"Isn't the title pretty much ""AMD & NVidia bad""?",hardware,2026-01-13 15:16:07,51
AMD,nzd1pn1,Although even then the thumbnail is framed negatively.   But I much prefer these to Steves endless negativity ragebait.,hardware,2026-01-13 15:01:19,4
AMD,nzcyycp,> finally a company that isnt bad   ftfy,hardware,2026-01-13 14:47:24,-26
AMD,nzgji2p,"Did you even watch the video? they are selling, and they are selling out, so much so that they want to ramp up production so they can push out more.",hardware,2026-01-14 01:15:59,5
AMD,nzfbt5x,I do not know if the numbering scheme from my workplace is common across the industry but the smt assembly would be in a class 5 or 6 clean room and the fabrication itself would be a 1 or 2 class clean room,hardware,2026-01-13 21:30:27,14
AMD,nzglawf,"I think it still matters to a certain point, thats why everyone in the factory is wearing a hat. The Factory boss decided to roll RNG dice and say *""Fck it, that small hat is fine, even tho wearing it is pointless now; I'll just pray nothing bad happen*"". lol  What hilarious is when you think about what going through uninformed factory-employee's head, after they saw some guy(Steve) walk-into the factory like that. Definitely a lot of ""WTF"" moment going through their mind lmao.",hardware,2026-01-14 01:26:15,3
AMD,nzdb445,"The factory boss told me not to bother tying my hair (""ä¸ç”¨ä¸ç”¨ä¸ç”¨, æ²¡äº‹å„¿æ²¡äº‹å„¿æ²¡äº‹å„¿. è¿™æ ·å¯ä»¥çš„"") when I started to put it under the hat... and after asking for a larger hat or hairnet.",hardware,2026-01-13 15:46:33,121
AMD,nze10ie,"> same thought, just feels disrespectful  It's a good thing you were there to personally witness the interaction so that we'd all know exactly how disrespectful Steve was being before holding everyone at gunpoint to force them to let him shoot the video without first fixing his hair.",hardware,2026-01-13 17:56:59,6
AMD,nzdfs8o,Nothing really surprises me with regards to gamers nexus at this point.  Edit: Downvote me all youâ€™d like. Theyâ€™ve been leaning incredibly hard into the rage bait type content of late.,hardware,2026-01-13 16:07:57,-53
AMD,nze0k30,I'm sure getting beauty tips from random redditors who have never left the basement is of utmost importance to Steve.,hardware,2026-01-13 17:54:57,14
AMD,nze76iz,yeah there is a reason the a310 cards they are making are 4 hdmi out.  Gotta know your market,hardware,2026-01-13 18:24:10,18
AMD,nzdm2vc,"Might be just the thing for older machines and a GNU Linux (or BSD like) migration. Or as a pass through GPU to Jellyfin, Emby, or Plex for media transcoding. Edit: or Small form computing tied together with a iGPU enhancing game performance...",hardware,2026-01-13 16:36:33,11
AMD,nzeeiko,I assume those are being marketed to OEMs who make digital billboard systems or something. No idea why else you would want 4 HDMI ports on a card.,hardware,2026-01-13 18:56:32,1
AMD,nzddzc6,"> Wasn't that the B310? That was supposed to be the $100 bracket.  It's the A310, which is Alchemist and it's pretty much dogwater for anything beyond being a 'display out' card. The claim that NVIDIA has abandoned that segment is stupid... They've had offerings in this segment for years, plus anyone smart will just go buy a used GPU, your money goes way further. For example, the GTX 1650 performs basically 10-15% better, has better drivers, better encoding and generally is better supported. It's older, but I mean Alchemist wasn't exactly impressive either when it released and pretty much Intel has moved onto Battlemage and Celestial driver optimisations instead.   Plus let's be real here I went and searched and I found only weird places tend sell the brand new A310, the only local computer shop I found selling it in Australia for instance is a big one which is good surprisingly, but they had it for $189 AUD, a total rip tbh. A used 1650 is like $100 AUD and a used 1650 SUPER is like $120 AUD. No reason to buy an A310 tbh, pocket the cash and move on. Or if you're really intent on spending around that much buying a used RTX 2060 for like $20 AUD more, so a total of $200-210 AUD is better. Then on the AMD side you have the RX 6400 which had an MSRP of $159 USD and it's again a solid 10-15% faster, but much better off buying a used 6500 XT or 6600. Neither company has abandoned the segment, they had offerings for years and the used market basically obliterated any point to buying a brand new card like this.  >  To be fair that's an almost useless tier nowadays because iGPUs are capable of outperforming the cards in those brackets.  Yep this too. Honestly, I mean it's cool they're showing how they make cards on this factory tour, but to be like ""NVIDIA and AMD abandoned this segment"" is stupid when it comes to the A310. Almost anything these days is better than an A310.  >  Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.  GT710 hasn't made sense for like 8 years at least, even when it was relevant people laughed at it, but it did the job for 'display out' and such which was all that mattered. GTX 1630 was okay but it was supposed to be $149 USD MSRP and it came out for like $200 USD in most stores due to GPU shortage at the time, not much NVIDIA could really do about that.",hardware,2026-01-13 15:59:39,-1
AMD,nzdyz47,They haven't made anything because the market has moved on. Intel might be making these but are they selling them?,hardware,2026-01-13 17:47:52,-12
AMD,nzdhs0m,"[Is it in reference to this moment in the video?](https://youtu.be/YwrUxG26ulk?t=648) If so, he doesn't say that as a literal quote, he says ""give up the segment"". Unless there's another quote somewhere else which I missed which may be possible or maybe it was edited out or cut from the video? I can't remember everything he said tbh but there was a lot of good information in this video and I think the title is better off without it. If it was called ""Intel Arc GPU Factory Tour with Sparkle"" I would have insta-clicked to watch anyways.",hardware,2026-01-13 16:17:04,13
AMD,nzdelag,You know what you're doing with the title... It's honestly unnecessary to use it on a factory tour video tbh.,hardware,2026-01-13 16:02:26,34
AMD,nzejqso,Stop defending your clickbait.,hardware,2026-01-13 19:20:08,12
AMD,nzf28eu,do you wanna address this then? Its kinda cringe ignoring the rest of the post  >**GN:**Â NVIDIA and AMD abandoned this segment!     >**B580:**Â $249 USD  >**9060 XT 8GB:**Â $299 USD  >**9060 non-XT:**Â $259 USD  >**5060:**Â $299 USD  >**5050:**Â $249 USD  Why include that in the title then too?,hardware,2026-01-13 20:45:49,1
AMD,nzg52uc,"Obligatory ""lol stupid pcmr amirite"" comment.",hardware,2026-01-13 23:56:47,0
AMD,nzdplnj,"The RTX 50 and 40 series are using the TSMC 4N node which is a custom version of the N5 node for NVIDIA. But anyway the N5, N5P, N4, N4P, N4X are all 5 nm class node, so have around the same price for the wafer. And I wouldn't be suprised that NVIDIA is paying less for these considering the volume compared to Intel orders.",hardware,2026-01-13 16:53:27,15
AMD,nzdpf4g,Well their gpus are much more expensive than amd & nvidia who arent even trying. When they try Intel wouldnt even have a chance  Nvidia increasing their entry gpu volume  [https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026](https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026),hardware,2026-01-13 16:52:06,4
AMD,nzdqnhb,Yep,hardware,2026-01-13 17:07:48,4
AMD,nzdueea,Thanks Steve,hardware,2026-01-13 17:26:35,30
AMD,nzdyjcv,But you still didn't to say hi to me at PAX West 2016 in front of the LEGO USS Missouri battleship...,hardware,2026-01-13 17:45:51,3
AMD,nzdtnpi,"makes sense, I coulda been more charitable in the way I said it",hardware,2026-01-13 17:23:03,2
AMD,nze79m5,"relax dude, steve already replied, no need to whiteknight",hardware,2026-01-13 18:24:32,-8
AMD,nzdtsrk,"he replied in a comment to me to say that he was told to leave it alone, I guess assembly isn't as careful as the initial production is.",hardware,2026-01-13 17:23:45,6
AMD,nzdjfsy,"Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN, and their recent consumer advocacy and stepping on some very powerful toes, your comment sounds an ***awful*** lot like an astroturfing smear campaign meant to breed sentiments against Steve & GN.",hardware,2026-01-13 16:24:37,11
AMD,nze6q5q,Imagine defending the hair of a guy who looks like he judges anime conventions in his spare time.,hardware,2026-01-13 18:22:13,-7
AMD,nzdq8iu,"I get it, for those with old boxes. But intel has great transcoding according to self hosters, and the powr consumption is much better vs old i5 pairing with those dedicated cards.",hardware,2026-01-13 17:05:17,8
AMD,nzej077,day traders love having a zillion stock tickers running.  i'm sure there's more applications where a heap of monitors is useful.,hardware,2026-01-13 19:16:47,7
AMD,nzeua57,A lot of digital displays make use of DP MST to avoid the use of home run cabling.,hardware,2026-01-13 20:08:25,3
AMD,nzelrbi,The a310 and a380 are fantastical for a media server!,hardware,2026-01-13 19:29:14,11
AMD,nzdi8nv,Your wasting a lot of words defending a company about to rerelease a 4 year old GPU (3060) because they canâ€™t get memory for the current model.,hardware,2026-01-13 16:19:11,10
AMD,nzdjgo9,"Sure it was paraphrased for the title, but that's just semantics.   AMD and Nvidia ""giving up"" vs. ""abandoning"" the segment mean the same thing either way, given Lucas' intention behind the statement.",hardware,2026-01-13 16:24:44,6
AMD,nzejtyf,Did you watch the video? I'm thinking not.,hardware,2026-01-13 19:20:31,0
AMD,nzfgwnz,He is ignoring the post because the poster didn't watch the video and is spreading BS. The segment they are talking about is $100 cards.,hardware,2026-01-13 21:53:45,7
AMD,nzdqmql,"maybe the cost for the raw wafer, but that's not all TSMC will charge nvidia for. You also need to account for yields, which could be different depending on the type of node.",hardware,2026-01-13 17:07:41,0
AMD,nze7n1w,But then I won't earn my free toaster after the 11th white knight attempt.,hardware,2026-01-13 18:26:08,-1
AMD,nzdxfeg,"> Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN  You have to be joking, right?",hardware,2026-01-13 17:40:46,7
AMD,nzdlwqs,Yeah. Iâ€™m definitely an astroturfing bot account. You got me. My profile certainly *reeks* of botting / astroturfing ðŸ˜‚,hardware,2026-01-13 16:35:48,-11
AMD,nze72qg,"Imagine being as shallow are you are while still posting on reddit behind an anonymous username.  Let's see how your hair looks, mate. You're giving off pure incel vibes here.",hardware,2026-01-13 18:23:43,10
AMD,nzdii5s,"You know I also talked about AMD right? Not just NVIDIA. Regardless, you think Intel isn't also going to have memory issues soon? They might just divert all memory they have to the SKUs that are selling.",hardware,2026-01-13 16:20:22,6
AMD,nzedwr3,>  a literal quote from Lucas  Does not line up with  > paraphrased,hardware,2026-01-13 18:53:49,8
AMD,nzdz5pq,Yields wouldn't meaningfully differ within the same family. Certainly not by enough to remotely cover for the die size difference.,hardware,2026-01-13 17:48:42,7
AMD,nzfzkw5,I'm starting to think that r/hardware is the circlejerk sub and I just haven't yet found the actual hardware sub that it's parodying,hardware,2026-01-13 23:27:14,1
AMD,nzdw1w0,You do it for FREE? Are you regarded?,hardware,2026-01-13 17:34:20,0
AMD,nzegtti,Intel B570/580 already use GDDR6 which is what Nvidia is trying to achieve with the 3060 release. Intel presumably won't be effected.,hardware,2026-01-13 19:06:58,3
AMD,nzel47j,"...Yes, that's why I said it's a semantics issue.  The meaning is the same: The paraphrased quote isn't a statement made by GN like u/KARMAAACS implies.   What makes it worse is that Lucas said that in response to Steve's question about ~$100 A310 cards and why they're still producing them (adding that they see a healthy demand for them from their customers).   It's like he didn't watch the video and just reacted to the title.",hardware,2026-01-13 19:26:20,-7
AMD,nzbjsg6,"It is worth pointing out that benchmark scores also depend on the TDP of the device, especially for MT tests. For X Elite, they used a 80W QRD, to showcase the max capability of the chip. Non of the OEM devices reached that high, so of course the benchmark scores were less.",hardware,2026-01-13 08:29:27,86
AMD,nzbhaqr,"Having bought a Ryzen AI Max+ 395 laptop last year, it always pleases me to look at these iGPU graphs.  In all honesty, I would consider a Snapdragon X2 Elite device if it came with first-class, out of the box Linux support if it comes in a package as quiet as a Macbook Pro or a Mac Mini.",hardware,2026-01-13 08:05:42,47
AMD,nzbljcb,Why are they comparing it to a year+ old M4 and not the current chip?,hardware,2026-01-13 08:46:12,25
AMD,nzbcz2k,"I wish there was a fanless design with those chips, and I wish Windows on ARM was treated as a first class citizen. I'd buy one of those in a heartbeat. Been looking for an upgrade from M1 but at this point I think I'll just get an M4 Air or wait for the M5 Air. Might have to snatch an M4 now though since the M5 Air might have a significant price hike due to rising cost of several parts.",hardware,2026-01-13 07:25:30,19
AMD,nzbqez9,"X2 Plus only reaches 4Ghz and it is only a bit faster than Panther Lake. X2 Elite reaches 4.7 or 5Ghz and it reaches Apple M4 or M5 level     also, I don't get why the reviewer wrote this article this way, X2 Plus is the budget platform yet it's trading blows with the top end chips of AMD/Intel in these benchmarks, in cinebench, the 10 core X2 matches the 12 core Ryzen AI 9... and this will be priced as a Ryzen 5",hardware,2026-01-13 09:33:57,8
AMD,nzbm5yc,"The 10-core X2 plus is meant for the 800$ laptop segment. Why is it being compared with Core Ultra 9 285H and Ryzen AI+ Max 395, which are meant for laptops with much higher price tags. At least it is worth a mention if the comparison is being made.  EDIT: The original PCMag article, from where the numbers in the linked articled are sourced, clearly makes that distinction about the class of processors it is likely going to compete with:  >The multithreaded version of the Cinebench test also showed some promising performance, with the X2 Plus 10-core coming within about 10% of the workhorse Intel Core Ultra 9 285H and the AMD Ryzen AI 9 HX Pro 375. We would have pegged the X2 Plus as something closer to a Ryzen 7 or Core Ultra 7. It's especially impressive, given that the Intel 285H is a robust H-class processor, not a U-series or V-series processor.",hardware,2026-01-13 08:52:13,9
AMD,nzefunm,"Pretty weak considering they have 3ch memory, more bandwidth and this is the perf to show for it?",hardware,2026-01-13 19:02:33,2
AMD,nzblt00,Why is there such a large difference in St between x2 elite extreme reference and x2 plus reference?,hardware,2026-01-13 08:48:46,4
AMD,nzbkv2z,"If it is so good, why QCOMâ€™s channel seniors left the company?",hardware,2026-01-13 08:39:46,-1
AMD,nzc8nnw,"Why the fuck are these chancers hellbent on dragging Apple, a completely irrelevant comparison, into PC hardware measurements? Apple could deliver a chip 4000 times stronger than the sun but it still doesn't run any software anyone cares about outside of Adobe, whose tech debt would still amount to a 1% advantage",hardware,2026-01-13 12:13:19,-7
AMD,nzf1g4d,"Qualcomm always performs well in native ARM 'synthetic' benchmarks. But...  The problem is, the real world is far more messy. 99.999% of Windows applications are still x86-x64 this results in performance and efficiency loss compared to the ideal benchmarks they show. There are countless games, and Adreno has relatively bad drivers and very few optimizations. Some people can't even use their printers and other accessories due to driver compatibility issues...  If you treat the WoA devices like Chromebooks, you'll have a great experience, but why would you ever want to spend $1000 to do that? If you plan to actually use your device for a nuanced workload instead of just web browsing and document editing, then just buy Intel or AMD.  Panther Lake looks like it will once again invalidate most of the improvements Qualcomm is making, just like Lunar Lake did to the X1 Elite launch. So I really don't see why anyone would early adopt WoA at this point except out of novelty.  Ps. Linux support for these Qualcomm chips are still worse than Windows support... So yeah.",hardware,2026-01-13 20:42:10,0
AMD,nzbcnm7,"Well im a really big fan of a lot of power in a tiny area ( Í¡Â° ÍœÊ– Í¡Â°). But no in all seriousness, the idea of having a single processor make the work of a gpu and cpu happen in one spot while keeping the power draw low and having performance match midrange things today is fun. Like I have Intel igpus for that, you get an alright processor with a igpu that can do so much productivity work its amazing! AMD got really close there with the 8000G series.",hardware,2026-01-13 07:22:36,-5
AMD,nzc5gxu,Dang that's a high TDP isn't it? Higher than panther lake even.,hardware,2026-01-13 11:49:02,12
AMD,nzbresm,"Qualcomm this time gave TDP for the tests and how much power it needs. The 18 core X2 Elite can go up to 100W in CPU like the Intel H series chips, but on same TDP, it has 2x the MT performance of AMD and Intel Platforms on Cinebench from 40W to 100W  [G-N5FcQXgAETuMr (1536Ã—850)](https://pbs.twimg.com/media/G-N5FcQXgAETuMr?format=jpg&name=large)     The X2Elite 12 core only reaches 50W",hardware,2026-01-13 09:43:38,27
AMD,nzbmq5u,another good point,hardware,2026-01-13 08:57:43,4
AMD,nzcz268,My 5600X doesnâ€™t even reach 80W. This rather deceptive marketing because thatâ€™s a lot of heat to pump out of a laptop without being thermal throttled.,hardware,2026-01-13 14:47:56,2
AMD,nzc0r0r,"> out of the box Linux support  Looking at the state of mainline support for X1, I'm not too optimistic about X2 (nor Qualcomm in general wrt. upstream support) :(",hardware,2026-01-13 11:08:54,27
AMD,nzce9j5,I mean the fact that it's slower than last year's chip is noteworthy.,hardware,2026-01-13 12:52:01,34
AMD,nzbnjcr,"Because X2 Plus is not a flagship chip. It's a lower end chip. X2-Elite series is the flagship and the base X2-Elite chip beats the M5, let alone the X2-Elite Extreme.",hardware,2026-01-13 09:05:42,-6
AMD,nzbk9p5,">and I wish Windows on ARM was treated as a first class citizen  That's Qualcomm's eventual goal it would seem, to be seen as competitor to Intel/AMD on an equal footing. However, that vision is still many years away from being realized, due to state of Windows on ARM.",hardware,2026-01-13 08:34:03,19
AMD,nzbvqs3,"Windows x64 isnâ€™t even treated like a first class citizen. The OS havenâ€™t had any meaningful improvements in years, if anything it gotten worst.",hardware,2026-01-13 10:24:33,19
AMD,nzcvn7b,"Are they still offering Windows on ARM? The first time they tried that, they killed it a lot faster.",hardware,2026-01-13 14:30:23,2
AMD,nzegupt,"We have a bunch of SD Thinkpads in our org. Honestly, they work great for  productivity use cases, and battery life is as good as it gets for Windows laptops.",hardware,2026-01-13 19:07:05,2
AMD,nzcpmn8,Genuine question but why do you want or need ARM specifically? Why not Lunar Lake or wait a bit more to see how Panther Lake fares.,hardware,2026-01-13 13:58:46,4
AMD,nzbromt,"Asus A16 is 1kg while being a 16"" device with these. I'm looking out for one of them with the new 94/98 SKU  (192 bit LPDDR5X with 48GB of RAM)",hardware,2026-01-13 09:46:18,1
AMD,nzbif7e,waiting for a wifi7 M mba seems forever already,hardware,2026-01-13 08:16:20,-1
AMD,nzboxtj,"Yup , better to get it now asap . The price hikes are already starting to reflect on mobile prices",hardware,2026-01-13 09:19:26,-5
AMD,nzc3hbs,The original PCMag article has a much more nuanced and positive take. This article seems auto-generated to me.,hardware,2026-01-13 11:32:46,12
AMD,nzc71is,">X2 Plus only reaches 4Ghz and it is only a bit faster than Panther Lake. X2 Elite reaches 4.7 or 5Ghz   Hmm. Why such a huge cut? Surely parametric yields arent that bad?  Intel Panther Lake top sku is 5.1 GHz, and the lowest one is 4.4 GHz.",hardware,2026-01-13 12:01:21,4
AMD,nzg61oe,"I think there is still a big of ""denial"" as to how performant ARM cores are now, and how behind x86 has gotten. Perhaps?",hardware,2026-01-14 00:01:58,2
AMD,nzbrqre,"Lower Max ST clock, X2 Plus is 4Ghz only. X2 Elite goes up to 5Ghz     I reckon that the X2P uses normal N3P libraries while on the X2E they said they use N3X for the Prime cores",hardware,2026-01-13 09:46:52,7
AMD,nzcbuib,Mor power,hardware,2026-01-13 12:35:56,1
AMD,nzbx26w,"What? There are so many possible answers that are compatible with the assumption that ""this is a good product"". Like: because its so good, the value of those people has increased and other companies pay them more so they leave for money. Or: They have worked very hard to make a good product for a long time, and now they want to go chicken farming. Or just: I dont like the company/environment so they are leaving, independent of the product. It might just not be causally related at all.",hardware,2026-01-13 10:36:38,4
AMD,nzcdj3x,"""MacOS doesn't run any software anyone cares about"" is only true if you only care about gaming.   Gaming is MacOS's weakness, but other than that I'd say MacOS runs most if not all software that most people care about. It certainly runs all the software I could need and I would argue that I have a far above average varied use of software. With they being said I still prefer Windows but software support is not what's holding me back from switching.",hardware,2026-01-13 12:47:12,10
AMD,nzcdzfq,"Does the job for most people. If the software you want isn't there, just buy intel/amd. Apple is a competitor so its good to have them",hardware,2026-01-13 12:50:12,5
AMD,nzclx6l,"You know... not everybody here is a gamer PCMR bro.  There are also CS/EE people who are interested in hardware. This place is one of the few places in the internet where we could hang out and have an intelligent discussion. However, with the death of sites like Anandtech, the quality of technical discourse has evidently declined.",hardware,2026-01-13 13:38:29,5
AMD,nzg73u2,">99.999% of Windows applications are still x86-x64Â   A lot of the MS Windows productivity stack has been ARM native for a while.   These SKUs are geared for productivity/home use. Gaming is a corner use case, and for that there are actual gaming laptops. Few people were buying Lunar Lake for gaming.",hardware,2026-01-14 00:07:43,1
AMD,nzc9zkz,>Dang that's a high TDP isn't it?  Not especially. Most of notebookcheck's 285H models have at least short term power boosts up to that power level or even higher.    >Higher than panther lake even.  PTL's max TDP is 80 watts.,hardware,2026-01-13 12:22:56,8
AMD,nzchg4k,QC tested with the same power requirements as Intel but most notebooks will be power limited to 50W and it will still be fine,hardware,2026-01-13 13:12:22,5
AMD,nzbw35z,and how much does the M4 Max pull at full tilt?,hardware,2026-01-13 10:27:43,11
AMD,nzdat63,">but on same TDP, it has 2x the MT performance of AMD and Intel Platforms on Cinebench from 40W to 100W  When comparing with CPUs actually making use of that TDP...not really. Pushing a Ryzen AI 7 350 to 100W is ridiculous.   It's more the other way around: The X2Elite 18Core seems to better scale downward below 40W.  https://www.computerbase.de/artikel/prozessoren/amd-ryzen-9-9955hx-intel-core-ultra-9-275hx-test.91825/seite-2#abschnitt_tdpskalierung_von_15_bis_180watt  https://www.computerbase.de/artikel/prozessoren/amd-ryzen-ai-max-395-test.95300/#abschnitt_tdpskalierung_von_45_bis_170watt_in_anwendungen",hardware,2026-01-13 15:45:09,4
AMD,nzdhqva,"Unlike Intel/AMD, Qualcomm doesn't define a 'TDP' for their SoCs. It is upto the device maker to set it.   What we are talking about is the maximum power the chip can draw, which is equivalent to Intel's PL2 power level.",hardware,2026-01-13 16:16:55,5
AMD,nzd84yk,It'll be for very short bursts unlike a desktop CPU which can sustain its clocks better.,hardware,2026-01-13 15:32:36,1
AMD,nzdnyqr,"my old 5600x would go up to 150w in cine bench, more than 14600k...",hardware,2026-01-13 16:45:08,1
AMD,nzcorxu,They at least seem to be trying harder for the X2 in the kernel and in mesa.,hardware,2026-01-13 13:54:10,11
AMD,nzczhwy,"The X2P is not a top chip. Apple only does flagship chips. Apple only does top chips for each formfactor/TDP, 8W on the A18/A19, 20W for the M5, 45W for the M5 Pro, 65W for the M5 Max  The M5 of the X2 lineup is the base X2E. The X2P and X2E 12 core occupy the same TDP range but one is much faster than the other. It's a choice because they need to sell their chips to OEMs and consumers, Apple only needs to sell laptops.  Basically QC X2E die is 20-30mm\^2 bigger than the M5 die, for the fatter CPU, but the GPU is the same performance. Compared to the M5 Pro, the CPU performance is equivilant, but the GPU performance is 30% lower. QC has to make bets on what Consumers and OEMs want and IMO they made the smart bets.  The X2P has a very weak GPU, but CPU wise the 10 core version is close to M5 level. Die size wise most likely it's 120mm\^2 making it cheaper to manufacture and so, sell it for cheaper.",hardware,2026-01-13 14:50:10,12
AMD,nzbpbp1,M4 is also not the flagship chip though. There are m4 pro and m4 max . And their flash ship is the ultra models and the current one is m3 ultra,hardware,2026-01-13 09:23:14,37
AMD,nzbwas7,The rival device to the Samsung galaxy book in question tested has M5 and thatâ€™s what matters here.,hardware,2026-01-13 10:29:40,1
AMD,nzd8nqk,Wish MS would focus on performance and snappyness but they seem to prefer bloat and slower performance than Windows 10 of all things.,hardware,2026-01-13 15:35:06,1
AMD,nzcjfzc,"I'm not in the Apple ecosystem. Everything I have is Android, other than this Mac. The Mac has just been too good to give up, and there seems to be no decent alternative. I've been waiting to see where WoA and Snapdragon goes, but so far seems to be going nowhere.",hardware,2026-01-13 13:24:19,5
AMD,nzcrrl9,"The thing is I might be weird but I really ***despise*** fan noise so I want a fanless laptop. I had a fanless Intel laptop before I switched to M1, and now I want to go fanless again. There are no good options on x86-64 still, so I'm hoping for some fanless ARM designs.",hardware,2026-01-13 14:10:11,9
AMD,nzbpimq,"Wifi 7 is pointless at the moment as none of the actual chips properly support its key features like MLO. All the current chips only support the Multi-Link Single Radio version of MLO, which can only connect to 1 band at a time and only acts as a load balancer.",hardware,2026-01-13 09:25:09,14
AMD,nzc89ae,"2 different dies, X2P uses a lower end die, X2E uses a fat 250mm\^2 one     On the X2E, they use N3X transistors for the highest performance. that might not be true on the X2P",hardware,2026-01-13 12:10:24,7
AMD,nzg5l14,"2 different dies:  \- Plus is the equivalent of the baseline M-series die for apple.  \- Extreme is equivalent to the Pro M-series die.   QCOM does not have a ""fat"" GPU die to compete with the Max M-series though.",hardware,2026-01-13 23:59:29,1
AMD,nzharjh,"Of course, there's a reason why Panther Lake's CPU performance is just not being discussed entirely.",hardware,2026-01-14 03:53:07,1
AMD,nzc8xcs,Their smartphone chips have the same cores reaching 4.6GHz and apparently doesn't use N3X.,hardware,2026-01-13 12:15:17,3
AMD,nzccua8,Dont think St rly scales with power by that much. Its how M5 will have similar st to M5 Max,hardware,2026-01-13 12:42:39,1
AMD,nzciei0,"Yeah makes sense based on the scores, which look similar to panther lake.   Can't wait to see a third party head to head.",hardware,2026-01-13 13:18:10,1
AMD,nzby5dk,\~50W cpu + gpu can go to \~+100W,hardware,2026-01-13 10:46:17,15
AMD,nzffzgw,The 100W part is was comparing it to Strix Halo and Intel,hardware,2026-01-13 21:49:32,1
AMD,nzgnwfc,"> Apple only does flagship chips  I find this opinion/position kind of hard to defend when the M4 Pro and M4 Max are both different, larger dies,  and absolutely sold as the premium, for-real-work upgrade. Apple does not market segment/differentiate based on TDP. You either need more power or you don't... and the M5 Max very much is just 3 M4's stuck together.  Even the price comparison doesn't really make sense--you can absolutely buy a brand-new, full price M4 laptop for ~$999 directly from Apple. That's not a sale price, that's not new-old-stock, 'oops nobody actually wanted to pay what we were overcharging' price, that's the same price it's been since it released. That won't even get you the lowest end, most-cut-down Strix Halo SOC inside a laptop.  Qualcomm can claim it's cheaper, but unless OEM's are overcharging to the tune of hundreds of dollars, I think the only vendor in question making $ here is Qualcomm.",hardware,2026-01-14 01:41:18,2
AMD,nzg54e8,FWIW the M5 Pro is a bigger die than the M5 as well.   I don't understand why they comparing the value tier SKY for this SD vs more expensive x86 SKUs. Though.,hardware,2026-01-13 23:57:01,1
AMD,nzgpw3d,"I see that but the M4 doesnâ€™t seem like it solves that- itâ€™s just an old flagship chip. And Iâ€™m not sure I totally agree, the lower end iPads have a low bin M5 with a core disabled. Cutting edge node is usually A chips. So itâ€™s a binned, â€œolderâ€ (for Apple) node and the lowest variant of the 4 (or 6, depending on how you count) versions of their lineup.  The M5 is in the iPad, seems like comparing to a current chip would be the thing to do given youâ€™re never going to have a perfect comparison- Iâ€™d think people are more likely to compare that when shopping rather than say a used iPad on eBay.  Not a great comparison either but Iâ€™d be OK comparing say the 5090 to the M5 Ultra whenever that is released even though it is not apples to apples. . .",hardware,2026-01-14 01:52:32,1
AMD,nzbs01n,"They are comparing in ST, in ST M4, M4 Pro and M4 Max are the same thing, within 2-3%",hardware,2026-01-13 09:49:23,8
AMD,nzf3t82,"Thats stupid to compare, x2p is for sub 800 devices, apple doesn't even sell m4/m5 for that much.  X2 elite is whats gonna compete with m4/m5 and x2 elite extreme with m4 pro.",hardware,2026-01-13 20:53:10,1
AMD,nzbvzbj,X2-Elite also is not the flagship chip. That is the X2-Elite Extreme (which is much better than M4 Pro in everything except GPU where it's about 30% slower).  M3 Ultra loses to M4 Max in single core CPU performance and some rendering tasks. But yeah it's still better in GPU than M4 Max (while drawing alot more power).,hardware,2026-01-13 10:26:44,-1
AMD,nzdjspm,"This is why I am looking forward to Google's upcoming Aluminium OS for PCs. If it functions like a Linux distro and can Linux software, I think Windows might bleed some serious marketshare.  Google atleast cares about the user experience, unlike Microsoft who has clearly given up even pretending to do so.",hardware,2026-01-13 16:26:15,6
AMD,nzdjwjd,Perhaps Aluminium OS will be our saviour.  [https://www.androidauthority.com/google-android-on-pc-qualcomm-snapdragon-summit-3600612/](https://www.androidauthority.com/google-android-on-pc-qualcomm-snapdragon-summit-3600612/),hardware,2026-01-13 16:26:44,2
AMD,nzd0y3r,"Its not weird I have m1 & m3 pro, the fanless is nice but fancurves on m3 pro essentially quiet",hardware,2026-01-13 14:57:28,1
AMD,nzeniul,Idk about fanless but I've heard multiple reviewers bring up unprompted how notably quiet panther Lake laptops are even under full load.,hardware,2026-01-13 19:37:20,1
AMD,nzd7k6r,"I see, I think it's not weird, I thought the ARM laptops for Windows had fans. Not sure if Intel will ever deliver something like that...",hardware,2026-01-13 15:29:51,0
AMD,nzhjogl,For the same reason why SDX2's GPU performance is just not being discussed entirely?,hardware,2026-01-14 04:52:49,1
AMD,nzce9si,https://www.tweaktown.com/news/108968/qualcomms-new-snapdragon-x2-elite-extreme-can-use-100w-plus-of-power-when-unconstrained/index.html,hardware,2026-01-13 12:52:04,1
AMD,nzco4lq,"X2E is much faster than Panther Lake, it's not close  Panther Lake only competes vs the X2P in this article, the 10 core version, the 18 core X2E is 2x of what Panther Lake can achieve in MT and +20% over ST  We can look at CPU config and see why  Panther Lake is 4P+8E+4LPE = 16 cores.  X2E is 12P+6E or 6P+6E  X2P is 6P+4E AFAIK",hardware,2026-01-13 13:50:40,1
AMD,nzc1bxt,"The CPU can reach higher in stress tests like these, Apple and QC are neck and neck in efficiency below 4.2Ghz+- on mobile, i wouldn't be suprised if this is the same",hardware,2026-01-13 11:13:53,8
AMD,nzg1xxo,"Where?  2x MT would mean Strix Halo and ""Intel"" (Whatever SKU) wouldn't even reach 1000 in CB24 MT at 100W TDP...and that's BS.",hardware,2026-01-13 23:39:59,2
AMD,nzgo04v,"> I don't understand why they comparing the value tier SKY for this SD vs more expensive x86 SKUs. Though.  Judging by the 1st gen, those are the laptops closest in price.",hardware,2026-01-14 01:41:53,1
AMD,nzgt7pv,The M5 in the ipads is the same exact chip with 1 core disabled. That is just optimising dies. Meanwhile AMD cuts 30% or 50% of a CPU die to sell it so they can upset their 8 core die for example,hardware,2026-01-14 02:11:17,1
AMD,nzc5ton,> X2-Elite also is not the flagship chip. That is the X2-Elite Extreme (which is much better than M4 Pro in everything except GPU where it's about 30% slower).   Coincidentally X2-Elite Extreme also is much worse than M4 Max in everything including GPU,hardware,2026-01-13 11:51:50,7
AMD,nzhksf5,Qualcomm has shared the gaming performance numbers and even let people test it out on their reference devices. Intel hasn't published any CPU benchmarks or let people run any yet.,hardware,2026-01-14 05:00:35,1
AMD,nzcwrox,100 Watts is not for ST.,hardware,2026-01-13 14:36:12,4
AMD,nzd2s93,"yeah -- the high TDP extreme version is a different class chip. Similar to strix halo, its a much bigger chip thats more closely a desktop chip than a mobile thin and light chip. When limited to the same TDP the X2E performance is significantly slower.",hardware,2026-01-13 15:06:42,4
AMD,nzgtpoz,"It depends which iPad, some have all cores.   AMD uses chiplets, a binned chiplet is 8 or 6 cores, so some combination of those is how many cores their products have.",hardware,2026-01-14 02:14:05,1
AMD,nzckklx,">everything including GPU  Source?  According to the numbers they have released CPU, MT performance matches M4 Max (Cinebench 2024), and ST performance exceeds the M4 Max (Geekebench 6).",hardware,2026-01-13 13:30:50,4
AMD,nzhmijp,"But no actual numbers against PTL, right? They showed some graphs against LNL and the ancient HX 370 at launch at that's it.  PTL is loosely the successor of LNL. Intel doesn't have a current gen competitor to SDX2EE right now though ARL HX can be considered from the previous generation.  AMD HX 370 is a heavily nerfed Zen 5 chip, bringing little to no improvement over Zen 4 at per core level. And as we all know, The Strix Halo 395 sweeps the floor against everything except Apple's top end chips.",hardware,2026-01-14 05:13:05,1
AMD,nzd7h8g,"Limited to the same TDP, X2E is still up to 2x on normal Panther Lake TDPs (X7/X9 is 60W TDP), but this is due to the much higher Perf/W QC and Apple have on their uarches.  The die size though you are correct, but if you count all tiles, the difference is simply that Intel allocated the die size to a much fatter GPU instead of CPU where they know they are behind. By going the GPU route, they secure very positive reviews because it carves a niche for them in the market where they are undisputed best.  Intel and AMD simply have lower perf/W. Panther Lake might fix it a bit, but e.g Lunar Lake on N3B was between 1.5-1.7x worse in perf/W vs 4nm X1 in ST",hardware,2026-01-13 15:29:27,1
AMD,nzcxlxg,"Their numbers are most likely from a ~130W reference platform (despite their ""laptop"" claims), while real retail devices will be running at 50 to 80W. It was the same with X1 benchmarks by QC - those results were NOT achieved by any real shipping device. If you take Mac Studio with M4 Max as a reference, which has similar TDP to their reference platform, X2 Elite Extreme is behind in both Cinebench 2024 as well as Geekbench 6.",hardware,2026-01-13 14:40:32,5
AMD,nzdk3kn,>X2E is still up to 2x on normal Panther Lake TDPs (X7/X9 is 60W TDP)  Do you have a source for this?,hardware,2026-01-13 16:27:36,5
AMD,nzen3z1,Panther lakes tdp is more like 45w max outside of bursts ofc..Almost all the benchmarks from Intel were at 45w and all sustained max tdps I've heard for panther Lake in laptops have been between 25-45w. Seeing they can handle like 100w+ gpus thermally it just seems to not use that much power like lunar lake and unlike arrowlake,hardware,2026-01-13 19:35:26,6
AMD,nz6kjy5,">Medusa Point keep pointing to RDNA 3.5 class graphics, sometimes described as RDNA 3.5+, and the same 8 CU limit shows up again.   Everybody point and laugh",hardware,2026-01-12 16:07:19,221
AMD,nz6q0n5,"8 CU RDNA3.5+ less than 16 CU RDNA3.5 in the 890M. Unless there's more $, higher clocks or frankensteined like the the PS5 Pro (IIRC RDNA2+RDNA4 ML/RT), I don't think there'd be any marketing material for gaming performance, ALSO when Nova Lake-H is expected to bump performance with Xe3P.  I also get that they have this new strategy with the IOD being it's own thing and adding a Zen6 CCD (that can be borrowed from DC/desktop) for max nT, but CPU competition is already rough with Snapdragon and Apple much less Intel.  The only thing I can think of they'd be proud of is a new NPU and well we all know how the market responds to that lol",hardware,2026-01-12 16:32:12,45
AMD,nz7091m,Why does AMD at 45% marketshare act like intel at 90% marketshare,hardware,2026-01-12 17:19:05,67
AMD,nz6q2yn,"The rdna 3.5+ is so infuriating. The chips like 7840U with 12CUs of RDNA3 were real good chips (still are) for casual 1080p gaming. Samsung even has put RDNA4 in the newest exynos 2600 if I'm not wrong. And still AMD doesn't give it all to the iGPUs, even if they have all the IP and those chips are new tapeouts anyways. If they did arch changes for rdna3.5+ they also need to revalidate the entire thing.   I don't see that there could be auch a big benefit selling basically 5 year old GPU arches... Sure they got a bit more efficient but stop joking around...   They save so little and lose so much trust.",hardware,2026-01-12 16:32:30,39
AMD,nz7va7m,Must be the OEMs fault,hardware,2026-01-12 19:39:36,11
AMD,nzctan1,"I think we've been gassing up amd too much, cause they keep putting out crap",hardware,2026-01-13 14:18:12,6
AMD,nz8uas6,Panther Lake auto win.,hardware,2026-01-12 22:23:19,14
AMD,nz9ucog,AMD how many years have you been selling the same iGPU?    AMD is truly not innovating lately,hardware,2026-01-13 01:34:47,10
AMD,nz7h15r,"I can only imagine when AMD planned these generations they looked at Alchemist and went, good luck with that, weâ€™re good for a couple of years. Perhaps the rapid development of the Xe graphics caught them by surprise.",hardware,2026-01-12 18:34:57,10
AMD,nzb0r0s,"Personally, coming from someone who can only afford a mid range laptop and has done some intensive research, I can't believe I'm going to end up going with the Core Ultra 5 125H (I know it's an old release, but it's still more than enough for my needs and pretty battery efficient). I used to be an AMD hardcore fan, but AMD's mobile mid range lineup (Ryzen 5s) GPU performance has been laughable since the days of the Core Ultra Series 1. Their saving grace was the GPU driver, but I'm sure Intel's team is not sleeping. And don't forget, XeSS3 is coming to Series 1 Core Ultra.",hardware,2026-01-13 05:43:10,3
AMD,nzejgaf,"with this attitude, even Wildcat Lake will win the budget segment. I've seen OEMs making less intel laptop in favor of AMD back in zen2 times and during Alder Lake times, intel could only compete in mass produced cheap stuff. They really had the chance to seize the market like desktop market",hardware,2026-01-13 19:18:48,3
AMD,nz6q1nr,DOA if this is not for Ryzen 5 and below only. Or with a fat IPC bump of 15%  The higher product though looks really good though for CPU,hardware,2026-01-12 16:32:20,11
AMD,nz6qnga,"i mean hey don't worry though i'm sure by now amd made a strong statement of ongoing support for rdna2 and 3 in regards to graphics both with longterm drivers and the latest features, RIGHT?  amd wouldn't release MORE older architecture apus AND have an int8 version of fsr4, but they won't release int8 fsr4 officially right?  that would be utterly insane and not sth, that amd would be doing right?",hardware,2026-01-12 16:35:05,7
AMD,nzbz38r,They nerfed the igfx so much because they thought their customers wouldn't need a lot of graphic performance & they need to create space for a large XDNA block (NPU).,hardware,2026-01-13 10:54:32,2
AMD,nz6ke0u,"Hello 996forever! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-12 16:06:34,1
AMD,nze56fp,I suspected what finally made AMD put its foot down about that windows scheduler BS was their big/LITTLE answer coming soon...,hardware,2026-01-13 18:15:30,0
AMD,nz6mucq,"AMD repeating the same shit Intel did in the past when they were the dominant brand with next to no improvement on the ""next gen"" lineup. How AMD is repeating this when this is exactly how Intel allowed them to catch up is beyond me.",hardware,2026-01-12 16:17:46,129
AMD,nz6z9mx,Steam deck 2 delayed til 2029,hardware,2026-01-12 17:14:34,23
AMD,nz7nviq,"My guess, based on what AMD has said about RDNA4 and UDNA, is that they didn't put the R&D into cutting RDNA4 down any smaller for iGPU usage. So far RDNA4 only has 2 die configs, 64 and 32cu. The 32cu config is exactly just 1/2 of the symmetrical 64cu config. And if you look at the die diagrams, it would take significant redesign to cut the size down further, far enough to fit into the space of 8-12cu RDNA3.5 spaces.  My guess is that UDNA is being designed for all sorts of configs, but has caused problem after problem for them... Just like every new GPU uArch family does for them. TeraScale, GCN, RDNA1. Each first iteration was a mess. I'm betting they wanted to get UDNA into iGPUs a lot sooner, and launch dGPU UDNA afterward. A guess.  NV hasn't been immune to this in the past. The FX Series, Tesla, and Fermi were all either under-performers, or problematic in their first iteration.   And weirdly Blackwell had a lot of initial driver issues. Way more than most people experienced with most generations of GeForce. I've used a variety of NV GPUs all the way back to 1999, and Blackwell was by far the messiest driver launch I can remember. Hardware is good, though, it seems.",hardware,2026-01-12 19:05:23,24
AMD,nz6kuhx,Should have called it RDNA 3.75,hardware,2026-01-12 16:08:39,39
AMD,nz6mx7e,So what is there plan to compete with Nova Lake? Just hope Intelâ€™s driver team dies of laughter?,hardware,2026-01-12 16:18:07,24
AMD,nz74coq,terrible,hardware,2026-01-12 17:37:51,9
AMD,nz7tzy1,"This is so laughably bad.  Any gpu without proper ml upscaling isnt viable anymore, especially low tier mobile shit that cant brute force good enough image quality.",hardware,2026-01-12 19:33:37,6
AMD,nzbfbk1,RDNA3+++. Its not the second iteration of RDNA3 here.,hardware,2026-01-13 07:47:16,2
AMD,nz7aex3,It's supposed to be a drop-in replacement so there's no improved ddr5 support. Makes no sense pushing out better gpus that remain memory starved I think.,hardware,2026-01-12 18:05:05,6
AMD,nzavz4o,so basically RDNA 3++? How very Intel of them.,hardware,2026-01-13 05:08:22,2
AMD,nz9ntal,More probably makes little difference without also doubling the ram channels.,hardware,2026-01-13 00:58:41,1
AMD,nzb2nzg,What if more CUs are pointless with the memory system and TDP limits?,hardware,2026-01-13 05:57:55,-1
AMD,nz6qtmd,>less than 12 CU RDNA3.5 in the 890M  Actually 16CU in the 890m. But it's memory bottlenecked so it only performs like 30% better than the 8CU 860m. But even if they bump up to LPDDR5x-10667 I don't see 8CU beating the 890m running on 8533.,hardware,2026-01-12 16:35:52,31
AMD,nz7am4k,"At this rate, it looks like Qualcomm will be having better iGPUs than AMD lol  (for mainstream 128b parts)",hardware,2026-01-12 18:06:00,14
AMD,nz75e7o,Corporations just do what their leadership sees as the most profitable way forward. For AMD thatâ€™s using their finite capacity at TSMC to make the products that have the best margins and apus for 1200 dollar laptops are not it. Market share isnâ€™t as good as scooping up profits from over investment in AI infrastructure.,hardware,2026-01-12 17:42:34,32
AMD,nzaq729,Intel currently has 79% of the laptop market share,hardware,2026-01-13 04:29:43,7
AMD,nz8m4uw,"It's worse, they're nowhere near 45% in laptop market share",hardware,2026-01-12 21:44:51,14
AMD,nz803w1,All money is in AI.,hardware,2026-01-12 20:01:42,2
AMD,nz6s2mj,">Samsung even has put RDNA4 in the newest exynos 2600 if I'm not wrong  This is what I'm eagerly waiting upon. If this is true (which has to be likely in some way since they've already marketed >50% in RT performance and some ML upscaler/framegen), then this is a real head scratcher for Medusa Point.",hardware,2026-01-12 16:41:30,15
AMD,nz71c0c,"Once again the consumer market is sacrificed on the alter of data centres. The RDNA 4 is made on tsmc 4nm which is still used for lots of enterprise skus, AMD aren't going to waste what allocation they can get of that node on consumer grade APUs. Intel has a chance to steal a march on AMD here because they can make their APUs on their own fabs and are not competing with an enterprise class product on their latest node... yet.",hardware,2026-01-12 17:24:00,9
AMD,nz6s6kn,"This makes me wanna get a laptop with Intel APU for my next buy, AMD can keep milking their RDNA 3.5",hardware,2026-01-12 16:42:00,13
AMD,nz705ds,Infuriating? I can never be infuriated by an inferior product when there's a superior one there for the taking.  Panther lake baby,hardware,2026-01-12 17:18:37,10
AMD,nz9w0zq,2023-2027 for the low power class  But the improvement from rdna2 igpu from 2022 to rdna2 in 2023 was already mediocre. Their last real jump was from vega to rdna2.,hardware,2026-01-13 01:43:59,10
AMD,nza8orb,"Well, this is the same AMD that dropped support for Vega GPUs in their drivers despite them still selling CPUs with Vega iGPUs as part of the ""Ryzen 7000 series"" at the time, and this is also the same AMD that thinks the Ryzen AI 7 445 deserves that 7 moniker despite only having 6 cores (with not even half of those being full cores rather than compact ones) and only having a 4 CU iGPU, so yeah they've been smoking some weird stuff over there for quite a while.",hardware,2026-01-13 02:51:24,8
AMD,nz8s8t5,"> Perhaps the rapid development of the Xe graphics caught them by surprise.  I don't know if you've seen the Battlemage technical powerpoint/presentation, but a lot of what they were trying/ended up implementing was the kind of thing you could spun up and then sell off a startup for; just for the juicy patents. It is actually really surprising that they got everything working. Intel mismanaging the team and Celestial taking *so* long to tape out/ship out is something else entirely, but Battlemage was actually a huge success as far as performance goes.",hardware,2026-01-12 22:13:23,3
AMD,nzeentz,"Please donâ€™t get a Core Ultra series 1 to use XeSS, those GPUs run the DP4A path.  Minimum Core Ultra Series 2 and above get XMX",hardware,2026-01-13 18:57:12,3
AMD,nz6t0gd,This is a replacement for the ai 7 450,hardware,2026-01-12 16:45:45,10
AMD,nz6r4si,Baby it's Ryzen 7. Their Ryzen 5 is still 6 core. Actually they are currently making a QUAD core mobile ryzen 5 (AI 330).,hardware,2026-01-12 16:37:16,28
AMD,nz6rh0d,Imagine if the only AI upscaler for AMD iGPUs will be XeSS?     That would be absolutely hilarious and depressing.,hardware,2026-01-12 16:38:48,15
AMD,nz7jz9s,">i mean hey don't worry though i'm sure by now amd made a strong statement of ongoing support for rdna2 and 3 in regards to graphics both with longterm drivers and the latest features, RIGHT?  Since almost all of you misunderstood the situation - AGAIN - the only thing AMD was dropping was specific DAY 1 optimizations for specific games on architectures more than two generations old(which does not include RDNA3 by the way).  Things that usually only amount to a small boost and often only in some situations/setups.  General driver support, optimizations, bug fixes and feature support has not been dropped.  Very little is actually going to change, and it's basically exactly matching what Nvidia has done for a very long time.  If any of y'all actually think Nvidia is optimizing new drivers to boost performance for Pascal, Turing or Ampere GPU's specifically for the latest game releases - they are not. lol   Plus, drivers for older architectures are generally pretty darn mature already.  There's simply going to be much less to squeeze from them, which is why it makes way more sense to focus on getting more out of newer architectures that still have more room for improvement.",hardware,2026-01-12 18:48:00,-5
AMD,nzc0qtk,"Out of the three Qualcomm Intel and amd, amd seems to be by far the least generous with die area (on advanced nodes)",hardware,2026-01-13 11:08:51,3
AMD,nz6nk45,Like even though Nvidia has 90% market share they still are not sleeping.,hardware,2026-01-12 16:21:01,66
AMD,nz6sini,and they're not even on desktop so pushing frequency through refinements is entirely useless too,hardware,2026-01-12 16:43:30,16
AMD,nz9hihv,"except Intel in the past actually know what dominant means, it means having absolute performance leadership and comanding over 85% of market share across all platforms.  AMD, dominant? Check their puny market share. It is a joke.",hardware,2026-01-13 00:24:54,15
AMD,nzbc99b,Yup. Itâ€™s not like AMD doesnâ€™t have enough APU/iGPU competency; they power consoles after all.   Next-gen products using last-gen graphics arch thatâ€™s already missing official support for some key software features; great.,hardware,2026-01-13 07:19:01,1
AMD,nzbfe91,"AMD was seen as an underdog, so in typical AMD fashion it intentionally squanders any goodwill it has.",hardware,2026-01-13 07:47:59,1
AMD,nzh3i74,"It's a natural consequence of generic HR processes being brain dead. When performance is based on metrics, and engineering is already strong enough, what moves the needle are salesmen, manipulators, and liars -- not your average engineer wanting to make a product for themselves. HR only cares about this year, not next year.  when everyone is fleeing Intel, how do you tell the difference between an ambitious manager, and a manager that contributed to Intel's decline? I guarantee you AMD didn't know or care during their COVID boom, and is going to end up in the same situation as Intel once it's fermented long enough.",hardware,2026-01-14 03:09:25,1
AMD,nz7x08y,"Wasn't it already? Valve said 2028 at the earliest, then mentioned 2029.",hardware,2026-01-12 19:47:29,18
AMD,nz9bk5m,Theyâ€™re probably more likely to go arm,hardware,2026-01-12 23:52:56,-10
AMD,nzbcv0e,"Blackwell is funny because the arch is not that different to Ada. The changes are mostly GDDR7 and new tensor core version; and higher power limits. And itâ€™s made on the same node process.   Itâ€™s the most incremental consumer NVIDIA generation, yet it also had the buggiest launch and initial driver quality.   It really makes me believe they failed to meet the deadline for more ambitious core changes, and just reverted to Ada but added some new pieces in. Hence, driver team having to pivot and less time to test/validate.",hardware,2026-01-13 07:24:28,4
AMD,nze4gc0,"So/or they've made the decision to focus effort on getting UDNA working rather then validating a iGPU RDNA 4 - tbh, makes sense, 3.5 is getting old, but it may make sense to focus on UDNA and then tape out iGPU versions of that once it's running.",hardware,2026-01-13 18:12:21,1
AMD,nz6m0gg,RDNA 3+++  RDNA3: Desktop (2022) and Phoenix (2023) and Hawk Point (2024)  RDNA3+: Strix Point (2H 2024)  RDNA3++: Gorgon Point (2026)  RDNA 3+++: Medusa Point (2027),hardware,2026-01-12 16:14:00,61
AMD,nz6nb50,Just shows how AMD will never release APUs supporting current gen FSR,hardware,2026-01-12 16:19:54,27
AMD,nz8cv3l,"That sounds a little too clear and simple for me, can we bring out the decoder wheels again.",hardware,2026-01-12 21:01:47,2
AMD,nz6o46z,"Don't think they care. As long as Venice still shits on next gen Xeon (99% it will), they're happy with DC money.",hardware,2026-01-12 16:23:34,29
AMD,nz6osu9,AMD won't have an answer in the mainstream ultrabook iGP segment (15-35w) to Panther until 2028+ never mind Nova. Desktop CPU wise I fully expect Zen 6 to retain gaming leadership and therefore DIY leadership. But definitely a collapse in laptop.,hardware,2026-01-12 16:26:41,19
AMD,nz6nj78,Expecting Intel to require 9000+ MT/s sticks to compete with AMD,hardware,2026-01-12 16:20:55,3
AMD,nz96k90,"Well, for this particular chip, there isn't much of a concern. Intel's reportedly keeping Xe3 for the lower end of the NVL stack. And tbh, rumors have Intel stalling a bit in this area going forward. RZL will probably reuse the NVL dies (so a mix of Xe3 and Xe3p), and even TTL is rumored to stick with Xe3p.",hardware,2026-01-12 23:25:55,1
AMD,nz6nswf,Marketing baby AMD has been the go to laptop GPU maker since the 6000 series,hardware,2026-01-12 16:22:08,-15
AMD,nz9c0ex,Intel can make Panther run that much faster using LPDDR5,hardware,2026-01-12 23:55:21,6
AMD,nzcu4rl,+++*  Phoenix (2023) and Hawk Point (2024)  Strix Point (2H2024)  Gorgon Point (2026)  coming soon: Medusa Point (2027),hardware,2026-01-13 14:22:35,3
AMD,nz9oc75,They can try giving more cache and increasing the memory speed. Look at Panther lake cache sizes and running LPDDR5x-10667.,hardware,2026-01-13 01:01:33,6
AMD,nzbcxws,Or just get better at supporting faster memory like Intel; and get more bandwidth.,hardware,2026-01-13 07:25:13,3
AMD,nzb2v1n,"Newer architecture, more cache, and higher memory clocks?   Like what Intel is doing.",hardware,2026-01-13 05:59:29,4
AMD,nzbghe5,"Then you fucked up your memory design. Altrough for AMD, that was an issue since Zen 1.",hardware,2026-01-13 07:58:03,2
AMD,nz7cnh1,">But it's memory bottlenecked so it only performs like 30% better than the 8CU 860m  yeah because they're amd so they don't even try to alleviate memory bandwidth pressure by increasing their gpus l2 cache beyond A WHOPPING 2MB. Meanwhile panther Lake gets a 16mb l2, an 8mb side cache to share and even access to the cpus 18mb l3 if it needs",hardware,2026-01-12 18:15:13,25
AMD,nz90s6d,"> Market share isnâ€™t as good as scooping up profits from over investment in AI infrastructure.  If they thought the AI workloads were really the future, they'd be crazy not to collect as much market share now with AI developer-enabled enthusiast hardware as they could get.  Nvidia is where they are now because basically every GeForce card they shipped from like 2007 to 2017 fully supported CUDA. AMD continues to not compete - today that means turning down a decent share of AI revenues post-2028.",hardware,2026-01-12 22:55:29,13
AMD,nz9kfe3,"finite capacity is manufactured by AMD themselves, TSMC has enough for AMD to book more. Just look at Nvidia, they sell way more chips than AMD; despite GPU's larger die, a GPU will never beat CPU in profit margin per die area. So there is no way AMD CPU departmant cannot outbid Nvidia.",hardware,2026-01-13 00:40:20,4
AMD,nzbhmiq,"I can't imagine sitting in leadership and thinking that doing the bare minimum is good for business. Even if there are better margins for what they're currently making, I'd rather crush the competition into dust by being better.",hardware,2026-01-13 08:08:49,2
AMD,nzd3pl0,"They could use 2nm Samsung capacity to laptop chips, it would infinitely better than this",hardware,2026-01-13 15:11:16,1
AMD,nzbgkva,"Who said laptop marketshare. I was talking everything. Server, embed, enthusiast/diy. Last I remember AMDs was in the 30s before Ryzen 9000.",hardware,2026-01-13 07:58:57,-1
AMD,nz79ntg,> exynos 2600  Was that intentional?,hardware,2026-01-12 18:01:42,8
AMD,nz7cp7u,"AMD produces monolithic laptop APUs for the most part, or at least laptop-specific I/O+GPU parts.  If they produce a laptop APU, thats a fixed amount of allocation going into laptops, it doesnt matter if RDNA3.5+ or RDNA4 is on there.  If it uses like 250mm^2 of N3 or whatever, then it doesnt matter if its RDNA3.5 or RDNA4, the 250mm^2 is not goint to DC either way.  This is not DC vs Mobile, this is just AMD not wanting to spend the 3 engineers for a week to validate RDNA4 on mobile or something...",hardware,2026-01-12 18:15:26,9
AMD,nz9lkr4,"laptop APU sells more profit than desktop, following that logic AMD should have abandon desktop first.",hardware,2026-01-13 00:46:31,3
AMD,nz7j1e9,"TSMC 4nm is basically just a 5nm family process that has been used for actual products since 2020!  They're not gonna use anything older than that.   Zen 6 is supposed to actually use TSMC 2nm.    And regardless, RDNA4 is not inherently tied to any specific process node.",hardware,2026-01-12 18:43:54,5
AMD,nz7c6a3,"> Once again the consumer market is sacrificed on the alter of data centres.  One thing this sub could stand to remember is that consumer products was never Intel's, Nvidia's or AMD's first line of revenue.",hardware,2026-01-12 18:13:03,4
AMD,nz8rb04,">This makes me wanna get a laptop with Intel APU for my next buy,  Good luck; Intel is aware they're the only premium APU option (unless you work with LLM's or other ML applications locally, the AI max chips can connect up a lot of RAM) and prices accordingly. I was trying to find a cheap Lunar Lake platform and the cheapest half-decent platform/config was like $1500 or $1600.",hardware,2026-01-12 22:08:54,-7
AMD,nz7hwlq,People want competition.  Gives customers more options and usually better value.,hardware,2026-01-12 18:38:51,4
AMD,nz7dprr,"Its just the sentiment that when enterprise makes money leave the consumers dead on the street.  Its great that Intel might have a strong mobile offering, but if all the companies would just drop consumers, as hard as AMD and Nvidia, as soon as enterprise prints money, thats just a bad market situation for consumers.  For AMD there is no real reason to dirty their history like this. They are just avoiding improvements for the fun of the game.",hardware,2026-01-12 18:20:03,2
AMD,nz7ar1h,3rd party reviews?,hardware,2026-01-12 18:06:37,0
AMD,nzajthx,Adjusted for Alchemist's actual release date Battlemage and Celestial are on target for standard GPU lifecycles. Biggest worrier right now is RAM-AGGEDON which has seemingly killed the 50 series super cards.,hardware,2026-01-13 03:52:18,3
AMD,nzfn750,"Thanks for the information. Well, I can upgrade to the Core Ultra 5 225H. It still doesn't have the Xe2 GPU, but itâ€™s said that the GPU now supports XMX. Sadly, the model with the 225H CPU loses the soldered LPDDR5X RAM and comes with slower DDR5 SODIMM (hey, itâ€™s a plus for upgradability though). I mean, the GPU itself is already faster than the one in the 125H, but I wonder how much performance loss from the switch to slower DDR5 RAM will affect the GPU performance.",hardware,2026-01-13 22:23:35,2
AMD,nz9glmf,According to that leak it seems that the ryzen 9 will have the same iGPU too.,hardware,2026-01-13 00:19:59,8
AMD,nz7j0gy,And it's terrible   At this rate every product is a ryzen 9,hardware,2026-01-12 18:43:48,10
AMD,nza9gk4,"Oh, they're coming out with a Ryzen ""7"" that's actually 6 cores too lmao, see the Ryzen AI 7 445. That stupid thing also only has a 4 CU iGPU too, not like the NPU is any faster either so I guess AMD marketing figured they can just do whatever the hell they like since they haven't all been fired yet evidently.",hardware,2026-01-13 02:55:35,1
AMD,nz70arf,for an office fleet 4c/8t is honestly more than enough.,hardware,2026-01-12 17:19:18,-12
AMD,nz6o2vm,"In NVIDIAâ€™s position you arenâ€™t trying to compete with the competition, youâ€™re just competing with yourself trying to get your existing customers to upgrade.   But in NVIDIAâ€™s case they do have competition in the HPC space so thatâ€™s whatâ€™s driving their innovation.",hardware,2026-01-12 16:23:24,50
AMD,nz7zazf,"Yes, but probably because valve knows theres no RDNA4 socs slated for release till then. I mostly commented in jest, but RDNA3 being shoved into every product is probably one of the main obstacles to a deck refresh.",hardware,2026-01-12 19:57:59,18
AMD,nz9o458,"Nah, they won't get 2x performance vs the OG Steam Deck with a similar power envelope by going arm.  Unless they've got some Apple-esque black magic with a side of extra Proton, anyway.",hardware,2026-01-13 01:00:20,7
AMD,nzbflba,There is no way a gaming handheld is going ARM without significant changes in how games run and then a decade or so of switching to those changes.,hardware,2026-01-13 07:49:47,1
AMD,nzbf33j,"> and new tensor core version  TBF, the way GPUs work these days, that IS a big deal.",hardware,2026-01-13 07:45:04,2
AMD,nzc6tmi,"Don't let the performance fool you. It's a lot different.  The ALUs are designed different to all cards made since rtx 20. The SM ALUs are more like Maxwell design derivative. That doesn't change compute throughput, but it changes behavior of occupation of math units.  They added a new hardware scheduler at GPC level.  New RT cores with several extra features  New tensor cores",hardware,2026-01-13 11:59:40,2
AMD,nze4ub0,My theory is losses in client driver staff who got out when the going was good bc bubble and/or possibly some serious problems with shit like maybe the new hardware scheduler.,hardware,2026-01-13 18:14:04,0
AMD,nzh8he8,> So/or they've made the decision to focus effort on getting UDNA working rather then validating a iGPU RDNA 4  Take a look at the layout of RDNA4. Reshaping that die would take a significant redesign. Either they did or they didn't design two completely different die layouts. The design time to reshape the entire GPU uArch on hardware wouldn't be something they'd do and then abandon. The cost (labor hours) would be nuts.,hardware,2026-01-14 03:39:06,1
AMD,nz76vac,RDNA3 family of products,hardware,2026-01-12 17:49:13,18
AMD,nz99ef5,RDNA 3++++......Would RDNA again!!,hardware,2026-01-12 23:41:20,11
AMD,nz6qunk,"*Announcement Day*  Media outlets: ""So does Medusa Point support FSR Redstone?"" (which in this time frame would probably be out for a \~year)  AMD rep: ""Uhh....""",hardware,2026-01-12 16:36:00,21
AMD,nz6piqh,"Their Epycs have been shitting on Xeons since Zen 2, but adoption is climbing very slowly. Revenue share is a lot higher, but unit share isn't. Their real money now is those instinct cards.",hardware,2026-01-12 16:29:57,23
AMD,nz6ode1,"Yeah, this is probably the unfortunate truth.",hardware,2026-01-12 16:24:43,2
AMD,nz78bsx,Nobody's stopping AMD from making a working IMC.,hardware,2026-01-12 17:55:44,35
AMD,nz8pzke,"Would they even use sticks? I thought Intel preferred high speed LPDDR5X, which are relatively cheaper than DDR5 IC's, for these high end thin & light, low power platforms.",hardware,2026-01-12 22:02:38,2
AMD,nza99ds,Panther isn't a drop in replacement for arrow-lake. Of course it supports faster memory.,hardware,2026-01-13 02:54:31,2
AMD,nzc24hi,Until LPDDR6 is available these APUs are limited to 128-256bit LPDDR5.,hardware,2026-01-13 11:20:52,0
AMD,nz8qqc4,"> Meanwhile panther Lake gets a 16mb l2, an 8mb side cache to share and even access to the cpus 18mb l3 if it needs  I was curious how Intel was getting such better perf out of Arc (not that I doubt it's efficient/decent) given the similar die sizes, this answers that, thanks",hardware,2026-01-12 22:06:10,12
AMD,nzbd5md,Itâ€™s funny because doubling L2 or allocating more die area to memory is relatively one of the simpler changes you can make engineering wise.,hardware,2026-01-13 07:27:10,6
AMD,nz9c5u0,Amd is pathetically stingy on die area even on an ancient node,hardware,2026-01-12 23:56:11,8
AMD,nzd3fvi,"Even the Qualcomm 8 Elite, yes a phone chip, has more L2 Cache for the GPU",hardware,2026-01-13 15:09:57,2
AMD,nzcut7k,TSMC has been the go-to excuse for AMD's lack of supply and lack of design wins every generation since zen 2 mobile back in 2020. Time and time again everybody else does just fine on TSMC's latest nodes (at times even more advanced than whatever AMD is using) and somehow only AMD isn't capable of it.,hardware,2026-01-13 14:26:07,2
AMD,nz79w0i,"Damn, autocorrect got me good.",hardware,2026-01-12 18:02:43,8
AMD,nz8ror3,"99% sure it's just that RTG is still relatively independent and didn't want to verify new IP blocks for laptop APU's, and AMD is eating too well to make demands or care. They're both printing money, so who cares? It's not like consumers could afford a better APU at this point in time anyway.",hardware,2026-01-12 22:10:43,5
AMD,nz9i8bm,">This is not DC vs Mobile, this is just AMD not wanting to spend the 3 engineers for a week to validate RDNA4 on mobile or something...  *""...And then, our strategy, okay, Strix Halo \[and\] Ryzen AI Max competes against that (Panther Lake 12 Xe), and it's better than that in terms of graphics performance, all of that. And then, for the mainstream of the market, that don't value that much graphics \[power\], because honestly, most of the people that are using Notebooks, that are outside of the creator or gaming spaces are, you know, they don't need that graphics performance.""*   [https://www.tomshardware.com/pc-components/gpus/amd-is-unphased-by-panther-lakes-big-integrated-gpu-its-not-even-a-fair-fight-to-compare-the-arc-b390-to-strix-halo-amd-exec-claims](https://www.tomshardware.com/pc-components/gpus/amd-is-unphased-by-panther-lakes-big-integrated-gpu-its-not-even-a-fair-fight-to-compare-the-arc-b390-to-strix-halo-amd-exec-claims)  AMD doesn't see value in Panther Lake's level of graphics performance. It's a strategic call -- they think gamers should buy Strix Halo and everyone else doesn't need playable 1080p on their iGPU.",hardware,2026-01-13 00:28:42,1
AMD,nz9fzj7,"Actually until relatively recently it very much was for amd and Nvidia, and still is half half for Intel.",hardware,2026-01-13 00:16:42,6
AMD,nz9kg5v,What makes a laptop with Lunar Lake a better option for what you're doing than something with Snapdragon X Elite?   I see laptops with the latter being sold from â‚¬900 with 32 GB of RAM and 8 cores and â‚¬1100 with 32 GB of RAM and 12 cores.,hardware,2026-01-13 00:40:27,3
AMD,nzhbo7j,"Intel is actually incredibly consistent at keeping prices stable across generations, almost regardless of the competitive landscape. One thing I always gave them credit for is that within the same product tier, a new generation may be priced the same or up to 10% more expensive, but usually nothing crazier.",hardware,2026-01-14 03:58:52,1
AMD,nzd4hof,"AMD ""Strix Point Refresh"" is DOA as a lineup.     Currently you either go Qualcomm for CPU and Perf/W or Intel for GPU and gaming. Perahps AMD Strix Halo for the GPU but honestly, i wouldn't. the RT performance and subpar upscaling will only make this GPU, in the long term, worse than the B390. this is my prediction",hardware,2026-01-13 15:15:03,1
AMD,nz9espj,Notebookcheck tested 3 games:  https://www.notebookcheck.net/Intel-Panther-Lake-with-Arc-B390-takes-on-AMD-Ryzen-Strix-Halo-and-GeForce-RTX-4050-in-our-first-gaming-benchmarks.1200743.0.html,hardware,2026-01-13 00:10:22,7
AMD,nzdfaa5,"[Celestial supposedly exited the design phase 7 months ago](https://www.tomshardware.com/pc-components/gpus/intel-arc-xe3-celestial-gpu-enters-pre-validation-stage), so we should really expect leaks on its silicon very soon if they're on track.  > Biggest worrier right now is RAM-AGGEDON which has seemingly killed the 50 series super cards.   I think Intel can make a lot of marketshare by pricing accordingly. A lot of 20/30 series owners and low end 40 series owners are looking to upgrade, and I believe Celestial can scale up to the point of a 9060 XT or 9070 to deliver them that level of performance.",hardware,2026-01-13 16:05:38,2
AMD,nzcwqcg,Another tweet says top Ryzen 9 up to 22 cores (8+12+2) while Ryzen 7 gets 10 cores (4+4+2). If true this is levels of starbucks upselling never before seen on mobile.,hardware,2026-01-13 14:36:00,3
AMD,nzaufrv,They truly stopped giving af about client,hardware,2026-01-13 04:57:47,2
AMD,nz75vvs,"Ah yes, Intel feeling vindicated",hardware,2026-01-12 17:44:48,23
AMD,nz9btm8,So is a $400 laptop with an Pentium.,hardware,2026-01-12 23:54:21,2
AMD,nzbh6si,it depends. In my office CPU bottleneck is common. When my script is running the 8c/16t CPU is fully loaded.,hardware,2026-01-13 08:04:40,1
AMD,nzb3af8,Cannibalise your own product - Jensen Huang  [https://www.youtube.com/watch?v=9OWpxVwL8YU](https://www.youtube.com/watch?v=9OWpxVwL8YU),hardware,2026-01-13 06:02:52,2
AMD,nzb2hlh,"Maybe they should consider Intel for their next Steamdeck, or even ARM, as crazy as it sounds",hardware,2026-01-13 05:56:32,1
AMD,nz9y3l5,"I think we are already there. Throw in a flagship smartphone SoC like D9500 or 8EG5 into the Steam Deck, ans you'll easily double the performance at the same power.  [https://youtu.be/3yDXyW1WERg?si=v53xylTx2vH7TQ2O](https://youtu.be/3yDXyW1WERg?si=v53xylTx2vH7TQ2O)  Valve is funding Fex.",hardware,2026-01-13 01:54:56,-8
AMD,nzbfd4p,"Yes, totally, we see this with FP8 DLSS4.5 today. Iâ€™m just commentating that this has been a very minor architectural revision on the other fronts, so buggy driver support around things like even basic display output (nothing to do with tensor cores) was surprising.",hardware,2026-01-13 07:47:41,1
AMD,nz6rmcf,"""We have amazing software. And we have amazing hardware""  ""Does the amazing hardware in question support the amazing software in question?""  ""...""",hardware,2026-01-12 16:39:29,34
AMD,nz9igjm,"I mean technically it does support Redstone. ""Analytical"" version, aka FSR3.   AMD is shuckin' and jivin'. True visionary of PR and marketing. As one wise man said: jebaited.",hardware,2026-01-13 00:29:54,3
AMD,nz8lwhz,"Revenue share for DC is huge, the amount of margins they can get means more cash for r&d",hardware,2026-01-12 21:43:49,1
AMD,nz9jjk6,"if you put that situatiuon in Nvidia shoe, Nvidia will be pulling agressively until they reach 80% market. It was only when they are 90%+ share they slow down a little.   Somehow AMD seems to be happy with their market share % now.",hardware,2026-01-13 00:35:38,-1
AMD,nz9srzk,Reading this immediately gave me flashbacks to the thousands upon thousands of /r/buildapc posts from Zen / Zen+ (i.e. Ryzen 1000 and 2000 series) owners asking for help due to their systems being unable to run stably while using XMP/EXPO profiles at anything over 3000MT/s.,hardware,2026-01-13 01:26:03,6
AMD,nz89k2i,"Sure, but those ram sticks are far more expensive still. You'd be paying more for equivalent performance.",hardware,2026-01-12 20:46:10,-9
AMD,nzauc89,"Consumers donâ€™t care, laptops with Panther start at 1100. OEMs also donâ€™t care all of them will default to Panther with all their mainstream & premium ultrabooks this year without question.",hardware,2026-01-13 04:57:07,2
AMD,nzcu9ej,Intel seems to be able to do a lot more with 128 bit LPDDR5x at the low power range (15-45w) just fine.,hardware,2026-01-13 14:23:15,2
AMD,nz9yita,>given the similar die sizes  Better node. Amd decided consumer plebs donâ€™t deserve anything better than refined 5nm (2020) in 2026.,hardware,2026-01-13 01:57:09,4
AMD,nzbxs9v,It isnâ€™t like amd hasnâ€™t done it. Theyâ€™ve relied on cache on desktop and server to compensate for their terrible IO dies.  Theyâ€™ve just decided the mobile plebs donâ€™t deserve to have more die area for cache. Even on a node as old as N4 at this point.,hardware,2026-01-13 10:43:02,3
AMD,nzbi33k,exactly,hardware,2026-01-13 08:13:11,1
AMD,nz87vb4,"Whoops, didn't mean to be rude. I thought I made the error when I quoted lol",hardware,2026-01-12 20:38:10,3
AMD,nz9fsyo,>It's not like consumers could afford a better APU at this point in time anyway.  The laptop OEM is proportionally hit much less hard than the diy space and there has already been Panther lake laptops announced for as low as 1100-1200. Full Strix point back in July 2024 only launched in laptops 1500 and up.,hardware,2026-01-13 00:15:45,3
AMD,nzbepe7,Problem is Strix Halo is priced as if it's made of 24 carat gold.,hardware,2026-01-13 07:41:33,5
AMD,nzbhsou,">AMD doesn't see value in Panther Lake's level of graphics performance. It's a strategic call -- they think gamers should buy Strix Halo and everyone else doesn't need playable 1080p on their iGPU.  Okay, where are the Strix Halo laptops that we can buy then? There's literally just the Flow Z13 and HP Zbook Ultra atm",hardware,2026-01-13 08:10:26,2
AMD,nz9mi71,"GPU performance, general compute. 1st gen X Elite only runs some software and not even better than Lunar Lake.  As a matter of fact, their advertised performance metrics were exclusively with the -84 SKU, which appears for all intents and purposes to be a Samsung exclusive.   Most of them are the -78 SKU, which decidedly *cannot* live up to the performance claims, and the -80 which is only a sidegrade.  Plus, most of these laptops are bad platforms. Tons of keyboard flex, unpleasant touchpads, lackluster screens... It's pretty clear they were being told by Qualcomm that people would spend big just to get an ARM laptop and if they *wanted* to do that, they can just go buy an Apple where they completely trounce Qualcomm, *and* the entire ecosystem supports the silicon.  Not enough people are spending ~$1300 on a laptop just to run a subset of the software they use to justify thinking about them. I'm kind of shocked you asked.",hardware,2026-01-13 00:51:32,3
AMD,nzaja69,">As usual, actual performance of the Arc B390 is likely to depend heavily on the power limit available to the iGPU and on the speed of the RAM in the respective laptop, since this also serves as VRAM for the iGPU.Â   Yeah that's not really a 3rd party review if the test system is provided by a 1st party source",hardware,2026-01-13 03:49:21,-5
AMD,nzd2x81,"Ryzen 7 getting a 10 core config would be suprising, this is literally them saying Ryzen 7 is staying 8 cores while Ryzen 9 will go from the range of 10 to 22...",hardware,2026-01-13 15:07:23,1
AMD,nz7sk77,"Itâ€™s evolving, but backwards",hardware,2026-01-12 19:26:54,7
AMD,nz9gbzr,There's a big difference between a basic machine that's meant to write emails and not much else and a top end SKU.    Also from an IPC and clock speed perspective 4C/8T Zen 6 is likely to be 40-80% faster than SKL in most tasks.,hardware,2026-01-13 00:18:32,-1
AMD,nza082e,"But will you get 2x the performance in say, Cyberpunk 2077 or Indiana Jones and the Last Circle without graphical artifacts?  That's where I have my doubts...I may be wrong--hell, I *hope* I'm wrong, but until someone releases an ARM PC that can play everything the Steam Deck can at higher framerates (or the same framerates at higher resolution), I'll continue to doubt.",hardware,2026-01-13 02:06:18,12
AMD,nzc3fur,Theres a huge difference between emulating DX on linux via proton and jumping to emulating an entire different ISA. The SOCs may have the raw performance but is proton on arm going to have that performance?,hardware,2026-01-13 11:32:26,3
AMD,nz9kuwl,Even at 90%+ Nvidia is still aggressively try to get their customers to upgrade. Amd is justâ€¦there,hardware,2026-01-13 00:42:40,0
AMD,nzckdui,"One of my first gen zens couldn't even do 3000, just 2933. Anything above that was unstable.",hardware,2026-01-13 13:29:47,2
AMD,nz9d8u3,Panther lake uses lpddr5x-10667. Nobody told amd to be stuck on 8533.,hardware,2026-01-13 00:02:01,7
AMD,nzb0i8g,"I don't care about that. I'm simply talking about why it's pointless to move to RDNA 4 when the RDNA 3.5 GPUs are still memory starved.   AMD does care about existing design wins though. They still can't, a decade after Zen 1, get OEMs to bother much, so allowing them to pop out strix point chips and call it a day (or better yet, offer multiple generations of chips on the same sku) matters.",hardware,2026-01-13 05:41:20,2
AMD,nzdczv1,"Arc is a lot better than just the node difference.  But yes, that surely isn't helping the situation, too :U",hardware,2026-01-13 15:55:10,1
AMD,nz8avqu,"You quoted me the way I intended to write, so it all worked out.",hardware,2026-01-12 20:52:24,3
AMD,nzcv8tj,And not usable in ultrabooks that run at <30w which is the majority of the pc market.,hardware,2026-01-13 14:28:19,1
AMD,nzcvfg0,They added 2 in CES - an Asus convertible and a Tuf laptop. That will be it for 2026.,hardware,2026-01-13 14:29:17,1
AMD,nz9p01q,"I would run Linux on them as I do on all of my other hardware across multiple architectures. And then price/performance is one of the main criteria for choosing a piece of hardware.   So apart from mainlining not going all that well with 1st gen X Elite and presumably a lot better with 2nd gen, I run the exact same software on all of my machines from SBCs all the way to full-blown desktops.   I don't know about the state of compute on 2nd gen and if it's competitive with Nvidia, AMD and Intel, but I guess we'll find out in a few months.   I'll probably get one of the 1st gen devices when those go on sale to clear the last remaining stock and see how things develop with 2nd and 3rd gen chips.",hardware,2026-01-13 01:05:12,1
AMD,nzav065,"Thatâ€™s stupid, by your logic all day 1 reviews are automatically â€œnot third partyâ€ because all of them use review sample sent by manufacturers before retail channel release. No laptop review has ever been representative of ALL laptops using the same chips, regardless of if itâ€™s a test sample or a retail unit.  What it does tell you, however, is the _ceiling_ of what the chip is capable of.   Anything else?  And that ceiling is far higher than the 890m. Boost the 890m to 80w running 64GB of 8533 ram and it wonâ€™t get close. Thatâ€™s all that matters.",hardware,2026-01-13 05:01:39,5
AMD,nzdfgce,There seems to be some conflicting information surrounding the supposed existence of 2 LPE cores on zen 6 apu,hardware,2026-01-13 16:06:25,1
AMD,nzai03y,Zen 6 is also not the product weâ€™re talking about,hardware,2026-01-13 03:42:18,3
AMD,nzazu3z,Is +40-80% vs full decade old stuff the best you can brag about?,hardware,2026-01-13 05:36:19,0
AMD,nzfnak6,"I don't doubt that, as I vaguely recall having to tell a few unfortunate losers of the silicon lottery that they'd have to give up on anything over 2666MT/s on their Ryzen 1600s.    Of course that might've also been because they were using four sticks of RAM (as Zen / Zen+ *really* struggled in such configurations), but this was well over 5 years ago so I wouldn't put too much stock in the accuracy of my anecdotes.",hardware,2026-01-13 22:24:03,2
AMD,nzb15n3,Then either a) their architecture is garbage and memory inefficient and/or b) they are pathetically stingy with cache sizes in mobile despite using an ancient node.,hardware,2026-01-13 05:46:18,2
AMD,nzdfntl,"It is, I only mentioned the node because they mentioned die size. If the die size is similar then the one with the denser node will obviously pack more in it.",hardware,2026-01-13 16:07:22,3
AMD,nz9swv5,">I would run Linux on them as I do  I don't know the current status of Asahi Linux, but I know quite a lot of it works already.  > And then price/performance is one of the main criteria for choosing a piece of hardware.  Then why are you spending $1300 on a low end Snapdragon Elite SoC when you can get an M4 for the same price with a better SoC, chassis, screen, keyboard, and touchpad? For $100 more, you can even stick to the same memory total. You can just say you're biased against them. There is no reason for a rational consumer to ever pick up the Qualcomm-based computer.  >I'll probably get one of the 1st gen devices when those go on sale to clear the last remaining stock and see how things develop with 2nd and 3rd gen chips.  Right now, compared to [what is essentially a top of the line -80 SoC,](https://browser.geekbench.com/v6/cpu/15218722) an M4 currently has [something like 30% faster single core performance and the same multi-core](https://browser.geekbench.com/macs/macbook-air-13-inch-2025-10c-cpu-10c-gpu), with a [75~80% faster GPU according to Geekbench's numbers.](https://browser.geekbench.com/opencl-benchmarks) The Snapdragon GPU's model name on that page is reported as 'X1E80100', if you'd like to compare.  Price to performance is almost incomparable, here. You are paying just as much for almost half the performance. Again, Qualcomm set the cost of the SoC's too high. There's no reason to buy them when an M4 is entry level. $1000 should have bought you the -84 SoC (which is some 20% faster on the GPU than the -78 or -80!) and a premium chassis, not the cheapest parts the OEM can spec out.",hardware,2026-01-13 01:26:48,2
AMD,nzbh22b,"> I don't know about the state of compute on 2nd gen and if it's competitive with Nvidia, AMD and Intel, but I guess we'll find out in a few months.  Compute performance was almost non existent on X1 GPUs. X2 is an improvement (new Adreno 8 architecture), but I'd wager it's still lagging behind AMD/Intel.   ""Obviously weâ€™ll have DirectX 12.2 and all the DirectX versions behind that, so weâ€™ll be fully compatible there. But we also plan to introduce native Vulkan 1.4 support. Thereâ€™s a version of that which Windows supplies, but weâ€™ll be supplying a native version that is the same codebase as we use for our other products. Weâ€™ll also be introducing native OpenCL 3.0 support, also as used by our other products. And then in the first quarter of 2026 weâ€™d like to introduce SYCL support, and SYCL is a higher-end compute-focused API and shading language for a GPU. Itâ€™s an open standard, other companies support it, and it helps us attack some of the GPGPU use-cases that exist on Windows for Snapdragon.""  https://chipsandcheese.com/p/diving-into-qualcomms-upcoming-adreno  > I'll probably get one of the 1st gen devices when those go on sale to clear the last remaining stock and see how things develop with 2nd and 3rd gen chips.  There's some amazing deals for X1 devices already; [$599 Zenbook A14](https://www.bestbuy.com/product/asus-zenbook-a14-14-fhd-oled-laptop-copilot-pc-snapdragon-x-plus-16gb-ram-512gb-ssd-zabriskie-beige/JJGGLH86J4)  I don't think waiting for 3rd gen makes sense, considering it'll probably be an incremental generation. 2nd gen fixes many of the flaws of first gen, with some nice upgrades across the board.",hardware,2026-01-13 08:03:26,2
AMD,nzbgxkd,"There are real concerns with review samples. While for GPU/CPU theres usually no issues, for monitors for example its not unheard of to ship review sample with a better panel then switch to worse panel for actual products.",hardware,2026-01-13 08:02:16,-2
AMD,nze1fop,"Most people don't brag about workhorse corporate machines. If it's something you're bragging about, it means other areas are lacking.      All they need to do is be cheap and turn on.",hardware,2026-01-13 17:58:51,0
AMD,nzb390i,strix halo is right there,hardware,2026-01-13 06:02:34,-1
AMD,nzbg6vt,"> I don't know the current status of Asahi Linux, but I know quite a lot of it works already  Only on M1 and M2. Newer M generations is still in progress.",hardware,2026-01-13 07:55:19,3
AMD,nzbx1f1,"For gpu and cpu, any issue associated with a review sample can only make the review sample look worse, and not better, hence reinforcing my point that it is the _ceiling_ of the capability of the chip.   Itâ€™s not like Ferrari sending reviewers a tuned version of their cars. Power consumption is monitored during reviews.",hardware,2026-01-13 10:36:25,3
AMD,nzg3upm,"Then like my other reply said, a $300 pentium laptop does the same job.",hardware,2026-01-13 23:50:12,1
AMD,nzbgfb0,Strix Halo costs more than entire intel laptop.,hardware,2026-01-13 07:57:31,5
AMD,nzbwrl7,What does that have to do with the target power envelop and cost of the chip in question in this thread? There are separate threads for Medusa halo (mid 2027). Go there.,hardware,2026-01-13 10:33:58,4
AMD,nzbi3ds,Explaining the concept of profit margins is not in my agenda this morning.   But if you think a 5nm chip costs more to produce than an 18A chip then I've got a bridge to sell you.,hardware,2026-01-13 08:13:16,-1
AMD,nyxvzvx,"Consumers electronics show: ""We don't care about you anymore, goodbye.""",hardware,2026-01-11 08:28:31,589
AMD,nyxxtjl,AMD realizes consumers don't have money.,hardware,2026-01-11 08:45:10,401
AMD,nyxvxut,CES = Corporate Electronics Show. AMD must have got the wrong memo thinking it should be selling AI.,hardware,2026-01-11 08:28:00,247
AMD,nyxvpgu,Consumer electronics = AI AI AI Datacenters Datacenters,hardware,2026-01-11 08:25:49,163
AMD,nyy5y9d,It's funny because I'm sure it would be the other way around if AMD was in Intel's position.  Companies don't care about us.,hardware,2026-01-11 10:01:16,58
AMD,nz0il00,"What are so many people going on about in these comments?   CES literally started as a trade show for vendors and distributors of video and audio equipment. When they transitioned to include computing products, there have been traditionally plenty of vendors presenting enterprise computing stuffs as well, since the 80s.   Some grown ass gamers think they are the center of the universe, to the point of assuming CES revolved around gaming in the past, somehow.",hardware,2026-01-11 18:27:56,44
AMD,nyyxl5v,Intel and AMD have switched places. AMD is more and more heavily used in the Enterprise which is extremely lucrative because of the margins so theyâ€™re more inclined to forget to keep their eyes on the ball in the consumer space. This is exactly what happened with Intel 15 or so years ago forcing AMD to focus heavily on consumer. Anyone who remembers the bulldozer flop remembers how it forced AMDâ€™s hand to solely focus on the consumer space after that for a good 10 years.,hardware,2026-01-11 13:44:47,30
AMD,nyyrgxi,"Nobody showed up for consumers, they all showed up for AI.",hardware,2026-01-11 13:04:35,27
AMD,nyxx4dy,"As if Intel didn't wish they could have talked more about AI. Unlike AMD who is at least minor player in the AI infrastructure space, Intel has plainly lost this business. The only relevancy they have is the foundry business and the latest rumor we heard is nvidia has decided against leveraging Intel process node.   So many bullshit opinions and illogical arguments in this article. Literally a waste of time. Cherry on top is the web page crash reloaded when I was about to finish reading, which speaks to the quality of tomshardware website in general.",hardware,2026-01-11 08:38:49,89
AMD,nyxz6w6,I mean no shit? Intel would talk about AI if they could.,hardware,2026-01-11 08:57:42,55
AMD,nz032dy,"Intel is today what AMD was before, the underdog. As such, they appeal to whoever they can for relevance. Rest assured they wouldnâ€™t be doing this if they had any relevance in AI.",hardware,2026-01-11 17:16:21,4
AMD,nyzh5l1,"This kind of shows where Intel thinks the market is going. Discrete GPUâ€™s are going to be replaced by chiplet iGPUâ€™s. Intel just proved you can play Cyberpunk on an iGPU at â€œdecentâ€ frame rates, how cool is that? If they can pull off gains like this over just a few years, then they can start really pushing iGPUâ€™s as a replacement for discrete on the low to middle end of the market, especially with improvements to their Xe3 software stack. The only issue with getting more performance is heat and VRAM. Keep in mind that the performance of Panther lake is being compared to a 4050, which is equivalent to what, a 2060 Super? So in 6 years we have the equivalent of that in a laptop iGPU? This is awesome!",hardware,2026-01-11 15:31:44,6
AMD,nyypflu,"Neither did Nvidia, theyâ€™re AI crazy.",hardware,2026-01-11 12:49:57,5
AMD,nyy6ryv,"These were not CES presentations they were the companies own presentations located in the same place as CES.  The sudden cheering for Intel just because of a presentation, no tests no reviews, yet they have routinely failed to deliver in the recent past, is mind boggling.  None of these companies are your friends.  The Handheld market is tiny and is not going to save Intel. These chips are not going to appear in $350 Steamdecks they are going to appear in $900 handhelds you and no one else can afford to buy.",hardware,2026-01-11 10:08:52,5
AMD,nyy76tb,CES is such a joke now,hardware,2026-01-11 10:12:39,4
AMD,nyy0gbk,Intel had nothing other than mobile chips.,hardware,2026-01-11 09:09:28,5
AMD,nyxv2xr,"Hello Antonis_32! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-11 08:20:02,1
AMD,nz25dhp,Intel has to - companies buy millions of Dell/HP/Lenovo business PCs and servers(HP sold their server business as HPE) - Dell is by far Intelâ€™s most important client. AMD does make server/enterprise CPUs but theyâ€™re more focused on the embedded ASIC(Xilinx) and enthusiast CPU market. Theyâ€™ll be fine with no CES.,hardware,2026-01-11 22:58:24,1
AMD,nz2rkjs,zen 6 cpu isnt due for a year wtf do u want amd to say?,hardware,2026-01-12 00:51:02,1
AMD,nz5cqjs,Intel's been missing from CES forever! Announcing junk every year and wasting our time and money.,hardware,2026-01-12 12:02:10,1
AMD,nz5ulkr,Check the GamersNexus Intel video. They also had a bunch of BS moments and still no news on the new desktop GPU.,hardware,2026-01-12 13:56:43,1
AMD,nzcz37n,CES hasn't stood for that for a while,hardware,2026-01-13 14:48:05,1
AMD,nzee7ev,"AMD executives, engineers, and shareholders are incentivized to maximize stock price. The main driver of stock price has been to bet on AI datacenters.   Iâ€™m a PC gamer and an investor. I have invested in this AI wave and can sell some of those investments to easily fund my purchase of PC parts at current valuations.",hardware,2026-01-13 18:55:09,1
AMD,nyya61q,"The only reason Intel talked about consumer electronics is because they are not that big of a player currently in the AI space, unlike nvidia and amd. Despite a shit ton of tax payer dollar.",hardware,2026-01-11 10:40:04,2
AMD,nz1nzt0,Intel cares about consumers?    Hahahahaha okay bro ðŸ‘  Tell that to those of us that sat on 14nm++++++ for years.  Tell that to 13th and 14th gen CPU owners who has massive degradation issues and Intel left out to dry,hardware,2026-01-11 21:34:59,0
AMD,nyy23t5,It's not the consumers electronic show any more. That name exists only in our hearts.,hardware,2026-01-11 09:25:07,-5
AMD,nz30wx6,Intel doesn't have much choice at this point.,hardware,2026-01-12 01:40:13,1
AMD,nyxye88,"Intel can announce all they want, so long as DDR5 costs eleventy gajillion dollars nobody is gonna buy it.",hardware,2026-01-11 08:50:24,-3
AMD,nyxvz7z,Corporates Eiiiaiiiii Showoffs,hardware,2026-01-11 08:28:21,0
AMD,nz3be4x,lol who would want to show up for what this sub wants ... everything has to be free or cheap and facilitate yearly upgrade cycles so everyone on Reddit can upgrade  You all should join the grifters on YouTube making videos about how AMD and NVIDIA are greedy.   Intel Showed up because they have nothing to offer the Data Centres not coz they love the consumers so much. They have nothing else on offer for Data centres .,hardware,2026-01-12 02:35:44,0
AMD,nyxwthe,"Oh, yeah, lap top chips. I'm so excited.",hardware,2026-01-11 08:36:04,-29
AMD,nyya6zb,"Intel did the same shit like AMD, but less annoying.",hardware,2026-01-11 10:40:18,-9
AMD,nyy7rhg,Consumers will show-up for AMD.,hardware,2026-01-11 10:17:52,-13
AMD,nz2ccu5,The reality is both are an order of magnitude behind Apple Silicon for the mobile market.,hardware,2026-01-11 23:34:41,-1
AMD,nz2hny2,CES means Government policy on cutting red tape to expedite building AI Centre's,hardware,2026-01-12 00:02:02,-1
AMD,nyz6ell,"In reality, Intel wanted to do what AMD did, but couldn't. They had to scream ""we are so back!!!"" to save face, and Tom's Hardware (as well as Reddit apparently) are lapping it up. K6, Core 1/2, Zen 1, and so on weren't handled like this; those were pitched as proper revolutions, and not only did everyone really believe it, Intel and AMD actually *delivered*!  We already know that AMD doesn't have anything new yet, because Zen 6 is still in development. They don't really *need* anything new yet either, because Zen 5 is still very good for anyone willing to buy into it now. They can wait just fine and talk about other things.  > A year ago, it wasnâ€™t clear what the future of Intel looked like, and just six months ago, there were legitimate questions about whether the brand would even survive.  It is still not even remotely clear what the future of Intel looks like, and the question of whether they'll be sticking around for very long is still extremely valid. The idea that Intel GPUs are still going to stick around under Nvidia's thumb is *beyond* hopeful.  Intel merging with AMD would have made more sense at this point, terrifying as that is. Reddit would be in flames, though.",hardware,2026-01-11 14:35:44,-10
AMD,nz0sfed,Intel making IMMENSE improvements in the PC space.   Lisa Su and AMD are trying to be her cousin,hardware,2026-01-11 19:10:39,-2
AMD,nyyh7mh,"Intel only did, because they aren't much use in AI . If they had much to do that in that area, they'd be pushing it too.",hardware,2026-01-11 11:43:31,261
AMD,nyz3qfi,Consumer electronics show: â€œDonâ€™t call us by our dead name.  Were CES nowâ€.   Consumers: ðŸ˜§,hardware,2026-01-11 14:20:46,41
AMD,nz9cwl7,"It's really unfortunate that computer technology is too uniform to allow two competing factions like this to exist in harmony. I wish it was more dedicated so I could tell these AI companies to go to hell, we don't want you people anymore in our consumer space.",hardware,2026-01-13 00:00:12,2
AMD,nz1br4s,That's how capitalism works.,hardware,2026-01-11 20:38:48,1
AMD,nyy57ar,And consumers donâ€™t want their new precious AI.,hardware,2026-01-11 09:54:19,150
AMD,nyya19i,"It's not consumers don't have money, just companies have a shit ton more.",hardware,2026-01-11 10:38:51,53
AMD,nz0ulf0,"Also AMD consumers are fixated on the 5800x3d to the point that a new one in box is more expensive than an AM5 motherboard and chip combo ($750 on ebay, wtf), so AMD gave them the only thing they wanted by announcing they're considering bringing the line back.",hardware,2026-01-11 19:20:25,6
AMD,nyy6vza,And that the Handheld market is tiny anyway.  Its kids cheering on future toys they won't be able to afford.,hardware,2026-01-11 10:09:54,10
AMD,nz0lm8n,AMD could have done the ryzen moment thing and just stuffed a ton of vram into their cards. Like they did with ryzen cpu cores and essentially force companies to program for ROCm. Market it as AI for all or something and just really mix it up against Nvidia. Helps gamers too. But they didnt.,hardware,2026-01-11 18:41:01,3
AMD,nz2gl70,"More like, AMD is following the big money just like Nvidia. What else would you expect of a massive billion-dollar public corporation?",hardware,2026-01-11 23:56:33,1
AMD,nz4mtik,And Intel realized that public opinion did in fact matter for their company and led to less sales on the whole,hardware,2026-01-12 08:06:24,1
AMD,nyyvscn,Gamers definitely have money to spend. They'd rather go hungry.,hardware,2026-01-11 13:33:29,3
AMD,nyywpqz,AMD realizing no consumer would want to buy their crappy GPUs these days anyway. only console manufacturers left. AMD is like chinese knockoff nvidia for 20% less cost or something.,hardware,2026-01-11 13:39:22,-13
AMD,nyxy8l3,They just made a small mistake on their decoder wheel.,hardware,2026-01-11 08:48:59,58
AMD,nyxzlu7,>CES = Corporate Electronics Show.   It's always fun when I say something on Reddit and it gets repeated by everyone else as if it was there own independent idea or thought.,hardware,2026-01-11 09:01:32,-122
AMD,nyyq1le,CES = Corporate Enterprise Slop,hardware,2026-01-11 12:54:25,49
AMD,nyyw56r,Consumers use products made with datacenters and AI. Itâ€™s not that complicated.,hardware,2026-01-11 13:35:46,-29
AMD,nz3eik8,They were switched 10 years ago and it was the other way around.,hardware,2026-01-12 02:52:31,9
AMD,nz45by3,probably but old Intel was always doing some kind of bullshit for R&D purposes so I'd imagine they'd still have something more than a refresh to show,hardware,2026-01-12 05:36:43,1
AMD,nzdcq5p,"I used my company credentials to get into the CES a couple times, and not once did I go look at computer stuff.  There's just so much more cooler stuff to look at like the A/V gear.",hardware,2026-01-13 15:53:58,2
AMD,nz2i6li,"i think AMD still maneged to outdo themselves.   we know you cant afford your 16gb ddr5 or ddr4 ram-kit lets present ""Helios"" with 31TB of HBM4 to cheer you upp",hardware,2026-01-12 00:04:40,3
AMD,nz4gt5k,CES revolved around electronics aimed at consumers. AI datacenter regulation is not electronics aimed at consumers.,hardware,2026-01-12 07:11:34,3
AMD,nz4s39t,"> Intel and AMD have switched places.  Yes, they both switched places. AMD with Intel, and Intel with TSMC and Samsung â€” Mainly due to excessive complacency on Intel's parts, after at least a full-blown decade of stagnation (in the end-user and client-space with quad-cores for a decade straight from 2006â€“2016), and years of hiccups and delays on process-technology (starting back then with 22nm).  Though I don't really see the point to mention it; AMD just filled a blatant innovation-vacuum Intel crafted over ages.",hardware,2026-01-12 08:56:15,1
AMD,nz4srgr,"> AMD is more and more heavily used in the Enterprise, which is extremely lucrative because of the margins. So theyâ€™re more inclined to forget to keep their eyes on the ball in the consumer space.  No. That's actually not the case, like not at all â€” AMD did **not** just shifted business over to enterprise for margins.  AMD since the Bulldozer-days *always* secretly prepared, to attack Intel fundamentally full-scale and with *everything on all fronts at the same time*. A *All-or-Nothing*-attack with that Â»All hands on deck!Â«-mentality from the start â€¦  Their chiplet-approach in and of itself shows, that AMD technologically laid down a fundamental path to success very early, which enabled to aim at virtually everything (from the top at the server-space & enterprise-business down to the end-user, and everything in-between) *from the start*, using *a single yet* ***universally*** *applicable architecture*, which was virtually engineered for years on end with AFAIK millions of man-hours, for being principally usable/applicable for every use-case and market there is, for AMD to go full scale and all-in in the first place â€” Nothing changed since.  *AMD aimed for enterprise and client from the get-go*. AMD's chiplets, engineered for years, is the obvious evidence.",hardware,2026-01-12 09:02:45,-4
AMD,nyy8ri4,">Still, itâ€™s a stark contrast from the AMD of even 12 months ago, and an even starker contrast to Intel. Under the leadership of Pat Gelsinger, the public-facing Intel quickly jumped on the AI boom. Presentations became winding events focused on road maps and geopolitics, as Intel tried to play a game it was struggling to be a player in.  I mean the article did point that out.  Intel still leads the market in client PC hardware sales. That's another relevancy they have, and I'm pretty sure they're glad everyone else gloated about AI. And also, mega companies failing to difersify and choosing to stick to their core market is a good thing. You wouldn't want to live in an alternate timeline where Intel bought Nvidia and 3dfx while they were on their last legs in the late 90s.",hardware,2026-01-11 10:27:04,35
AMD,nyyb9uk,>Unlike AMD who is at least minor player in the AI infrastructure space  AMD is becoming a pretty major player in the AI space now tbh,hardware,2026-01-11 10:50:07,17
AMD,nyy19a6,"thats not a rumor there were articles about it recently, NVidia after testing their node pulled out!  Not the first time this happened with a major company and Intels nodes. Sure the nodes have gotten better by now, but they still have a steep road ahead to keep up with TSMC and to some degree with Samsung!",hardware,2026-01-11 09:17:03,-15
AMD,nyxzovx,Does Intel even have a compute platform built yet?,hardware,2026-01-11 09:02:19,12
AMD,nyy7tx7,AI AI AI AI...,hardware,2026-01-11 10:18:30,0
AMD,nyyansy,"Well Sell didn't and they absolutely could spend time talking about ai servers or what not. It turns out ai just isn't a selling point to anyone but the stock market. They might as well move CES to New York so the hedge fund managers don't have to fly as far, because that seems to be the only audience companies seem to care about.",hardware,2026-01-11 10:44:34,-1
AMD,nz0up55,What was there to prove? You could already play Cyberpunk on an AMD iGPU years ago.,hardware,2026-01-11 19:20:52,13
AMD,nyzow14,"i could totally see how x86 laptop would win against arm in general in this section, if they scale the igpu decently enough with good efficiency",hardware,2026-01-11 16:09:00,3
AMD,nyyblif,">The sudden cheering for Intel just because of a presentation, no tests no reviews, yet they have routinely failed to deliver in the recent past, is mind boggling.  Intel actually did setup a bunch of [testing laptops for Panther Lake at CES](https://www.digitalfoundry.net/news/2026/01/intel-panther-lake-mobile-graphics-entry-level-desktop-performance)  the actual suspicious part is the fact that they were only letting people test the GPU with games, not the CPU",hardware,2026-01-11 10:53:03,26
AMD,nyy8cpd,"The laptop market isn't tiny though. Mobile chips go into a lot of things. I personally ordered over 100 Nova Lake laptops for work, because being able to run meetings all day on a charge is awesome and it's plenty performance for office use.  Very excited to try out Panther lake this year. Not everything is about gaming.",hardware,2026-01-11 10:23:18,12
AMD,nyyba18,"That's what mostly CES has been used for. Nvidia wouldn't introduce their top GPU in it, neither would AMD as pre-Christmas period is a good sale period and newer hardware sell-out easily.",hardware,2026-01-11 10:50:10,11
AMD,nyy44uh,Bro creepy leather jacket man jensen gave a presentation on how nvidia helps palantir at CES. There was literally nothing for consumers. I am surprised he easnmt booed off the stage.,hardware,2026-01-11 09:44:16,42
AMD,nyy2fpp,Intel is a chipmaking companyâ€¦what else would they bring??,hardware,2026-01-11 09:28:16,36
AMD,nyydbsy,Aka the biggest segment of worldwide pc shipment by far,hardware,2026-01-11 11:08:46,12
AMD,nyy2dct,"Yeah, how dare they focus on the most common consumer rather than enthusiast gamers.      It's like, there's a bunch of consumers that use laptops over desktops these days.    ""According toÂ [Statista Consumer Insights](https://www.statista.com/insights/consumer/), 37 percent of U.S. adults still have a desktop PC in their household, which is relatively low compared to 68 percent who own a laptop""  Also seeing something come out of their long await 18A process is interesting.",hardware,2026-01-11 09:27:38,46
AMD,nyybrax,"Yeah, instead they should have brought massive AI GPU racks to the **consumer** electronics show, that **consumers** can totally afford to buy!   That being said, Intel probably would have done the same if they had the same footing in the AI market that NVIDIA and more recently AMD now have.",hardware,2026-01-11 10:54:31,10
AMD,nz3ftoc,"a year would actually be quite long, Zen 5 launched July 2024. A successor two years later at least for the H2 mobiles would be nice. October/November 2026 desktop release would be reasonable and fit previous cadence, tho there are rumours about a N2X delay.  I think the biggest issue is the lack of a dGPU refresh and the non-existend RDNA4 iGPUs. RDNA5 is likely H2 2027, so sticking with RDNA 3.5 hurts. Most APUs/mobiles will only run a more modern gen starting in mid 2028, if they can even ramp enough beside some Halo SKUs, dGPU and consoles (not even counting AI-GPUs). with the current lack of FSR4 for RDNA 3.5, that's dissappointing, tho they think about backporting.  dual stacked V-cache (each CCD with 2 V-cache layers) and better memory controller/IOD would've been awesome as well.   rumours and leaks obviously had nothing on the radar, so it's not suprising nothing came from CES, but still, Lisa booked a keynote - and brought basically nothing. at last years CES they delayed RDNA4 - so it's the second disappointing keynote in a row.",hardware,2026-01-12 02:59:42,1
AMD,nz4hufv,2018 called and wanted their jokes back,hardware,2026-01-12 07:20:49,4
AMD,nyykwym,"This is just blatantly false and ""doomposting"". I've seen several people post the same thing as you and here is a small list I compiled of consumer related electronics that were shown:  * Intel showed Panther Lake, which looks really nice. * Pretty much all the big laptop manufacturers showed off their next generation laptops, a lot of which seem pretty nice (a lot of focus on reparability this year). They also showed off a bunch of cool concepts like the computer-in-a-keyboard from HP, the rollable laptop from Lenovo, the dual-screen laptop from Asus. * A lot of monitor news. * A lot of PC case related stuff were shown. * IKEA has launched a bunch of matter devices for home automation as well as a 10 dollar speakers that can pair with up to 100 units. * Motorola showed off their Razr Fold phone. * Nvidia's DLSS 4.5 update seems pretty big and will work on older generations as well. * LG and Samsung both showed off new TVs. The latter showed off micro RGB. * A lot of audio-related stuff like new soundbars, wireless speakers from Samsung, Onkyo and so on. * Razer announced some gaming related stuff if that's what you care about. A new stream deck keyboard, a new controller and so on. Hyperkin also announced a modular clamp-on controller. * A robot vacuum that can climb stairs. * A bunch of car related things were announced as well, such as Ford saying L3 driving will come in 2028. * L'Oreal showed off a flat iron that uses IR light. * Asus showed off a Wi-Fi 8 router. * Lego showed off ""smart bricks"" which seems neat.  I could keep going but I think you get the point. There is a ton of stuff from CES that are consumer related. Just because you were hoping for specific things (probably gaming related) from a handful of specific companies (AMD, Intel and Nvidia) does not mean the entire show is bad or no longer about consumer electronics. There were over 4100 exhibitors at CES this year. Believe it or not, but there were plenty of consumer related stuff being shown.",hardware,2026-01-11 12:14:46,37
AMD,nz4hzjm,It is consumers electornics show and it will always be so no matter how much they will attempt to change the name.,hardware,2026-01-12 07:22:06,1
AMD,nyy0v8w,"Even at these prices the cost increase of a entire laptop or PC is not exactly a deal breaker. It might push you down the performance tiers, but outside of the low end there is a still a machine to match your budget.   DDR5 cost roughly as much when it first launched, and people still bought and built PCs back then.",hardware,2026-01-11 09:13:24,8
AMD,nz4i6jb,if you dont need super mega overclocked modules DDR5 costs arent that crazy.,hardware,2026-01-12 07:23:50,1
AMD,nyyt31o,Their event wasn't actually at CES. It was just in Vegas during the same week.,hardware,2026-01-11 13:15:43,2
AMD,nyz8n4o,You mean none of them. Don't pretend that AMD hasn't ditched consumers for more money the moment they could.  Corporations aren't your friend.,hardware,2026-01-11 14:47:50,13
AMD,nyxy9pb,Laptops are by far the largest part of the consumer computer market.,hardware,2026-01-11 08:49:17,27
AMD,nyxxt0j,Can't tell if you're being sarcastic or not but Intel actually shipping products with 18A is at least exciting to me. Very interested in knowing how the backside power and ribbonFET technologies pan out,hardware,2026-01-11 08:45:02,26
AMD,nyxzisl,"As someone in the market for a be laptop, I actually am excited. It's great to see laptops improving for the millions of people who buy them each year.",hardware,2026-01-11 09:00:45,12
AMD,nyxxqf8,it is cool! mobile advancements are big.,hardware,2026-01-11 08:44:22,14
AMD,nyy14ex,I am. Maybe they finally have a competitor to the M series so Apple fans can shut up and cry about ARM being so good.,hardware,2026-01-11 09:15:45,6
AMD,nyywluw,This is r/hardware what the fuck else do you want,hardware,2026-01-11 13:38:41,-1
AMD,nz4ydp4,">Â They don't reallyÂ *need*Â anything new yet either, because Zen 5 is still very good for anyone willing to buy into it now.  Desktop yea, mobile might be cooked though. CPU perf is fine, battery life and iGPU perf does not look competitive at all.",hardware,2026-01-12 09:57:42,3
AMD,nz0h1ps,I hope Lisa sees this bro.,hardware,2026-01-11 18:21:19,2
AMD,nyzv8nz,"Yeah, Why do people get this idea that these companies are doing us any favors? As soon as the winds shift, they'll be gone.",hardware,2026-01-11 16:39:17,101
AMD,nz1nprv,"They also did because, during the span of two generations, they managed to lose most of their enterprise market and almost crash the company. Their last hail Mary is to cater to consumers.",hardware,2026-01-11 21:33:40,21
AMD,nz2yu2b,"Yeah that's for sure the reason, but regardless it's good for us as consumers and we'll have to take what we can get right now.  Intel could gain some big market share in the CPU and GPU gaming sector if they play the next couple years right.",hardware,2026-01-12 01:29:20,3
AMD,nz4mnez,"Also, they had less to gain since their chips were already faster. Its like commending a company for making a fast car when they never had one, vs a company that only makes fast cars.",hardware,2026-01-12 08:04:50,1
AMD,nyzrwng,"You say that as if AMD matters too. Spoiler alert, Ai products aren't much use to 90% of the people buying them. All LLMs that most people are used to run in the cloud on insane hardware and not local.  You can run some very small models on these new AMD system but there really aren't very many main stream options for them. I'm just waiting for the bubble to burst so that I can snap some of these up on the cheap.  AMD's ROCm platform is a distant second to NVIDIA. The software support isn't their and AMD is trying to force people into their overpriced enterprise gear instead of supporting their GPUs.  Unless you've dealt with trying to implement your own local LLM stuff, none of this matters. Microsoft is just paying OEMs to brand shit as Ai, as if people need, or want it. Let alone have any idea what they'd use an NPU for.   I do think NPUs are great if you want to keep your data local. But the amount of work and effort is high, furthermore these NPUs are weak. You'll still need a 16gb GPU to run anything larger.",hardware,2026-01-11 16:23:28,-10
AMD,nz0vc9v,You're right. CPUs don't matter at all.,hardware,2026-01-11 19:23:48,-5
AMD,nyzt284,"There is way more to see at CES than overhyped PC gear. If you've never been, it's hard to appreciate. I've worked several shows there just never have had a chance to walk the whole show floor because it is so fricken massive.  My favorite is getting a sneak peak at future production display technologies. I worked some of the shows where OLED was on display nearly 10 years before consumers could afford it. Some of the cutting edge stuff is truly a marvel.",hardware,2026-01-11 16:28:57,33
AMD,nyyocd6,"Also, can't buy CPU when no RAM.",hardware,2026-01-11 12:41:51,68
AMD,nz8o0ek,"You don't get it, it's called AI 400 because it can do AI and it can do 100 more AI than AI 300 which consumers are gonna love because 400 > 300",hardware,2026-01-12 21:53:26,4
AMD,nyztlny,"And if you want it, go with NVIDIA. They know they can jack up the price because their software that more advanced than AMD. Setting up AMD systems is so much more of a pain in the ass. I'm going to get a used 3090 this year because new GPUs are insane.",hardware,2026-01-11 16:31:31,-17
AMD,nyyks96,"or they get bank loan and if they fail to pay back, the bank will be in trouble then the gov will bail the bank out.",hardware,2026-01-11 12:13:42,26
AMD,nz4n2f5,"The big problem with the chips is that they just take too long to make. Which is driving up cost big time. Instead of just creating the chip in one go, it takes months and many cycles to get a chip built. Which I believe is ultimately not sustainable. Especially if they want to make this AI bubble not burst soon. Because it requires a lot of processing power, which is just not worth the investment for many products.",hardware,2026-01-12 08:08:39,1
AMD,nyyczu8,"Handheld is tiny, but the laptop market isnâ€™t.",hardware,2026-01-11 11:05:44,16
AMD,nz33djo,That would be fucked for anyone who doesnâ€™t want to upgrade.,hardware,2026-01-12 01:53:19,3
AMD,nz4g6xs,companies will literally pay double for Nvidia so they could avoid ROCm. ROCm is still a broken mess.,hardware,2026-01-12 07:06:12,2
AMD,nz4n55r,"Its too expensive, otherwise we would've gotten more vram already",hardware,2026-01-12 08:09:21,1
AMD,nz1pdmj,"AMD has a lot more than GPUs. Their X3D  processors are in *high* demand among gamers, and just about every handheld has a Ryzen APU.",hardware,2026-01-11 21:41:27,4
AMD,nyybk5n,"That joke is so obvious, thousands of people are making it independently of you.",hardware,2026-01-11 10:52:43,42
AMD,nyyofjy,That's actually hilarious that you think you're the first to come up with this one. Big main character energy,hardware,2026-01-11 12:42:31,18
AMD,nyy05z6,"Because replacing ""Consumer"" with ""Corporate"" is such a unique idea that only you could've come up with it in the first place.",hardware,2026-01-11 09:06:48,93
AMD,nz07eez,"You may have thought it up yourself, but recognize that it is the lowest hanging fruit there is there, and other people saying it also thought of it themselves. I made up a joke about Caligula when I was a teenager, but I don't think I'm the first person to ever go ""Why did Caligula appoint his horse a senator? Because he always voted NEIGH!""",hardware,2026-01-11 17:36:55,4
AMD,nz4ggyu,You should be happy people liked what you said.,hardware,2026-01-12 07:08:36,1
AMD,nyzj79u,This is brain dead.  This is a trade show to show off consumer electronics.   Full stop.,hardware,2026-01-11 15:41:42,20
AMD,nz4ikkj,"CES revolved around audio and video, including high end professional products, the general public wasn't even allowed to attend until the mid 90s.  Computer gaming is as unrelated to the original audience of CES as data center products.  It has been a general trade show for anything to do with electronics for almost 40 years at this point.",hardware,2026-01-12 07:27:25,3
AMD,nyzlwsm,"Yeah the article underestimated the incentives AMD has in deprioritizing the client segment, but this segment is intel's lifeline. And now that intel's not held back by 10nm delays, AMD will find a harder time to compete, compared to the high end server market where Intel is struggling.",hardware,2026-01-11 15:54:48,6
AMD,nz0gifd,"AMD is a player. But they are an order of magnitude smaller, at least, in terms of revenue in that segment vs NVDA. For example.",hardware,2026-01-11 18:18:58,2
AMD,nz4gxyg,If by becoming major you mean lost 2nd place to become 3rd because an in-house chip beat them?,hardware,2026-01-12 07:12:45,1
AMD,nz1s4r8,"I think he was more speaking about AMD being *at least* the minor player in the AI-space, compared Intel.  Since Intel aren't even really partaking in the whole AI hardware-market for businesses after all and is virtually absent with basically no hardware present (at least *not* in terms of actual enterprise-grade HPC- or AI-hardware), apart from their non-selling Gaudi-accelerators â€” The only real thing making it to market in several years of ever-delayed and often thrown-out road-maps, even if it turned out to be a complete dud and non-seller.  Because apart from their lackluster client-NPUs, which at first couldn't even qualify for the minimum Windows-requirements for anything AI (which is a huge embarrassing fail in and of itself, blatantly showing how Intel trails behind), there's no real AI-/HPC-hardware from Intel at all.  ---- AMD in the other hand, sports a whole line of AI- and HPC-hardware for the enterprise, and is the only one remotely close to nVidia, powers the world's #1 and #2 supercomputers these days and gets government-contracts for that.",hardware,2026-01-11 21:54:18,-5
AMD,nyywlg1,">thats not a rumor there were articles about it recently  Unless those articles had official confirmation, then it's quite literally still just rumors.",hardware,2026-01-11 13:38:37,7
AMD,nyy5tar,Yeah nvidia used samsung for the 30 series i believe  as well,hardware,2026-01-11 09:59:59,-4
AMD,nyy4u40,I think that's OpenVino,hardware,2026-01-11 09:50:51,12
AMD,nyy5gl2,"Yes they do. They have a few generations of their Gaudi DC AI HW, and they have a far more solid SW stack than AMDs (OneAPI).   Problem for intel is that nobody cared. NVDA/CUDA is too entrenched in the learning space.",hardware,2026-01-11 09:56:42,23
AMD,nyy4mpn,I don't think it matters too much because Intel's dedicated GPUs still aren't good enough to do all of the fancy AI GPU nonsense that both AMD and Nvidia could do.,hardware,2026-01-11 09:48:54,6
AMD,nyy0w0l,"Can any ""high IQ"" Redditer explain the downvotes? What exactly is the issue with asking if intel has a CUDA equivalent?",hardware,2026-01-11 09:13:35,1
AMD,nzbsu1l,"The difference is AMDâ€™s mobile GPUâ€™s have stagnated.   They havenâ€™t had a noticeable improvement since their first RDNA 3 based iGPUâ€™s in like 2023. Strix Point uses RDNA â€œ3.5â€ which is shockingly poor in terms of gains from the RDNA 3 iGPUâ€™s which launched in 2023.   Just for context, Intelâ€™s Lunar Lake iGPUâ€™s were Xe2 and [were beating Strix Point last year.](https://www.notebookcheck.net/Intel-Lunar-Lake-iGPU-analysis-Arc-Graphics-140V-is-faster-and-more-efficient-than-Radeon-890M.894167.0.html)  I get it sounds â€œcontroversialâ€ to some people in here, who see the success of AMD in handhelds and such, but again, most of those handhelds were made back when these RDNA 3 based iGPUâ€™s launched. The Z1 was RDNA 3 and is used in most handhelds because, at the time in 2023, it was the best option available.  Theyâ€™ve sat twiddling their thumbs after securing one win in 2023 that everyone else has caught up or surpassed them in mobile GPU efficiency and performance per watt.  Strix Halo has the same issue, the only difference theyâ€™ve done is increased memory bandwidth and increased the power. So in terms of perf/watt itâ€™s still bad despite the fact that it is a good performing GPU.   â€œRDNA 3.5â€ is nonsense. AMD seemingly canâ€™t seem to make a better iGPU with any measurable performance/watt improvements than what they did in 2023.  Theyâ€™ve improved their CPU performance, with Zen 5 based processors, but again just disappointing iGPU improvements.",hardware,2026-01-13 09:57:24,1
AMD,nyywjq7,"> the actual suspicious part is the fact that they were only letting people test the GPU with games, not the CPU  Not suspicious, it's simply that the CPU is nothing special, no more than a node sidegrade and a very minor IPC bump.  Meanwhile the GPU is actually a good step forward, also because of a lack of real competition.",hardware,2026-01-11 13:38:19,14
AMD,nyyl9xd,"And I think they were only letting people test the X7 and X9 parts (with has the 12 core GPU), not the normal u7 and u9 parts which will be far more common and only has a 4 core GPU (almost half of what's in Lunar Lake).     Panther Lake looks nice but depending on which chip you compare it to it seems like it will be a side-grade or even a downgrade compared to Intel's previous generation, and they aren't letting anyone test those areas.",hardware,2026-01-11 12:17:43,11
AMD,nyysu8d,"But you need to be careful when ordering Intel SKUs. The top Nova Lake SKU has 3x the power draw of the rest of the line. Similarly,  Panther Lake has wildly different power characteristics based on the SKU but they at least tried to disambiguate them a bit better visually than in Nova Lake.",hardware,2026-01-11 13:14:04,1
AMD,nyyddsu,The name is literally consumer electronics show. Itâ€™s more like corporate buy my Ai thing now,hardware,2026-01-11 11:09:17,-3
AMD,nyyv610,DLSS 4.5 is really good ...,hardware,2026-01-11 13:29:29,10
AMD,nyymehn,That jacket was so gaudy as if it was made out of a 'fabulous' croc,hardware,2026-01-11 12:26:47,5
AMD,nyyobyi,Alien space ships of course.,hardware,2026-01-11 12:41:46,6
AMD,nyyxctj,">Also seeing something come out of their long await 18A process is interesting.  As somebody who hates laptops, this is the actual exciting part of what Intel is doing right now.  Intel closing the gap on process node is what we should all want to see.",hardware,2026-01-11 13:43:22,6
AMD,nz0ft1v,"Fill in the blank, ""OpenAI is the world's largest _______ of NVidia hardware.""",hardware,2026-01-11 18:15:49,1
AMD,nz4di2p,but rdna 4 was release so stop crying with your entitlement garbage.  they announced 9850x3d and ppl still whine. never happy,hardware,2026-01-12 06:42:54,-3
AMD,nz4kvum,This makes no sense. But good try,hardware,2026-01-12 07:48:36,-2
AMD,nyyveyz,"Sir this is /r/hardware, yours supposed to hate technology here",hardware,2026-01-11 13:31:06,19
AMD,nyys50t,"> There were over 4100 exhibitors at CES this year. Believe it or not, but there were plenty of consumer related stuff being shown.  Well said.",hardware,2026-01-11 13:09:14,15
AMD,nyyqzg5,Yeah monitors and TVs in particular had a great show.  I'm thinking of pulling the trigger on a new monitor and feel almost spoiled for choice with the selection of new displays I have to choose from this year.  God I'm glad there's at least one space in the consumer PC (or related) market that has some real fkin competition or at least isn't subject to a cartel.,hardware,2026-01-11 13:01:11,10
AMD,nyyyyus,">This is just blatantly false and ""doomposting"".   Strictly speaking it's true, CES means CES now and doesn't stand for anything",hardware,2026-01-11 13:53:15,10
AMD,nyzfd5j,"I think this comment isn't being quite representative. I think everyone here can acknowledge that there were some consumer related thing, but anyone who has followed CES knows that time attention ratio wise, this is the least consumer focused CES there has been, hence all the warranted jokes and criticisms.  Even the PC cases this year were far less spectacular than typically.",hardware,2026-01-11 15:22:50,-1
AMD,nz4y9gz,Funny thing is that Intel in their own performance index page for PTL are announcing they are matching the snapdragon x elite *gen 1* in ST perf/watt.   Seems like ARM fans can keep bragging about ARM being so good for at least another year.,hardware,2026-01-12 09:56:35,2
AMD,nyy5k7y,"They donâ€™t have a competitor to the M series. The day that happens you will see an endless line up of fanless laptops, then youâ€™ll know they have one.",hardware,2026-01-11 09:57:38,2
AMD,nyy5stj,"To be honest I very much doubt it. Feels like M is on a completely different lvl to anything right now. The fact that m1 could run cyberpunk by using a translation layer using 35wâ€¦   And this is a general cpu, not a highly game optimized one like steam deck.",hardware,2026-01-11 09:59:52,-6
AMD,nz5k61x,"Sure, it's just not many care about AMD laptops to begin with. I don't know if AMD wants to focus there right now.",hardware,2026-01-12 12:54:59,1
AMD,nz5pyue,"It's just so crazy how the default is to never care about anything, and to assume that anyone doing so is malformed in some way.  I hope nobody from *any* PC-related business *ever* looks upon this hellhole we call Reddit for any reason.",hardware,2026-01-12 13:30:35,0
AMD,nz4bo26,"> Yeah, Why do people get this idea that these companies are doing us any favors? As soon as the winds shift, they'll be gone.  Indeed, they're likely regular folks like us.  If we don't pay them what they're worth they'll go with whoever offers the best deal.  PC gaming aint it since AI came into the scene.  Lucky are those who completed their dream rig the day before price increases occurs.  They're good for the next 5-10 years. By then maybe PC gaming will go back to pre-AI prices?",hardware,2026-01-12 06:27:17,9
AMD,nz3elqj,Most of Intel's revenue is consumer. AMD makes comparable revenue in their Data Center segment compared to Intel DCAI. Intel is still 4x AMD in client revenue.,hardware,2026-01-12 02:53:00,6
AMD,nz0p9mk,A distance second but stil a significant player. Nvidia doesnt produce enough to saturate the market. Additional second rate compute is better than no compute so AMD still has loads of costumers.,hardware,2026-01-11 18:56:42,14
AMD,nz4fwbr,There was always way more to see at CES. but that way more is consumer products. Not AI datacenter products that some of the companies wanted to push this year.,hardware,2026-01-12 07:03:39,4
AMD,nzbze45,I only start caring if itâ€™s AI is over 9000,hardware,2026-01-13 10:57:12,2
AMD,nz0atlu,">Setting up AMD systems is so much more of a pain in the ass.  Depends what you use them for, but for general everyday gaming they aren't.",hardware,2026-01-11 17:52:55,14
AMD,nz4g01q,"> Setting up AMD systems is so much more of a pain in the ass.  setting them up isnt, supporting them when inevitable issues arrise is though.",hardware,2026-01-12 07:04:33,1
AMD,nyyo1l6,"If you borrow 1 million dollars from a bank, you have a problem.  If you borrow 18 billion from multiple lenders, they all have a problem: https://www.reuters.com/business/finance/banks-lend-18-billion-oracle-tied-data-center-project-bloomberg-news-reports-2025-11-07/  > Nov 7 (Reuters) - A consortium of around 20 banks is providing a project finance loan of about $18 billion to support the construction of a data center campus linked to Oracle in New Mexico, Bloomberg News reported on Friday.  > Sumitomo Mitsui Banking Corp [RIC:RIC:SUMFGI.UL], BNP Paribas SA (BNPP.PA), opens new tab, Goldman Sachs Group (GS.N), opens new tab, and Mitsubishi UFJ Financial Group (8306.T), opens new tab are administrative agents on the deal, the report said, opens new tab, citing people with knowledge of the matter.  > The four lead banks have enlisted other banks and will now sell the debt to additional banks and institutional investors through a retail syndication process, with commitments expected by late November, according to the report.  > U.S. tech firms are ramping up investments in data centers to meet soaring demand for computing power, driven by increasingly complex artificial intelligence models such as OpenAI's ChatGPT. The New Mexico data center campus is part of the Stargate initiative, a $500 billion push to build AI infrastructure across the U.S., led by OpenAI, SoftBank Group and Oracle, the report said, adding that Oracle is expected to be a tenant at the new site.  > Pricing is being discussed at 2.5 percentage points over the secured overnight financing rate and the loan is expected to carry a four-year maturity, with two one-year extension options, according to the report.",hardware,2026-01-11 12:39:36,26
AMD,nz7s533,"By taking the money from taxpayers aka, consumers",hardware,2026-01-12 19:24:59,1
AMD,nzceahp,The noise on reddit isn't being caused by laptop usage though.,hardware,2026-01-13 12:52:12,1
AMD,nz359td,A ton of people would have bought a $600-700 9070 XTX with 32 GB of VRAM including me. Thus forcing Nvidias GPUs and cuda-centric companies to reevaluate pricing and programming and everything.,hardware,2026-01-12 02:03:29,-2
AMD,nz4gcg6,handheld are a tiny market compared to other mobile. They do have good gaming processors.,hardware,2026-01-12 07:07:31,1
AMD,nyych8p,"Is it really a joke? CES stopped calling themselves Consumer Electronic Show a long time ago, they rebranded to just CES.",hardware,2026-01-11 11:01:00,7
AMD,nyy0c4s,Could anyone have made the joke? Sure. Did they make that specific joke before I did? Not that I know of.,hardware,2026-01-11 09:08:24,-112
AMD,nz0gfqb,"> â€¦ the lowest hanging fruit there is â€¦  Ironically, mildly related, consumers where always the lowest hanging fruit for them, until investors started aiming for the very crown at the top of the whole game â€” The world's economic tree shaken once financially, and big corporates started to fall down like figs for investors to run after â€¦  Now it's just a craze of who picks up the biggest fruits first, for the juice afterwards.",hardware,2026-01-11 18:18:38,0
AMD,nz09mnh,"I wonder who's right, some redditors who don't know a single thing about business or CES themselves?   >Why Exhibit at CES 2026 Elevate Your Brand at the Worldâ€™s Most Powerful Tech Event  >When youâ€™re looking to get your innovations noticed, nothing compares to CES. Techâ€™s biggest stage puts your solutions in front of **B2B and B2C buyers**, investors, media outlets, prospective partners, influencers and industry heavy hitters from across the globe, at exactly the right moment. Seeing new products is the No. 1 reason people attend â€” which means they come ready to explore and engage with the tech thatâ€™s changing how we live.",hardware,2026-01-11 17:47:24,-12
AMD,nyy59pj,I was referring to general purpose compute APIs like Nvidia's CUDA or AMD's ROCm.,hardware,2026-01-11 09:54:57,4
AMD,nyy5pdd,"This is a weird sub. Sometimes it seems named ironically, as a lot of posters here seem to literally hate tech/hardware.",hardware,2026-01-11 09:58:58,16
AMD,nzfa14x,I'm pretty happy with my Strix Halo here.,hardware,2026-01-13 21:22:23,1
AMD,nyz8dbk,"Tbh, mostly looking forward to the 10 Xe core SKU. Seems like it shouldn't be priced as high as the flagships and still have decent GPU performance.",hardware,2026-01-11 14:46:22,2
AMD,nyzwrm7,"> The name is literally consumer electronics show  it's not. The name is literally just ""CES"". They dropped the full name years ago",hardware,2026-01-11 16:46:25,2
AMD,nz031pc,"Nvidia: ""we're helping this company create complete data profiles of every human being on the planet including facial recognition, BUT HERE'S A LITTLE THING FOR YOU GAMERS!!!!""",hardware,2026-01-11 17:16:16,-5
AMD,nz3f2wp,If you got a 40 or 50 series before the prices went stupid.,hardware,2026-01-12 02:55:40,-1
AMD,nz4hxtb,"RDNA4 does not appear to be released in the mobile segment, as they are still making RDNA3+++ dies instead.",hardware,2026-01-12 07:21:40,5
AMD,nz67ejc,"The new X3D didn't add another layer of cache, just ~3% speed bump. RDNA4 never came to APU and we are likely stuck with RDNA3 until 2028. Many RDNA3 chips will likely be sold even longer, even tho they already started cutting software support and so far haven't released FSR4. That's not a strong Roadmap.   Qualcomm and Apple showed more efficient chips as well and there is no sign AMD will counter them anytime soon, likely 2028 as well.",hardware,2026-01-12 15:04:29,1
AMD,nz4l0j2,"Talking about 14nm++++ in 2026 makes no sense, so we are even.",hardware,2026-01-12 07:49:49,1
AMD,nz4i3xt,"CES stands for Consumer Electronics Show and it will *always* stand for it. Them attempting to change the name does not change what it stands for, it only makes them act like idiots trying to deny it.",hardware,2026-01-12 07:23:13,-1
AMD,nyy5pta,We barely know shit about Panther Lake. Perhaps it's not OEM ready yet.,hardware,2026-01-11 09:59:05,1
AMD,nyy65k0,"> The fact that m1 could run cyberpunk by using a translation layer using 35wâ€¦   Low 1080p with upscale? I mean, yeah, it is nice, but not that much of a win. Plus, it really benefits having an actual native port.",hardware,2026-01-11 10:03:09,3
AMD,nz11aqd,"Windows to me is more pain in the ass than Linux, due to all the Windows annoyances. And on Linux AMD works much better than Nvidia. You don''t even need to worry about drivers.",hardware,2026-01-11 19:50:32,14
AMD,nz16ycb,What if a borrow over 30 trillion and increase by defecit?,hardware,2026-01-11 20:16:19,5
AMD,nzciqc5,Reddit isn't real world,hardware,2026-01-13 13:20:07,1
AMD,nz35fc3,9070XT is currently $700â€¦.  AMD doesnâ€™t control ram prices lmao.,hardware,2026-01-12 02:04:18,11
AMD,nyypzww,"It's a criticism in the form of a wordplay, kind of a jokey way of giving them a jab.  But I think this is more about people heaping crap on that guy.",hardware,2026-01-11 12:54:05,2
AMD,nyy3hfq,"[At the very least, 2013.](https://community.sap.com/t5/technology-blog-posts-by-sap/is-the-consumer-electronics-show-turning-into-the-corporate-electronics/ba-p/12995064)",hardware,2026-01-11 09:38:10,90
AMD,nyyyjjz,Humility doesn't cost you very much effort but it would sure pay off instead of acting like this. Embarrassing.,hardware,2026-01-11 13:50:39,19
AMD,nyza95b,It always feels nice when people are confidently wrong and run headfirst into a gotcha like this. Eapecially since this was so easy to Google.,hardware,2026-01-11 14:56:26,20
AMD,nyyd93t,lmao what a wet blanket,hardware,2026-01-11 11:08:05,20
AMD,nz0dd6k,Yes surely that means being in a government policy maker and discuss AI data center regulation minutiae,hardware,2026-01-11 18:04:35,4
AMD,nz4gld0,"we know for a fact that CES themselves are having identity crisis and is wrong about what the show is for, so that leads what, redditors to be right?",hardware,2026-01-12 07:09:40,1
AMD,nyyd8ry,They love hardware. But exclusively gaming hardware. They hate any tech or hardware thatâ€™s irrelevant to gaming and/or â€œstealâ€ resources from gaming hardware. This is PCMR 2.0 and has been for quite some time.,hardware,2026-01-11 11:08:00,23
AMD,nz4hfvu,It is. CES having identity crisis about their own name does not change who they are.,hardware,2026-01-12 07:17:12,1
AMD,nz0faq6,"The power company provides them power. The eater company provides them water. People create yhe software. Government provides infrastructure.  They getting CPUs, Memory, motherboards, server racks, network infrastructure, etc. from various companies. But it's Nvidia that are the bad guys.",hardware,2026-01-11 18:13:30,7
AMD,nz4hl61,"""There was nothing for consumers""  ""Points out a thing that was for consumers""  ""Quick, redirection into outrage!""",hardware,2026-01-12 07:18:31,6
AMD,nz4hohh,GPU prices are normal to this day.,hardware,2026-01-12 07:19:20,4
AMD,nz70bk0,"Prices are still fine in Europe, at or under MRSP. Prices are just dumb in the US thanks to your guys toddler president.",hardware,2026-01-12 17:19:24,1
AMD,nz9vhzg,idk if u noticed but AMD is a lot smaller than nvidia and are always behind in mobile,hardware,2026-01-13 01:41:08,1
AMD,nz9v9dm,looking for apus thats ur first problem,hardware,2026-01-13 01:39:50,1
AMD,nz4g2z9,"Depends, is that internal deficit or external? internal pretty much doesnt matter.",hardware,2026-01-12 07:05:15,1
AMD,nz9pycq,I mean I meant last March when RDNA4 was launched. A 32 GB Card costing the same or less than a 5070 ti would have sold to hobby builders and forced the 50 supers to be launched.,hardware,2026-01-13 01:10:26,2
AMD,nyz17am,Awww shit now _thats_ bringing receipts.,hardware,2026-01-11 14:06:28,33
AMD,nyz7unv,u/BlueGoliath,hardware,2026-01-11 14:43:37,22
AMD,nz0h01r,It affects sales so obviously yes.,hardware,2026-01-11 18:21:07,-7
AMD,nz5jxqm,CES is in crisis because of the internet makes these things unnecessary to get press coverage. The wrong thing you just made up.,hardware,2026-01-12 12:53:28,1
AMD,nz053ef,"And it has to be laptops. If you dare say anything about it being a bad form factor for high performance computing, or how much more expensive and cut down it is compared to desktops, you get downvoted into a smoking hole in the ground.  Joke's on them, the tiktok generation will eschew laptops for phones and they'll be the boomers for wanting something better.",hardware,2026-01-11 17:25:58,8
AMD,nz0efot,"That makes sense.  Having attempted good faith discussions about tech/hw (coming from academia/industry), it was bizarre witnessing literal emotional meltdowns about something as random as digital design concepts/tools/components.",hardware,2026-01-11 18:09:32,7
AMD,nz4j6px,"And only love hardware in terms of clock/cycles.  They hate ""proprietary tech"", so anything novel or not general purpose is hated as well.",hardware,2026-01-12 07:33:04,1
AMD,nyyx02h,"That's not what is happening here, though?",hardware,2026-01-11 13:41:08,0
AMD,nz4j7wn,You are joking right?   The teirs have gone up 100-250$ each over the last two gens. The Xx90s even more so.  And the resell is ridiculous,hardware,2026-01-12 07:33:22,-1
AMD,nzb9sdy,That would be a valid argument if AMD wasnt spending tens of billions on stock buybacks.,hardware,2026-01-13 06:57:26,1
AMD,nz0qfuf,"Bro went radio silent, lmao.",hardware,2026-01-11 19:01:49,16
AMD,nz15qpd,You've lost the plot,hardware,2026-01-11 20:10:41,5
AMD,nyzbexi,It is.,hardware,2026-01-11 15:02:34,9
AMD,nz4jp90,"No i am not joking. Once you adjust for inflation the prices are about the same. A 1070, arguably the best card of a decade, cost more adjusted for inflation than a 5070 does. The 5090 is the only one that breaks that, but anyone who cares about affordability already does not care about the 5090.",hardware,2026-01-12 07:37:47,1
AMD,nzcucbv,lol so braindead,hardware,2026-01-13 14:23:41,1
AMD,nz1bap1,Thatâ€™s not a substantive reply,hardware,2026-01-11 20:36:40,1
AMD,nz4kcng,"Once you adjust for inflation? Will you adjusting peoples wages along with that?   And yea, 1070 was amazing, they were high quality. They weren't fucking catching on fire.  They are lower quality, for a higher price (call it inflation or not). And yes the price impact is felt when wages in comparison are stagnant.",hardware,2026-01-12 07:43:44,0
AMD,nz1dims,These aren't consumer electronics.  They're hardly tangentially related.  It doesn't matter if it affects their sales.   It doesn't belong at this trade show.,hardware,2026-01-11 20:47:04,4
AMD,nz4l5mz,"Obviusly, as we all know wages have exeeded inflation almost globally. Average purchasing power has increased.   The 5070s arent catching fire. In fact there has yet been a single piece of evidence of the new connector creating a fire.",hardware,2026-01-12 07:51:09,2
AMD,nz4r3t9,"Prove it. Just for the US. Please prove wage growth has exceeded inflation and purchasing power has increased.   And I love how you have to specifically pick out just the 5070 for not catching fire. What about the multiple reports that 50 series are seeing premature failures, and don't forget about the 50 disaster from about a year ago when they fucked cards with a bad driver and shipped cards with defective silicon.",hardware,2026-01-12 08:46:57,-2
AMD,nym1ypx,Given the increased RAM costs a Super refresh would only make sense if existing sales were being impacted by consumers demanding more VRAM.,hardware,2026-01-09 15:38:02,245
AMD,nymeohf,"*""Somehow, this is AMD's fault.""*",hardware,2026-01-09 16:35:07,214
AMD,nylyn77,"Why do so many people think nvidia cares about anything AMD does? Even if AMD released something do people really think nvidia would act on that?  There was never a super series announced.   The real problem is modern media and how they make money. Tech media needs stuff to talk about and interest is high. There are a lot of self proclaimed leakers who conveniently present new ""leaks"" on a schedule. In reality, they can make up whatever they want. Even the ""best"" get like 40-50% of their broader claims right.    But in their story, they're never wrong. If they said something that didn't happen they blame some company for that. That's their business model.",hardware,2026-01-09 15:22:45,258
AMD,nym3ami,With the price of RAM and no doubt VRAM now I don't think they'll bother with a Super series this time.,hardware,2026-01-09 15:44:07,35
AMD,nynidr3,"Itâ€™s ok Jensen, we didnâ€™t need another lesson on why competition good, monopoly bad.  Thanks anyway.",hardware,2026-01-09 19:32:39,8
AMD,nym8v59,"At this point, Nvidia only cares about keeping their AIB partners on life support as they ride the AI cash cow. If their gaming cards had more competition - they'd feel the need to launch super cards and keep their AIB partners happy.Â    But as it stands, they'd have to massively raise those card's price since they use premium SKU dram modules. So the lack of performance competition doesn't necessitate it, and the cost of goods increase wouldn't make them very competitive anywayÂ    Gaming is their side chick they give enough attention to as to not lose them, but has no intention of making them a priority.",hardware,2026-01-09 16:09:10,25
AMD,nypo8wp,"I can see why VRAM prices are a problem for the lesser cards, but for the 5080S thereâ€™s such a cavernous price gap between the 5080 and the 5090, surely they can fit it in.",hardware,2026-01-10 02:01:27,6
AMD,nyoj9v4,Damn. Was hoping for a 5080 with 24gb and a bump in CUDA to match the 4090,hardware,2026-01-09 22:23:44,4
AMD,nyn5fe4,I think this is the biggest opportunity for Intel to get market share. Once in a decade even. They're not seeing much takeup for AI on their end so they should have room for GPUs to feed the hungry.,hardware,2026-01-09 18:34:42,12
AMD,nyocxae,"""offers no competition"" is a bit much. The 9000 said has been pretty solid and kept prices near MSRP for a good length of time. Regardless of what the steam survey says, it's held top spots on Amazon, Newegg and mindfactory   If there were no competition there wouldn't even rumors of a super series.",hardware,2026-01-09 21:53:43,8
AMD,nymz61h,"The Super models were only launched because the OG models were dogshit. The OG 2080 8GB got so much criticism for being worse than the 1080 Ti 11GB because it cost $100 more for equivalent performance but less VRAM. The 1660 Super was a 1660 Ti with a $50 price drop. At $230 it was probably Nvidia's last great budget GPU. The 4080 Super was all about the $200 price drop to $1000 and nothing else. *(1-3% performance delta that you couldn't detect in a blind test.)*  The only 50-series model that really needs a Super is the 5080, just so that there's a current equivalent of the 4090. Oh, and a 5090 Super that's $300 cheaper but that's a delusional fantasy. 5090s are rarely going for the $2K MSRP anyway.",hardware,2026-01-09 18:07:00,9
AMD,nymu01h,"Having a super series really isn't a standard as they've only done it with 2 of the last 4 RTX line and it wasnt sequentially.  AMD really only doesn't offer a competitor to the 5080 and 5090, but are very competitive with the rest of the 50 series. This is in addition with FSR4 being somewhere between DLSS3 and 4 for image quality.",hardware,2026-01-09 17:43:51,6
AMD,nym6ac5,My 5080 will be enough until 2035,hardware,2026-01-09 15:57:36,14
AMD,nyo2e96,This new narrative it has something to do with AMD is laughable.  It very obviously has nothing to do with AMD's lack of competition given Nvidia have been 90%+ of the consumer market for a long time now and didn't even bother to use their best product in years to present a challenge.,hardware,2026-01-09 21:05:07,2
AMD,nyooyt8,Yet AMD's 9000 series is selling very well according to Amazons sales data:  https://www.amazon.com/Best-Sellers-Computer-Graphics-Cards/zgbs/pc/284822,hardware,2026-01-09 22:51:55,4
AMD,nymc8st,"Maybe not popular opinion but I always felt that Standard, Standard Ti, Standard Ti Super (or just Super) is over saturating the naming for a card series. Just to stick to either:  * Standard then Standard Ti OR * Standard then Standard Super  Don't overcomplicated things, if there's enough development to the chip then just up the name, don't add some weird postfixes.",hardware,2026-01-09 16:24:16,4
AMD,nyndbx0,guess AAA game studios will actually have to learn how to optimize.,hardware,2026-01-09 19:09:37,3
AMD,nymth77,\*\*cough\*\* Intel B770 \*\*cough\*\*,hardware,2026-01-09 17:41:31,2
AMD,nyn8hhn,"I am not even sure why they have a super planned.  I feel like the original Super was released only because no one was buying Turing/2000 series. it was overpriced and offered minimal rasterization gains while the raytracing was too weak to be used. The same applied to ada where the whole stack felt lackluster except for the very expensive 4090.  Given the higher prices of hardware these days, I expect people to be upgrading less often. Even enthusiasts may choose to skip 2 generations instead of maybe upgrade every other gen - so we are looking at maybe 5 years of usage for a single card. Given this, the amount of vram end up being a more important factor when making a purchase, but this is not something that can really be solved economically these days given the memory shortage.",hardware,2026-01-09 18:48:06,2
AMD,nyotwzl,"""offers no 2026 competition.""  Says people who have no idea what AMD has planned for 2026, only what they've said they have planned.   Do I think AMD is launching anything new this year in the consumer dGPU space? Probably not. But this is a prime example of ""making news out of no news.""",hardware,2026-01-09 23:17:33,2
AMD,nynhpr7,indefinitely in this case means permanently at some point.,hardware,2026-01-09 19:29:37,1
AMD,nyocyl8,"I've felt for a while now that the Supers didn't make sense. With RAM costs skyrocketing, an insurmountable lead in market share and a decent lead in this generation, what reason do they have to make more consumer GPUs?  A reminder that you should never make plans around unannounced hardware. I've seen people for *months* being told to wait for Super release before buying a GPU. Total nonsense.",hardware,2026-01-09 21:53:53,1
AMD,nyog20t,"Alright intel: there are some juicy market shares to grab, itâ€™s now or never.",hardware,2026-01-09 22:08:19,1
AMD,nyogr38,"Never liked the idea of Supers anyway. They should do a solid lineup from the get-go, and then improve that with Ti's later. Now some folks wait for the Supers as if it was some automatic upgrade for those smart enough to wait for them.",hardware,2026-01-09 22:11:36,1
AMD,nypi3d6,reminds me of 30 series supers getting canceled. I guess only even numbers make it. 4070 ti super was a great deal.,hardware,2026-01-10 01:27:58,1
AMD,nyttc0e,"Reading this, I hope that my refurbished 4070 super could last these turbulent price waves.",hardware,2026-01-10 18:26:38,1
AMD,nyxgf28,"Out of 20 series released, only 2 times Supers existed at all, so Supers were never a guarantee.",hardware,2026-01-11 06:12:31,1
AMD,nzfqf6v,Um the 3070 does work. You just have to tune some settings down now and not Ultra 4k everythingâ€¦ Iâ€™m not defending Nvidia. I think I even agreed that they skimped on ram but you are exaggerating and saying a 3070 wont run games anymore. If I somehow got a rx480 with 16GB of ram it would still not perform as well as a 3070 with 8Gb.,hardware,2026-01-13 22:39:30,1
AMD,nyma3qf,"I don't think Nvidia could care less about what AMD is doing. Even if AMD *did* release something, I very much doubt it would perform well enough to beat anything above a 5070 Ti.",hardware,2026-01-09 16:14:43,0
AMD,nyoocsq,"no reason to release, and amd is a blip against nvidia right now...       nvidia fears broadcom and apple and google more than amd",hardware,2026-01-09 22:48:50,1
AMD,nym3ue7,Y'all think the 6 series GPU gets released on time next year?,hardware,2026-01-09 15:46:37,1
AMD,nymfbna,I don't think anyone minds.,hardware,2026-01-09 16:37:57,1
AMD,nyosrm2,Does anyone know why AMD doesn't develop a GPU that can compete with Nvidia? What's stopping them?,hardware,2026-01-09 23:11:32,1
AMD,nymn1lc,"They will release super series when they have more ai slop tech to gatekeep, requiring the new hardware.",hardware,2026-01-09 17:12:24,0
AMD,nymphhl,Radeon hasn't had competitive marketshare since the 5850. Nvidia doesn't care about actual performance.Â  Radeon is a tarnished name.,hardware,2026-01-09 17:23:24,-1
AMD,nym29kq,Maybe games will be optimized now,hardware,2026-01-09 15:39:25,-3
AMD,nynnksf,Yeah and this time it is better than for ex. 30/40 series. The 5070Ti having 16GB is quite good compared to the past when we got 3070Ti 8GB and 4070Ti 12GB...,hardware,2026-01-09 19:56:27,51
AMD,nynv6n6,"November was one of the worst months of GPU sales ever, stretching back decades. But Nvidia also doesn't care because the same space for an inference chip fetches 10x as much as the consumer GPU. Neither of them is selling many GPUs right now.",hardware,2026-01-09 20:31:37,14
AMD,nypjgd0,"it could make sense the ""other way"" as well. For example.  Nvidia could release  5080 Super that use 5090 die( GB200) but with 384bit bus 24GB make the availability of 5090 scarce. (this saved 8GB vram)  a 12GB 5070 Super that GB203 & replaces 5070Ti, making 5070 Ti EOL. (saved 4gb vram)  a 12GB 5060Ti Super that use GB205, EOL 5070. (saved 4GB vram)  a 8GB 5050Super that use GB205 use 8GB GDDR6, EOl the 5060 that use GDDR7.  (saved GDDR7)",hardware,2026-01-10 01:35:37,4
AMD,nypwqk0,"NO,  this assumes, that nvidia cares about consumer sales at all even a bit.  they don't!  you know what they might do?  a free trial to geforce now, so you can get to enjoy OWNING NOTHING and get accustomed to that.",hardware,2026-01-10 02:48:27,0
AMD,nyneyqq,"Plus, RDNA4 was AMD's response to the 50 series. 9070xt came out 9 months ago. It's way too soon for the next gen.",hardware,2026-01-09 19:17:03,64
AMD,nynn7c1,"When ""Nvidia - $50"" backfires if Nvidia doesn't release anything.",hardware,2026-01-09 19:54:45,14
AMD,nytmoc3,"Can't release an ""Nvidia -$50"" card if Nvidia doesn't release one first. How would they know how to price them??",hardware,2026-01-10 17:55:47,1
AMD,nynfemf,It is. If AMD planned on releasing competitor to the 5090 we wouldnâ€™t have this discussion,hardware,2026-01-09 19:19:04,-32
AMD,nymoi85,"> Even if AMD released something do people really think nvidia would act on that?  Back in the competition days these two companies would watch like hawks and spring fucking *anything* to ruin the other's product launch. A surprise SKU, a price cut, whatever.  It's been quite a while since the market was in that kind of state however.",hardware,2026-01-09 17:18:59,48
AMD,nylzs4e,"Right? None of these companies are giving focus to the consumer side this year for the simple reason that the AI craze has taken over electronics supply, CES made things pretty clear on this.  That's already pretty established, and yet since media needs ""news"" to get clicks and ad revenue, they will keep publishing *something* and calling it news to get clicks, no matter whether its truthful or relevant.",hardware,2026-01-09 15:27:57,49
AMD,nym3tur,4060ti 16gb prices were slashed beforehand the launch of 7800xt and 7700xt. They do care about when competition hits hard enough and they couldnt ripoff too obviously.,hardware,2026-01-09 15:46:33,33
AMD,nyo1aer,Esp true about Youtube Grifters going on about Nvidia.,hardware,2026-01-09 20:59:57,6
AMD,nyoucoq,"Because they always did. Jensen doesn't want to lose 0.01% of market share to anyone. If they had 98.95% of the market, Jensen would melt down over 98.94%.",hardware,2026-01-09 23:19:54,5
AMD,nyodbvz,"Jensen has been in the GPU business since it's beginning and regardless of how much they make in data center, he'll guard the GPU performance crown fiercely.",hardware,2026-01-09 21:55:32,3
AMD,nym0un3,">There was never a super series announced.   I don't fault anyone for thinking it was extremely likely for the 5000 series. The last 3 generations all had super releases and by all appearances was their new/current model. The supers in all generations launched at the same or lower price than the og card, so there seemed to be some standardization with the approach too.   If we're being objective and unbiased, it was more likely than not that a super release was going to happen or was at least planned.  Edit: petting kitty is totally correct, there wasnt a 30 series super, just TIs",hardware,2026-01-09 15:32:52,12
AMD,nymllz6,">But in their story, they're never wrong. If they said something that didn't happen they blame some company for that. That's their business model.  You say that as if we don't have prototype 3080 20GBs and 4090 Tis.",hardware,2026-01-09 17:05:54,4
AMD,nym8hi2,"What would more likely happen is AMD exploiting the situation to raise prices of their new GPUs. Is the AMD xx70 card as fast as a 5080? They'll just release it at $949, and wait until their generation to drop to the $599 it should be.",hardware,2026-01-09 16:07:28,2
AMD,nyn5u48,Nvidia would only care if somehow AMD wrangles the market and offers some kind of already-widely-supported alternative to CUDA at a cheaper price. That's what would need to happen anyways,hardware,2026-01-09 18:36:29,2
AMD,nyrwart,1. Makes up some random rumors about the super series 2. Keeps posting about it as if it's real 3. Truly believes about the rumor 4. Gets mad that the imagined super series is nowhere to be found  Gaslighting 101 everyone.,hardware,2026-01-10 12:14:00,2
AMD,nym41h7,If people think AMD is stopping Nvidia from capitalizing on fomo customers...,hardware,2026-01-09 15:47:30,4
AMD,nyoizam,I argue a lot more Leaks are true than we know but plans change.,hardware,2026-01-09 22:22:18,1
AMD,nypodv4,"Yeah, these companies will do everything to keep their names in the news cycles. Super refreshes that are a minor improvement are the perfect tools to do that.",hardware,2026-01-10 02:02:12,1
AMD,nyokoin,Because Nvidia reacts to AMD all the time.,hardware,2026-01-09 22:30:39,0
AMD,nyn94a2,"I do not think Lip-bu Tan announced development of any new products that werenâ€™t already started under Pat. So, I am not sure if Intel has plans after Nova Lake in developing their own products and not just shifting to full datacenter and Fab business",hardware,2026-01-09 18:50:51,3
AMD,nypbgkb,Intel missing the boat once again. Didn't announce any new desktop/gaming-class GPUs and gave no signs of releasing one anytime soon. I guess we're stuck with Nvidia for the near future,hardware,2026-01-10 00:51:04,6
AMD,nyr6yo7,"> so they should have room for GPUs to feed the hungry  The problem is that their GPUs are too expensive to produce for the price they sell them at. If anything, it might have gotten worse as more memory was a selling point for them.",hardware,2026-01-10 08:24:29,4
AMD,nyotxpm,">The only 50-series model that really needs a Super is the 5080, just so that there's a current equivalent of the 4090   A 5070 super with 16gb vram or a 5060 super with 12gb would've been nice too, but the ram situation happened...",hardware,2026-01-09 23:17:39,8
AMD,nyp3bd1,Current equivalent to a 4090 is a 5070 and its â€œ4090 Performance!â€,hardware,2026-01-10 00:07:56,1
AMD,nym8f75,2030\*,hardware,2026-01-09 16:07:11,15
AMD,nym9ajm,Damn respect if you can use a 5080 for 10 years without upgrading,hardware,2026-01-09 16:11:06,9
AMD,nymavx4,I pulled that off with my 1070 till last month. Now I have a 5070 Ti and I'm going to do the same.,hardware,2026-01-09 16:18:11,4
AMD,nyq0308,"i mean no, because they refused to give you 32 GB vram,  but also no, because it might have melted by then.  another great planned obsolescence move by nvidia ;)  can't keep your card forever if it eventually sets your home on fire!",hardware,2026-01-10 03:07:38,-3
AMD,nyo4eqw,*suffixes,hardware,2026-01-09 21:14:32,2
AMD,nyxixkw,"I dont mind there being Ti and Supers but i dont like the ""Ti Super"" option. That just sounds silly.",hardware,2026-01-11 06:32:45,2
AMD,nyndq7u,"""ChatGPT, optimize my game for 10% better performance!"" /s",hardware,2026-01-09 19:11:25,6
AMD,nyxitgq,as if that has ever happened in gaming history.,hardware,2026-01-11 06:31:49,1
AMD,nymfgt3,"AMD & Nvidia are basically equal in performance-per-watt.     I don't care that AMD isn't releasing any 600W, $2000+ ""halo"" card.    The vast majority of people can't afford it.",hardware,2026-01-09 16:38:35,3
AMD,nyxjaoo,Skill issue.,hardware,2026-01-11 06:35:47,3
AMD,nypk3uc,"Market segmentation, if both companies cover a specific range that satisfies all types of customers (AMD the affordable mid range, Nvidia the expensive high range) then they can have proper profit margins over the whole range of products.       And the R&D costs would probably be too high for AMD to risk it, entering the market segment Nvidia is dominating in.   Plus both of them now fully jumping into the AI hype train. I don't think high end AMD cards will be there for the immediate future.",hardware,2026-01-10 01:39:16,2
AMD,nyso8q5,"It's expensive, and risky. The size of Nvidia dwarfs AMD",hardware,2026-01-10 15:08:37,1
AMD,nyyfrey,Except they do? The 9070 XT and 9060 offer similar performance to 5070 TI and 5060 for lower prices. And FSR4 closed the gap to DLSS.,hardware,2026-01-11 11:30:47,1
AMD,nyq0er3,"now that would be kind of hard using the exact same dies, BUT there is a way of course.  you just make that ai slop shit tech require a bunch of vram by itself and BAM 16 GB cards are bye bye.",hardware,2026-01-10 03:09:34,0
AMD,nyp4ti6,They need to shut the Radeon division down and move it to the U.S.,hardware,2026-01-10 00:15:52,2
AMD,nymlmh9,Lol,hardware,2026-01-09 17:05:58,11
AMD,nyoc37f,5080 makes no sense tho. It's pretty close to 70ti in perf while costing quite a bit more. Giving it 24gb to differentiate itself would help.  There are already games which will top out 16GB VRAM buffer with DLSS features and RT turned on. Like Indy.,hardware,2026-01-09 21:49:54,34
AMD,nynomm8,"Yep  there isnâ€™t a need for a 50 super refresh. The 50 series itself is just a refresh of the 40 super series there isnâ€™t much of a performance improvement nor VRAM improvement from the 40 super cards, just more AI with more power draw, and faster vram mostly.  RAM prices are too high too, I suspect it wouldnâ€™t sell that well and it might instead take money/RAM away from AI cards.",hardware,2026-01-09 20:01:17,26
AMD,nyq5t1q,How is 5070Ti quite good when it offered barely any meaningful performance improvement from 4070ti super? 4070ti super also had 16GB.,hardware,2026-01-10 03:41:50,7
AMD,nypyqd0,">The 5070Ti having 16GB is quite good compared to the past when we got 3070Ti 8GB  this is wrong and it ignores what actually happened.  the 3070/ti cards were at the time pointed out by professional reviewers to be still fine for rightnow, but with the looming ps5 the 8 GB vram should become an issue relatively soon and IT DID.  and now a 5070 ti with 16 GB is better how exactly?  in 2 years the ps6 is coming out with 30 or 40 GB memory. if the ps6 comes with 40 GB, then 16 GB is EVER WORSE than 8 GB was when the ps5 came out.  nvidia is AGAIN massively artifically limiting the lifespan of the 5070 ti by giving it just BARELY enough vram for rightnow to break with the soon to arrive new console generation.  to match a ps5's 16 GB unified memory you needed at minimum 12 GB vram.  to match a 40 GB ps6 assuming the same 3/4 conversion, you'd need at least 30 GB of vram!  where are those 5070 ti 32 GB cards and 24 GB cards, or hell 48 GB cards! (3 GB on both sides)  and again to be clear to not have the 8 GB vram issue all over again you need 24 GB if the ps6 comes with 30 GB or 32 GB if it comes with 40 GB.  nvidia and amd know this.  they just hate you and want to sell you hardware, that will become broken in 2-3 years already.  the fact, that you call this ""quite good"" is just ignoring reality and terrible.  can  you please start calling out scum companies shitting on us?",hardware,2026-01-10 02:59:41,-1
AMD,nyp5b3q,"The unfortunate part for consumer GPU customers is... that's probably what Nvidia wants during a supply shortage. Sell only the bare minimum, and even then at extremely high margins for the few that cares enough to pay up, and reallocate all remaining resources to high margin datacenters.  Every wafer spent on Geforce is one less wafer spent on AI accelerators before the current demand frenzy goes away.",hardware,2026-01-10 00:18:25,14
AMD,nyo0zak,"Sauce? Because I read, that november was one of the best months of GPU sales ever.",hardware,2026-01-09 20:58:30,3
AMD,nypnx7f,"Thatâ€™s more of a 5080 Ti, but I do wish they would do that. It basically already exists (with extra VRAM) as the RTX Pro 5000.",hardware,2026-01-10 01:59:47,3
AMD,nyqkjcv,"> 5080 Super that use 5090 die( GB200) but with 384bit bus 24GB make the availability of 5090 scarce. (this saved 8GB vram)  Why though? The DRAM wafers are the problem, doesn't matter if you buy 2 or 3GB modules. May as well use the cheaper (half price?) GB203 die, given the lack of competition at this level.",hardware,2026-01-10 05:18:04,2
AMD,nypzoki,Gross.,hardware,2026-01-10 03:05:15,2
AMD,nz6s7fg,honestly owning nothing is pretty awesome. Selling your unneeded stuff is annoying. The issue is that booking everything as a service is pretty expensive.,hardware,2026-01-12 16:42:07,1
AMD,nyxgu9b,and yet Nvidia is the only one actually producing consumer cards in large numbers and the only one actually developing consumer software that isnt a complete shitshow like redstone.,hardware,2026-01-11 06:15:50,1
AMD,nynoqyw,Good point  We donâ€™t need new GPUs if theyâ€™re gunna cost way more with the ram shortage,hardware,2026-01-09 20:01:51,18
AMD,nyxj1k6,"According to techpowerup, rdna5, a middling update is rumored for 2026...  A placeholder before udna.",hardware,2026-01-11 06:33:40,1
AMD,nyno92s,"AMD needs to take care of its features first, the performance itself is not the problem. They need to bring FSR on par to DLSS Transformer and add frame-gen with comparable low latency and quality NV has. If the 9070XT had this then it would be great, but right now it just suffers too much from the lack of DLSS and FG support.",hardware,2026-01-09 19:59:32,9
AMD,nynpxcg,Maybe in a year or two when RAM prices have hopefully become more reasonable  It would have to be good enough to pay for the more expensive VRAM  But I think they will just wait till the 10000 series. 90 series just released within the past year. Even the 50 series itself is essentially like a Super series refresh to the 40-super series. Not huge performance uplift. Not worth releasing something until they show substantial improvements.,hardware,2026-01-09 20:07:17,1
AMD,nyqebym,"I don't know why you're downvoted so much. AMD has been fumbling their high-end video cards ever since the Fury X. I personally don't think AMD spent enough effort on the biggest strength they have left, which is their raw rasterization performance. Sure, you'll get many people here crying about how they don't have special features like DLSS and better ray tracing. But there are plenty of people who don't give a shit about that sort of thing, and just want a raw power house to pump out frames.  If AMD had been more competent over the years, Nvidia wouldn't be able to get away with charging so much for GPUs like they have the last 2 generations.",hardware,2026-01-10 04:35:39,-5
AMD,nyxhigi,"we havent been back in the competition days for over a decade, though.",hardware,2026-01-11 06:21:11,2
AMD,nyms418,Goiven that there hasn't been relevant competition from AMD since RTX released nvidia stopped caring about anything AMD did.,hardware,2026-01-09 17:35:17,7
AMD,nymf6x9,"Nvidia is making far more money from AI, but their consumer segment is still making them billions and they're not going to simply drop it.  If AMD had a product that was solidly one-upping Nvidia, say if they had done a refresh on the 9070xt or something, then Nvidia very likely would have responded with at least one new skew or a price drop.  Even if they're not nearly as competitive as I wish they were, AMD does keep Nvidia in check at least somewhat lol.  Of course, with the DRAM shortage I doubt anyone will make a refresh this year, but if one does the other probably will too.",hardware,2026-01-09 16:37:23,24
AMD,nyxhodq,I dont see why this lie keeps getting repeated literally the same week in which both companies released significant improvements for consumers.,hardware,2026-01-11 06:22:30,2
AMD,nymmari,"Right, it's not accurate to say they dont care about the competition at all, though I think it is fair to say that Nvidia will not revolve their whole strategy around the competition, either.",hardware,2026-01-09 17:09:02,18
AMD,nym1800,30 series had no super,hardware,2026-01-09 15:34:37,31
AMD,nyxi0jm,20 and 40 series were the only series in entire Nvidia history that had Supers.,hardware,2026-01-11 06:25:13,3
AMD,nymawse,"It's weird that there was no Super series for the 30 series. They released the 3080 12gb and 10gb, so the 12gb could have been called the 3080 SUPER. Laptops got a 320 bit GA103 die that never made it into any product in the full configuration and was always cut down in really weird ways. Always 256 bit.  Even cut down as far as to put into some rare 3060ti cards. They could have easily been the 10GB RTX 3070ti SUPER. It's like it's a die they made, and then regretted it, and tried to make it disappear. Even the 3060 had room for more enabled SMs. It really feels like they just cancelled that while series as well because the crypto and COVID boom was already enough, so no need to put in more effort.",hardware,2026-01-09 16:18:18,5
AMD,nym579a,"nvidias super series is just a refresh. There are a couple reasons to do refreshes for a manufacturer:   1. You typically get better chips over time. At the beginning you typically don't exactly how yields and binning will be, after some time you can typically sell higher binned products because you know how it'll work out. Even if it's a few percent, it's a small improvement for hardly any cost. Especially when the next generation is far in the future.  2. Price drops. The vast majority of cost appears in development, not per unit cost. After selling enough units and your rnd is paid for you're pretty much only making profits and can easily drop prices. It's typically better to introduce a new, cheaper product than just lowering prices in the market.      The RTX 5090 likely wouldn't get a refresh, why? It would only threaten their compute cards.    The RTX 5080 is already using the full GB203 die.    The 5070ti is already super close to the 5080.    The 5060ti is already using the full die.   A refresh would only make sense for the 5070ti (essentially sell the current 5080 as a 5070ti super), 5070 and 5060 unless they want the same performance with more vram.    We don't know how far away the next generation is.    Blackwell was never a good candidate for a refresh, never really made much sense.",hardware,2026-01-09 15:52:46,1
AMD,nyxi6ja,A proptotype in a lab does not mean a planned release.,hardware,2026-01-11 06:26:36,1
AMD,nyr6tp0,Intel's not going to abandon the client CPU market. That much is certain. Client *dGPU* is another matter.,hardware,2026-01-10 08:23:12,2
AMD,nywq9rt,Do you think jaguar shores will actually happen after the disasters of the previous 3 gens in a row?,hardware,2026-01-11 03:24:55,1
AMD,nymccfg,honestly with diss far old cards can push much further.,hardware,2026-01-09 16:24:43,14
AMD,nymdaw8,"I used my 5700xt for just about 6 years before replacing it with a 5070ti this year. And given how slowly things are improving year over year now, I'd be surprised if I *couldn't* use that card for 10 years, barring some critical hardware failure with it.  There'd have to be some major technological shift at this point, which if it happens and there's insane improvements then that's great!",hardware,2026-01-09 16:28:58,5
AMD,nymkm0e,I'm on my 10th year as a 1080TI user,hardware,2026-01-09 17:01:21,9
AMD,nymnu7k,"I'm using a GTX1070 right now.  It's.....not great. lol  I think it'll get a bit easier going forward, cuz the next consoles are inevitably going to be very disappointing, at least if they release within the next couple years and dont cost $1000.  There wont be any huge uplift in performance this time around because of the need to hit a price point that wont make people scream.  PS5 Pro already shows how little they're able to improve over time compared to what the PS4 Pro offered in a similar time frame(and without a huge increase in price...).  And not only will the PS6 specs not be that amazing, but we're bound to have another like three years of cross-gen titles as well.  So even people with something well less than a 5080 will likely still be capable of playing the latest games decently enough for a good while.  I think a 5080 will probably be very in line with what PS6 offers in general, really.  Bit sad, honestly.  Sure, it's fine for getting more longevity out of your existing PC, but this is all because prices and especially performance per dollar improvements are becoming so bad.  It was much better when tech and graphics/games were advancing faster, but you could at least buy an upgrade to keep up at an entirely reasonable cost.",hardware,2026-01-09 17:15:58,7
AMD,nyne9k6,"my buddy used the next gen up beefed up...or maybe it was THE 8800GT for like damn near a decade.  i remember customers plopping desktops on the counter and flumes of dust coming out and it still worked, they would be like 8 and 10 year old towers we built them and they were ready to upgrade. mostly office and email and maybe some FB or YT but, they'd buy a mid range or higher from us and they would age quite well for such simple use.  i still have some 2008 era stuff i still use. a little slow but, doable, the little core2duo laptops don't like the telemetry and data collection on amazons website but i can buy stuff.",hardware,2026-01-09 19:13:52,1
AMD,nynvosr,There are lots of people still happily using their 1080 from 2016. Also consider that generational advancement has become milder and milder with each generation. His 5080 will most likely be made obsolete by software lockouts before anything else.,hardware,2026-01-09 20:33:59,1
AMD,nysf1n8,My GTX 1080 wants a word.,hardware,2026-01-10 14:17:35,1
AMD,nyxihpy,"on the contrary, he will be the futures equivalent of a ""guy with 1080 crying ray traced game dont work on his ancient hardware""",hardware,2026-01-11 06:29:09,1
AMD,nym9uo0,"Im using a 1060, works fine on 1080p.",hardware,2026-01-09 16:13:36,-1
AMD,nymo2zw,"Aye me too, super surprised how good it does for 4k. I bought it for max settings 1440p but holy shit can it run 4k pretty easily with max settings and frame Gen.  Got that wicked 4k monitor deal from Samsung directly that came out to 130 after student discounts and honeygold. Taking my 5070ti gains even further from the 1070 that ive passed down to my bro for his first rig.",hardware,2026-01-09 17:17:06,2
AMD,nz5xmxm,"the last of us ps3, thats it, unless you count sega genesis games made after 2010 BUT, i speak of herasey!!",hardware,2026-01-12 14:13:23,1
AMD,nyp495t,"People will not pay for a $2000+ AMD gaming GPU no matter what performance it offers due to the lack of features compared to Nvidia. At some price point, raster performance becomes less important and people care more about the productâ€™s, longevity, adaptability to different workloads, and long-term driver support. For example, most people buying a 5090 are not buying it just for gaming. You do not have AMD customers today saying they are buying an AMD GPU to play games and do other workloads with the same GPU.",hardware,2026-01-10 00:12:53,8
AMD,nympibs,"Seriously, 9070 Ti is almost on par with a 5070 Ti, 9070 is better than a 5070 in almost every workload and on top of it is as efficient with unlocked framerates and more efficient when capping framerates according to some reviews. FSR4 closed the gap to DLSS. This was AMD as competitive as it ever was - and it's still not enough to make Nvidia or their customers care. The Super revisions aren't coming because Nvidia calculated it (for the time being) doesn't make them enough money, simple as.",hardware,2026-01-09 17:23:31,5
AMD,nyxj0jd,> AMD & Nvidia are basically equal in performance-per-watt.   only if you stick to 2018 or older software.,hardware,2026-01-11 06:33:26,2
AMD,nyq1yre,"this already is doing the lying work for those shit companies.  implying, that amd couldn't release a truly high end card, UNLESS it costs 2000+ us dollars.  that is just absurd.  now they very much would want to charge those absurd prices with exploding margins,  but they absolutely DO NOT have to.  amd was charging 700 us dollars for a 357 mm2 die on a 5nm family process node with last generation 16 GB memory.  they could have charged 400 us dollars and still made a decent profit.  so again you could get a proper high end card for 650 us dollars or 700 us dollars just fine. a proper big die with a big memory bus and a proper amount of vram.  the idea, that graphics cards need to cost over 1000 us dollars is insanity, that nvidia started to push and of course amd would love to take that up as well.  but don't make those freaking excuses for them.  NO a high end card can cost 650 us dollars or 700 us dollars as they did in the past!",hardware,2026-01-10 03:18:48,1
AMD,nyneuyo,The R9700 32gb card does come close.,hardware,2026-01-09 19:16:34,0
AMD,nyxje94,If they use 3GB chips for these theoretical Super series than AI is exactly what it would be aimed at. Same dies but with 50% more VRAM.,hardware,2026-01-11 06:36:36,2
AMD,nyqynca,"Well, at least it's cheaper than 4080. 4080's $1200 price tag was just too much. It drove most gamers to either 4070ti or 4090.",hardware,2026-01-10 07:09:15,6
AMD,nyou2q1,>  Giving it 24gb to differentiate itself would help.  Differentiate itself by making it 10-20% more expensive? It should have launched with more than 16GB 100% but a refresh with more memory now is pointless.,hardware,2026-01-09 23:18:24,-3
AMD,nypreyz,"> The 50 series itself is just a refresh of the 40 super series   > just more AI with more power draw, and faster vram mostly.  That and a whole new package. It almost seems like not a refresh at all when you put all that together.",hardware,2026-01-10 02:18:39,6
AMD,nypw7ch,Memory bandwidth on the 50 series is significantly higher. Generational real performance increase generally match memory bandwidth increases and they do for the 50 series.,hardware,2026-01-10 02:45:30,8
AMD,nyr9o4e,"This is pure wishful thinking, with the current RAM prices you will be lucky if the ps6 comes with 24GB.",hardware,2026-01-10 08:49:38,11
AMD,nyq7lqp,"You understand that consoles use shared memory both as RAM and VRAM ? Current gen has 16GB so next might have 32GB. That is still less than having a 5070 Ti + 32GB RAM, not to mention on the PC you have everything running at higher clock speeds + you have more powerful CPU.   Pointless comment from you",hardware,2026-01-10 03:52:34,19
AMD,nyxgl0v,8Gb VRAM is still not an issue. Despite all of your and others dooming.,hardware,2026-01-11 06:13:49,-1
AMD,nyo1jwp,"> ""1.6 million units of video game hardware were sold in the US in November 2025, which is the lowest total since 1.4 million back in 1995.""  https://www.videogameschronicle.com/news/hardware-sales-and-physical-game-spending-in-the-us-just-had-the-worst-november-in-30-years/  And three more reporting the same thing, game and game hardware sales well down, to nearly 1995 levels.  https://www.avclub.com/video-game-hardware-sales-november-2025  https://www.ign.com/articles/video-game-physical-software-and-hardware-sales-just-had-the-worst-november-in-the-us-since-1995  https://www.gamespot.com/articles/hardware-sales-in-the-us-just-had-worst-month-in-30-years/1100-6537041/",hardware,2026-01-09 21:01:10,5
AMD,nz6spf6,"i mean you can simplify your life and still own the shit, that is important to you.  dealing with subscription bullshit is just more stress and more shit to deal with.",hardware,2026-01-12 16:44:21,1
AMD,nyxol06,Zero confirmation for any of that. I would trust the latest stuff from various leakers instead pointing to a RDNA5/UDNA launch in late 2027,hardware,2026-01-11 07:21:07,1
AMD,nyp12hh,"FSR4 is good and pretty close to DLSS, the problem right now is not enough games have it implemented natively so you have to do workarounds like use optiscaler",hardware,2026-01-09 23:55:51,16
AMD,nyq01my,"The DLSS and FSR 4 are now at a point that unless you are pixel peeping, you will not notice the difference in normal use. Yes DLSS 4 is slightly better, but it's not a noticeable difference in the same way it was with FSR 3.",hardware,2026-01-10 03:07:24,4
AMD,nyodr0a,"Feature adoption is as big a problem as the features themselves for AMD. So, so many big games have DLSS support but only FSR2 support, which is complete dogshit.  XeSS has better adoption than FSR3 and it's for a GPU brand with essentially 0% market share.",hardware,2026-01-09 21:57:29,4
AMD,nynn6sa,"While I do not have a quote from Nvidia putting it clearly, I do think the decision to make the 3060 a 12GB card instead of 6GB was in part thanks to AMD, where the lowly RX series already had 8GB for years.",hardware,2026-01-09 19:54:41,17
AMD,nyms2pp,"Remember that Nvidia worrying about the threat of Vega (that ultimately ended up being overblown) is part of why the 1080 Ti was only(""only"") 699$ despite easily being the best card on the market that wasn't a Titan or Quadro.",hardware,2026-01-09 17:35:07,16
AMD,nyxhxc6,Jensen has said in the interview that they know what AMD is planning and AMD knows what they are planning. He also stressed this is normal in the industry.,hardware,2026-01-11 06:24:30,2
AMD,nym1m1s,It did get a weird model like the 3080 12GB,hardware,2026-01-09 15:36:24,14
AMD,nymazfp,"It sort of did: the 3090 Ti was a 3090 SUPER, the 3080 Ti was basically a 3080 SUPER, same with the 3070 Ti for the 3070, 3060 8GB was basically a 3050 SUPER too. The name sure wasn't 'SUPER' for those products, but they did do some sort of a refresh via a new SKU that largely never made sense to buy because prices were crazy inflated.",hardware,2026-01-09 16:18:37,16
AMD,nym1x5j,Hot damn youre right! My bad idk why I was thinking it did. I guess it's just my perception that it's been standard since the rtx cards.,hardware,2026-01-09 15:37:50,3
AMD,nym63n3,But they did release the LHR versions the following year after 30 series. So a sort psuedo refresh.,hardware,2026-01-09 15:56:46,1
AMD,nyytkyn,"That was already addressed and mentioned in the comment you're responding to. That's correct, they released a higher vram upgrade card in the 30 series but didn't call it a super.",hardware,2026-01-11 13:19:03,1
AMD,nymlwp9,"They essentially DID do a Super series, just without the naming.   3090Ti = 3090 Super  3080Ti/3080 12GB = 3080 Super  3070Ti = 3070 Super",hardware,2026-01-09 17:07:14,4
AMD,nyoetl6,"The refresh rumors were that 5070, 5070ti, and 5080 were gonna switch to 3GB VRAM modules now that they were more readily available, not that there would be much of a change to the overall binning. Maybe an inconsequential mild clock bump.  Now the rumors are that since the Super series was (rumored to be) just a VRAM increase, and now there's a RAM shortage, that it's just not viable any more.",hardware,2026-01-09 22:02:30,3
AMD,nywvxyz,"Hell if I know. If you pushed me on it, I'd lean towards ""yes"", they'll release *something* on Xe4, eventually. But your guess is good as mine.",hardware,2026-01-11 03:57:28,1
AMD,nynm8ok,"I just replaced by 1070 with a 5070ti last summer, it played all the old games I bought just fine, but the last several years I have been console first. I have a feeling the 5070ti and its 16GB VRAM is going to last a long time at 1440p.",hardware,2026-01-09 19:50:23,2
AMD,nymo9sn,"I'm using a 1070.  It is not fine for modern, demanding games whatsoever, even at 1080p.    Comments like yours are a bit dishonest and you know it.  You're not exactly playing Star Wars Outlaws on your GTX1060, are you?",hardware,2026-01-09 17:17:56,10
AMD,nymdvj8,Yeah to play 10 years old games.,hardware,2026-01-09 16:31:30,6
AMD,nzb96d8,"of you want to speak about optimizing for outdated consoles i think the winner will be gta5, which invented a new technique of reading data from hard drive and disk at the same time in order to provide sufficient data reading rate and prevent stutter. This technique then was never used again because people realized SSDs exist.",hardware,2026-01-13 06:52:08,1
AMD,nyxj2ij,>  9070 Ti   time traveler detected.,hardware,2026-01-11 06:33:53,2
AMD,nyo29a2,Can you use newest FSR outside of Call of Duty? FSR4 is still waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaay behind DLSS 3 or even 2.,hardware,2026-01-09 21:04:28,1
AMD,nymxxeq,"AMD engineer: hey, we closed the gap with nVidia! Now it's the time to go even faster foward, right?  AMD marketing: actually no. Let's squander ALL THE GOOD WILL we accumulated lately by ending support for Zen 2 cards. Thank you.",hardware,2026-01-09 18:01:24,0
AMD,nympzxu,"And they did this while not using the stupid fire hazard power plug, too.",hardware,2026-01-09 17:25:41,3
AMD,nyyv5iv,"NO, a 24 GB 5070 ti or 24 GB 5060 ti certainly would NOT be aimed at the pure ai slop machine, but as gaming cards and nvidia might market ai bullshit for the gaming cards then, that breaks on 16 GB vram easily.  just to be clear nvidia and amd are as you already know massively upselling anyone, who needs a decent amount of vram for anything! amd is selling you 32 GB cards just fine, but that will be a vastly higher margins pro card please! so does nvidia for all the chips.  but that is not what we are talking about here. here we are just talking about a vram increase for gaming cards to make them less shit and how nvidia would market around them.  ""nvidia bullshit ai assistant"" for example is one thing, that already eats a bunch of vram, which upsells you to more vram, IF you were to run it (i mean you shouldn't run it, but it is an example)",hardware,2026-01-11 13:29:23,0
AMD,nypg1ib,"5080 is a fine product. 16GB VRAM is more than enough for gaming, and content creation / rendering for most use cases. Only local AI is affected.",hardware,2026-01-10 01:16:22,-7
AMD,nyrmk0e,"because they gave even less cuda cores   a 4080 was at least 60% of a 4090, the 5080 is 50% of a 5090  and it gets less and less as you go down  the 5060 is more or less what a 50 class card typically is.  and it would have been a killer card had it been a 50 class card because it can do 1440p 60 even with the hobbed vram it has as an entry level 200 dollar first GPU in 2025, but god damn it was sold as a 60",hardware,2026-01-10 10:49:37,3
AMD,nyytfut,"a whole new package? wym?  for gamers, fake frames from MFG is largely irrelevant as it introduces input lag that makes it feel like you might as well do cloud gaming. DLSS is enough AI in gaming.  I guess if youâ€™re using the cards for AI in particular, not gaming, then yeah, 50 series all the way, but if youâ€™re not, itâ€™s a bad value compared to buying used 40 super cards.  10 fps extra on average per game, and thatâ€™s coming from already high fps, so the percentage boost per game is pretty small. https://youtu.be/1x8oOY-sAWU?si=9yLUv_E7JrZsEqmh",hardware,2026-01-11 13:18:05,1
AMD,nyyz0mn,10 fps extra on average across all games? Not a big difference. https://youtu.be/1x8oOY-sAWU?si=9yLUv_E7JrZsEqmh,hardware,2026-01-11 13:53:32,1
AMD,nyrb12c,"can you like think this through a bit more?  the ps6 is NOT getting produced rightnow. it is getting produced in about 2 years.  in 2 years the ai bubble could have long popped, or the bubble still bubbles, the memory insanity could be still over.  remember, that it was the openai scum, that stole all the world's memory pretty over night, which created the current absolute dystopia.  and in 2 years when the ps6 will get produced and come out this could all be over.  if it is NOT over, the ps6 will rather get delayed, than anything else.  from the options at hand sony can either launch a 30 or 40 GB ps6, they can NOT launch a 20 GB ps6, because it would be barely a memory step up from the ps5 pro, which would not give you any ""next gen"" experience.  the ps5 pro already has 18 GB, which is 16 GB + 2 GB of slow ddr5, that background stuff (os, dl tasks) can get thrown onto to free up more memory for the game alone.  so there are again 3 options! 30 or 40 GB ps6.  if that is not possible, it would get delayed.  there is no 24 GB ps6 even possible. it WILL be 30 or 40 GB base on the memory setup.  and again no one is producing a ps6 rightnow. it will get produced late 2027 at the earliest.  the memory prices exploded in a month and they can certainly be back to sanity in less than 2 years.  \_\_\_  also in case that isn't clear, sony isn't throwing together a new custom apu 5 months, before they produce it. it has been in the works for a while. rdna5 is designed strongly for the ps6 and the chip already got leaks.  the things left to decide is, when is it gonna get produced, what clocks will it have, what parts will be disabled for yields AND what memory configuration and again they have the 30 or 40 GB option.  it is one or the other. i hope this made it very clear.",hardware,2026-01-10 09:02:21,-2
AMD,nyqyiw5,Nor will games start to be made exclusively for PS6 hardware right away. It's only about now when new games no longer launch on PS4 (= can be designed to require more than 8GB total RAM),hardware,2026-01-10 07:08:10,5
AMD,nyq982k,"maybe you could actually read?  >to match a ps5's 16 GB unified memory you needed at minimum 12 GB vram.  unless you don't know what unified memory stands for?  and to say it slower for you.  to match the UNIFIED 16 GB memory of the ps5, you need at least 12 GB VRAM on your graphics card.  this is due to differences in how the ps5 handles memory, being more efficient, no duplication, etc... etc...  those are the results of practical testing. to again match the ps5's memory setup in your desktop or laptop you NEED at least 12 GB vram.  or a 3/4 conversion rate.  >so next might have 32GB  this is also wrong, we already know from leaks, that the ps6 as i said will have either 30 or 40 GB unified memory.  and yes those numbers make sense as it is 10 chips 5 on each side with 3 or 4 GB memory per memory chip.  > on the PC you have everything running at higher clock speeds + you have more powerful CPU  this is completely meaningless, which you'd know if you ever watched any video on vram issues.  you NEED enough vram, what speed it is running on is not the issue, you just need enough.  and again if we apply the same math, that worked for the ps5, then you need for the ps6 games generation in your graphics card 22.5 GB vram (24 GB in what is possible) to match 30 GB unified ps6 memory, or 30 GB vram to match the 40 GB ps6 version.  and again this is acounting for how console memory works in regards to being unified in regards to how it handles it differently, etc... etc...",hardware,2026-01-10 04:02:40,-7
AMD,nyyvf3b,"ah yes you can't run 1080p medium anymore, but hey please ignore all that and enjoy your broken games, because some games can still run at 1080p low right with a broken amount of vram? on your at least 1440p monitor we can assume.  the future is now! please go ahead and defend trillion dollar company scams some more.",hardware,2026-01-11 13:31:07,-2
AMD,nyr9bks,"This is console sales, not GPUs.",hardware,2026-01-10 08:46:26,10
AMD,nyxgy09,FSR4 is as good as DLSS3. Which is great. Except we now have DLSS 4.5.,hardware,2026-01-11 06:16:40,2
AMD,nyxh552,"Let me paint you the picture from a game developer perspective. You are making a game. You want to make it run well with a GPU. You ask Nvidia AND AMD for help. Nvidia sends you an engineer or even a team if the game is big enough to help you adopt your code to Nvidia GPUs. AMD either ignores you, or throws a temper tantrum that you dared to ask Nvidia to help.  Guess which GPU is going to be supported by the game.",hardware,2026-01-11 06:18:16,2
AMD,nyp23bv,"The bigger problem for nvidia was having to classify their 2nd biggest chip in 30xx series as the 3070/3070Ti, since 6800XT was much faster than it.    Otherwise, it could have become the 3080 outfitted with 16GB of VRAM and 3060 would not have looked so abnormal in the lineup.",hardware,2026-01-10 00:01:21,11
AMD,nyu8ihi,I remember being in highschool and lusting after a 1080 ti and thinking how much money that was for a gpu. Haha we didn't know how good we had it. Now I'm stuck debating if I want to just bite the bullet for ddr5 ram or upgrade to a better ddr4 system haha,hardware,2026-01-10 19:38:29,3
AMD,nynbo7k,"Nah , the reason was that the 980ti was 649 and a 10% price bump is quite good already. Bumping it faster wouldâ€™ve scared more customers.   Nvidia didnâ€™t give a shit about vega.",hardware,2026-01-09 19:02:04,1
AMD,nymnusn,"Yeah, Ampere dropped right around the Ethereum craze before it moved to proof of stake, so the entire lineup from top to bottom was permanently out of stock at anywhere close to MSRP.  During that time, nVidia cobbled together mad SKUs from whatever leftover cut down dies they had with whatever GDDR was available at the height of the COVID supply shock.   So thats how we ended up with a 12gb 3060 and an 8gb 3060ti which was almost a 3070 but back then you couldn't find either for sale at less than 3x MSRP. You had to wait for an ETH crash, the first of which occurred in August 2021.   I still remember keeping a damn xlsx spreadsheet tracking ETH price weekly. Only time I ever graphed anything with a perfect correlation. Then August rolled around and that was the first time I could actually check out a graphics card (any graphics card) without a 502 Bad Gateway error thanks to all the damn bots.  Absolutely mad time to buy a GPU.",hardware,2026-01-09 17:16:03,11
AMD,nymlgsb,"Pretty much.  I think Nvidia didn't consider the SUPER line any more than a one-off at the time, which is why they didn't go back to it for 30 series(which also offered pretty reasonable value out the box anyways).  And with 40 series, I think Nvidia came in with the clear intent of offering worse value up front, only to give better value with SUPER variants later on.  You can just tell looking at how they specifically cut down their parts and what gaps there were.  And it's not like 5nm was some totally new node either, it had been in products since 2020 already and by all accounts had great yields and mature characteristics.  They absolutely intended on milking things and skimping on initial value for better margins.",hardware,2026-01-09 17:05:15,10
AMD,nymlyjc,"So we have a 5070 super, problem solved",hardware,2026-01-09 17:07:28,3
AMD,nyn9nji,3080Ti 20GB = 3080Ti Super Duper,hardware,2026-01-09 18:53:11,2
AMD,nyxi3lu,Tis is the OG super series before they were called Supers and been around a lot longer.,hardware,2026-01-11 06:25:55,2
AMD,nywzat0,I'm still astounded how bad Ponte Vecchio turned out even with a state funded supercomputer at stake,hardware,2026-01-11 04:17:34,1
AMD,nymu684,"Is anyone playing Star Wars Outlaws? I'm playing AoE 2: DE, AoE 4 and LoL. It's a 1060 3gb even, so the cheaper version.   Every now and then I play one of the free Prime or Epic games, all run fine (including Hogwarts).",hardware,2026-01-09 17:44:38,-4
AMD,nymlm9l,"I had a GTX 770 for 11 years, until 2024. I was still playing a few recent games like Honkai Star Rail and indie games. But also DOOM Eternal and the modern Wolfenstein games. All that with 2GB VRAM.",hardware,2026-01-09 17:05:56,4
AMD,nynole7,"KCD2, released 2025 and pretty good looking, runs around 30fps on a 1060 6GB.      CP2077/PL is from 2020/2023, is still a very good looking game and it runs fine, albeit very ugly, on a 1060 6GB.        Stalker 2, released 2024, also runs at 30fps+ on a 1060 6GB.      Just because it can't run Full RT games or Star Citizen at 120fps doesn't mean its useless for still many games that are not 10 years old. It is also the listed minimum requirement for Stalker 2 (2024) and KCD2 (2025).",hardware,2026-01-09 20:01:08,4
AMD,nymh0jw,I am using my 970 to play Hogwarts,hardware,2026-01-09 16:45:26,2
AMD,nymtrts,"I can play Hogwarts fine (free game recently), I mostly play AoE 2: DE, AoE 4 and LoL.  All recent games or games that are updated frequently.   I did upgrade my CPU to a 5600 so that helps. Not upgrading until it dies, because why, it works.",hardware,2026-01-09 17:42:50,1
AMD,nzcvi37,5 and not 4?,hardware,2026-01-13 14:29:39,1
AMD,nysr8a8,"You can add FSR4 into almost any game that uses DLSS2/3/4, FSR2/3 or XeSS...",hardware,2026-01-10 15:24:30,0
AMD,nyxj4vc,"you mean RDNA 2 cards? Zen 2 is still getting new releases, unfortunatelly.",hardware,2026-01-11 06:34:26,2
AMD,nz4exps,a 24 GB 5060ti would be useful only for AI as it is far too much VRAM for that chip for gaming.,hardware,2026-01-12 06:55:14,2
AMD,nyscz73,"Indiana Jones already brushes up against the 16GB cap at 4k. Its concerning that a brand new high end GPU is already seeing memory bottlenecks in the year of its release. Sure you can optimize your settings to keep it within 16GB of usage but that really shouldnt be necessary with a  brand new $1200 GPU  Nvidia are pushing super memory intensive features, like path tracing, frame gen etc, then selling you GPUs that have barely enough RAM to make use of them in current games leaving out any room for future games and features. We just have to pray their software team can cut the vram used by these features enough to give your GPU longevity.",hardware,2026-01-10 14:05:37,10
AMD,nypyvpw,hell there nvidia marketing employee,hardware,2026-01-10 03:00:33,15
AMD,nyytjnf,Why are you getting downvoted? What you said is all factual.,hardware,2026-01-11 13:18:48,1
AMD,nyrb91q,"It's coming out in two years, hardware is getting finalized about right now and components needs to be sourced in this year for the first batches, it's very much affected by the DRAM crunch.",hardware,2026-01-10 09:04:28,7
AMD,nz4f0gm,with 8 GB  you can run 1080p high texture resolution with no issues. You just cant run all the extreme resolutions which you wouldnt be able to see on 1080p anyway.,hardware,2026-01-12 06:55:53,1
AMD,nyxldat,lol no,hardware,2026-01-11 06:53:14,1
AMD,nywpfe3,"Rdna2 was truly a magical generation for amd standards, even if it took console stakes and essentially a significant node advantage (TSMC N7 vs Samsung 8nm aka rebranded 10nm) to get there.",hardware,2026-01-11 03:20:12,3
AMD,nyxin48,So the newest game you play (AOE4) is from 2021.,hardware,2026-01-11 06:30:23,1
AMD,nymnz11,I shudder to imagine what you've had to do to the graphics to achieve that at even 30fps. lol,hardware,2026-01-09 17:16:36,3
AMD,nymnjav,"Its just the randos that bought a card every  year thinking they needed a new gen to keep up with gaming.   Meanwhile 1000 series owners riding what 8 years now where they finally need to upgrade.  I laughed at all these ""oh the 50 series is only 10% over the 40 what a waste of Nvidia greed""  My 1070 did enough at 1440p on many games especially the free fps games you wouldn't want max settings on. And they only just recently cut driver support for the 10 series.  20-30 getting life support with new driver support too",hardware,2026-01-09 17:14:36,3
AMD,nyxj7hk,not without jumping through hoops 99% of consumers dont even know exist.,hardware,2026-01-11 06:35:02,3
AMD,nyxjxcp,"Yes, RDNA 2. My apologies.",hardware,2026-01-11 06:41:02,1
AMD,nz4hyk9,">as it is far too much VRAM for that chip for gaming  this is complete and utter bullshit.  that is the same garbage, that people said about 16 GB vram when the 30 series cards came out.  how's the 3070/ti holding up again with its 8 GB vram today? oh yeah a broken dumpster fire designed by nvidia to force customers to upgrade purely for vram reasons.    you NEED the amount of vram you need as long as the gpu can play the game at all.  this means NO MATTER the gpu performance, if the gpu can still play the game and people will use it, which they will, it NEEDS enough vram, which will be at minimum 24 GB vram when the ps6 comes out to match it (if the ps6 gets just 30 GB memory)",hardware,2026-01-12 07:21:50,0
AMD,nyre8o1,"NO, not memory.  are you excited to draw up contracts about memory, that isn't getting produced yet + doing so during the memory apocalypse. that surely would be a banger deal and so smart!  you can not buy any 32 Gbit gddr7 (4 GB) chips yet, which need to exist, BEFORE sony can make the decision on whether to use 30 or 40 GB options.  so NO, not memory. memory capacity is absolutely NOT locked in yet.  and the idea, that they would have it locked in is again absurd.  just to be clear, YES the current memory insanity can strongly effect things, but the outcomes are 30 GB, 40 GB or the delay option as well.  so YES the current memory insanity, if it is still going on by then could delay the console and could result in a 30 GB console, instead of a 40 GB console and thus holding all of gaming back (thx ai bullshit)  but it can NOT result in a 24 GB console and it can also not result in an at least theoretically possible 20 GB console, because that would be a complete stagnation as the ps5 pro already has 18 GB and sony and especially mark cerny aren't complete idiots and certainly won't do an ""xbox series s"", which if you don't know didn't have enough memory, by quite a bunch and resulted in developers actively hating that piece of shit massively, prevented games from getting to xbox as a whole and held back all of gaming.  so again NO. there will be a 30 or 40 GB ps6.",hardware,2026-01-10 09:33:01,-2
AMD,nz4ihxm,"this is factually incorrect and you already know this, because you almost certainly already saw this video:  [https://youtu.be/IHd95sQ-vWI?si=WfYJzwMqEAy3oB2e&t=1928](https://youtu.be/IHd95sQ-vWI?si=WfYJzwMqEAy3oB2e&t=1928)  which shows, that at 1080p medium settings the game breaks.  you are WRONG. you can no longer run at 1080p with high texture quality anymore, you can not even run at 1080p with all medium inc texture quality.  please face reality in this regard and demand proper amounts of vram.",hardware,2026-01-12 07:26:45,-1
AMD,nyxkv0e,"This sub still didn't think that AMD could compete despite PS5 clocks showing that the RDNA2 would easily clock at 2.2GHz. RDNA1 was good but it took too much power to get close to 2GHz, and even beating 2080Ti was thought to be a silly claim.    But even I was surprised at how high RDNA2 could reach. My 6800XT ran at 2.6GHz and the latter RDNA2 chips were close to 3GHz mark.  AMD have sort of stagnated since then. RDNA3 would have looked much better if it could go over 3GHz mark easily like RDNA4 does now.",hardware,2026-01-11 06:48:56,0
AMD,nymq6u4,"Medium settings, with low shadows  and 1080p",hardware,2026-01-09 17:26:32,5
AMD,nz4iovn,"The 3070ti is holding up just fine. Its niether broken nor on fire.  The minimum amount of VRAM people actually NEED for new games is about 6 GB. Everything else is a matter of settings. There is yet a game that on max settings can utilize 16 GB, so 24 GB is ludicrous claim.  Also PS6 will not have 30 GB of memory. thats kinda silly.",hardware,2026-01-12 07:28:30,2
AMD,nytyc01,you need some help :D,hardware,2026-01-10 18:49:36,5
AMD,nz4jjnk,A badly done remaster from a company know for outdated broken game engines stutter... at maximum settings?,hardware,2026-01-12 07:36:23,2
AMD,nz5ubsu,"I have not seen anyone with proof of needing more than 16GBs of VRAM.   The VRAM argument is fucking annoying at this point because people treat it like they are always pushing the limit. 9070XT has 16GB of VRAM. You might want 24GB for 4k but if we look at the steam hardware survey, such a low amount of people are running 4K screens. People are happily playing games on hardware as far back as the GTX 10 series. I know a guy getting by on a 1050 right now bless his soul.  In 2015 we had 4Gb cards running games that objectively looked better than the games now. Prime examples being Metal Gear Solid V, Star Wars Battlefront, Battlefield 1. Many others Iâ€™m sure. 10 years later we have quadrupled VRAM, get worse performance in games, get worse image quality from AI upscaling, horrible antialiasing, and frame generation, and then still think adding 900 gigs of VRAM is a necessity.  Yes NVIDIA skimps on VRAM. Id say its cards handle the VRAM it does have very effectively though and by the time you need more VRAM, it is time to consider a new GPU. I havenâ€™t hear of people not being able to play games because of lack of VRAM since the late 2000â€™s early 2010s. Again, people playing on 1050s in the big 2026.",hardware,2026-01-12 13:55:13,1
AMD,nzcwhgh,">There is yet a game that on max settings can utilize 16 GB, so 24 GB is ludicrous claim.     indiana jones does already.  >Also PS6 will not have 30 GB of memory. thats kinda silly.  what else will it have? what's your source on it not having at least 30 GB of unified memory?  reliable leaks point to it either having 30 or 40 GB of memory based on whether sony choses 3 or 4 GB gddr7 memory modules.  you have better sources, then please provide them.  and 30 GB is less than a doubling from last generation and the ps4 16x the ps3's memory.  so 30 GB is the low number and we should HOPE, that sony goes for 40 GB as it is better for everyone.",hardware,2026-01-13 14:34:42,1
AMD,nzb91ie,"> You might want 24GB for 4k  Do you have an example of what game can use (not just allocate) 24 GB for 4k?  >In 2015 we had 4Gb cards running games that objectively looked better than the games now. Prime examples being Metal Gear Solid V, Star Wars Battlefront, Battlefield 1.  as someone who has recently replayed MGS5 last year, you are looking through nostalgia glasses. The games visuals were very simplistic compared to modern games. I think a lot of people dont actually remmeber what those games looked like. Indy game from last year is objectively the best looking game ever from a technical standpoint. Guess what, its mandatory RT.  DLSS eliminated aliasing problems and with quality preset its better image quality than native thanks to that aliasing removal.",hardware,2026-01-13 06:50:58,1
AMD,nzcnwyw,">and by the time you need more VRAM, it is time to consider a new GPU.  this video is almost 3 years old now:  [https://www.youtube.com/watch?v=Rh7kFgHe21k](https://www.youtube.com/watch?v=Rh7kFgHe21k)  it shows the VERY STRONG at the time and still quite strong 3070 get completely broken and unplayable in lots of games at settings, where the gpu itself can get great fps.  for example the last of us part 1 1440p high quality settings, so what you'd expect to run just fine on your quite new card of course.  runs at 86 fps with excellent 1% lows of 76 on the rx 6800.  meanwhile the 3070 is unplayable broken with 30 fps 1% lows. (this is a sign of extreme stuttering and freezes in the actual game)  so you are wrong.  the 3070 and the 3070 ti are broken today, because of their missing vram.  nvidia knew EXACTLY what they were doing, when they refused to sell 16 GB versions of the 3070 and 3070 ti.  and the fact, that you defend this still today, when the video exposing this scam came out almost 3 years ago is absurd.  \_\_\_  and in regards to visual quality, that is partially a separate discussion mostly about forced blur into games.  you could have a modern game, that is free from temporal blur reliance, that is using forward rendering and gives u the option to use msaa and if that game has decent quality textures, you will need more than 8 GB vram.  remember, that games are developed to get the most out of the ps5. to match the ps5 you need about 12 GB vram at least.  nvidia knew this, amd knew this and they released 8 GB vram broken cards and are still doing this to break to fuck with people.  >I have not seen anyone with proof of needing more than 16GBs of VRAM.  indiana jones breaks with 16 GB vram at very very high settings already, BUT that wouldn't be a big issue. the big issue is the 30 or 40 GB ps6 train coming down the tracks, that WILL push vram requirements up massively yet again just like the ps5 did, just as people predicted the ps5 would as they advised against 8 GB vram before the ps5 ever came out.  and you are not buying a graphics card for TODAY, you are buying it for maybe 5 years or longer, which will be 3 years into the ps6 generation, at which point 16 GB vram will possibly be very terrible by then.",hardware,2026-01-13 13:49:32,1
AMD,nzbbg64,"The 4k comment was not definitive. I said that because I donâ€™t have much time on my own 4k setup. I wasnâ€™t suggesting you MUST have 24Gb I said you might want, â€œmightâ€ expressing my uncertainty. My overall point was the VRAM argument is always overblown in my opinion.  I disagree. I played MGSV recently, it is not nostalgia. I offered other examples as well. I do not agree with you on the quality of upscaled images with DLSS. Does Stalker 2 look better than MGSV? I suppose in some ways yes, but then as you play it, itâ€™s as if something got stuck in your eye. The image isnâ€™t stable, you shut off a flashlight and wait a moment for Lumen to catch up and make all the light particles disappear rather than it be an instant lights off. Same thing as in MGS Delta, a better apples to apples comparison, it looks good in a still image but when you move around and look at the foliage or off in the distance you see the specs flashing about and the blur/ai fighting with itself. These things are all noticeable and there are communities of people coming about because itâ€™s almost a complete disappointment to find out a game you are interested in is releasing on UE5.  I think my point stands, MGSV looked better native, compare its native image to MGS delta on a native setting. Why are we rocking these high tier, massive VRAM cards, but relying on AI and upscaling now? Rendering a 480p native res image upscaled to have something â€œplayableâ€.",hardware,2026-01-13 07:11:51,1
AMD,nzctl7e,"If youâ€™re buying a gpu for 5 years or longer, and itâ€™s 2026, and the 3070 came out in 2020, doesnâ€™t that meet your requirement?   Iâ€™ll take a look at the video you linked. I have a friend still playing on a 3070 with no such issues as games being completely broken.  You are saying if Nvidia put all the VRAM on the card, it would play every game maxed out Ultra at 4k. You can play TLOU1 with a 3070.  [Indiana Jones on RX480 8GB](https://youtu.be/2xwi-rfLnug?si=KQzJmufOYAqS_cCA)  I mean idk what we are arguing about here. Good cards will go a long way. Yea Nvidia skimps on Vram but people exaggerate it being an issue for them. If a RX 480 can play this, I know a 3070 can.",hardware,2026-01-13 14:19:46,0
AMD,nzerdb9,"I wasnâ€™t expecting the claim of 82% gain vs Strix Point to hold up in third party reviews, but here we are. The gap is pretty astonishing, almost painful considering APUs used to be AMDâ€™s unique thing.  Also, interesting point they raise: because Panther Lake gets the full featureset of XeSS 3 (SS + FG), the user experience can even match that of Strix Halo.",hardware,2026-01-13 19:54:50,59
AMD,nzei5l6,"It's somewhat ironic that Intel's GPUs seem to be the most exciting things about their SoCs now, specially if you consider the recent deal with Nvidia...",hardware,2026-01-13 19:12:57,55
AMD,nzem2zz,"So far Intel is only showing off gaming performance on the iGPU, and it looks good. But I can't wait to see full benchmarks.     Intel's 200 series is actually 4 different lines:   * Lunar Lake (Core Ultra 200 V series) in premium thin and light segments and handheld segments * Arrow Lake (Core Ultra 200 H series) in mainstream laptops * Raptor Lake re-refresh (Core 200 series) in gaming laptops with discrete graphics * Meteor Lake Refresh (Core Ultra 200 U series) in cheaper thin and light laptops     I'm curious to see how it compares to all 4. I expect Panther Lake to easily beat the Arrow and Meteor lake chips, but I'm curious if it can beat Lunar Lake in power consumption (or at least come close) and beat Raptor Lake in single thread/gaming performance.",hardware,2026-01-13 19:30:41,18
AMD,nzewamc,"Nice first look with Digital Foundry. Always believed Lunar Lake was a sidegrade to the HX370/Z2E (890M), but this seems like a real upgrade. (Also didn't realize Intel themselves were the ones giving the power measurement tool to media outlets, very cool).   Important to note, **these tests are done at \~60W package power.** I would've loved to see a smaller power envelope, but we'll just have to wait. In three games, CP2077, Doom TDA, and SotR. Similarly power configured HX 370 and Strix Halo, Panther Lake (B390 12 Xe3 cores), is **2x faster** than the former and \~**23% slower than the latter** (Strix Halo being on avg, 30% faster). AMD announced Ryzen AI MAX+ 392 and MAX+ 388 with smaller CPU configs but with the full GPU, so it'd be interesting where that takes them, as far as I know the Asus TUF Gaming A14 has the 392, and I could probably count on one hand how many laptops/tablets has Strix Halo lmao.  Very respectable showing. Though, I really wished Intel had updated their ML upscaler since there only have been incremental updates with XeSS 2, and so far is only on par with DLSS CNN, where FSR4 (and INT8) and DLSS4/.5 are superior. Don't get me wrong, XeSS3 and it's ML M/FG is great and I reckon has no frame pacing/jitter issues compared to Redstone FG, but this was a small miss from them.  So as I anticipated for a while, PTL is very exciting stuff, not only their GPUs in gaming, but HW acceleration has been something they've always worked on so interested to see QuickSync performance on media editing, and even perhaps advances for 3D graphics/ray tracer improvements.",hardware,2026-01-13 20:17:51,19
AMD,nzfxuor,"Some numbers extracted:  ### **Cyberpunk 2077 (1080p Ultra, RT Reflections + Sun Shadows)**  | Processor | Native | XeSS Balanced / FSR3 Performance | XeSS Balanced + Frame-Gen / FSR3 Performance + Frame-Gen | | :--- | :---: | :---: | :---: | | **Core Ultra X9 388H** | 29.05 FPS | 55.96 FPS | 96.6 FPS | | **Ryzen AI 9 HX 370** | 15.6 FPS | 32.5 FPS | 58.5 FPS | | **Ryzen AI Max+ 395** | 35.96 FPS | 79.3 FPS | 116 FPS |  ### **Doom: The Dark Ages (1080p Ultra, Static Res)**  | Processor | Native | | :--- | :---: | | **Core Ultra X9 388H** | 33.3 FPS | | **Ryzen AI 9 HX 370** | 16.34 FPS | | **Ryzen AI Max+ 395** | 43.3 FPS |  ### **Shadow of the Tomb Raider (1080p Highest, Ultra RT Shadows)**  | Processor | Segment 1 | Segment 2 | Segment 3 | | :--- | :---: | :---: | :---: | | **Core Ultra X9 388H** | 42.64 FPS | 39.6 FPS | 37.12 FPS | | **Ryzen AI 9 HX 370** | 24.3 FPS | 19.8 FPS | 20.8 FPS | | **Ryzen AI Max+ 395** | 67.96 FPS | 52.3 FPS | 52.3 FPS |  ### **Core Ultra X9 388H vs. Discrete GPUs**  | Game | Core Ultra X9 388H | Radeon RX 6600 | GeForce RTX 3050 | | :--- | :---: | :---: | :---: | | **Cyberpunk 2077 (Native)** | 29.05 FPS | 28 FPS | 34 FPS | | **Cyberpunk 2077 (Upscaled)** | 55.96 FPS | 60 FPS | 65 FPS | | **Doom: The Dark Ages** | 33.3 FPS | 37.5 FPS | 36.5 FPS | | **Shadow of the Tomb Raider (S1)** | 42.64 FPS | 59 FPS | 51 FPS | | **Shadow of the Tomb Raider (S2)** | 39.6 FPS | 38.5 FPS | 44.4 FPS | | **Shadow of the Tomb Raider (S3)** | 37.12 FPS | 42.7 FPS | 45 FPS |",hardware,2026-01-13 23:18:01,10
AMD,nzg74v1,The only thing I am curious/interested in is performance per watt. If Intel delivers this then I'll likely get this as server parts.,hardware,2026-01-14 00:07:52,4
AMD,nzexvl8,These look amazing. Hope this is the start of Intel back competing properly. AMD showing signs of complacency lately and need a big kick up the bum.,hardware,2026-01-13 20:25:18,9
AMD,nzen1km,did the video say what desktop rtx 3050 they tested? it is 6gb or 8gb?,hardware,2026-01-13 19:35:08,3
AMD,nzgffmp,"Impressive, I wonder what the different in memory bandwidth to the GPU is here given that Strix Halo is 4 channels to Panther Lake's 2 (even if the per channel throughput is higher).  That said, there's a process advantage going from TSMC 4 (which is a TSMC 5 derivative) to TSMC 3e, to temper the conversation a tad.",hardware,2026-01-14 00:52:54,3
AMD,nzgih4i,Wonder of a variant of this is what the next Xbox may have been based off of.,hardware,2026-01-14 01:10:08,3
AMD,nzequ1c,steam deck 2 candidate?,hardware,2026-01-13 19:52:25,11
AMD,nzgac2y,"Next gen version of this will probably end up being my laptop then, and Iâ€™ll finally be free from the curse of dGPUs",hardware,2026-01-14 00:25:07,4
AMD,nzh3os2,If I'm buying a laptop this year it's definitely going to be a Panther Lake Intel,hardware,2026-01-14 03:10:30,2
AMD,nzexn9h,"Panther Lake slaughters Strix Point, like it's not even a competition.  As for Strix Halo, at the same 65w Halo is 30% faster, but I wouldn't even call Halo the winner here. Halo has an absolutely massive GPU die and barely performs better at this wattage while PTL is around the same size as Strix, and such a difference in die size affects product costs and company margins.Halo is only in a few products, and that list shrinks significantly when you're looking for a thin small laptop or handheld. Since most people will use upscaling, Xess2>FSR3 (FSR4 not supported), so image quality and stabilization will be worse on Halo. Don't get me wrong Halo is still good and still shines in its own way, but it doesn't actually win against PTL imo unless you only look at FPS and ignore every other factor.  As for the 3050 and 6600 (desktop) comparisons. Lets just say it's +-10% depending on the game and settings. With such close performance it essentially shows that lower end mobile dGPUs are about to become a relic. You end up using like twice the power, and pay more money for essentially the same performance as a PTL iGPU  TLDR; Panther Lake looks like it'll be the best laptop chip to buy, at least in gaming, and seems like Intel is back to making good products.",hardware,2026-01-13 20:24:14,10
AMD,nzf928q,"1080 ultra RT @ 20fps may be 2x faster than strix point but is still unplaybale, apart for making intel look better than it really is no one would use these settings.  intel showing cherry picked benchmarks is usual but DF doing it for them is questionnable.",hardware,2026-01-13 21:17:55,-20
AMD,nzf1s19,"Incredibly embarrassing for AMD as their main revenue driver in Gaming is SoC (Sony, MS) and Steam Deck/Machine. I'm not upset though...bout time we had true competition for APUs and SoCs.",hardware,2026-01-13 20:43:41,23
AMD,nzexbtk,AMD could easily make an APU with enough CU in the level of ps5 or xbox - but they dont. Since xbox is belly up I still hope that they beg AMD for an APU like this with no exclusivity.,hardware,2026-01-13 20:22:45,8
AMD,nzgfkri,"This is ray tracing, which I'd imagine is a lot better on Intel. I'd imagine that gap would fall to around 50% in raster. I'm still wondering how this would perform at like 25w for handhelds.",hardware,2026-01-14 00:53:42,2
AMD,nzel0kv,The recent deal with NV has nothing to do with their iGPU in Panther Lake SoCs tho..,hardware,2026-01-13 19:25:53,31
AMD,nzfmv60,Nvidia knows it has monopoly. AMD is happy with being Xbox and Playstation SoC supplier and being the backup in industrial machine learning when someone can't afford Nvidia.,hardware,2026-01-13 22:21:58,5
AMD,nzf02sb,"Doubt it will beat Raptor Lake refresh in single thread, though a lot of Core 200 is not Raptor Lake, but Alder Lake since itâ€˜s a refresh of the U and H series CPUs of 13th gen, which were basically rebadged 12th gen CPUs with the Raptor Lake labeling..",hardware,2026-01-13 20:35:43,1
AMD,nzf01rx,"It for sure will. [https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/3.html](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/3.html)  They've already talked about CPU performance a while ago, and while people still brag on about the on package memory for Lunar Lake, it effectively was a one off, as it was best for the OEM partners to do their own thing without enforcing it on Intel's end. Even without, Intel has made good efficiency improvements for PTL compared to LNL.  PTL is their new baseline for future performance, on all fronts it's better than what they made in the past.",hardware,2026-01-13 20:35:36,0
AMD,nzfv1iv,"Xess 2 is much better than fsr3, which is what strix point uses. God fsr3 is shit",hardware,2026-01-13 23:03:08,7
AMD,nzhiwgq,What power level for the GPU in the dgpu tests?,hardware,2026-01-14 04:47:23,1
AMD,nzgenmo,>AMD showing signs of complacency lately  It's not even a recent occurrence. That's just Radeon being Radeon tbh.,hardware,2026-01-14 00:48:37,4
AMD,nzhjiep,"It has been going on for years with CPU for 3 years now too. AMD started ignoring consumers as soon as Intel was no longer a threat, and focused on datacenter. Core counts, prices, performance all stagnated.",hardware,2026-01-14 04:51:38,1
AMD,nzh16i9,Crushing CPU isn't enough?,hardware,2026-01-14 02:56:04,-2
AMD,nzf6qgh,Performance is similar to that rx6600 so it's for sure the 8gb version.,hardware,2026-01-13 21:06:57,8
AMD,nzfwqvi,8GB 3050.  The 6GB 3050 is substantially slower than the RX6600  https://www.techpowerup.com/review/nvidia-geforce-rtx-3050-6-gb/31.html,hardware,2026-01-13 23:12:08,4
AMD,nzexv04,"The didn't explicitly state, but searching an old video of theirs, they do have the 8GB model on hand",hardware,2026-01-13 20:25:14,2
AMD,nzg4839,From what Iâ€™ve heard itâ€™s a pretty expensive package. You probably will only see it in the 1000+ dollar ones from rog and Lenovo.   Maybe in a few years if the price drops enough,hardware,2026-01-13 23:52:13,3
AMD,nzfvtnb,Steam Deck 2 will be an ARM device more than likely.,hardware,2026-01-13 23:07:15,0
AMD,nzf3yy6,"The reviewers did note fairly that the wattage is a notable part of it - Strix Point isn't really designed to be run at 65W, and Strix Halo isn't really a 65W part either - Strix Point is being run above its positioned power envelope, and Halo below.  Halo has quite a bit of performance left up the wattage curve.  The native performance was surprisingly pretty strong, but it's with the modern XeSS solution that you really see it shine - it's much better than FSR3 on the AMD mobile offerings in terms of quality and performance.  Think this part will overall be the one to beat.  Strix Point is dated, and Halo is too expensive and rare and power hungry.  Think INTC knocked it out of the park, and there is still room to iterate on the model.",hardware,2026-01-13 20:53:54,16
AMD,nzf04ic,8060s in Strix Halo is about 6 times bigger than Xe3 in Panther Lake.,hardware,2026-01-13 20:35:58,7
AMD,nzfgff1,They explicitly say that you wouldn't play at those settings without upscaling. The main point is demonstrating performance delta in a GPU limited scenario,hardware,2026-01-13 21:51:34,19
AMD,nzeyl65,DF has done this topic with Strix Halo vs PS5  [https://youtu.be/vMGX35mzsWg?si=R\_OhawEsXOw2hei-&t=869](https://youtu.be/vMGX35mzsWg?si=R_OhawEsXOw2hei-&t=869),hardware,2026-01-13 20:28:39,15
AMD,nzezg23,You mean Strix Halo?,hardware,2026-01-13 20:32:44,10
AMD,nzexw8y,Have you heard of Strix Halo? Thatâ€™s what youâ€™re describing,hardware,2026-01-13 20:25:24,15
AMD,nzgmee8,"Isnâ€™t it still embarrassing something from the company known for poor graphics, is a lot better in ray tracing than whatâ€™s from the formerly ATI though.",hardware,2026-01-14 01:32:39,11
AMD,nzeozo9,"Yes, but how can we be sure it won't impact further development? Only history will tell.",hardware,2026-01-13 19:44:03,18
AMD,nzf3e7d,"It means future Intel iGPUs will use Nvidia dies, and further kill off driver support for Intel iGPU drivers",hardware,2026-01-13 20:51:13,-9
AMD,nzfzdpb,"IIRC, there was a RPL-H that was new silicon, but the same L2 size as ADL.",hardware,2026-01-13 23:26:10,3
AMD,nzf0qh6,"> on all fronts it's better than what they made in the past   Intel, at least, isn't making that claim.",hardware,2026-01-13 20:38:50,-1
AMD,nzglxok,"Depend on games, in forbidden west and GOWR, xess is very blurry, fsr is better.",hardware,2026-01-14 01:29:55,-1
AMD,nzgmiac,You can just use FSR INT8 and it will be better than Xess 2.,hardware,2026-01-14 01:33:17,0
AMD,nzffsai,makes sense ye,hardware,2026-01-13 21:48:37,2
AMD,nzeyb5x,i see thanks,hardware,2026-01-13 20:27:21,1
AMD,nzhjmdj,Youâ€™re thinking of lunar lake. Panther Lake is relatively cheap. Mostly in house and small. No on-package memory either.,hardware,2026-01-14 04:52:24,1
AMD,nzfrwii,"> Think this part will overall be the one to beat  For laptops and mini-pcs, this looks like an outstanding product. As for handhelds, the only question is how it scales to lower wattages.   >Halo is too expensive and rare and power hungry.  Strix Halo doesn't make sense as a product at all. It's expensive to produce and lacks memory bandwidth. For gaming you're bettter off getting a dGPU. For AI, it's not good either because although it has a lot of memory, most of the AI applications that could potentially utilize that much memory also need high memory bandwidth which Halo doesn't have.",hardware,2026-01-13 22:46:58,8
AMD,nzf2dbw,"If we're talking about the chiplets/tiles, the 8060s is found with the IO, while PTL has the IO on a separate platform tile. It's more 3x, PTL iGPU+IO < STX-H IOD.",hardware,2026-01-13 20:46:26,9
AMD,nzfvdqn,"Sorry I'm dumb, when you say 6 times bigger you mean the physical chip size?  So pamtherlake can more easily fit in handhelds, unlike strix halo?",hardware,2026-01-13 23:04:55,0
AMD,nzezaw3,1. Its very expensive 2. bandwidth limited 3. it really should've been RDNA4  \[This got fixed\] 4. it was always paired with 16 cpu cores which simply bloated the power profile but it finally got fixed with 388.     Still missing the first 3 issues.,hardware,2026-01-13 20:32:04,21
AMD,nzhcuuu,"Kind of. AMD didn't think it would take off, and even a lot of gamers at first thought it was a joke. Nvidia pushed hard, and implemented tech like upscaling, and frame generation to make it more, and more viable. They steered the industry in this direction, and AMD kept heading straight. Intel followed the captain of the ship instead. Nvidia. AMD is course  correcting, but won't be totally on track until likely RDNA5.",hardware,2026-01-14 04:06:33,0
AMD,nzexm5s,"The way Nvidia is treating PC users right now, I am not sure their influence would be positive on Intel. For all we know they are just going to drag Intel along into the AI slopfest.",hardware,2026-01-13 20:24:05,-3
AMD,nzf1fvq,"For what specifically? For the end user, performance has gotten better. In the CPU segment, for peak CPU perf, I can see ARL-H/HX still being relevant but we have NVL-H in the future.",hardware,2026-01-13 20:42:08,2
AMD,nzgrsoq,Are you talking about the dp4a version of XeSS?,hardware,2026-01-14 02:03:17,5
AMD,nzgmvxr,I used fsr4 using optiscaler on my lego2 and it tanked fps by like 20%+. Is that what you mean,hardware,2026-01-14 01:35:29,2
AMD,nzh78gl,"Not really. We need to consider Intel can use XMX version of XeSS and it's much better than DP4A version. Tom defended their decision to call all XeSS the same. I still disagree. To this day people need to add ""and we're talking about X version"" where X is either superior Intel native XMX or general purpose DP4A. Plus INT8 version of FSR4 has slightly worse IQ compared to FP8 and has a massive perf hit compared to FSR3 on RDNA3.  [AMD Tried To Hide This From You - FSR 4 INT8 on RDNA 3 & 2 Tested](https://www.youtube.com/watch?v=yB0qmTCzrmI)  In HUB testing INT8 FSR4 is 1 to 2 tiers slower compared to FSR3. ie FSR3.1 Quality nets you the same FPS as FSR4 INT8 Perf in some titles. By the time you reach image parity between Panther and Strix Halo the former might be even faster thanks to better up-scaling.",hardware,2026-01-14 03:31:31,2
AMD,nzfv8ou,It isnâ€™t too bad on the memory bandwidth side - it has a quad channel setup - but it certainly wonâ€™t beat GDDR with a wide bus. It smokes PTL in that respect though,hardware,2026-01-13 23:04:11,6
AMD,nzfzo15,> while PTL has the IO on a separate platform tile  And some (most notably memory controllers/PHYs) on the SoC/CPU die. Would need a die shot to attempt a proper apples to apples comparison.,hardware,2026-01-13 23:27:42,5
AMD,nzfwdbv,yes and yes.,hardware,2026-01-13 23:10:10,5
AMD,nzhao8o,Doesn't have tk be RDNA4 if they would've just release FSR4 on RDNA3,hardware,2026-01-14 03:52:33,1
AMD,nzfe9jz,"> For what specifically?   They haven't said battery life is better than LNL, and obviously there's some aspects of CPU perf vs ARL H/HX. Overall, a very solid product. Just not a clean sweep.Â    > but we have NVL-H in the future   Of course, N2 NVL should solve pretty much any high perf concerns. Just focusing on PTL.",hardware,2026-01-13 21:41:39,-1
AMD,nzgo3vp,"First, the cost of FSR4 INT8 is not a simple percentage, it depends.  Seconds, Z2 Extreme has 16 CUs, and Strix Halo Pro has 40 CUs, the cost is not the same on both.  Third, without any details of the game, frametime and fps, your comment does not add anything at all.",hardware,2026-01-14 01:42:27,0
AMD,nzfi5k8,"That's fair, CES coverage and their slides got me thinking there was really no improvements but rather was similar  >Of course, N2 NVL should solve pretty much any high perf concerns. Just focusing on PTL.Â   Honestly, thinking about nodes, I find somewhat amusing a lot of focus was on graphics, Xe3, on a TSMC node",hardware,2026-01-13 21:59:28,-1
AMD,nzfxqz8,"> Honestly, thinking about nodes, I find somewhat amusing a lot of focus was on graphics, Xe3, on a TSMC node  Hah, not *entirely* a coincidence, but even from an IP perspective, Xe3 is a huge jump while both CGC and DKT and minor revisions. I assume the NVL release will be a lot more CPU-centric by comparison.",hardware,2026-01-13 23:17:27,-3
AMD,nzgcpfe,"My laptop has a 3050 4GB  And now an APU is stronger than that, holy shit",hardware,2026-01-14 00:38:01,10
AMD,nzes2kb,Rough comparison:    Strix Point - 50%      Panther Lake - 100%      Strix Halo - 130%,hardware,2026-01-13 19:58:04,20
AMD,nzgyupq,What's the power difference at the wall?,hardware,2026-01-14 02:43:02,1
AMD,nzfcjrb,"All the benchmarks are using RT ?  I understand this is kind of representative of future titles, but I think the results are going to be way different in raster... (RX 6600 is way above RTX 3050 in raster, and AMD iGPU won't be so far behind Panther Lake)",hardware,2026-01-13 21:33:50,2
AMD,nzghcrm,Yes and also those laptops with these APUs cost twice than your laptop at release. I had my 3050 laptop bought in 2023 brand new for 600$.,hardware,2026-01-14 01:03:44,3
AMD,nzgt029,That APU is a ton of silicon so donâ€™t feel bad. Do think theyâ€™re promising tech tho.,hardware,2026-01-14 02:10:05,1
AMD,nzf579c,Really? Gotta watch the video now that is seriously impressive,hardware,2026-01-13 20:59:40,5
AMD,nzgzvjl,"They set all the chips to 60W, so I would say less than 100W from the wall for total system power.",hardware,2026-01-14 02:48:44,2
AMD,nzgomi0,"In addition to being more forward-looking, they also stated they didn't want to do the same tests everyone else would be doing (i.e. not using RT).",hardware,2026-01-14 01:45:22,6
AMD,nzfg2h9,"I think it would still be pretty far behind even without RT, since Intel did mix in a healthy bit of raster into their benchmarking for that graph at CES, but weâ€™ll have to wait for final reviews to see.",hardware,2026-01-13 21:49:55,4
AMD,nzgqi1n,"Yes I can imagine.  But mine also have a battery that lasts at max 2h, so now im seriously considering investing in one of these when it launches",hardware,2026-01-14 01:55:59,6
AMD,nzh5jwx,"Technically they shouldn't be, dGPU laptops have way more components and require better PSU, but the economics of laptops are very quirky. Thin and Light laptops with better CPU than i3 are targeted at the premium market while low end gaming laptops have crappy components and thin profit margins.",hardware,2026-01-14 03:21:29,3
AMD,nzf6o68,"Note they are all run at 65W, halo would commonly be run higher and point a bit lower.",hardware,2026-01-13 21:06:38,16
AMD,nzhep7n,Are you sure about that? I didn't catch that in the video.,hardware,2026-01-14 04:18:40,1
AMD,nzgswty,That's still a pretty good showing for PTL considering the cost premium for Strix Halo,hardware,2026-01-14 02:09:34,4
AMD,nzgvpw1,No question this is in my opinion a game changer. An absolute ton of games can and will now be run on PTL X chips. Itâ€™s a big deal.,hardware,2026-01-14 02:25:20,3
AMD,nyvrtkt,"Great combo, if you go on Amazon you can get the upgraded version Thermalright Phantom Spirit for around $35, or for a tad more the Phantom Spirit Evo! My wife has the regular one and I have the Evo version, both rock.   As far as the 5800xt goes, great choice! I've beaten myself up countless times for not upgrading my wife's 5700g when the 5800xt was selling for $125 last year. Regardless upgrade and sell your old 3600 with the wraith prism that's included with the 5800xt. Easy $75 to put back in your pocket.   You have a good plan ðŸ‘  Also my wife's case has the 160mm clearance and it works perfect. Her ram sticks were a tad tall so I the first fan up a tad and doesn't affect anything! Or you could move front fan to the back and only lose 1-2 degrees of efficiency ðŸ¤Ÿ",buildapc,2026-01-11 00:19:26,11
AMD,nyvtjkv,"""My CPU is running very hot and some games that I have played for years are starting to freeze my computer and I have to forcefully turn it off and back on.""   This usually means a CPU repaste rather than a replacement...try that before you drop a Benjamin plus.",buildapc,2026-01-11 00:28:34,12
AMD,nyvxdvx,>My CPU is running very hot and some games that I have played for years are starting to freeze my computer and I have to forcefully turn it off and back on.  Have you tried reapplying the thermal paste on the CPU cooler? Thatâ€™s the most likely issue.,buildapc,2026-01-11 00:47:54,3
AMD,nyvyv16,Thatâ€™s about as good as you can do since the x3d CPUâ€™s are so expensive now. Can you run your ram at a higher speed? Youâ€™ll see a good improvement if you can,buildapc,2026-01-11 00:55:17,3
AMD,nyw0g80,You donâ€™t need an upgrade. You need to remaster it and maybe get a better cooler. Iâ€™ve had a 3600 for 5 years and it champs.,buildapc,2026-01-11 01:03:44,3
AMD,nyvrvm2,Very good plan! That's gonna be your best bet for sure,buildapc,2026-01-11 00:19:43,2
AMD,nyw280s,Do consider getting a 3200 Mhz RAM kit if possible later down the road. It makes a noticeable difference.,buildapc,2026-01-11 01:13:28,1
AMD,nyxevq4,kinda sucks that I saw a 5800xt going for like 150 a few months ago but I figured it would be even cheaper in like 6 months or a year when I was planning on upgrading.,buildapc,2026-01-11 06:00:33,1
AMD,nyvroua,"Both great choices. Unless you trip over an msrp 5800x3d, that's the best you can get without going ddr5",buildapc,2026-01-11 00:18:45,1
AMD,nywsatr,i like the aesthetic of the one on amazon with the digital display and its only like 10 bucks more than the non digital display one.,buildapc,2026-01-11 03:36:42,1
AMD,nyzvuoq,Only downside is that it will block my second ram slot. I will have to remove my third ram stick and downgrade back to 16gb ram.,buildapc,2026-01-11 16:42:09,1
AMD,nyvy4nv,"I figured this might also be the case, but I do not mind upgrading as I can afford the new components.",buildapc,2026-01-11 00:51:32,5
AMD,nywnp1e,I would check the mounting of your CPU cooler.  Uneven pressure can cause overheating as well.,buildapc,2026-01-11 03:10:44,1
AMD,nyvy5up,"I figured this might also be the case, but I do not mind upgrading as I can afford the new components.",buildapc,2026-01-11 00:51:42,1
AMD,nyvz0nh,"I am not sure, I have never tried increasing ram speed.",buildapc,2026-01-11 00:56:06,1
AMD,nyw0sjk,"It does work good! Like I said, it has almost been 6 years and it is still going strong.",buildapc,2026-01-11 01:05:36,1
AMD,nywtneg,"I almost bought that one, but reviews were inconsistent about the reliability of the display so I played it safe and got the PS Evo off eBay for $40 total, been satisfied so far!",buildapc,2026-01-11 03:44:31,1
AMD,nz05atv,I guess I'm a little confused. The fan on the cooler will block the ram stick? If that's the case you can move the fan to other side of the cooler if that doesn't bother your aesthetic,buildapc,2026-01-11 17:26:57,2
AMD,nywinir,"I would say 5800xt. It is cheaper than 5800x for some reason and a Lil faster. Buy a beefy air cooler tho, nx600 or pa120se",buildapc,2026-01-11 02:42:30,2
AMD,nyvznkp,"You may want to try anyway, in case you have other faulty components that are causing the freezing/crashing (eg could be bad mobo, ram, PSU)  If one of your other components is bad, that may change your purchasing decision. _Eg if itâ€™s a bad mobo you may want to switch to Intel and get a faster 14th gen CPU for the same price._  Also, right now you canâ€™t really sell the CPU in good faith as â€œworking conditionâ€ because it could well be faulty. That changes how much money you get back from selling it.",buildapc,2026-01-11 00:59:28,2
AMD,nywx9ru,"the display model is just the ps evo with a magnetic attachment that has a single connector for the mobo. you could probably buy the magnetic attachment/ display thing separate if you wanted. i just look it because i get curious about temps. i dont have to like run a separate program to see where im peaking at.   it is a software start at windows (there is a linux program as well) but the software is barebones, hence the ability for fans to make a linux program.",buildapc,2026-01-11 04:05:28,1
AMD,nz05k46,I am just looking at pictures of videos of it. I will have a tech expert install it for me and they can decide the best way to orient it.,buildapc,2026-01-11 17:28:09,2
AMD,nyx9opl,Is the wraith prism that it comes with not sufficient enough?,buildapc,2026-01-11 05:22:26,1
AMD,nz0ko9g,"I'm sure they'll get it set up for you! Enjoy whatever upgrades you go with, I think you have a good plan",buildapc,2026-01-11 18:36:59,2
AMD,nyyfozn,"No, they run quite hot, and a twin tower, dual fan is likely what op will need, possibly an aio if planning on an oc",buildapc,2026-01-11 11:30:10,1
AMD,nz0vmi1,"I've been running a 5600x paired with a 6750xt for 1440p gaming since July 2023. Performance has been quiet good.  The only game that came close to making out my cpu was Bordelands 4, and that recently had a performance optimization patch that helped a lot.",buildapc,2026-01-11 19:25:04,3
AMD,nz0vj57,"The r5-5600 would be within an average of about 8% slower than the r7-5800XT in terms of gaming headroom.  They are essentially the same CPU with the r5 series just having two cores disabled.  Because of that the performance difference for most games is down to the boost clock.  When the r5-5600T was $100 USD it really was a bargain.  With your GPU playing on a 1440p monitor you would likely never know the difference.  Keep in mind the r5-5600, 5600T, 5600X, and 5600XT are all essentially the same thing.  Likewise the r7-5700X, 5800X, and 5800XT are all the same CPU at the core level.",buildapc,2026-01-11 19:24:39,2
AMD,nz12yyr,Those tests in this video are with a 4090 as a GPU.       And the difference is none existing.      You only have a low-mid tier GPU. Which makes the difference even smaller        https://youtu.be/7L9rPNSuPCA?si=IJH-kWoa_f94T5AR.          Pick the cheaper CPU.             Makes no sense to pay more for 1 fps,buildapc,2026-01-11 19:58:04,2
AMD,nz0x9n0,"23% price difference is not horrible, but it's a minor benefit for gaming, you'll be fine either way.",buildapc,2026-01-11 19:32:14,1
AMD,nz15l3z,"You are, in actuality paying the 23% more for two extra cores. The 5600 is a six core, twelve thread cpu, the 5700 is eight core sixteen thread cpu. Many games donâ€™t really use above six cores, yet. If you can afford it I would really recommend going with the 5700. If you are missing performance, then you canâ€™t really overclock the cpu to have two extra cores? The 5700 will win in every multi core scenario compared to the 5600.",buildapc,2026-01-11 20:09:58,1
AMD,nza2nnn,"THANKS TO ALL, finally found a 5700X TRAY at good price, so bought it with a cooler",buildapc,2026-01-13 02:19:12,1
AMD,nz0v7nx,"Yaaas, 5700x 9060xt gang!",buildapc,2026-01-11 19:23:13,1
AMD,nz0vzkh,"Id just go with the 5600xt. The 5600x vs 5700x is a ~10fps difference, and with the 5600xt the gap will be even smaller. Plus the included cooler is nice",buildapc,2026-01-11 19:26:45,1
AMD,nz199y7,Ryzen 5 5600 is enouph for the 9060xt,buildapc,2026-01-11 20:27:13,1
AMD,nz0uoep,If youâ€™re playing 1440p and more intensive games go with the 5700. If 1080p or mainly low intensity games 5600,buildapc,2026-01-11 19:20:47,0
AMD,nz0udrp,"LMGTFY links / telling people to ""google it"" is not allowed in this subreddit, as they are not useful answers. Please resubmit your comment with an actual link to the answer.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2026-01-11 19:19:28,1
AMD,nz15zy3,Absolutely goated price to performance build,buildapc,2026-01-11 20:11:54,1
AMD,nz56yqc,thinking of upgrading to 9060xt from 2060. What is your opinion on the card? I play on 1080p online FPS and 2K for single players,buildapc,2026-01-12 11:15:55,1
AMD,nz12nl6,This makes no sense,buildapc,2026-01-11 19:56:38,-1
AMD,nz13tus,Are you going to elaborate on why or just be contrarian? I have a 5600x and 9060xt at 1440p and some games Iâ€™m definitely bottlenecked by my cpu. So I suggest to this person they should potentially get a better cpu than the one I run if theyâ€™re playing games that are more intensive.  OP didnâ€™t give much info in their post on what they play/are wanting out of their system so I gave a very broad response. Either say something of substance or do everyone a favor and bow out,buildapc,2026-01-11 20:01:55,0
AMD,nz1hlg8,"the difference between both those CPUs is \~1-5fps on average (with a 4090)   [https://youtu.be/7L9rPNSuPCA?si=L2PQZBG4hUkUk2kf&t=409](https://youtu.be/7L9rPNSuPCA?si=L2PQZBG4hUkUk2kf&t=409)  a 1.440p resolution would be more taxing on the GPU.   makes no sense to get the 5700 because of the higher resolution.   but overall, the 5600 is just as good",buildapc,2026-01-11 21:05:37,1
AMD,nz587sk,"Looks good, glad you got it sorted out. Yeah, i also had to do the step bios updates on my old X370 Pro4 for Zen 2/3 support. In the end i left in the 1600X anyway as it seems to run W11 24H2 just fine (even though its an unsupported cpu). The rig is just a general use living room computer.",buildapc,2026-01-12 11:26:30,3
AMD,nz59bcw,Good upgrade and your current mobo VRMs should be sufficient to handle the 5600 if you don't overclock,buildapc,2026-01-12 11:35:32,2
AMD,nz5yvmp,"Not unheard of with b350/b450 support for Vermeer, glad it went well and you managed to sort it out. Had to go trough something similar, although in my case I didnt have any issues with the peripherials after flashing the required bios. In all fairness though props to AMD for their chipset support...in comparison Intel at the time were changing sockets and chipsets every chance they got, the fucking cunts",buildapc,2026-01-12 14:20:02,2
AMD,nz65bqu,"Have you actually ever updated the bios before? When I started out with my 1600 and x370 motherboard, it took close to a year before there was finally a bios that ran my memory at 3200mhz with that cpu. Was real fun being on zen for the first time.",buildapc,2026-01-12 14:53:55,2
AMD,nz5j35d,"Cheers! It's stressful when it doesn't boot, but it's fun to figure out all the steps and get them done correctly",buildapc,2026-01-12 12:47:48,2
AMD,nz5j94r,"Thanks!   Can I ask how you know that? I don't know the first thing about VRM, but I gave up on trying to figure that out when I couldn't even find it on the spec sheet. I'm impressed at you living up to your username haha",buildapc,2026-01-12 12:48:56,1
AMD,nzbw09s,Yeah I actually updated it about a week prior. I ended up trying to manually set memory timings with little to no knowledge and the mobo factory reset itself haha,buildapc,2026-01-13 10:26:59,1
AMD,nz5p1no,If it is compatible with Zen 3 Ryzen 5000 CPUs then it should atleast support a 6 core CPU! You can check by benchmarking your CPU and if it doesn't hit or stay at its rated clock frequencies without thermal throttling then you can blame the VRMs maybe. Check VRM temps in HWinfo,buildapc,2026-01-12 13:25:16,2
AMD,nz9emnh,"A 5800xt and 9060xt 16gb should be right around that price and be good upgrades. If you have a decent used market by you, try to pick up a 3080(ti) - I see those for around $325 by me. Used cpus are fine too, just avoid anything with a G suffix, the 5500 and 5700. A used 5700x or 5800x would be perfect.  Of course if you find a $200 5800x3d lol....",buildapc,2026-01-13 00:09:28,14
AMD,nz9f418,9060xt should leave you some room for a decent cpu upgrade. perhaps a 5700x,buildapc,2026-01-13 00:12:04,3
AMD,nz9g4wc,"Cpu: 5600/5600x/5700x/5800x/5800xt (check local pricing) Gpu: 9060xt 16gb/5060ti 16gb  If you're going used, same cpu's but maybe a 5700x3d/5800x3d if you can find them for around 200$ or lower. Good used GPU's to look for are the 3080/3080ti, 4070(super), 4070ti(super), 6800xt, 7800xt/7900 gre/7900xt (make sure they're going for a good price).",buildapc,2026-01-13 00:17:30,3
AMD,nz9fs1z,I've got a 3080 10gb I'd sell if you're interested,buildapc,2026-01-13 00:15:37,1
AMD,nz9h6sw,https://pcpartpicker.com/list/dHnfcx,buildapc,2026-01-13 00:23:08,1
AMD,nz9j7oo,I bought a 7800XT and it took me WELL into 1440 gaming and is quite comfortable with 4K @60hz.,buildapc,2026-01-13 00:33:52,1
AMD,nz9m7vk,"I have the r5 3600 currently with a stronger gpu. Its def the gpu thats make your cpu feel slow. I would just put in a 5060ti 16gb for $399 and call it a day. Also I current run 1440p with my set up so you can get a 1440p 180hz display for like $200 and you are golden. As for power supply, there are some good 650-750 watts out there for 70 bucks.",buildapc,2026-01-13 00:49:59,1
AMD,nz9mw74,"ryzen 7 5700, its around $190 and a 9060xt 16gb for like $370.  You can also go for a ryzen 5 5600x for like $130",buildapc,2026-01-13 00:53:40,1
AMD,nz9t0ki,Second hand RX 7600 and 5800XT cpu. Even a RX 6600 works very well for 1080p games.,buildapc,2026-01-13 01:27:23,1
AMD,nzaqyc7,"used 5700x for 150 or less. That's an achievable price on ebay so works well as a high. Facebook Market, ebay, Offerup and r/hardwareswap  are all worth checking.  You can find people selling used 9060xt 16gb cards on ebay for 330ish and it's hard to argue with them for that kind of price.",buildapc,2026-01-13 04:34:39,1
AMD,nzatffh,Try for this.  Choose Your Parts - PCPartPicker https://share.google/vzSufZwCtZfFKdKw6,buildapc,2026-01-13 04:50:58,1
AMD,nz9emoq,Buy a 9070 and drop it in,buildapc,2026-01-13 00:09:28,0
AMD,nz9f76d,"Option 1, 5700x3d for $300 and the remaining on a gpu....will leave you room to upgrade to a better GPU in the future .....CPU will be less of an issue as you increase resolution. Option 2, Ryzen 5600 and whatever you want to spend on a gpu....I just upgraded my 5600 because it was bottlenecking a 4070ti super at 3440x1440. If your going with a 5600, you probably won't get full utilization out of a higher end card should you upgrade again at some point",buildapc,2026-01-13 00:12:32,0
AMD,nz9f7d9,Agreed. This would be my path also.,buildapc,2026-01-13 00:12:33,2
AMD,nz9vq5v,Yeah I juat came frim i5 8400 and 1060 6 GB to base 5600 and 5060 ti 16 gb.  Wowzers.  Won't upgrade again until 2031 or so.,buildapc,2026-01-13 01:42:22,1
AMD,nzap5f0,"Second your advice, although X3D chips are $350 plus. Unfortunately not a reasonable option on am4",buildapc,2026-01-13 04:23:10,1
AMD,nz9j9dq,Iâ€™m running the same CPU as you.,buildapc,2026-01-13 00:34:08,1
AMD,nzad87y,"Skip the 5700. It's not a downclocked 5700x, it's a 5700G with the iGPU disabled, so it's got less cache as well as being slower. 5500 is like that too, and should be avoided unless you're OK with a iGPU-less G-series.",buildapc,2026-01-13 03:15:52,1
AMD,nz9k8m2,"I third this, came to post it if it hadn't been already (about the 5800XT + 9060 XT 16GB being where I'd put my funds)",buildapc,2026-01-13 00:39:19,2
AMD,nzawpjk,"As someone who threw a 4070 Ti Super at a 3600 and used it for a year, I'd throw in a beefy GPU like a 9070XT, 9060XT 16GB, or 5060 Ti 16GB at it *while accepting there are some newer releases out there and future releases that it will struggle with.* It may work for some people, it may not work for others. A minimum, I think it's something that everyone who is budget restricted should give a shot; especially if you're more of a patient gamer. For me, a 5700XT to a 4070 Ti Super felt like a brand new computer that didn't really need anything else.   In my case, I never actually naturally encountered a title that my 3600 couldn't run that I really wanted to play. Sure I saw some in benchmarks showing that some titles experienced sub-70FPS, but seeing it is different than actually encountering it in my natural course of gaming. Plus.. it's pretty much the same as seeing a title on PS5 or Switch that interests me, but I'll probably never play because I don't have a PS5 or Switch.  Sure some titles would also perform better than my 3600 could when paired with a 4070 Ti Super, but for the most part 90FPS or higher feels fine to me and I only have a 144hz 1440p monitor that I can't really tell the difference at between 110FPS and 144FPS. DLSS feels perfect fine and really carries the 3600 pretty far.   In all honesty, Hell Let Loose was the only title that I ever really naturally encountered that made the 3600 feel a bit tired, but by all means it was playable.   I did eventually move on to first a 5600 and then a 7600x, but that was more of a no-brainer financial decision. I found super good prices on both when prices were good ($110 on the CPU/mobo/RAM side of things after selling my old mobo/RAM/CPU). In all honesty, I couldn't personally say I've naturally encountered any game-changing improvements with either the 5600 or 7600x. Again, there definitely are some out there. Especially BF6. But you can stretch a 3600 extremely far and get great results (until you run into something that you flat out can't that's under 60FPS).   I've since played a couple games that my 3600 probably would have struggled with, but in all honesty they were titles I thought were overrated and could have lived without and that was my experience playing them. Had I upgraded from a 3600 to 7600x with the current prices and knew better, I'd probably have pretty bad buyers remorse.",buildapc,2026-01-13 05:13:33,1
AMD,nytscim,There will be bottleneck but it varies depending on what games you run. Hardware canucks did a video on this so you can get a good idea of how much the bottleneck is  https://youtu.be/TXKyQYiLro8,buildapc,2026-01-10 18:22:02,2
AMD,nytsgxs,"It is certainly pretty balanced for 1440p and I must say you hit the bullseye with that combo. As with PCIe. x16 you are using all cpu power with that configuration.  The only couple changes I would do is a 5700/800 or non existing X3D and a 9070 non xt, But at that resolution you are good until AM6.  I will only upgrade if I do content creation or video editing.",buildapc,2026-01-10 18:22:36,2
AMD,nytvnwz,I have a Ryzen 5700x with 9070 XT and it runs great at 1440p.,buildapc,2026-01-10 18:37:22,2
AMD,nyuqhz2,"How much is the 5600? Lately, it's been going over $150, at which point just get the 5800XT and be done. Used, for a good price, it'll be fine, though.",buildapc,2026-01-10 21:08:29,2
AMD,nz1cr1f,In CPU games it will Bottleneck it pretty hard.,buildapc,2026-01-11 20:43:29,1
AMD,nytus7r,Thanks!,buildapc,2026-01-10 18:33:21,1
AMD,nytubm5,"Thanks! Good to hear itâ€™s balanced for 1440p. Iâ€™m mainly doing AAA single-player, so this reassures me ðŸ‘",buildapc,2026-01-10 18:31:13,1
AMD,nyu1svn,"Yes, that is a combo added with the ram and ssd he has in mind seems just right.",buildapc,2026-01-10 19:05:58,2
AMD,nyut4ov,"Around $175, the used market for Ryzen CPUs is kinda non existent where I live, but thanks for the advice! Did find a Ryzen 7 5700x for like $160 so I think I'll go with that and if I'm not mistaken it's pretty similar in terms performance(at least in gaming) to the 5800XT.",buildapc,2026-01-10 21:21:38,1
AMD,ny89nom,"The 9800x3d doesn't have any competition, and miles ahead from the 14600k  If you can afford a new platform (mobo/cpu) go ahead",buildapc,2026-01-07 16:59:42,14
AMD,ny88wy2,Get a more reasonable Z790 board and just use your already existing 14600K.,buildapc,2026-01-07 16:56:21,4
AMD,ny899hg,"Why is that board so expensive?  Also, do you already have the ddr5 ram?  If you're buying ram, just get a ddr4 lga1700 board and buy ddr4 ram.  If you're buying ddr5 ram anyways, I'd probably go all out with the 9800x3d.",buildapc,2026-01-07 16:57:55,2
AMD,ny8a0is,I already have 32gb ddr5 5200 ghz ram,buildapc,2026-01-07 17:01:19,2
AMD,ny8azwh,"I know i5 14600k will work with rtx 5070 16 gb or if I get 5080, my issue is it worth it stick with lga 1700 motherboard where I don't have lots of options to upgrade the cpu or just go with amd motherboard",buildapc,2026-01-07 17:05:51,1
AMD,ny8dlpv,If you already have an old pc with the intel cpu why are you buying another motherboard? Just put the gpu in that build and youâ€™re good to go.   14600k is a pretty good cpu for gaming you probably wonâ€™t have any issues with the 5070. Only if you play crazily cpu intensive games like Tarkov will you see a significant difference with an AMD x3d chip in most cases. This is a pretty good video to see for cpu scaling: [Hardware Canucks](https://youtu.be/TXKyQYiLro8?si=ZOKjBBvTqtMOiuny),buildapc,2026-01-07 17:17:41,1
AMD,ny8g1nf,"This is such a weird post tbh.Â   You already have a working pc with a working LGA 1700 mobo just because you got a gen 5 NVME doesnâ€™t mean you need an entire new platform. The NVME will work on a gen 4 slot, sure youâ€™re not getting full speed but honestly for gaming itâ€™s not gonna make that much of a difference.  Save your money and use the parts you have seems useless to upgrade.",buildapc,2026-01-07 17:28:41,1
AMD,nya3j3i,just use the 14600k and get a better gpu isntead,buildapc,2026-01-07 21:46:57,1
AMD,nyarush,"Donâ€™t bother upgrading CPU, 14600k is plenty powerful - instead of spending extra on 9800x3d get 5070ti/9070xt - it will net you far more performance than cpu upgrade",buildapc,2026-01-07 23:41:39,1
AMD,nybm9dl,the 5070 has 12 gb of vram did you mean the 5070 ti? if you have the extra money i highly recommend the 9800x3d i recently switched from an i9 12900k to the 9800x3d,buildapc,2026-01-08 02:18:19,1
AMD,nyhp6i8,"Thanks everyone for there advice, I end up with using the ddr5 I have with i5 14600k and I purchased  Gigabyte NVIDIA GeForce RTX 5070 Ti Gaming Overclocked Triple Fan 16GB. I think this setup will be enough for now and maybe by the end of this year or next year I will switch to the newest amd 9950x3d when they released it. I think my i5 14600k setup will be enough for now for 2k gaming and video editing. Thanks everyone for your help.",buildapc,2026-01-08 22:58:13,1
AMD,ny8h9yj,I see this bundle for $680 AMD Ryzen 7 9800X3D + TUF Gaming X870-PLUS WiFi,buildapc,2026-01-07 17:34:14,2
AMD,ny8a8a7,I already have 32 gb ddr5,buildapc,2026-01-07 17:02:20,3
AMD,ny8e09d,The old pc motherboard is standard one and not for gaming or support gen5 nvme,buildapc,2026-01-07 17:19:30,1
AMD,ny8qi63,"Make sure you can afford the ram too, 2x16 ddr5 6000 is gonna be another 500",buildapc,2026-01-07 18:14:37,4
AMD,nyas5eq,"Put those money into higher tier GPU instead - more performance to gain, 14600k + 5070 ti will be faster than 9800x3d with 5070.",buildapc,2026-01-07 23:43:10,1
AMD,ny8axfb,"Then it's up to you, but get a cheaper board regardless lol.  I think if I were you, I'd buy a used 7800x3d for $300(I see one right now on ebay), get a $150 am5 board with pcie 5, then sell the 14600k on ebay for $240.  The 7800x3d is not only a really fast CPU, but it is also less affected by your slow 5200 ram due to the large vcache. You could spend the extra $100 for a used 9800x3d if you want.",buildapc,2026-01-07 17:05:32,2
AMD,ny8fm72,"Unless you have money to burn, it is perfectly fine for gaming. Do you know the model?  Gen 5 m.2 is basically useless for gaming and probably wonâ€™t matter for a while. Unless you really need gen 5 for a specific use case but gen4 and even gen3 ssds are still very fast.  Just make sure your power supply can handle the 5070 (>650w is ideal) and it has the proper connections for it(12vhpwr 12 pin) and I see no issues with it",buildapc,2026-01-07 17:26:43,1
AMD,nyaj9my,Micro Center has $680 bundles that include 6000 RAM,buildapc,2026-01-07 22:58:31,2
AMD,ny8c12y,"Reason for that board is expensive because it supports NVMe Gen5, I have Crucial T700 1TB Gen5 I am planning to use",buildapc,2026-01-07 17:10:32,1
AMD,ny8g56y,Power supply is CORSAIR RM1000e ATX 3.1 PCIe 5.1 Ready Fully Modular 1000W Power Supply,buildapc,2026-01-07 17:29:08,1
AMD,ny8ct69,"In that case, go with AM5... B650E boards are like $140-$150, comes with pcie 5 nvme, and has PCIE gen 5 primary slot.  Some b850 boards are also like this, but they are only guaranteed to have nvme pcie gen 5, not the primary slot. So you'd need to do a quick google on the b850 boards to check.  I'm sure there are cheaper LGA1700 boards with pcie gen 5, but I don't know how they're labeled.",buildapc,2026-01-07 17:14:05,2
AMD,ny9lt6u,"Don't need to get an expensive AM5 board to get pcie gen 5. Lots of great options like B850's that support gen5. I used to always think the top end chipset was just inherently better (thanks Intel marketing) but you'll get just as good of an experience getting a cheaper board, that includes all the features you need.",buildapc,2026-01-07 20:31:04,1
AMD,ny8gxgp,"Money not that issue, kind my worry now is if I stick with lga 1700 not that much options for cpu update in the future. With amd it's the newest one and kinda no need to buy new cpu or motherboard for a while. I5 14600k is good but not better than 9800x3d. I see bundle of AMD Ryzen 7 9800X3D + TUF Gaming X870-PLUS WiFi for $680",buildapc,2026-01-07 17:32:40,1
AMD,ny8h6cy,"In that case Iâ€™d just replace the gpu. Save the money or consider moving it into a better gpu like the 9070xt for an even better gaming experience (more vram, higher frames etc.)  Thatâ€™s a pretty high wattage psu in that pc, does it have a gpu in it already?",buildapc,2026-01-07 17:33:47,1
AMD,ny8ljrt,"Yeah but itâ€™s not like you are investing more money into the lga1700 system, you already have it.  You also donâ€™t gain anything by moving to am5 right now, if you wanted to switch later and get a newer x3d chip you would just shift the buying of the motherboard to then. You already have the ddr5 which is the hard part.  Iâ€™m not seeing the logic unless you really want the x3d right now. I donâ€™t really see the value in replacing such a recent and high performing cpu.  At the very least you could try out the pc with the gpu and decide if you really need to upgrade the cpu. Does that computer have a gpu already? Iâ€™d be wary of spending lots of money for minor improvements.",buildapc,2026-01-07 17:53:09,1
AMD,ny8o2t6,"Not yet, planning to get rtx 5070 16gb this month.",buildapc,2026-01-07 18:04:07,1
AMD,ny8qj5f,"Ok then yeah, Iâ€™d say go ahead and get a 5070 for that pc. The 5070 is 12gb so if you really want 16gb your best options are the 9070xt or 5070ti",buildapc,2026-01-07 18:14:44,1
AMD,nyb5s0e,"Put the 680$ for the amd platform upgrade towards a higher tier gpu and keep the 14600k, donâ€™t buy a 12gb card in 2026. Find a 5080 for msrp before itâ€™s too late!",buildapc,2026-01-08 00:51:46,1
AMD,ny8qv5m,"Do you think it worth it to go to 5080, or wait for a new GPU release",buildapc,2026-01-07 18:16:12,1
AMD,ny8s25g,5080 gives not much performance increase at a pretty big premium. Wouldnâ€™t really recommend itâ€™s not a very sensible choice imo.  Probably not a good idea to wait on the gpu.. if you want one get it now. Prices are going up and may not come down for a while. The 9070xt can be found for around 650 and the 5070ti for around 750.,buildapc,2026-01-07 18:21:25,1
AMD,ny8tnfl,Where you find 5070ti for $750?,buildapc,2026-01-07 18:28:18,1
AMD,nzdzjyt,Most likely power supply issue (not enough headroom/faulty) or cpu cooler isnt sufficient.,buildapc,2026-01-13 17:50:31,84
AMD,nze16kp,This is a symptom of a PSU failing.,buildapc,2026-01-13 17:57:43,44
AMD,nze1ns0,+1 for potential PSU failure.,buildapc,2026-01-13 17:59:50,22
AMD,nzdzag3,You'll have to post complete specs including the power supply. Could be a power supply that doesn't have enough wattage for all the components. Could also be thermal throttling meaning either the CPU or the GPU is getting too hot.  Provide specs for the following:  CPU  GPU  Motherboard  RAM  PSU  SSD or hard drive  PC case  Cooling. How many case fans? Intake or exhaust? Air cooler or AIO liquid cooler?,buildapc,2026-01-13 17:49:18,5
AMD,nzdzi3d,"Heya! I had a similar issue recently, found out that my PSU started slowly dying and couldnt give enough juice to my rig during demanding moments. I did see you wrote that they changed everything except gpu and ram, id say reseat the ram as it could be static charge or just a simple reseating issue! Otherwise im almost sure it must be the power supply. Good luck on the repairs!",buildapc,2026-01-13 17:50:16,5
AMD,nze3oyx,"Could be dying PSU, but a dying GPU can also cause system shutdowns. That was the case for me anyway. I kept trying things until I finally swapped GPU and never had another crash.",buildapc,2026-01-13 18:08:57,4
AMD,nzeaggg,No one's really mentioned it enough but it could be the socket of the outlet it's plugged into I would absolutely have a electrician or YouTube how to make sure it's not a shitty connection or failing. Let alone a inside wall fire happening,buildapc,2026-01-13 18:38:34,4
AMD,nzdz00d,Does he have a way of monitoring CPU temperature? It sounds like overheat protection kicking in.,buildapc,2026-01-13 17:47:59,9
AMD,nze1f8o,"It may be shutting down due to power issues. Check to make sure the internal PCI-E connectors are properly connected both to the graphics card and to the power supply itself (I would dis- and reconnect these connectors just to be sure).   Also, if this power supply is older and/or was already close to capacity, you may start to see issues under load as power supplies degrade over time.  A cheap power supply tester runs about $20, and is a worthwhile tool to have on hand for issues like these.",buildapc,2026-01-13 17:58:47,3
AMD,nze28em,Psu,buildapc,2026-01-13 18:02:25,2
AMD,nzeisix,"look out for gpu overheating, may need to reapply thermal pads/paste",buildapc,2026-01-13 19:15:49,2
AMD,nzf41fx,"> Over the last 2 weeks his pc has completely shut down anytime he plays games. Games like expedition 33 and it takes 2.  Only when playing games?   > He's done everything from new windows install to replacing everything but his GPU and his ram. We really don't know what to do. Any tips?  Well if you even replaced the power supply...  Does the motherboard have an iGPU? You can unplug your video card and try to do a light game with just the iGPU  > PC case Factual design meshify 3 black solid  Check how sensitive/depressed his power button is.  I experienced something very similar with my Fractal Design Torrent where it shut down randomly from me doing anything and after a ton headache and replacing parts, I found the issue was my case's power button.",buildapc,2026-01-13 20:54:14,2
AMD,nzgbgg4,Had this exact same problem a few months ago with the Corsair RM850x. They sent me a replacement and I haven't had the issue ever since.,buildapc,2026-01-14 00:31:14,2
AMD,nze2g8a,"Change your psu. Not enough power. Buy good brand. Good is Seasonic, Super Flower, Corsair RMx or hxi series, NZXT",buildapc,2026-01-13 18:03:24,1
AMD,nze02j9,"I had the same issue, and it was one of the modular cables running to the GPU. I replaced it with a spare from the kit and no issues since.",buildapc,2026-01-13 17:52:49,1
AMD,nze3v2j,CPU AMD Ryzen 7 9800X3D 8-core  GPU AMD Radeon RX 9070 XT  Motherboard Asus x870 plus wifi  RAM Trident z5 (2 of them)  PSU Corsair RX1000  SSD or hard drive Samsung SSD 990 Pro 2TB  PC case Factual design meshify 3 black solid  Cooling. How many case fans? Intake or exhaust? Air cooler or AIO liquid cooler?  RMX Series RM1000X with 3 fans,buildapc,2026-01-13 18:09:43,1
AMD,nze41ii,Its cool youre posting for your guy but the nerds here need specs,buildapc,2026-01-13 18:10:30,1
AMD,nze5kmb,Have him check and monitor his CPU and GPU temps.  There is a thermal limit in place which will shut down the PC to protect the components if they get too hot.  (Often this limit is 95C),buildapc,2026-01-13 18:17:14,1
AMD,nze912r,"If I had to guess it's caused by game mode in bios which reduces throttling and can cause shut downs if cooling isn't sufficient, else I'd try disabling xmp in bios, reducing dram frequency, if that doesn't work either look if voltage settings are too high / low, last resort low effort fix if nothing worked is to get the battery out of the motherboard and put it back in after 5 mins.",buildapc,2026-01-13 18:32:14,1
AMD,nzeaewd,PSU failure/ issue,buildapc,2026-01-13 18:38:22,1
AMD,nzeaxmy,"another +1 for failing PSU or at least bad PSU connections (not fully seated or ""clicked"" in. (Which leads to not enough power transfer if not making a great connection.)",buildapc,2026-01-13 18:40:40,1
AMD,nzec8fd,"How long has he had the PC? Did he build it or prebuilt?  Given it's not a bottom of the line psu I'm guessing he built it.  Is he using two pci-e cables to the GPU...not a daisy chain 2 connectors on one? Are all connections secure? and to an appropriate port on the PSU, some are labeled for sata or lower power connections vs. PCIE for CPU or GPU use.  Defintitely check ram QVL list for the mobo, and then timings/expo profile.  Generally EXPO II profile is more stable.",buildapc,2026-01-13 18:46:24,1
AMD,nzecta0,"Had similar case, my GPU was overheating. Applying thermal paste fixed it. Can check my thread: https://www.reddit.com/r/pcmasterrace/comments/1pl3ach/black_screenmax_fan_crash_rtx_3090/",buildapc,2026-01-13 18:48:58,1
AMD,nzed482,"Maybe overheating CPU or faulty power supply, assuming the wattage is enough for what you have.",buildapc,2026-01-13 18:50:18,1
AMD,nzeen5g,"Did he just upgrade his GPU by any chance?  I had this exact issue a few months ago when I bought new 9070XT  The PC would shut down completely (as if I pulled the plug), only when I played certain games (It takes two being one of them which I see that you mentioned).  I tried:   1- Reinstall Drivers and BIOS  2- To replace my PSU (1000W).   3- Re-seat everything and unplug, replug everything from the PSU  4- Reinstall Windows.   Nothing worked.  I put in my old 2070 again, and the problem stopped.  I RMAed the GPU, and once I got my new one, the problem has been resolved.  They could not replicate the problem when I sent it for the RMA, they were going to send me back the GPU that I sent but I insisted that my PC is now working fine with the 2070, and it wasn't with that GPU, and they eventually sent me a working GPU.",buildapc,2026-01-13 18:57:07,1
AMD,nzef3pp,"Run a stresstest on the RAM, with AIDA64 for example. RAM running at higher than base speeds (so running with an XMP/EXPO) profile is still a bit finicky. Sometimes it will boot, but crash when put under load. Running a RAM only stresstest will let you confirm/deny this as the source of your problem.",buildapc,2026-01-13 18:59:10,1
AMD,nzejrqq,what kind of psu? Everyone says failure but u could have a good unit that doesnt have enough power on the rails for specific part. Buddy of mine i play with would have pc crashes on intense games or if we turned his settings maxed. I asked him his power supply and looked it up and it was more than enough wattage but digging into the rails they didnt supply enough power. His gpu wanted over 300 watts and the rail only supplied a max of 280 or something so once the card was being pushed itd just shut down. May be a problem with u especially if its a prebuilt like his was,buildapc,2026-01-13 19:20:15,1
AMD,nzek7zi,I had this problem a few years ago and it turned out to be the GPU instead of the PSU. Computer was fine until I played a game. It would run for about 15 minutes and then shutdown.,buildapc,2026-01-13 19:22:18,1
AMD,nzekobf,"Whats his RAM timings? those sticks are single rank (8gb, 16gb or 24gb per stick) or double rank (32gb or more per stick)? if they are double rank probably the CPU can't handle it, try lowering the speed to 5600 and see if it keep happening.  [9800X3D just supports 5600 on single rank and double rank](https://www.amd.com/en/products/processors/desktops/ryzen/9000-series/amd-ryzen-7-9800x3d.html), BUT... most people can run 6000 (sweet spot) stable using single rank sticks... double rank is just silicon lottery.  He should try to run a stability test of CPU+RAM like AIDA64 or OCCT to see if he is getting any errors.",buildapc,2026-01-13 19:24:21,1
AMD,nzeoegn,Anecdotal but I had a RMX1000 too and it failed after 8 months. Was playing on my TV before my computer just blacked out. I got the warranty replacement for it but it took a long time so I also splurged out on a 1600W PSU instead. I run 5090 + 9800x3d and do a lot of gen AI work.,buildapc,2026-01-13 19:41:21,1
AMD,nzer1bq,either PSU or bad cpu chip whea errors or messed up ram timings,buildapc,2026-01-13 19:53:19,1
AMD,nzer25l,"You could check Event Viewer logs for any power/voltage related shutdowns, and can use HWInfo for example to monitor power outputs from the PSU. Not seeing any anomolies doesnt guarantee a good PSU, but this does not cost a lot of money or time to investigate.",buildapc,2026-01-13 19:53:26,1
AMD,nzes7ed,Clean it from dust.,buildapc,2026-01-13 19:58:41,1
AMD,nzesfqy,too hot maybe....  repaste cpu and gpu + undervolt/underclock your cpu/gpu/ram from bios + use power saver mode,buildapc,2026-01-13 19:59:45,1
AMD,nzewuqn,"My machine did this for a while and I had no idea why. I would start a high demanding game and the screen would freeze and then the pc would restart. I ended up cleaning the old thermal paste off the cpu, reseating the CPU fan with new thermal paste and did the same thing with my GPU. It restart or freeze ever again. Do that before you assume a part is broken.",buildapc,2026-01-13 20:20:29,1
AMD,nzf0lt4,If heâ€™s replaced everything except GPU and RAM then one of those are likely the culprit. Can help rule out RAM by running Windows Memory Diagnostics tool. Try turning off XMP in bios.  Consider drivers as the issue. After fresh windows install only download essential software to test (like Steam).,buildapc,2026-01-13 20:38:13,1
AMD,nzf1kw9,Check wether you have 2 PCIe cables going on from your PSU to the GPU or only 1 with a double ending.,buildapc,2026-01-13 20:42:46,1
AMD,nzf2jg6,"Out of curiosity, are you sure it's just the PC shutting off? Do you have anything else plugged into other nearby outlets that also turn off?  I say this because I used to have an issue where my window unit AC and PC were plugged into the same power circuit in my apartment. During the summer, when the AC was on and I was trying to play somewhat demanding games on my PC, they would try to pull too much power and trip my breaker.  You'd probably notice if you were tripping your breaker, but it's also possible there's problems with your electrical wiring. Perhaps try plugging the PC in to an outlet connected to a different circuit. Just throwing this out there as a (weird) possibility.",buildapc,2026-01-13 20:47:15,1
AMD,nzf2wtf,When this happened to me it was one of my RAM sticks having an error but I also got a blue screen of death with each crash so that might not be your issue.,buildapc,2026-01-13 20:48:58,1
AMD,nzf454e,I had this problem and was stumped for weeks.  This video was exceptionally helpful and finally helped me identify the somewhat simple fix.  https://youtu.be/1EYLzD2GytA?si=PZXWA8aLthlVy828  It ended up being C-states,buildapc,2026-01-13 20:54:42,1
AMD,nzf7sx5,Try a new power cable for the GPU before replacing the PSU. Especially if thereâ€™s a sharp bend in the cable where it comes out of the GPU.,buildapc,2026-01-13 21:12:02,1
AMD,nzfa116,look at your CPU temperature. Maybe you left the plastic protective film on the cooler.,buildapc,2026-01-13 21:22:22,1
AMD,nzfco8h,It can also be the RAM if the RAM is unstable. Try running without XMP/AMP turned on at stock ddr5 ram settings. Then try each RAM stick individually.,buildapc,2026-01-13 21:34:24,1
AMD,nzffxlk,"If all else fails, try disabling core integrity in windows security. This helped me in a similar situation after I went as far as replacing the PSU.",buildapc,2026-01-13 21:49:18,1
AMD,nzfqsov,"When you say it shuts down, do you mean it just cuts out and loses power as if the pug was pulled out the wall socket? If so I had that years ago. Tell him to take the GPU out and run with the integrated graphics on the CPU for a while. Won't be able to run a great deal but just play stuff it can handle. See if it keeps happening. When I had this I replaced the PSU first but it still happened. Took the GPU out for a few weeks and had no issues. Put a new GPU in and never had another problem. Cooling was not a problem, no overclocks either. Something on the GPU was causing power loss though.",buildapc,2026-01-13 22:41:23,1
AMD,nzfthry,"I just had this issue a few weeks ago, also with Expedition 33. Check the power cable going into the GPU. If, at the connector, it's bent too much the pins might not fully connect and you'll have power distributed unevenly - causing some pins to overheat. This will then cause the PSU to shut down if it believes your PC is overheating.   It took me awhile to figure it out but after taking off the side panel and readjusting the cable it has never done it again.   Of course, this is just a possibility but worth trying. Good luck!",buildapc,2026-01-13 22:55:02,1
AMD,nzg3rxy,"When I had this issue, it was bad ram causing problems. You may want to look into testing it (each stick individually) to rule that out",buildapc,2026-01-13 23:49:48,1
AMD,nzgam6r,"check for loose/missing connections on the mobo/gpu, try running with only one stick of ram, take a sniff from the PSU vent to see if something has been cookin' inside, google around how to disable all the power saving BS in windows, do away with all power saving shit in BIOS as well, start rolling back GPU drivers, check for known issues with your motherboard / BIOS but dont fuck around reflashing it unless you know what you're doing and you are aware of the risks, pay attention to hotspot temps while running a benchmark like heaven, dont really think event viewer will log an overheating core",buildapc,2026-01-14 00:26:40,1
AMD,nzgd5dl,"Others are correct in pointing out a power supply issue.   One thing to also check is if your PC is hooked up to a UPS/APC. If it is, the fault may lie there.",buildapc,2026-01-14 00:40:24,1
AMD,nzgdot0,"Also check if your gpu fans are running. Had it not long ago they weren't, and learned that way a gpu can shut down your system as well if it is overheating.  For some stupid reason amd decided to set the fan curve to all 0's, and your description fits my experience",buildapc,2026-01-14 00:43:21,1
AMD,nzh5eza,"I have the RM1000i not x, this exact problem happened to me when ever my psu switch from single 12v rail to multiple 12v rails, I solve it by switching back to single rail.   So from my experience it's 100% psu failing, rm1000 come with 10y, I recommend that you check your warranty and claim it.   *if you can get ""i"" it's my choice from corsair, and thier hx1000i shift is my next psu ðŸ˜…",buildapc,2026-01-14 03:20:42,1
AMD,nzh7oip,either psu or overheating problem,buildapc,2026-01-14 03:34:15,1
AMD,nzh7wnp,"PSUâ€™s cooked probably. Had the same thing just happen to me, most of the same symptoms as you do",buildapc,2026-01-14 03:35:36,1
AMD,nzh8mc9,Is it only when playing games? Or will it shut down when idling in windows as well? I had a similar situation which drove me bonkers and I troubleshot for days with similar to what you are experiencing. Although mine restarted rather than just shut down.  It ended up being a faulty wire from my mobo to my pc case power button. It was tripping my system.   I ended up disconnecting and turned my pc on via the mobo power button (was in early stages of building a new pc to bother swapping out the wire),buildapc,2026-01-14 03:39:57,1
AMD,nzh9nfz,I had a similar issue here recently and it ended up being an alert set within iCUE. It was hard to find out what in event viewer other than it saying it triggered a shut down. But iCUE had an alert set to shutdown PC if it got to 50Â°. Turning that off fixed the issue.,buildapc,2026-01-14 03:46:15,1
AMD,nzejj8v,I had an issue where the AMD Adrenalin Software somehow became incompatible with the driver that was installed. Froze my computer and lost connection with my second monitor. Uninstalled and re-installed Adrenalin and everything was back to normal.,buildapc,2026-01-13 19:19:10,1
AMD,nzdzwwm,Sounds like overheating.  Remove and re-mount the CPU with new thermal paste?  Are all the case fans/CPU fan/GPU fan working correctly?,buildapc,2026-01-13 17:52:08,0
AMD,nze11cq,Check temps. Get nzxt cam or something like that. It monitors you and cpu temps. If it's getting to hot it thermal throttles then shits down to prevent damage,buildapc,2026-01-13 17:57:05,0
AMD,nzebfqn,"Defective power supply is the most likely answer.   Defective motherboard is the second most likely.   CPU overheating and shutting down IS possible, but pretty unlikely (unless they're not using a cooler at all! Which, as I look at it... has OP stated which CPU cooler is actually being used?)",buildapc,2026-01-13 18:42:53,36
AMD,nzec5tf,This is also what happened to a friends PC when he needed to redo the thermal paste on his GPU. Everything was fine until there was demand on it.,buildapc,2026-01-13 18:46:05,16
AMD,nzf0unn,"I am having the exact same issue as OP and I noticed it only happened while I had some stream going on my second monitor while gaming which leads me to believe it's a PSU issue. I replaced my motherboard first because it was having other issues and I thought perhaps it was related but at very worst I get a motherboard with working USB headers. It went away (or so it seemed) but then it happened 3 times in one night this last weekend while gaming and watching NFL on my second monitor.    My plan is to replace my PSU next week and just not have videos playing on my second monitor in the meantime. Everything on my PC has been replaced since this started happening except the PSU (through random upgrades like RAM, CPU, GPU etc).",buildapc,2026-01-13 20:39:22,1
AMD,nzfv15c,Or a cooling issue. My PC shut down because my case fans weren't ramping up properly (incorrect settings in BIOS and it was like an oven in there. Must've been motherboard sensors triggering it because the CPU & GPU temps were in normal range.,buildapc,2026-01-13 23:03:04,1
AMD,nzfwdie,"I had this exact same issue and replacing the psu resolved it. Old psu had plenty of power for the rig, but was just failing.",buildapc,2026-01-13 23:10:11,1
AMD,nze4zdt,"Yeah, we used mine gpu which is a Geforce RTX 3070 Ti and it took an hr to crash instead of the usually 5 minutes.",buildapc,2026-01-13 18:14:41,2
AMD,nzeatom,"We already checked the outlet, and checked the power usage. But its still a concern.",buildapc,2026-01-13 18:40:11,1
AMD,nzdzdbl,"Yep, it never goes above 50 cÂ° sometimes in high demanding games it goes to 70Â°c but when we go to the event viewer it doesn't say anything about overheating",buildapc,2026-01-13 17:49:40,3
AMD,nze9707,what voltage would you recommend thenn?,buildapc,2026-01-13 18:32:57,1
AMD,nzegens,"Huge disclaimer to make sure you get the cable that came with that PSU. But yes, a broken cable could do what OP's bf is seeing.",buildapc,2026-01-13 19:05:05,1
AMD,nzea0bi,"I saw in another comment that you mentioned having no crash log in event viewer right?  I was having the same trouble, could you try running some low demanding games? (Something like stardew or terraria could work). Personally for me, i was able to play some games like payday 2 and vermintide 2 for hours but everytime i launched fortnite or arc raider it would hard crash my pc after barely 5-10min idle.  I also saw your problem seems to stem from the GPU you said, faulty motherboard maybe? Even new components can be faulty sometimes. Especially if you checked with another GPU and the only difference was the time it took to crash. If you have another pc, check both gpu in that other computer to verify!",buildapc,2026-01-13 18:36:34,1
AMD,nze986e,"850W PSU is the minimum for that CPU/GPU combo. 1000W is recommended so you should be fine wattage wise. It looks like a gold model. Keep in mind the platinum and titanium versions are always better.  Looks like the Fractal case has 3 fans in the front? Any others? Rear exhaust, top exhaust, etc.  Got any more specs on the RAM? how many GB and what is the rating? 3200Mhz, 6000Mhz, etc. Were the 2 sticks a matched pair? Meaning were they sold as a kit. You can run into a lot of problems adding more RAM even if it is the exact same model if it isn't bought together as a matched kit.  I'm not sure what you are using to monitor temps. I would use Afterburner to monitor CPU and GPU temps. You can also set an on screen display to monitor temps in game with the included RTSS software. It will also allow you to set a custom fan curve to keep the GPU nice and cool.",buildapc,2026-01-13 18:33:05,0
AMD,nze4jnt,Fixed that!!! I was waiting on 2 things that I didnt know that I had to bug him about,buildapc,2026-01-13 18:12:45,1
AMD,nze63c0,He normally has an app open to see what the temps are never gets above 70 C,buildapc,2026-01-13 18:19:29,2
AMD,nzef5c9,"Built, he upgrades the parts every year or so. Ill ask about the rest of this",buildapc,2026-01-13 18:59:22,1
AMD,nzef9ee,More than enough,buildapc,2026-01-13 18:59:52,1
AMD,nzefewq,"He got his gpu over a year ago, he only started having this problem over 2 weeks ago",buildapc,2026-01-13 19:00:33,1
AMD,nzefkee,We did do a normal stress test but not one directly for ram,buildapc,2026-01-13 19:01:15,1
AMD,nze39pk,"Also check if the plastic is off the block, preventing the paste from actually working",buildapc,2026-01-13 18:07:04,3
AMD,nze45k3,"Yep, its a new cooler, all fans are working overtime in some games. Its a brand new PSU aswell",buildapc,2026-01-13 18:11:00,1
AMD,nzem5ao,"I had same issue, checked everything. My issue was wrong apply of thermal paste so once it got used up and cpu was bare bones without paste it started shutting down. First i changed psu but problem persisted. And once i removed aio cooling i saw how the paste got smushed all over the place but not much left on cpuâ€¦ once i changed the shutting down stopped",buildapc,2026-01-13 19:30:59,1
AMD,nzeeqvd,Thermaright FC140 CPU Air cooler,buildapc,2026-01-13 18:57:34,6
AMD,nzem31i,Yeah I had this issue myself when the fan on my GPU started to fail and I didn't notice.  This was back in the days of single-fan GPUs though so YMMV.,buildapc,2026-01-13 19:30:41,3
AMD,nzf2vni,"Yep. Was thermal paste for me, but my system was older. I redid the thermal paste on the GPU and CPU and haven't had an issue since.",buildapc,2026-01-13 20:48:49,1
AMD,nzehnup,The fact that it still happens with your card makes it seem like the PSU. The reason it happens faster with his card is because it uses more power.,buildapc,2026-01-13 19:10:45,8
AMD,nze783g,Yeah if the crashing one is 5+ years old especially it seems likely that's the issue. Interesting that your GPU also crashed though... Maybe multiple failure points.,buildapc,2026-01-13 18:24:21,1
AMD,nze4zfl,"That actually sounds very normal.   Sorry was autocorrected without realizing it.  Has he gone into his bios to check ram timings for his specific ram? My sisters pc was having this issue for months.... But we live 7 hours apart and she doesn't have enough confidence to check ram timings.   When I was down for Christmas, I found the right setting for her ram on her and cpu, gigabyte mono and RAM. It's been rock solid since.",buildapc,2026-01-13 18:14:41,4
AMD,nzeurqv,Wattage. Recommended 850W but that's just the minimum.,buildapc,2026-01-13 20:10:43,3
AMD,nzf379v,1000W is fine,buildapc,2026-01-13 20:50:20,1
AMD,nzftxhl,"Personally, I use PC Part Picker for everything. Let's you know exactly what you need for your rig - like voltage.",buildapc,2026-01-13 22:57:18,1
AMD,nzeal9k,"Also, check their BIOS dates! Ive heard there was issue with older bios for that motherboard (quick search rn gave me some videos)",buildapc,2026-01-13 18:39:09,1
AMD,nzebla3,Hes been able to play Fallout new vegas and Octopath traveler 2 with little to no issues,buildapc,2026-01-13 18:43:34,1
AMD,nzebo6x,He actually has 4 ram sticks but only using a matching set. I dont know exactly what the ram sticks are.,buildapc,2026-01-13 18:43:55,1
AMD,nzem3lc,"I'm running a very similar setup, same cou/GPU with an Asus board.  I had ram issues, a small voltage increase and docp profile II solved my issues.  I wasn't crashing every game though, only occasionally, but did crash running certain synthetic benchmarks every time.  This was a crash/freeze though for me, not a power down/shut off.",buildapc,2026-01-13 19:30:46,1
AMD,nzeg0zj,Have you repasted the CPU thermal paste?,buildapc,2026-01-13 19:03:21,1
AMD,nzgzqg8,"Like others have said, the most likely issue is the PSU (It was the first thing that I replaced figuring that it was the issue).  If he changes that and it still crashes, you might want to check the GPU, although if you don't have your old one or access to another one, that might be difficult.",buildapc,2026-01-14 02:47:56,1
AMD,nzeixzq,"A normal stresstest will usually put a load on everything, so you can't confirm/eliminate the cause. A stresstest per component, like the one from AIDA64, will let you do that.",buildapc,2026-01-13 19:16:30,1
AMD,nzek44m,Sounds like a defective PSU then?,buildapc,2026-01-13 19:21:49,1
AMD,nzeg3mz,"Totally enough for a 9800X3D (assuming it's running properly). I'm going back to defective power supply being the most likely cause, followed by defective motherboard.   (In truth, either is possible, and I'd call it a coin toss as to which is the most likely scenario. But since the Power supply is generally easier to replace than a motherboard, I'd start with the power supply)",buildapc,2026-01-13 19:03:42,10
AMD,nze8yph,He just got his gpu about a year ago when his old gpu completely failed,buildapc,2026-01-13 18:31:56,2
AMD,nze5sab,"Im not sure if he's done that yet, but I will definitely ask him",buildapc,2026-01-13 18:18:11,2
AMD,nzecsw8,> Has he gone into his cups  Can you clarify what you mean by this? Autocorrect BIOS or am I IOoTL?,buildapc,2026-01-13 18:48:55,2
AMD,nzegekt,Yep,buildapc,2026-01-13 19:05:05,1
AMD,nzegk1r,We just replaced the motherboard like 2 days ago,buildapc,2026-01-13 19:05:45,2
AMD,nzegmzy,We are thinking its the ram or the gpu at this point,buildapc,2026-01-13 19:06:07,1
AMD,nze9f6c,"Hmm that's not really old enough to wear out unless he's mining crypto or something. If the crashing card also has a bigger power draw, it could point back to the PSU again.",buildapc,2026-01-13 18:33:57,1
AMD,nzeqsur,"I searched google and came up with the following.  To enable   [EXPO](https://www.google.com/search?client=firefox-b-e&channel=entpr&q=EXPO&ved=2ahUKEwiy5uCNpYmSAxXXHDQIHQN4NHsQgK4QegQIARAB) on your Asus X870-Plus WiFi motherboard, enter the BIOS (Delete key), press F7 for Advanced Mode, go to the **Ai Tweaker** tab, set **Ai Overclock Tuner** to **EXPO I**, then save and exit (F10) to apply the optimized RAM settings for higher performance. T",buildapc,2026-01-13 19:52:16,3
AMD,nzeffg9,"likely an automistake, since 'in his cups' is a (mildly archaic) metaphor for being drunk",buildapc,2026-01-13 19:00:38,3
AMD,nzeftch,I was autocorrected. I meant BIOS. She has an AMD cpu on a Gigabyte. Had to switch to profile 1 EXPO settings.,buildapc,2026-01-13 19:02:23,2
AMD,nzelezw,"RAM or GPU is certainly *possible*, but RAM being defective doesn't generally cause *shutdowns* as much as it causes crashes and reboots.   Defective graphics cards, similarly, don't usually cause shutdowns. If they're defective, you'll usually see crashes, reboots, or the display simply winks out while the rest of the machine stays on.   If you've replaced the motherboard already, it's VERY likely to be a defective power supply.",buildapc,2026-01-13 19:27:41,14
AMD,nzeuojv,"You can check the RAM by trying one stick at a time, and then the other.",buildapc,2026-01-13 20:10:18,3
AMD,nzfioy1,It has an integrated gpu so you can just take the graphics card off and test it. If it is a power supply issue it would probably work if you're not using the big gpu since it would be low power. If it still crashes then it might be a different issue.  You can also load up afterburner and turn the power budget down and watch the monitor. Try a benchmark software that runs the gpu hot and see if it crashes when it hits a certain watt pull. If it crashes immediately then it's not the gpu getting hot for sure.  You can also pull one stick and just use one ram stick at a time to see if it is that.  You can also do a portable build of linux and run stuff on there and see if it still crashes.,buildapc,2026-01-13 22:01:58,2
AMD,nzeilgr,"Haha yeah I thought it was a pretty funny/fitting autocorrect. If I had to go into BIOS to check my RAM timings I'd want to be in my cups, as well. It usually helps.",buildapc,2026-01-13 19:14:55,3
AMD,nzgkun3,This. A dead stick of ram usually causes shut downs and BSOD that are inconsistent with no discernible pattern.,buildapc,2026-01-14 01:23:39,3
AMD,nzg5494,"I used to have this problem with an AMD card. I swapped to a nvidia and I've been fine ever since.  Full shutdowns randomly while gaming. Ever since covid, QC has gone to the gutter with more products.",buildapc,2026-01-13 23:56:59,1
AMD,nzfr43h,"For gaming I would have preferred the 9700x, but at the same time it's still a good price for what you received.",buildapc,2026-01-13 22:42:58,6
AMD,nzgbc3b,"I game on an R9 7900x and a 6950xt at 1440p and I don't typically findy CPU noticeably affecting anything in a negative way. I also don't do super super high refresh gaming, most of my games are running between 80-150 fps with a mix of medium-ultra settings depending on the title and how old it is.",buildapc,2026-01-14 00:30:35,1
AMD,nzaq2fv,"I agree with the other commenter.   Typically I prefer the 9070XT in slightly cheaper $1,600-$1,800 gaming builds.   By $2,000 I'd prefer a 5070 Ti even if it used a 7800X3D.",buildapc,2026-01-13 04:28:54,4
AMD,nzapduh,"If you save a few bucks and get the 7800x3d combo you're extremely unlikely to ever actually care.  You could get a montech century II power supply for like 40 bucks off. Microcenter usually lets you throw in a part for building your own PC if you're having them do it, but if you're doing it I'd do it for sure. [https://www.amazon.com/MONTECH-Century-II-High-End-Cybenetics/dp/B0F3XW1J16?t](https://www.amazon.com/MONTECH-Century-II-High-End-Cybenetics/dp/B0F3XW1J16?t)  It's a solid build all around tho",buildapc,2026-01-13 04:24:39,1
AMD,nzat2u9,The ssd is great. Overall great prices,buildapc,2026-01-13 04:48:39,1
AMD,nzauktd,Looks good,buildapc,2026-01-13 04:58:44,1
AMD,nzemnkg,I would get the G758. Better price per part under 2k than buying the parts,buildapc,2026-01-13 19:33:20,1
AMD,nzb5vac,Any brand recommendations for the 5070ti?,buildapc,2026-01-13 06:24:01,1
AMD,nzb5tbj,Isnâ€™t the 7800 less powerful and runs hotter as well?,buildapc,2026-01-13 06:23:34,1
AMD,nzbeaa1,"It's not much of a difference, 10-20% depending on the game if you're raw CPU speed bound which is pretty rare, especially if you're playing at 1440p or 4k. Personally I'd rather have the 100 bucks and that's even with being a strategy/sim player who'd probably benefit the most, if you play AAA it's almost laughable.",buildapc,2026-01-13 07:37:40,1
AMD,nz27xjt,"Buy the G.Skill, and underclock it. Set it to the EXPO profile, but then change the speed to 6000MT/s (3000MHz), before applying settings. It'll be fine.  It's probably just been hanging around longer, as most people are buying JEDEC stuff, or looking for 5600-6000 XMP/EXPO.",buildapc,2026-01-11 23:11:27,21
AMD,nz27g90,worst case you run it at slower speeds,buildapc,2026-01-11 23:09:00,8
AMD,nz27ibu,"It's fine, but you might need to underclock it to 6000mhz. There's plenty of guides on how to do that.  Faster is better, but less stable.",buildapc,2026-01-11 23:09:17,4
AMD,nz5d84d,"With the 2:1 dividor uclcl to mem you get after 6200/6400 depending on your luck in the silicone lottery , you need to have 8400 rams to compensate for this and be again as performant as with a 6000 kit",buildapc,2026-01-12 12:05:54,2
AMD,nz27nqy,It'll work fine but xmp might be unstable.,buildapc,2026-01-11 23:10:04,1
AMD,nz290fr,"If they have expo profiles you can just set your motherboard to accept them. They usually have a full speed expo, then they have a lowest speed set for expo profile 2. That tends to be closer to 6000 cl 30",buildapc,2026-01-11 23:17:05,1
AMD,nz2ied3,"you can set ram to any speed you want, ram doesn't have clock so its not locked to any frequency. Your memory controller sets the speeds.",buildapc,2026-01-12 00:05:46,1
AMD,nz33qhx,"it'll run at the highest speed the cpu/mobo can support, so yeah cheaper is better in this case",buildapc,2026-01-12 01:55:12,1
AMD,nz467vq,"I would go with the G.Skill Trident RAM, underclock it to 6000 MHz, and tighten timings since that RAM has SK Hynix A-Die and benefits from it more than a 5600 or 6000 MHz standard kit possibly could. You should be able to set CAS latency (CL) down to 30, so in this case input 30-38-38-38-96 at RAM 1.4V, SoC 1.2V, and FCLK 2000 MHz in BIOS.  Then test for stability with TestMem5 using Anta777 Absolut preset, up to 6 cycles across 3 hours; y-cruncher with both VT3 and VST benchmarks being run to test infinity fabric, there are shortcuts to do it in 1 hour; then finally OCCT with AVX-512 (or SSE if not available) to heat soak your memory for an hour.  If you pass these, then you got some RAM worth its weight in gold. Just make sure you got some fans pointed at them!",buildapc,2026-01-12 05:43:31,1
AMD,nz4938o,X3d chips do not benefit a lot from expensive fast ram. Do not overspend unnecessarily,buildapc,2026-01-12 06:06:13,1
AMD,nz4ddzr,I hope you got a good cooler.. btw MSI official website still has there ridiculous sale going on .. they have the 850w and 1000w PSU and some motherboards on sale for 50% off . I canâ€™t see the sale being up much longer itâ€™s alrdy been up since Black Friday,buildapc,2026-01-12 06:41:56,1
AMD,nzcx7a6,6000MHz is ideal for Ryzen. Anything higher is diminishing returns and potential instability.,buildapc,2026-01-13 14:38:26,1
AMD,nz7v8fv,AMD 100%. Iâ€™ve used zbrush and ue5 and both work great.,buildapc,2026-01-12 19:39:22,5
AMD,nz7bocp,"Idk about ZBrush, but for the rest, AMD wins easy, there should be some Benchmarks for that App.",buildapc,2026-01-12 18:10:48,3
AMD,nzevo2v,https://www.pugetsystems.com/labs/articles/intel-core-ultra-200s-content-creation-review/?srsltid=AfmBOoqwnN9xO491CDG7m9GtABSgJyUkTKLoToeH4s1VtPeZbfq3yrKu#Game_Dev_Virtual_Production_Unreal_Engine  Intel takes the cake for Unreal Engine.  AMD takes the cake for gaming value.  Zbrush is super light and can run on anything modern.  So pick based on your priority - gaming or game dev.,buildapc,2026-01-13 20:14:54,1
AMD,nzayfn3,Any changes should I make in the cart ?,buildapc,2026-01-13 05:25:54,1
AMD,nzcn3kj,"The only think that I could say to you is that the PSU is very mid, check this: [https://docs.google.com/spreadsheets/d/1akCHL7Vhzk\_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/htmlview?usp=sharing&pru=AAABm9ukk8E\*ZtFdQNsr-FiDumyjGFrw4A#gid=1973454078](https://docs.google.com/spreadsheets/d/1akCHL7Vhzk_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/htmlview?usp=sharing&pru=AAABm9ukk8E*ZtFdQNsr-FiDumyjGFrw4A#gid=1973454078)  And with the Monitors something similar, check Techspot and RTings, because most Monitors look good in paper but then they are shit, I'm not sure if the ones you selected are good or not.  With the rest, as I said, I don't know what you need and don't need for ZBrush, so I can't really help too much, but AMD is gonna release a new CPU 9950X3D, might be worth it to wait for it.",buildapc,2026-01-13 13:45:03,1
AMD,nys45av,"The 265k is a good option for premiere and after effects.  It outperforms the 9900x handily and is very close to AMD's best, the 9950x3d in both suites.  Cheaper and better performing is a great combo so I'd recommend that.  Intel screwed up badly with the 13th and 14th Gen and many people would tell you to buy a 7500f over a 285k for the same price so you've got to look at benchmarks to actually look at the cpus performance.  Speaking of the 285k it is significantly better at Premier, but that is a steep asking price and the 4 extra e cores don't help much for after effects.",buildapc,2026-01-10 13:11:46,15
AMD,nyscal4,"The 265K is an excellent productivity CPU and still good at gaming, I would probably go with the 265K in your case.",buildapc,2026-01-10 14:01:38,4
AMD,nys3g99,"[https://www.cpubenchmark.net/compare/6326vs6171vs5031/Intel-Ultra-7-265K-vs-AMD-Ryzen-9-9900X-vs-AMD-Ryzen-9-7950X](https://www.cpubenchmark.net/compare/6326vs6171vs5031/Intel-Ultra-7-265K-vs-AMD-Ryzen-9-9900X-vs-AMD-Ryzen-9-7950X)  9900x is a horrid buy for productivity unless you are deadset on upgrading within 2 gens or you already have am5. 265k is an absolute beast in productivity and value (cheaper than a fucking 9700x in a few reigons)     Even in gaming the 9900x is only 6fps ahead on average at 1080p, at higher res the diffrence closes even more  [https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html)     Oh and I know some uneducated swine that just knows how to say ""intel runs hot"", in multicore the 265k is 0.1 points behind the 9900x whilst being 2.5points ahead in singlecore and gets much better average frames per watt  [https://www.techpowerup.com/review/intel-core-ultra-7-265k/24.html](https://www.techpowerup.com/review/intel-core-ultra-7-265k/24.html)",buildapc,2026-01-10 13:07:06,8
AMD,nys6kd4,Theyâ€™re very close. Iâ€™d check the benchmarks for the software you use most often.,buildapc,2026-01-10 13:27:24,3
AMD,nyse7z6,"You get the same average gaming and productive results give or take certain games and applications with both of these, with the 265K being a clearly better for Premiere, much better value, and the motherboard PCIe connectivity is far better. Only reason to get the 9900x is to upgrade the CPU later while remaining on the same board, with the current AM5 boards being behind in PCIe lane management.  My z890 bord I can have a full PCIe 5x16, M.2 5x4, M.2 4x4 and 2x TB4 ports all to the CPU. Everything else on the board the chipset handles it all without throttling down anything. Look at any single chipset AMD board and you'll see conditions like the M.2 5x4 throttles to 5x2 mode if you use your USB4 ports, using you extra M.2 slots deactivates some PCIe slots, and a bunch of your physical x16 PCIe slots are electronically only in x2 or x1 mode instead off all x4. Some AMD board get around this with double chipsets which brings the board idle power up while the AMD CPU is already an idle power chugger. Then you got PCIe 4.0 x8 uplink from chipset to CPU in Intel z890 instead of x4 on AMD.",buildapc,2026-01-10 14:12:50,3
AMD,nyslqw0,"Have a read of https://www.pugetsystems.com/labs/articles/intel-core-ultra-200s-content-creation-review/   I've been using a 265K system for a wide range of gaming and non-gaming tasks for over a year and have been happy with it, and its stability.   I don't think the 285K is worth it for most users. Perhaps if you can justify how much more quickly it will perform income-earning tasks, and therefore pay for the incremental cost...",buildapc,2026-01-10 14:55:13,2
AMD,nysmex5,"If you donâ€™t care about upgradeability, and you want to use a bunch of NVMe SSDs that you already own, and you donâ€™t mind paying the outrageous price for DDR5 RAM, then yeah, Intel Arrow Lake (like the 265k) is a good choice here!   If you donâ€™t mind spending for a new cpu and motherboard in one year from now, even upgradeability wonâ€™t matter, youâ€™ll be able to go Intel Nova Lake, which is rumored to have a big jump in performance, especially for gaming with itâ€™s bLLC.  No, donâ€™t go 285k, not worth it, unless thereâ€™s a dealâ€¦ and there could be (or on the rest of the Arrow Lake parts) bc of how badly they sell, so check your retailer.",buildapc,2026-01-10 14:58:51,2
AMD,nyt8nek,Iâ€™m running the 265K and honestly itâ€™s a beast. Best value CPU for productivity and gaming.  Iâ€™m the type of person who will play a game while installing another one for shits and giggles and surprisingly this chip handles it amazingly.  Intel fucked up the 14th gen and as a result these new chips are super underrated. Underpriced too!  Just donâ€™t buy a Gigabyte board.,buildapc,2026-01-10 16:48:55,2
AMD,nys2puf,"Get the Intel.  Itâ€™s far more stable, no memory training like AM5 and it has quicksync hardware encoding used by Adobe and other video encoding.  Yes AMD gets 140fps instead of 119fps, but some of us really donâ€™t care.  Those people probably going to buy an 8K TV because 8 is bigger than 4.",buildapc,2026-01-10 13:02:05,-1
AMD,nysd0z1,Intel everyday.  Amd CPUs keep failingâ€¦,buildapc,2026-01-10 14:05:54,-2
AMD,nys9rjb,"I have a 64 core â€¦ i got it specifically for an app with Cpu only render. burst is better on the intel chips so as you render your working frame in real-time,  it will render fasterâ€”however AE started adding background thread timeline rendering, so the threadripper will get me a timeline preview complete faster while i work.",buildapc,2026-01-10 13:46:45,5
AMD,nytc2hs,"> Even in gaming the 9900x is only 6fps ahead on average at 1080p, at higher res the diffrence closes even more  You can pull at least 20% more fps out of a 265k if you overclock the cpu all around and your ram a bit. The cores are decently fast and limited by the latency added by the tile design, you can offset a lot of that with a ring + d2d overclock and low latency ram (tuning trefi/trfc)",buildapc,2026-01-10 17:05:08,2
AMD,nyvz6di,"Yeah this is a thing that annoyed me about x870 boards. Then when I looked into intel, they dont have these issues, plus they get full fat thunderbolt. Even some b860 boards got everything I need. Any reason to go with a z890 board over a b860 other than overclocking?",buildapc,2026-01-11 00:56:56,1
AMD,nyvza67,Why no Gigabyte board?,buildapc,2026-01-11 00:57:30,1
AMD,nyso0r6,"Raptor Lake was not that long ago... That said, I also have a 265K and itâ€™s been rock solid",buildapc,2026-01-10 15:07:26,3
AMD,nyw0o1k,"Intel boards haven't mandated TB4 on the entire lineup like AMD did for x870 with USB4 on same TB4 spec. Something to lookout for. I didn't even look at the B860 boards.  I benchmarked Lossless Scaling off of the iGPU which is a niche use case where the 265K can be ad advantage over a 9800x3d. Niche, but I made practical use of it when an Unreal Engine 5 game was stuttering on 1440p 60fps locked perfect frametimes, used the iGPU to frame gen 120fps and made it smooth. DLSS frame gen would stump down to 114 maximum and frames bouncing around making the phrase ""trade real frames for fake frames"" happen. [https://www.reddit.com/r/losslessscaling/comments/1m7ywk7/core\_ultra\_265k\_igpu\_results/](https://www.reddit.com/r/losslessscaling/comments/1m7ywk7/core_ultra_265k_igpu_results/)",buildapc,2026-01-11 01:04:55,1
AMD,nyw3q0z,"Iâ€™ve had 3 in the past 2 months. Theyâ€™re low quality (feel flimsy) terrible support(it cannot be that difficult to provide detailed diagrams, most other brands do) terrible software and drivers. My last straw was my B860i board crashing when xmp profile was enabled for my 48gb ram kit.  Switched to an MSI board and the difference is night and day in every aspect. I can finally run my expensive ram kit at its advertised speed.",buildapc,2026-01-11 01:21:59,1
AMD,nzgll89,In my opinion I think the sooner you can build the better youâ€™ll be. The future outlook doesnâ€™t look good for us. I would bite the bullet as soon as you can.,buildapc,2026-01-14 01:27:54,3
AMD,nzglmzw,"Tbh I'd get something like a 5060ti for now instead, and keep the rest of the PC bank.  If that doesn't work, upgrade your Intel processor. having 12th gen RN is good, as you can fit two additional generations of Intel in that socket, which is a real luxury.",buildapc,2026-01-14 01:28:11,2
AMD,nzgpu8s,"You have a board that would support a better 12th gen CPU, Iâ€™d pick up like a 12700(can be non k,K, KF , or S idc) and a better graphics card.  That will easily last you until hopefully all this current trend blows over enough to normalize.",buildapc,2026-01-14 01:52:15,1
AMD,nzgma0g,"I was thinking of that but Iâ€™m afraid Iâ€™m being too hasty right now, since my current still runs somewhat well right now, but I get what ya mean, I am really tempted too.",buildapc,2026-01-14 01:31:56,2
AMD,nzgmgyh,"Wow that I didnâ€™t know, but if I upgrade my CPU should I upgrade my GPU to prevent any potential bottlenecks perhaps?",buildapc,2026-01-14 01:33:04,1
AMD,nzgn5n6,"I totally understand. My current build is pretty solid still. 3060 ti, ryzen 5 5600x 16gb ram. I was planning on a new build spring of 2027. I just figured in my situation, I donâ€™t see next year being any better but a lot worse for me. So I was like OK let me just go ahead right now , especially since Iâ€™m having tons of fun in arc raiders. If I was to wait till 2028 , then I might have not built now. Goodluck to you in what you choose.",buildapc,2026-01-14 01:37:03,1
AMD,nzgorsx,"Did you mean the opposite?   ""Bottlenecking"" depends on so incredibly many variables, the biggest one being in what you play.   You don't want GPU wait CPU time (CPU bottleneck) as that causes stutters and hitches in FPS during gameplay.   If the GPU is the limiting factor, that is usually a good thing.   CS2 or other e-sports titles will strain the CPU more due to the high number of frames, than 4k Cyberpunk.",buildapc,2026-01-14 01:46:13,1
AMD,nzgnl41,"Thank you bruv for the insights, and hopefully you got a rig that didnâ€™t hurt the bank ðŸ˜­ðŸ˜­",buildapc,2026-01-14 01:39:30,2
AMD,nzgp79x,"Ohhh I see, okay yeah that brings a lot of clarity right now, still torn on what to get in that case, any recommendations?",buildapc,2026-01-14 01:48:40,1
AMD,nzgp7v9,"Ohhh I see, okay yeah that brings a lot of clarity right now, still torn on what to get in that case, any recommendations?",buildapc,2026-01-14 01:48:45,1
AMD,nzgpxmt,"First, get a 5060ti / 9060XT. Try that, with what you play.  If it doesn't work out and you get stutters during gameplay because CPU is constantly at 100%, upgrade that too.  There is just too much potential in your rig in this market. The 3050 was honestly not a very good card (terrible value when it came out compared to the 3060 that iirc had twice the VRAM) and the Intel gen you are on also leaves a lot of room for upgrades.",buildapc,2026-01-14 01:52:47,1
AMD,nzguxny,"I see yeah the 3050 is bad today, bought it cause I was a broke student and happy at the same time since it was me first PC :D It is now showing its underperformance though lol, I have heard much praise for the 9060XT for its price so, Iâ€™ll definitely give that a go!!!",buildapc,2026-01-14 02:20:56,1
AMD,nywo7jq,"The 5700X3D would be an upgrade, but it's hard to find that chip at a decent price.  The 5600X is already pretty good, why are you considering an upgrade?",buildapc,2026-01-11 03:13:39,10
AMD,nywoyt8,"If you can find it or the 5800X3D for a good price, yeah good upgrade.",buildapc,2026-01-11 03:17:50,4
AMD,nywoxqf,"Kinda depends on your GPU/resolution, otherwise for CPU based / extra multicore it's good.",buildapc,2026-01-11 03:17:41,1
AMD,nyws0nu,"The Zen 3 3D CPUs like the 5700X3D are decent upgrades over regular Zen 3 but the pricing is completely prohibitive  Unless you get lucky and find a legit one for cheap somehow, I wouldnâ€™t consider it",buildapc,2026-01-11 03:35:00,1
AMD,nywscxp,Hmm would an upgrade to an rzyen 7 or ryzen 9 still on am4 be good perforamnce wise ? Wihout x3d?,buildapc,2026-01-11 03:37:03,1
AMD,nywppjd,I play cyberpunk with many mods and over all the cpu dosnt keep up with my demands anymore i am almost always cpu bottlenecked  in r6 cod helldivers now i have a 3060 i can upscale if i rly need to but not if my cpu is the problem..,buildapc,2026-01-11 03:21:41,-1
AMD,nywq0t9,1080p  i have a 3060 but my cpu is almost alwys the bottleneck,buildapc,2026-01-11 03:23:30,1
AMD,nywwoa7,"Would make almost no difference in most games, more cores isnâ€™t helpful",buildapc,2026-01-11 04:01:49,1
AMD,nywwky7,"Very interesting, I wouldn't normally suspect a 3060 to bottleneck a 5600X.  In some games it could be a factor of the number of cores. I don't play cyberpunk, but apparently it does utilize more cores. You may therefore find more performance even by going to a 5700X or 5800X. In most games, performance should be the close to the same across all of the 5000 series.  There is nothing wrong with the 5700X3D and its a great chip, its just hard to justify at the price its currently available at. You could probably upgrade your GPU with the leftover cash from choosing the 5700X over the 5700X3D.  I would always recommend considering your own use case though, and looking at benchmarks to see if any potential upgrade is worth it to you.",buildapc,2026-01-11 04:01:17,2
AMD,nywqifd,I don't get how a 5600 can choke a 3060 eh.   You use dual channel ram?,buildapc,2026-01-11 03:26:18,1
AMD,nz4zujj,yeah gpu is low on priority bcs as i said my cpu is almost 90% the problem  unfortunately,buildapc,2026-01-12 10:11:35,1
AMD,nywqzcr,"I see your list of games, only for R6 a better CPU would matter at boosting higher refresh rates.",buildapc,2026-01-11 03:29:02,1
AMD,nywr9uy,Yes xmp bla bla  cod is cpu heavy .  Cyberpunk is cpu heavy  even more with mods  so in cyberpunk my gpu is nor aly at 90 or so never max i still get 30 fps at this poibt with all the mods thats bcs my cpu is dying  if my cpu wasnt dying i could upscale the gane to get more gpu performance and id be at 60 probably fps but i cant upscale bcs my cpu is crying already,buildapc,2026-01-11 03:30:45,-2
AMD,nz4zk6c,no bcs warzone cpu bottleneck my cpu is 80-100% usage that aside in cyberpunk i have 30 fps not bcs of grapgics bcs my cpu is not capable of handling the modded game yes my gpu is not great but i could use a upscaler for more fps if my cpu could handle it but bcs i have cpu bottleneck everywhere i cant as i stated above almost every game is cpu bottlenecked with my system,buildapc,2026-01-12 10:08:55,1
AMD,nxgkf8w,"Good for what? If you mean good for the price, 9060xt offers better fps per dollar if you have RT off.",buildapc,2026-01-03 16:19:38,2
AMD,nxgkiug,Yes that is good.,buildapc,2026-01-03 16:20:07,1
AMD,nxgmd6n,It will game.,buildapc,2026-01-03 16:28:55,1
AMD,nxgmr1r,Yes.,buildapc,2026-01-03 16:30:43,1
AMD,nxgnx69,The 7500F combos well with everything that isn't a 4090/5090,buildapc,2026-01-03 16:36:14,1
AMD,nxgo4o1,"Depending on what your use case is, you may be better off with a 16GB version card, especially if you're planning to be at 1440p/2160p for resolution. So, in this case it's still a good pairing but I would consider the AMD Radeon 9600 XT 16GB which can typlically be found around the same price as the 5060 Ti 8GB, or the 16GB version of the 5060 Ti or the RTX 5070 if you can swing it.",buildapc,2026-01-03 16:37:12,1
AMD,nxgottp,"Its a perfectly fine cpu and gpu.  I wouldnâ€™t recommend it for 4k gaming, but a 7500f and 5060ti (preferably 16 gig of vram vs. 8) will do reasonably well at 1080p or 1440p.  The next question is price.",buildapc,2026-01-03 16:40:26,1
AMD,nxgqawp,"is the Intel Corei5-12400 better? my budget is like 1500â‚¬, probably more because I want to upgrade it over time. CPU 's and GPU prices are rising so that is my main priority",buildapc,2026-01-03 16:47:21,1
AMD,nxgtbzt,>is the Intel Corei5-12400 better?   No and nope.  Think about a 9070 non-XT,buildapc,2026-01-03 17:01:25,1
AMD,nxijify,"The 12400 usually is a little better than a 5600x, worse than a 7600x.  A 14600k is usually as good, or a little better than a 7600x in gaming with the cost of efficiency.    The benefit of LG1700 with the 14600k, is you can find moboâ€™s that use DDR4.",buildapc,2026-01-03 21:55:17,1
AMD,nzd2hb5,Itâ€™s fine.,pcmasterrace,2026-01-13 15:05:12,3
AMD,nzd2l52,Solid option under $200. Not many good deals on the 7800x3d unless near a Microcenter.,pcmasterrace,2026-01-13 15:05:44,2
AMD,nzd3thz,"I game at 4K/144Hz with an RX7900XTX and the same CPU and mobo as you. It runs with High settings or better in all games.  Up until Christmas I was 4K gaming with the same card and a 12400 on a Z690 board, which also ran very well.  The new rig was a PITA to set up: AM5 is very picky with series 9000 CPUs, especially with fast DDR5 and reused NVME drives.",pcmasterrace,2026-01-13 15:11:48,1
AMD,nzd53lo,"It's fine, I got the 7600x combo at microcenter and matched it with a 9060xt 16gb for my dad, and my nephew was using it for cyberpunk for hours at 1440p, played and looked great",pcmasterrace,2026-01-13 15:18:00,1
AMD,nzd5dyd,Its a great entry level cpu.,pcmasterrace,2026-01-13 15:19:24,1
AMD,nzd91ee,"I'm rocking the same CPU, but with a 5070 Ti. I totally agree that this CPU isn't talked about enough. I'm more than happy with it.",pcmasterrace,2026-01-13 15:36:55,1
AMD,nzdahkx,Coming from a i5-7600k to a 9600x I think it's a damn good CPU!,pcmasterrace,2026-01-13 15:43:39,1
AMD,nzdc5gk,"Yes, quite good. It's not good anymore only for new builds as ddr5 ram is very expensive so AM4 is way cheaper",pcmasterrace,2026-01-13 15:51:19,1
AMD,nzdo4kw,>because nobody talks about it i  What?  Great combo btw.,pcmasterrace,2026-01-13 16:45:52,1
AMD,nze31yr,"Especially in 4K, as the general workload will be heavily GPU dependent. Wife's got a 9600X and 5070 Ti at 4K no issues.",pcmasterrace,2026-01-13 18:06:07,1
AMD,nzdtww5,For 1080p itâ€™s one of the best CPU price to performance wise. And pairs well with mid range GPUs like 5060 Ti 16gb / 5070 but for 4k definitely want something stronger.,pcmasterrace,2026-01-13 17:24:18,1
AMD,nzduhtx,Yeah iâ€™m using a 5060 TI 16gb with the 9600x and It gives me 120+ frames on Arc Raiders in 1080p and Balanced DLSS,pcmasterrace,2026-01-13 17:27:01,1
AMD,nzej5th,"Might be completely wrong, but didnâ€™t amd integrate some sort of physx emulation in their gpus? So basically they should be capable of it given they arenâ€™t ancient or unsupported of this feature?   Like I said Iâ€™m a bit ignorant about physx but thatâ€™s what I once heard and would love to know more",pcmasterrace,2026-01-13 19:17:29,0
AMD,nzelrs8,"Nah, AMD can only run PhysX on cpu. This isn't a big deal in later versions of physX when it became more CPU friendly, however if you try to run an old PhysX game like Batman Asylum it will just run like poo (as it did for us Nvidia 5xxx series owners before latest drivers).",pcmasterrace,2026-01-13 19:29:17,2
AMD,nzfbj8r,Interesting.   Have you ran any benchmarks of this? Share some videos on/off with the PhysX on CPU vs. the 1030?,pcmasterrace,2026-01-13 21:29:12,1
AMD,nzassiv,What happens if you reset to default all the settings in adrenaline?,pcmasterrace,2026-01-13 04:46:44,1
AMD,nz85jje,"Just buy it for the board ram and cpu then change out the aio and the psu, and the case too if you want.",pcmasterrace,2026-01-12 20:27:08,4
AMD,nz87wy1,The new pc you listed is muuuuuch better return it and get the 9800x3d build.,pcmasterrace,2026-01-12 20:38:24,4
AMD,nz82zbv,"I would always recommend building a PC yourself. The 9070XT is a very good Price-to-performance card and while it has some AI features it is about equal to a 7900XT (5% faster). About the watercooling, AIOs are really safe, not like custom loops. Also, I'm pretty sure if it destroys your pc they will replace everythiing that was destroyed but the odds of that are like 1/1000000000 (I use watercooling)",pcmasterrace,2026-01-12 20:15:11,5
AMD,nz855ek,For 850 bucks you're not doing better and tbh you did pretty good in this market,pcmasterrace,2026-01-12 20:25:20,2
AMD,nz9a6do,Iâ€™d return and get the other one you listed.  So much better.  Iâ€™ve been using AIOâ€™s for a decade or so and have never had a leak.,pcmasterrace,2026-01-12 23:45:31,2
AMD,nz9oqtk,Yeah the case on this current one is horrendous to work on the internals with. I dont even know what the psu is because its buried somwhere in the bottom section of the case.,pcmasterrace,2026-01-13 01:03:48,1
AMD,nz88uxi,"Thanks, do the 9070 xt's have problems with burning out their power connectors?",pcmasterrace,2026-01-12 20:42:52,-1
AMD,nz861l0,"I've always built my own in the past. I think pre-builds started making more sense with AI ruining the market. I hate the idea of a pre-build but here we are. About the water cooling, does it have antifreeze in it? Theres a good chance this thing could be in below freezing temps if I'm out of state for work.",pcmasterrace,2026-01-12 20:29:27,1
AMD,nz881l6,"Thanks, I'm mainly annoyed that I cant apply the $165 in best buy credits to the cost of the PC. Kinda had a feeling the associate was making crap up, my fault for not verifying I guess. Like, what the hell am I gonna get at best buy for $165?",pcmasterrace,2026-01-12 20:39:01,1
AMD,nz85dnm,Thanks lol. Bad deal on the PC I got?,pcmasterrace,2026-01-12 20:26:24,1
AMD,nz893xv,No they don't have anywhere near the power draw that 5090 or 4090's do.,pcmasterrace,2026-01-12 20:44:04,3
AMD,nz8lb60,"Itâ€™s a glycol mix, nothing to worry about",pcmasterrace,2026-01-12 21:41:07,2
AMD,nz890lq,"Do they expire? If not then just buy some games there. If they do, it'd pay for most of a 2tb ssd  As far as the RAM goes 16 is probably fine for now but if you want just pick up a single stick 16gb with either the same or better specs and they'll likely pair up fine.    You could have really done a lot worse buying a 850 dollar PC on light at best research overall. If you want to finance a 2k one you can but you're in a good spot overall imo and if it's the one I'm thinking of it's even just using retail parts. Have you booted it up yet? How's it doing with the games you actually want to play?",pcmasterrace,2026-01-12 20:43:37,2
AMD,nz8b3kg,Also you don't need to worry about waterdooling at all. Most of us all watercool our pc's with aio's and they're totally fine,pcmasterrace,2026-01-12 20:53:26,3
AMD,nz9q4o0,Thank you. Any idea of the freeze point? Depending if its ethylene or propylene glycol and the mix ratio the freeze point could be all over the place.,pcmasterrace,2026-01-13 01:11:23,1
AMD,nz9rae8,Its working pretty good for me for the older games I play. I tried mining XMR with it and was disappointed though. Would have been 3 months for a $13 payout. The energy cost today makes most mining non-profitable but I think of it as a space heater that helps pay for its energy cost. I'm mainly worried this might be my last chance for a long time to upgrade to a modern PC. I don't think the market is getting better any time soon. And mixing ram has very mixed reviews so also worried about buying a $200 ram stick and finding out they wont pair,pcmasterrace,2026-01-13 01:17:48,1
AMD,nz9plhh,I guess if if the watercooling was installed by someone that cared it would be ok. This current one had a broken screw holding a panel on and the rear cooling fan was half ass plugged in to where the lights came on but it wouldnt spin up. I had to reroute the cables to have enough slack to plug it in properly. Idk if the water cooling systems use compression fittings but want to assume its probably installed half ass and will have to chase all the connections to make sure theyre torqued properly,pcmasterrace,2026-01-13 01:08:29,-1
AMD,nz9vh97,No idea those details are never given  I know with Infloor heat I use 30% glycol mix which nets me -10c to -15 which is more than enough insurance where I am at. However I doubt the ratio is that high in an aio. Probably closer to 10% which would give you around 0 to -5c there about.,pcmasterrace,2026-01-13 01:41:01,1
AMD,nz8jf1r,"There are a few good upgrades you could do. For what you want though it sounds like a new graphics card is the main thing you need. I would recommend either a 9070 XT or a 5070 Ti, both of which can be handled by a 750W supply (you should double check what psu you have)  As you can see [here](https://www.techspot.com/review/3048-nvidia-5070-ti-vs-amd-9070-xt-with-dlss-fsr/#Cyberpunk_2077), the 5070 TI is a bit faster in Cyberpunk. That's at 1440p with the ultra ray tracing preset. However the 9070 XT is also over 100 cheaper, which makes the 5070 Ti 13% faster for 20% more money. Both are good options imo  You also could upgrade to 32GB of RAM, and/or upgrade the CPU. If you're happy with the frame rate your CPU can get in Cyberpunk though, pushing the graphics settings is mainly a GPU thing. I'd say upgrade the GPU first and see how that goes, then reevaluate whether you want to upgrade anything else",pcmasterrace,2026-01-12 21:32:25,1
AMD,nz70ms5,upgrade cooler and undervolt. plus a 9950x3d is CRAZY for a 4070ti super,pcmasterrace,2026-01-12 17:20:48,3
AMD,nz6z95v,"No that's absurd, waste of money.",pcmasterrace,2026-01-12 17:14:30,2
AMD,nz6zkxm,I bet you can just drop voltage. At least my raptor lake had excess voltage out of the box.  But if it's below 100 Â°C then does it really matter?,pcmasterrace,2026-01-12 17:16:00,2
AMD,nz70gce,You can likely drop voltages a lot. My 14900k does 5.7/4.6 at 1.23V for example,pcmasterrace,2026-01-12 17:20:00,2
AMD,nz710k9,"I don't think that it's worth it as you won't really gain any performance.  I would highly recommend undervolting the 14900k to decrease its temperatures; there are many guides for doing this online. As an example of the effectiveness of undervolting, my 9800x3d with a quiet fan profile went from 80-85C under load to 60-65C from a modest -23% curve with no performance loss",pcmasterrace,2026-01-12 17:22:33,2
AMD,nz71wxf,"At some point you are going to have to learn how your hardware works, rather than trying to buy good performance. Either learn how to undervolt your cpu, improve your cooling or set a lower power limit. Buying a new hardware platform is an absolute waste of money even if you are a nepo baby.",pcmasterrace,2026-01-12 17:26:39,1
AMD,nz7bcwb,"You should just turn off Hwinfo if the temperature is bothering you. Modern it/R9 CPUs are built to run close to thermal limit to give you the highest possible speed for the task.   Literally the only possible way of getting them way under that limit is a delidding mod and a direct die water cooling custom loop, but that most likely won't get you more performance.",pcmasterrace,2026-01-12 18:09:23,1
AMD,nz7dgxb,Without more details itâ€™s hard to tell but you shouldnâ€™t expect any better performance from a 9950X3D than you currently have.  Might even be worse,pcmasterrace,2026-01-12 18:18:56,1
AMD,nz7dwr7,"It may be that either the cooler isn't quite up to the task for an i9 or that your computer case doesn't have adequate airflow for such a powerful processor.        I used to use an Antec Nine Hundred when I had processors less powerful than an i5. When I upgraded to a motherboard with an i5 processor, I couldn't get the Antec cool enough no matter what I did. So, I sold it and bought a Phantex Enthoo case. I installed the same motherboard in that case, filled all the vent bays with fans and everything was fine. I even used the same CPU cooler on the motherboard that it already had in the Antec case.    I think the Enthoo case having vents in the bottom and top as well as the usual front and back made a big difference.",pcmasterrace,2026-01-12 18:20:55,1
AMD,nz7epz5,"As others have probably already pointed out the newish Ryzen 9s will make just as much or more heat just find a quiet fan or fans for the air cooler or radiator or lots of/bigger fans and keep them turned down low and itâ€™ll be quieter, more cores will help with stuttering if itâ€™s cpu related, have some cores working on editing and another will be running the mouse.",pcmasterrace,2026-01-12 18:24:35,1
AMD,nz7h9cv,"What are your drives? It seems to me you have other issues that will just transfer over to the AMD. Intel is still the undisputed king of productivity and the 14900k is no joke, at best you'd be side-grading. I wonder if you're saturating your drives and/or memory and causing the studders.  You need to run some telemetry (HWinfo or the like) and see if you're 100% on drives or memory, or possibly both.",pcmasterrace,2026-01-12 18:35:59,1
AMD,nz89aon,For workloads you canâ€™t beat the 9950x3d and your temps will go down especially if you tweak PBO with a -20 offset. I just made the switch after my 14900kf shit the bed even after updating the microcode and manually setting all my settings from day one.,pcmasterrace,2026-01-12 20:44:56,1
AMD,nz74qvn,"The heat isn't really an issue I just stated that as I'm used to lower temps as I came from an i7... My main concern which I probably should've stated better was for workflow. During heavy editing session, I can get lag/delay whenever I do a bulk of photos, masking with the delay can get annoying and with some video editing I notice a slight lag also. Like I'll be clicking to mask and my mouse would freeze for a bit then come back or it'll have some input lag causing editing to be annoying as it's not as precise due to the lag. Was wondering if going amd would be better for that type of work",pcmasterrace,2026-01-12 17:39:38,2
AMD,nz7dck5,"Pasting from another reply lol   The heat isn't really an issue I just stated that as I'm used to lower temps as I came from an i7... My main concern which I probably should've stated better was for workflow. During heavy editing session, I can get lag/delay whenever I do a bulk of photos, masking with the delay can get annoying and with some video editing I notice a slight lag also. Like I'll be clicking to mask and my mouse would freeze for a bit then come back or it'll have some input lag causing editing to be annoying as it's not as precise due to the lag. Was wondering if going amd would be better for that type of work",pcmasterrace,2026-01-12 18:18:23,1
AMD,nz7qs85,"I currently run the arctic freeze 360, as mentioned the heat isn't really a big concern just more of something Ive yet to get used to coming from an i7.. main concern I put in the edit is when I do workflow things unfortunately",pcmasterrace,2026-01-12 19:18:46,2
AMD,nz7qhio,I have a 1TB m.2 from Samsung. Has around 568gb of free space available.,pcmasterrace,2026-01-12 19:17:22,1
AMD,nz8a69m,"I'll prob look into and see if I can find a super in-depth guide, as I've never done more than just OC using bios pre built settings for my i7 so doing anything voltage wise and stuff is a new field for me",pcmasterrace,2026-01-12 20:49:04,1
AMD,nz8u1w0,Biased opinion in this subreddit aside. Intel is still generally king for workload where amd is king for gaming. Long as the 14th gen doesn't kill itself like the 13th which I'm pretty sure it will id run what you have for now. If and when the 14th burns itself out id make different decisions based on tech available at that time.   Came from intels until the 13th burned itself out and I shoved a 12th in its slot and built an entirely new 9800X3D pc for gaming.  Sidenote if it does burn out you can put an i7 12th gen in the board instead of buying a new pc.,pcmasterrace,2026-01-12 22:22:07,1
AMD,nz7h10a,"That is very likely not a platform issue. I would start by isolating usb devices. Leave only mouse and keyboard and see if it persists. It's not the most likely cause but it's incredibly easy to check.  Monitor cable may also be at fault. If you have a replacement make sure it's rated for the bandwidth and if not you may have to buy one. In case you have to buy one to check,  you can move this step to later in diagnostics",pcmasterrace,2026-01-12 18:34:56,1
AMD,nz84u4n,"I see. I have a similar problem with my PC in that most of the time, it runs cool, below any threshold that I worry about.  I know that my CPU isn't supposed to hit its performance limit until it reaches 100 degrees Celcius. Most of the time, it doesn't get above 60 C, even in gaming. Except when I play my currently most demanding game for my system - Oblivion Remastered.  In Oblivion Remastered, if I tried to play it with all the settings at maximum, the CPU temps would get really close to 100 C and I would see weird glitches in the game and stuttering at times.  I quit playing it for a while, but then I read somewhere that if you turn the game visual settings down a tick below max on each one except the ones for hair and the one that makes moving things like cloth and such look more realistic - those you turn way down - it will make a big difference. It sure did. Now, the game doesn't heat my system up beyond 80 C and then only for a brief time.  I'm running the game with a 2060 Super, so I'm trying to use the ray tracing effects in the game, but it's a pretty demanding thing for my setup to do, especially with hair and cloth modeling turned on max.  In terms of the processors you are asking about, the Core i9-14900K and the Ryzen 9 are quite comparable in terms of specs. i9 has more of some things and Ryzen 9 has more of other things, but the net result is so close that I don't think it justifies a CPU swap in terms of performance. In terms of heat production, the i9 is a little bit cooler and uses a little less power than the Ryzen 9.  I have noticed that trend myself over the years. Every time I upgraded to a signifcantly more powerful CPU, I had to make other changes to deal with the computer getting hotter.",pcmasterrace,2026-01-12 20:23:52,1
AMD,nzfavz8,"Your laptop.   Not that either would be a gaming machine by 25/26 standards. But for everything else, your laptop is actually really solid.",pcmasterrace,2026-01-13 21:26:19,2
AMD,nzfcv4v,The desktop will probably do better in games. I'd personally divert a little bit of what you're saving up for a better GPU in the interim.,pcmasterrace,2026-01-13 21:35:15,2
AMD,nzfjnrf,Depends on the game,pcmasterrace,2026-01-13 22:06:35,1
AMD,nzflo73,Your laptop has ddr4 as opposed to ddr3. Your ram speed is much faster too Your cpu is 4 years old and your dads is 12 years old. Both your dads gpu and your igpu are weak and not for gaming but you can definitely play games.  Your igpu in games: https://youtu.be/QAzl6wzHGSY?si=mbhA1XCp320Fq8r5 Your dads gpu in games (720p): https://youtu.be/kx1-o00NEyg?si=XOp5FIrC_vdHqGVR,pcmasterrace,2026-01-13 22:16:12,1
AMD,nzfdvfq,"assumiing you plan to game on it -the move here would be to take the pc from your old man, sell it - then use the money to put together a budget egpu to use with your laptop.   The CPU and ram situation on your laptop is actually perfectly serviceable, by adding a gpu to that mixture you should be able to have a fairly competent 1080p gaming experience for not too much coin. keep your eyes out for a used 5700xt or a gtx 1080. both can be picked up quite cheap on the current used market and still make for great 1080p gaming cards.",pcmasterrace,2026-01-13 21:39:53,1
AMD,nzfasyx,"The i7 would probably be better (all hail the budget king haswell) but the 660ti would be an bottleneck, so the pc would be better if it had an better gpu",pcmasterrace,2026-01-13 21:25:56,-2
AMD,nzfpxre,Im aware neither is ideal for gaming but as a teenager without a job I have to take what I can get,pcmasterrace,2026-01-13 22:37:04,1
AMD,nzfpsjp,"War thunder, pubg, and maybe sparking zero (if I can eek enough fps out of that one to be tolerable) are my heaviest gpu wise  Rimworld, factorio and dwarf fortress are probably my heaviest cpu games",pcmasterrace,2026-01-13 22:36:22,1
AMD,nzfp9vz,"My dad wants me to keep it intact because he wants it back after I get new build up and running, he doesn't care if I upgrade parts of it in the meantime but he wants all the parts from the og system back at the end of the day  Also what's an egpu",pcmasterrace,2026-01-13 22:33:47,1
AMD,nzfihh4,I really doubt a 12 year old 4 core cpu will be better than a 4 year old 6 core cpu and the laptop also has ddr4 ram as opposed to ddr3 on the desktop.,pcmasterrace,2026-01-13 22:00:59,2
AMD,nzfr7q4,Cpu games will be better on the laptop. Gpu games will be better on the desktop.,pcmasterrace,2026-01-13 22:43:28,1
AMD,nzfvp27,"you can find out what an e-gpu is by googling it.  however in that case, you could take the pc from your dad, sell the laptop and use the money to upgrade the GPU in the PC. a gtx 1080 should pair pretty well with the 4790 and give you a pretty decent 1080p gaming experience. (you'll need to make sure the power supply in the PC has the correct pcie plugs to power your intended GPU)  NOTE WELL - 4th gen doesn't support tpm2.0/trusted platform, meaning you cannot run windows 11 on it natively. If you play multiplayer games with kernel-level anticheat (like valorant for example) - you'll need to run the pc on windows 10.",pcmasterrace,2026-01-13 23:06:35,1
AMD,nzfjlyt,Kid named mobile power restrictions,pcmasterrace,2026-01-13 22:06:21,1
AMD,nzfk1y3,Mr money bags talking btw,pcmasterrace,2026-01-13 22:08:27,1
AMD,nzfizu0,"Good luck trying to upgrade that laptop, ever",pcmasterrace,2026-01-13 22:03:24,-1
AMD,nzfsdbo,I was planning on moving my laptops ssd over if the desktop was better  Which has the better overall performance,pcmasterrace,2026-01-13 22:49:20,1
AMD,nzfj739,"Good luck trying to upgrade that desktop, ever",pcmasterrace,2026-01-13 22:04:22,2
AMD,nzftadn,Probably the desktop. Also I recommend checking the psu wattage and upgrading the gpu. The 660 ti is holding back the i7 quite a bit.,pcmasterrace,2026-01-13 22:54:00,1
AMD,nzfjata,Kid named new motherboard,pcmasterrace,2026-01-13 22:04:52,0
AMD,nzftk7u,i'd imagine,pcmasterrace,2026-01-13 22:55:23,1
AMD,nz32t1q,Great build! That 7600X and 7700XT should run great.,pcmasterrace,2026-01-12 01:50:17,6
AMD,nz32amk,![gif](giphy|D6WuLOKOpR2fK),pcmasterrace,2026-01-12 01:47:35,5
AMD,nz3dlif,"Almost the same as mine, except I have a 6700XT. Has worked great for me.",pcmasterrace,2026-01-12 02:47:30,5
AMD,nz31unl,Just hope you got the 7700XT for cheap,pcmasterrace,2026-01-12 01:45:13,3
AMD,nz3k0ar,We like it,pcmasterrace,2026-01-12 03:22:22,2
AMD,nz39p5y,Ah. A fellow 7700XT enjoyer. Very nice.  https://i.redd.it/cdkcld79xtcg1.gif,pcmasterrace,2026-01-12 02:26:51,1
AMD,nz32fhh,"dont worry, about 100 dollars less than msrp, there werent any 9060xt's available anyway that werent 8gb models",pcmasterrace,2026-01-12 01:48:18,2
AMD,nz3ed0w,"nice, which brand did you get",pcmasterrace,2026-01-12 02:51:39,1
AMD,nz3p2g9,"Same as you, XFX. Been pretty happy with it.",pcmasterrace,2026-01-12 03:50:07,1
AMD,nxrxis0,25 processes ðŸ˜­,pcmasterrace,2026-01-05 06:33:54,21
AMD,nxrye1q,Why is there only 1GB of RAM seen by the OS? Is that the max amount the first AMD64 could handle at the time? I thought the 64-bit architecture was theoretically supposed to be able to handle up to 4TB of RAM?,pcmasterrace,2026-01-05 06:41:08,5
AMD,nxru9l1,Oh wow.  You're going to upset a lot of Intel fan-bois by stating fact that AMD64 destroyed Intel's Itanium CPUs.  Here's the back story as told by the guy that created Task Manager.  https://www.youtube.com/watch?v=WyX8TO3awfw,pcmasterrace,2026-01-05 06:07:24,22
AMD,nxry1mg,Itanium had lackluster performance for its own native software as well!,pcmasterrace,2026-01-05 06:38:16,4
AMD,nxs3ims,"The Greatest Generation had a word for what IA64 became, a ""boondoggle"". It's where something shifts and drifts so far from its original intention, becoming so complex and expensive along the way, that it no longer fulfills any of it.  What Intel wanted and what HP wanted were well aligned. HP wanted a server/high-performance computing (HPC) product to replace the older PA-RISC, while Intel wanted a monopolised line of products it could name its price on.  Itanium essentially killed every other RISC architecture of the 1990s. MIPS was sold off by Silicon Graphics, and Compaq (owners of DEC) ended work on the Alpha, both intending their future HPC machines to be IA-64. Of the RISC workstation platforms of the 1990s, only SPARC remained in development, by Sun Microsystems (soon to be gobbled by Oracle).  The point behind IA64's ""EPIC"" (explicitly parallel instruction set computer) architecture was to move almost all the CPU's instruction handling front end into software, let the compiler handle it all once then it didn't have to be done every time the code ran. So the instruction stream had to already have all its parallelism worked out, the compiler needed intimate details of the CPU's architecture.  By the time Merced actually shipped, Intel was de-hyping it in every presentation. The narrative became ""Wait for McKinley, though buy Merced now if you want to get familiarity with IA-64"". Merced had become so large on silicon that Intel's 180 nm process couldn't actually manufacture it so it starte being cut back. IA32 hardware support was first to go, but then the caches got trimmed, the exact same problem AMD was to later have with Bulldozer - and the exact same outcome.  On natively compiled IA-64 bit code against natively compiled IA-32 code a piddly little AMD K6-III from three or four years earlier at just 450 MHz was up to twice as fast as Merced running at 800 MHz. In other tests, Merced was four times faster than AMD's market-leading Athlon.  Well, anyway, we're getting a bit long and you're getting a bit bored. McKinley, the version Intel had wanted to make, fixed most of this and really was a HPC floating point monster. McKinley went more or less unchanged to 130 and 90 nm as Madison, which was dual-core and had larger L2 caches.  The future was actually looking bright for Itanium in 2001, even after Intel had cut its revenue forecast for Itanium from $35 billion in 2002 (1999 projection) to $7 billion by 2005 (2001 projection). Actual sales figures, by 2005, were barely over $2 billion.  Three things conspired against it.  1. If you wanted Itanium, you went to Intel, and Intel's pricing was catastrophic. You could buy a quad-socket Xeon system for the price of a dual-socket Itanium, and the Xeons would be faster. 2. Intel had not invited AMD to the Itanium party, when IA64 took over the world, as Intel felt was its entitlement, it would be an Intel monopoly. AMD disagreed here, so AMD extended IA-32 into AMD64, a drop-in replacement for existing systems in almost all cases. In AMD's K8-sporting Opteron's first year, it out-sold Itanium four to one: Michael Dell canned his exclusivity agreement with Intel and started selling Opteron servers. Dell wanted to sell servers, after all. 3. Very much unwilling to have Intel in control of the destiny of Windows, Bill Gates went on stage and told the world that WindowsXP's future updates would support one 64-bit architecture, and it was AMD's. Intel's share price was down over 5% by the time Gates had done speaking!  Microsoft did make a version of the NT6 codebase for IA64, but never a client version, only ever Server 2008 (NT6.0) and Server 2008R2 (NT6.1)  Itanium did lumber on, with the basic Madison architecture being more or less unchanged as it shrank to new processes, gained more cores, etc. The last Itanium, the 9700-family, was ""Kittson"" in 2017. The highest model ran at 2.66 GHz and had 8 cores. Intel's top end Xeon of 2017, based on Skylake-SP, had up to 28 cores.  The final shipping system was the HPE Integrity server/HPC, which stopped accepting IA-64 orders in 2021 and, since 2015, had been the only shipping IA-64 product. Linux removed IA-64 from mainline kernel support in 2021, Linus Torvalds quipping:  *""HPE no longer accepts orders for new Itanium hardware, and Intel stopped accepting orders a year ago. While Intel is still officially shipping chips until July 29, 2021, it's unlikely that any such orders actually exist. It's dead, Jim.""*",pcmasterrace,2026-01-05 07:25:14,6
AMD,nxso76w,"Is this in a VM?  I remember trying to install Windows 7 on a 1st gen Ryzen, a config that is officially supported and it absolutely did not like the USB implementation.    I had to modify the Win 7 ISO with drivers to get any input device working at all.   So I kinda don't see someone installing such old software on real hardware.",pcmasterrace,2026-01-05 10:38:06,2
AMD,nxy0h9c,"System starts up with that CPU ...   Nearly 4Ghz, 8 Cores, 16 threads and drawing 65w without overclocking.  ... Yep, looks normal.  That would look like science fiction in a 2003 system  - from memory and quick research you would need dual Xeon's to even come close to the performance for a higher power draw - But at least the ram quantity would look sane.",pcmasterrace,2026-01-06 03:29:32,1
AMD,nxs89iy,Not even arch is that light haha,pcmasterrace,2026-01-05 08:08:36,9
AMD,nxse1rp,No copilot and other bloatware. Awesome build.,pcmasterrace,2026-01-05 09:03:31,3
AMD,nxsqe2d,Probably because it's a beta-build. The official support at least on the os-side is 16Gb or 64Gb (with PAE on). Or he got that freaky 1Gb DDR5-Stick or just a VM.,pcmasterrace,2026-01-05 10:56:59,5
AMD,nxu4hdd,It's running as a VM.,pcmasterrace,2026-01-05 16:03:32,3
AMD,nxtocps,It's a virtual machine,pcmasterrace,2026-01-05 14:44:29,1
AMD,nxrx6jm,Not an Intel fanboy but OP's fact checking sucks. The licensing agreement between Intel and AMD preceds the IA-64 debacle and IA-64 never meant to be backwards compatible to begin with.  But I would have loved to be a fly on the wall at Intel's board meeting after AMD's announcement of AMD64. I guess there was a lot of panicking.,pcmasterrace,2026-01-05 06:31:03,8
AMD,nxte51j,> Oh wow. You're going to upset a lot of Intel fan-bois by stating fact that AMD64 destroyed Intel's Itanium CPUs.   On what planet do you actually see anyone being even slightly annoyed by this lol,pcmasterrace,2026-01-05 13:47:24,5
AMD,nxtnqm5,who installs win 7 on ryzen xddd,pcmasterrace,2026-01-05 14:41:12,1
AMD,nxu58ij,"Ah I see, my bad",pcmasterrace,2026-01-05 16:07:04,1
AMD,nxrxliz,"If you've not already done it, I do suggest watching the video that I linked.  That guy is a former developer at Microsoft during that time - again, he is the guy that created Task Manager, so he had true insight.",pcmasterrace,2026-01-05 06:34:32,3
AMD,nxz3fg4,">and IA-64 never meant to be backwards compatible  It's still a big reason it never caught on, however.",pcmasterrace,2026-01-06 08:23:56,1
AMD,nxyf08n,"I personally know people in the IT industry that believe ""Intel makes the best everything.""  I work with 1 of them on a daily basis.",pcmasterrace,2026-01-06 05:00:36,3
AMD,nxtqjsl,I did on an AliExpress B450 mobo it just to try.,pcmasterrace,2026-01-05 14:55:57,2
AMD,nzdmt4s,That looks like my previous build.,pcmasterrace,2026-01-13 16:39:53,2
AMD,nzdnvc5,"decent, personally wouldv got a little cheaper cooling for a 5070ti, but to each their own",pcmasterrace,2026-01-13 16:44:42,1
AMD,nz8xa56,"Check the cpu pins for any bent pins. If all are straight or not broken off, your motherboard is pobably, broken.",pcmasterrace,2026-01-12 22:37:56,2
AMD,nz8ychz,"I did, nothing out of place, I'll look into another mobo",pcmasterrace,2026-01-12 22:43:13,1
AMD,nzgv4ia,you went from a 9060XT to a 6800XT?,pcmasterrace,2026-01-14 02:22:02,2
AMD,nzgw9f7,"ðŸ¤·â€â™‚ï¸ do you like it? If so, so do I.   *(Boyfriend answer?)",pcmasterrace,2026-01-14 02:28:25,1
AMD,nzhhk5r,Solid,pcmasterrace,2026-01-14 04:38:10,1
AMD,nzgzbbb,"Yes, 9060XT 8gb to 6800XT 16gb",pcmasterrace,2026-01-14 02:45:37,1
AMD,nyjbo9a,[https://www.asus.com/support/faq/1038568/](https://www.asus.com/support/faq/1038568/),pcmasterrace,2026-01-09 04:08:46,3
AMD,nxvht28,"It's definitely a good backup card for non-iGPU systems. Also great for tinker PCs with a spare PCIe slot, as you watch it steadily grow into a 42U Homelab.",pcmasterrace,2026-01-05 19:48:57,2
AMD,nxvjh0v,Is it already time for me to fall into the homelab rabbit hole...,pcmasterrace,2026-01-05 19:56:34,2
AMD,nxvjw7h,![gif](giphy|oWjyixDbWuAk8),pcmasterrace,2026-01-05 19:58:30,2
AMD,nzgr8p1,Should be a great chip for overclocking,AMD,2026-01-14 02:00:10,3
AMD,nxxa4mv,Should fit a good hole in the line up,AMD,2026-01-06 01:05:16,51
AMD,nxxufn2,Iâ€™d love a max+ 388 in an itx motherboard to make a killer 1080p steam machine. The 395â€™s cpu is overkill but an 8 core cpu + 8060s + 32gb ram would be awesome. And even better if amd decided to release fsr4 int8.,AMD,2026-01-06 02:55:19,40
AMD,nxx8w1k,FSR4n't.,AMD,2026-01-06 00:58:39,135
AMD,nxx8hg6,Should've had these from the jump.  The prices are gonna suck...,AMD,2026-01-06 00:56:27,33
AMD,nxxfv3c,I like how they didn't announce a single product using these.,AMD,2026-01-06 01:36:07,24
AMD,nxy5bzs,"at this point idc any innovations (if there's any to begin with) if I can't afford it, are people that rich nowadays that nobody is hyped for mid-range stuff? Not saying I like mediocre performance, but I do like seeing a good price to performance kind of product",AMD,2026-01-06 03:58:01,7
AMD,nxxapco,It's going to be embarrassing when fsr5 isn't backwards compatible with rdna 4,AMD,2026-01-06 01:08:22,19
AMD,nxzd1gc,"Nice to see that they are real but there's still only 2 laptops available after a year, and no 16 inch option.  Lenovo has still yet to have their event so it's still early to say there aren't any new strix halo laptops coming but it's not great so far. Personally I'm just after an efficient lightweight 16 inch laptop with more than 8gb vram and that's ideally not over Â£2500.  Ryzen AI 388 + 32 or 64GB ram would be ideal, at least if it ends up being expensive, I don't have to worry about running out of vram.",AMD,2026-01-06 09:56:21,4
AMD,nxzk04g,8 cores is all u need. actually why not offer 85c cores only as the c cores would still be way more than enough for such a gpu with all the powerlimits/thermal limits.,AMD,2026-01-06 10:58:51,3
AMD,nxybevt,I like the 388+.  Am I crazy for wanting this on a handheld? Basically a cheaper GPD Win 5.,AMD,2026-01-06 04:36:29,6
AMD,nxy4rx4,More RDNA3 crap.  AMD is Radeon's worst enemy.,AMD,2026-01-06 03:54:39,7
AMD,nxxofr1,"All I want from AMD is a balanced APU, a solid CPU and a solid GPU. I genuinely donâ€™t understand this weird obsession companies have with slapping a midrange GPU next to an absurdly overkill CPU. Why canâ€™t we get a Ryzen 5 or Ryzen 7 level CPU paired with something like an 8060S?   Instead, itâ€™s always a top of the lone Ryzen 9 or Intel i9 equivalent paired with whatâ€™s basically a midrange GPU. All that does is jack up the cost for no real benefit and push the final price way higher than it needs to be. A 12 core CPU driving a 60 TFLOP GPU is still massive overkill, and it just feels like wasted silicon and money to me.",AMD,2026-01-06 02:22:26,5
AMD,nxzd536,"iGPUs are becoming an exciting field. The Intel ARC B390 is showing itself to be a bloody awesome little iGPU & if this progress continues, I wager my next PC will probably just be a mini PC with an iGPU.",AMD,2026-01-06 09:57:17,3
AMD,nxy6c9h,Hopefully a laptop drops with one of these below or around 2k so I can get one device for gaming and productivity on the go and at home,AMD,2026-01-06 04:04:06,1
AMD,ny3awz1,"Now put it in in a proper laptop with a 17"" screen and a full keyboard (i.e. with numeric keypad and no annoying multi-function limitations).  Strix Halo has been so badly wasted.",AMD,2026-01-06 22:25:06,1
AMD,ny49b5d,"Paper launch without any brand got it, waiting for a tablet 12-14"", 388, 64GB",AMD,2026-01-07 01:21:54,1
AMD,nyc016l,There damn well better be dozens of mini PCs and laptops using the Max+ 388 with NO discrete GPU. But let's just wait and see how OEMs fuck this up. They always do. It is the perfect laptop/tablet/miniPC chip. I want it. Now.,AMD,2026-01-08 03:31:25,1
AMD,nxyblhe,I like the 388.  Am I crazy for wanting this on a handheld? Basically a cheaper GPD Win 5.,AMD,2026-01-06 04:37:43,0
AMD,ny08ms5,"This would be interesting to see on a handheld. Considering that we're also seeing the new X Intel CPU on handhelds, I wonder if AMD can beat it in price/performance since it's already faster",AMD,2026-01-06 13:49:35,0
AMD,nxxlts8,So a hx 370 upgrade with 8060 the graphics. Okay how much?,AMD,2026-01-06 02:08:16,-1
AMD,nxxetpu,"the 395's 16 core CPU is just absurdly overkill for most people, the 388/392's 8/12 core CPUs are much more sane (and definitely cheaper)",AMD,2026-01-06 01:30:33,51
AMD,nxyk7sb,If you're planning on steam os you'll have fsr4 through decky.,AMD,2026-01-06 05:38:13,12
AMD,nxzdx3r,IMHO it won't be cheap enough over CPU + dGPU combo - and it's system you can upgrade.,AMD,2026-01-06 10:04:35,5
AMD,ny2jzde,"In my experience, the 385 already fits the bill. I got the Framework Desktop mobo, shopped around for deals on the other components, managed to get a sub-4L build for about $1000. Installed Bazzite, logged in to Steam, and was gaming within minutes.  The only lingering flaw - and granted, it IS a pretty major flaw at the moment considering the use case - is that Sleep doesn't work properly and seems to cut power to both USB and the network card even after waking. I'm confident I'll resolve whatever the issue is, but until then, being sure to power down after every session isn't a very difficult thing to do, especially since the only thing I'm using it for is gaming.",AMD,2026-01-06 20:20:35,1
AMD,ny63ifb,We have the same idea ðŸ˜….   388 steam machine + jellyfin server sounds banger if the price doesn't suck,AMD,2026-01-07 08:54:02,1
AMD,ny1yzpm,"I use it on my GPD Win5 anyway. Still the best upscaling (1080p, performance mode)",AMD,2026-01-06 18:44:55,2
AMD,nxxgqzr,"AMD's CES stream is still an hour away, they'd prolly save product announcements for the stream",AMD,2026-01-06 01:40:53,11
AMD,nxxkrqf,"Strix Halo is ridiculously expensive and needs super fast RAM, I don't expect to see any remotely affordable products with that any time soon.",AMD,2026-01-06 02:02:34,13
AMD,nxz1rei,"And then Asus / Lenovo are gonna shadowdrop a midrange model or two, with Lenovo being months behind Asus. Typical AMD release basically.",AMD,2026-01-06 08:08:19,3
AMD,nxzu291,The ASUS TUF 14 is going to have a version with the 392.,AMD,2026-01-06 12:18:48,2
AMD,nxzv50y,the [Asus TUF A14 is actually getting the Ryzen 392](https://youtu.be/h27w0PXFBgk?si=C15cREUl4y4NaBYp&t=307)! the literal midrange gaming laptop  https://i.redd.it/2nywodrp0qbg1.gif,AMD,2026-01-06 12:26:30,4
AMD,nxyxyrp,"these kind of are mid-range, even low end, but pricing probably won't be",AMD,2026-01-06 07:33:16,1
AMD,nxzavfw,"This should bring down prices to sane levels. Only enthusiasts were getting 8060s for gaming when it was paired with 395. With these, much more saner options, Iâ€™d expect laptops and handhelds around 1k, although ddr5 price will mess this up.",AMD,2026-01-06 09:35:52,1
AMD,nxxgqef,People should call AMD out more for this behaviour. At least Nvidia backports DLSS back to 20 series.,AMD,2026-01-06 01:40:48,24
AMD,nxzfe3u,the [Asus TUF A14 is actually getting the Ryzen 392](https://youtu.be/h27w0PXFBgk?si=C15cREUl4y4NaBYp&t=307)! the TUF A14 series has never went over Â£2000  https://i.redd.it/pnrflrgzepbg1.gif,AMD,2026-01-06 10:17:57,7
AMD,nxzbz22,"Lol cheaper, if people actually want this i expect it to be priced the same or more honestly. AMD is Nvidia when it comes to iGPUs.",AMD,2026-01-06 09:46:20,4
AMD,ny08wml,"GPD win 5 already has Max 385 version, what do you actually mean by ""cheaper"" lmao.  GPD with Max+ 388 will be more expensive than their Max 385 version",AMD,2026-01-06 13:51:06,3
AMD,ny09e3d,My first thought reading about this too,AMD,2026-01-06 13:53:47,1
AMD,nxycku8,Wrong order lol.,AMD,2026-01-06 04:44:15,1
AMD,nxxvfd8,"Iâ€™m hoping the 8 core model is the sweet spot. The 395 is way overkill in the cpu department and ram department for the average user.   Not all of us want gobs of ultra fast ram to run hot dog/not hot dog. Some of use just want a decent apu for low power, quiet couch gaming.",AMD,2026-01-06 03:00:48,6
AMD,ny0jfbe,thats what the 388's gonna be   it's an 8 core CPU with 8060s,AMD,2026-01-06 14:47:50,1
AMD,nxz6kkm,kills low end gpus and cpu sales is a likely cause,AMD,2026-01-06 08:54:05,0
AMD,ny5uc3f,It won't happen. Framework tried to do this with amd but it needs the fast soldered ram,AMD,2026-01-07 07:30:13,1
AMD,nxxze0o,"nah, this is a full fat desktop ccd, not strix point",AMD,2026-01-06 03:23:22,8
AMD,nxxt2tn,395 is around $1500-2000 so probably 1k plus since hx370 is very expensive still,AMD,2026-01-06 02:47:54,1
AMD,nxxf7c3,Agreed. Although I did buy a 395+ framework desktop ðŸ˜‚,AMD,2026-01-06 01:32:36,22
AMD,nxxlffp,395 isn't meant for most people.,AMD,2026-01-06 02:06:06,13
AMD,nxzz8d7,I was looking into this and there is a use case you may be missing. This will give you steam machine performance in a 2-3L PC case. Getting a dGPU into something like the MS-A2 (closest comparison I can come up with easily) requires jumping through a ton of hoops and will be more expensive besides.   This has a very niche use case compared to what you suggested and if money is a concern at all your solution is probably better.,AMD,2026-01-06 12:54:03,6
AMD,ny7i55m,You will most likely be able to grab a cheap Chinese mini pc which will beat any cpu + gpu combo in terms of value and size . As a 1080p steam box it would be great heck even the current 870m is decent enough for 1080p gaming . An 8745h machine is like $350 for a fully built system.  Iâ€™m not suggesting itâ€™s for everyone but there is definitely a strong use case,AMD,2026-01-07 14:52:19,1
AMD,nyjsc1d,Looking for the stability of a APU device. The simplicity and stability is worth a premium price imo.,AMD,2026-01-09 05:59:34,1
AMD,nxzq1lh,I guess we wait and see,AMD,2026-01-06 11:48:37,0
AMD,ny2p7nn,I've been using an 8700g for the past couple of years and it's fine for 1080p/low gaming but in all these SFF systems it's the GPU that's the bottleneck.  I have no doubt the 8050s is a great improvement on the 780m but I'd still rather have the 8060s,AMD,2026-01-06 20:44:59,2
AMD,nz1c3p6,"How many watts does it run at, and how long does a battery charge last?  I'd like to maintain a stable, true 60 FPS for 2-3 hours without it feeling like a turbine.  A low preset would be fine, especially in modern titles like Cyberpunk, Gothic Remake, etc.",AMD,2026-01-11 20:40:28,1
AMD,nxxn5ta,Read the article. Amd didn't announce any device.  These are likely from the full press kit released ahead of the keynote.,AMD,2026-01-06 02:15:33,9
AMD,nxxlneo,â€œNeeds super fast ramâ€  Why? Any ram should suffice or is this a downside of an APU?,AMD,2026-01-06 02:07:19,1
AMD,nxzbo78,"Seems like you are right, as I just heard from the Youtuber Crimsom Tech that Asus TUF A14 will have a variant with Strix Halo.",AMD,2026-01-06 09:43:28,2
AMD,ny0ttzf,OOOOh yeah baby. If we get 'full performance' over a USB Type C connection im in.,AMD,2026-01-06 15:38:40,2
AMD,ny09adv,"Honestly their most surprising announcement, and that's counting the Zephyrus Duo, the GoPro Proart PX13 and the Flow Z13 x Hideo Kojima colab",AMD,2026-01-06 13:53:14,1
AMD,nxxhz8b,they will just get DLSS 4.5 too now.,AMD,2026-01-06 01:47:30,9
AMD,nxy5c43,"People even excused AMD for *actual* fake pricing.  I remember launch day of RDNA4 people here and on r/radeon going to buy the card, but the subsidized stock was gone so they bought it for $800... when their whole reason for buying was the fake MSRP.",AMD,2026-01-06 03:58:02,8
AMD,nxxugid,Right but at one point they didnt back dlss to the 10 series. They can't. Amd also cant just slap the same fsr on rdna3 without drawbacks. They gotta make a version for rdna 3.5. Rdna3 should be getting one as well.,AMD,2026-01-06 02:55:27,3
AMD,nxzo2jg,"Thanks so much for sharing this, it completely flew under my radar, it's definitely one I'm going to be keeping my eye on as I liked the design on the 2025 model. I would ideally prefer 16 inch but I love how this laptop has 2x m.2 so it will make it really easy to just slot my old SSD in and go.  I'm not worried much about the price, I wouldn't mind going a bit over Â£2000 for a 64gb model, definitely one I'm going to keep an eye on.",AMD,2026-01-06 11:33:07,2
AMD,nxzkthf,They said 16 inches,AMD,2026-01-06 11:05:54,1
AMD,ny1rkdh,The 385 has a worse GPU than the 395. The 388 doesnâ€™t.,AMD,2026-01-06 18:11:45,2
AMD,nxycs1l,Nah I phrased it deliberately.  ATi was a top tier graphics card company; AMD bought them and turned them to shit.,AMD,2026-01-06 04:45:36,7
AMD,nxzsdza,"I was considering a framework desktop, how are you liking it?",AMD,2026-01-06 12:06:26,4
AMD,ny01npc,"Yes, if you are into very-SFF then this can do things other builds can't. Current Framework and Chinese Strix Halo 395 boards are priced at like $2700+ and with current DRAM shenanigans 388 with 32GB RAM may loose the price advantage it initially would have. Steam Machine will win this one by a big margin I guess.",AMD,2026-01-06 13:09:28,4
AMD,ny5m7nj,"i mean, a low profile 5060 will fit in a case thats only very slightly larger, I put one in a dragon range minisforum a few months ago.",AMD,2026-01-07 06:21:45,1
AMD,ny9otu9,How would that compare to something like the Minisforum HX99G? Â It can be found pretty inexpensively refurbished.,AMD,2026-01-07 20:44:22,1
AMD,nyimun9,"I'd love a tablet like the Misinforum V3, but revamped with a proper kickstand like the ASUS ROG Flow Z13 and Surface Pro have, an OLED screen, a detachable Bluetooth keyboard like the Surface Pro 13 has (Flex Keyboard), and the DP-in the V3 has, with the Max+ 388 chipset.  That'd be my dream portable device, hands-down.",AMD,2026-01-09 01:52:52,1
AMD,nxxphl5,I CAN HOLD OUT HOPE IF I WANT!,AMD,2026-01-06 02:28:09,6
AMD,nxy5k61,They only announced the AI MAX chips,AMD,2026-01-06 03:59:23,3
AMD,nxxm3gn,"Massive iGPU that shares bandwidth with the CPU. Both need a ton of bandwidth so if you just put ""any RAM"" you're gonna lose a ton of performance.",AMD,2026-01-06 02:09:43,17
AMD,nxxtumz,The major thing holding igpuâ€™s back from better performance is ram speed - ddr5 just isnâ€™t fast enough to work as vram. Whatâ€™s unique about strix halo is that it uses soldered lpddr5x instead of ddr5 modules to improve gpu performance.,AMD,2026-01-06 02:52:06,5
AMD,nxxua01,Appreciate the teachings of my fellow redditors,AMD,2026-01-06 02:54:28,1
AMD,ny0unmi,"the 2026 TUF is claiming up to 95W maximum power usage, so even 100W chargers can give full performance",AMD,2026-01-06 15:42:30,2
AMD,nxzlpej,Remember Zen5% as well,AMD,2026-01-06 11:13:30,2
AMD,nxxvnpj,"That's what people are expecting.  Sure we all know that FP8 isn't on RDNA3... But there's other instruction sets that could work, there's Some AI hardware there to be used there at some level. No one is expecting to get FSR4 on an rx 580 or an RX 5700XT... maybe 6000 series, but 7000 series should've been guaranteed.  It's the LEAST they could do.",AMD,2026-01-06 03:02:06,6
AMD,nxzsp5u,"yea idk why not a single tech journalist or content creator noticed while they were on Asus's booth, literally only Crimson Tech noticed",AMD,2026-01-06 12:08:47,3
AMD,nxz8xly,"they weren't top tier, they were also below nvidia most of the time. same as today really.",AMD,2026-01-06 09:17:04,4
AMD,nxycxhf,Ohhhh fair enough. I thought you meant that Radeon are holding AMD back (which they are rn).,AMD,2026-01-06 04:46:37,2
AMD,ny0aol9,"Radeon's most successful period was under AMD from 2008-2013. They carried AMD for a lot of that time while their CPU division faltered.  Also ATI were in financial trouble when AMD bought them, so it was unlikely they could have continued without a buyout.",AMD,2026-01-06 14:00:57,1
AMD,nxzwjzh,"It's cool. Using it exclusively for llm models. It acts as the backend for OpenWeb uI. Since the nic is only 5Gb, I keep the models on the device. Eventually I'm going to put it in another case so I can use the x4 slot for a 25G card and centralize my model storage.  But performance is strong all things considered. It doesn't chug when trying to generate a result. I think the biggest delay is the time from when it loads the model requested into memory before it starts processing",AMD,2026-01-06 12:36:20,6
AMD,ny04gbm,"Looking through the CES news, I saw that AMD teased a reference dev box. They called it the Ryzen AI Halo. A AI Max+ 395 with 128GB. If there is even just a hint of them dropping this thing with a ""reasonable"" price tag in Germany, I'll just grab one of those.  My hopes for a decent sub-2L chassis with a 388 and 64GB of RAM feel rather slim. I rather avoid the possibility of filing a warranty claim against a Chinese brand.",AMD,2026-01-06 13:26:04,1
AMD,ny05p0g,"Iâ€™m still waiting to see what we actually get from the steam machine personally, We still need specs and price point. My current htpc gaming setup is 15L which is tiny considering what itâ€™s packing (3900x+3090) but I wouldnâ€™t be opposed to something in the USFF if itâ€™s at a â€œreasonableâ€ price and competes in the same ballpark.",AMD,2026-01-06 13:33:10,1
AMD,ny6s86h,"Honestly, the ms-02 ultra is what I was considering if using dgpu but it is almost twice the size. I would be hard pressed to beat the 3090 in my 15L so it needs to be a usff case or be close to matching my current rig in a slightly larger form factor (2-5L).",AMD,2026-01-07 12:23:55,1
AMD,ny9xcsd,Performance wise I have no clue but design wise it fills the same general market.  Hits in the same ballpark as a mobile 3060. The AI chips supposedly play in the 4000 series mobile range so better I guess.  I didnâ€™t even know the HX99G existed until now and as a refurb in the sub $500 range barebones I donâ€™t think I would be disappointed.,AMD,2026-01-07 21:20:26,1
AMD,nz02q2o,ASUS ROG Flow Z13 has already launched 385+ 8060s version,AMD,2026-01-11 17:14:44,1
AMD,nxznsii,They also announced the 400 series with actual product pictures  https://www.servethehome.com/wp-content/uploads/2026/01/AMD-CES-2026-Ryzen-AI-400-Availability-800x450.png,AMD,2026-01-06 11:30:50,3
AMD,nxxmh9n,"Still learning this whole AI chip deal. The power efficiency of Core Ultra cpus interest me, but at what cost is always my first thought.",AMD,2026-01-06 02:11:48,1
AMD,nxy921b,It's really that they use 8 memory channels rather than the typical 2,AMD,2026-01-06 04:21:09,2
AMD,nxzn0yw,"The usage itself is not unique, the unique thing is that it's mandatory. You won't find Strix Halo with SO-DIMM memory because AMD doesn't allow it.",AMD,2026-01-06 11:24:36,-1
AMD,nxxye8m,"There is INT8 and I used to work on there last I know is that them corporate heads dont want to backport to be released RDNA3/RDNA3.5. Their RDNA4 was stopgap and fooled consumers enough to buy them and they might not even know if RDNA4 will be even supported in the future, haha.",AMD,2026-01-06 03:17:39,4
AMD,nxz9c8p,Theyâ€™d lagged a bit but they still competed in the high end and in the early 2000s made halo products like the 9800 XT. Since their acquisition theyâ€™ve exclusively been â€œthe cheaper alternativeâ€ and since Turing theyâ€™ve been generations behind.  The only new graphics tech I can think of from AMD era Radeon was the TressFX hair rendering (which basically only ran on Radeon cards).,AMD,2026-01-06 09:20:59,4
AMD,nxyd39f,"Oh they def are, but when AMD bought ATi, AMD CPUs were crap while Radeon was still good.  They basically coasted on Radeon's residual quality until their CPUs got good then stopped caring.",AMD,2026-01-06 04:47:41,2
AMD,ny0865s,How easy for it to set up. What model+ config did you use if you don't mind me asking,AMD,2026-01-06 13:47:03,1
AMD,nyilmz4,What is the 25G card for?,AMD,2026-01-09 01:46:23,1
AMD,ny464pc,That ryzen ai halo box is for developers not general consumer,AMD,2026-01-07 01:04:38,2
AMD,nxyxijq,"it's a 256 bit interface instead of the usual 128 bit, ie. 8 32-bit channels vs the normal 4 (which are usually still referred to as dual channel since we are used to talking in 64-bit channels)",AMD,2026-01-06 07:29:07,6
AMD,nxzvtpk,Framework said they asked amd about upgradeable ram (sodium or lpcamm I dont recall exactly) and amd tried but couldn't get the signal integrity right without sacrificing bandwidth (and let's not kid ourselves those large igpus are dependent on the insane 256GB/s bandwidth that strix halo has) At least that is what I remember reading back when the framework desktop was announced,AMD,2026-01-06 12:31:15,3
AMD,ny0mzob,"Do you enjoy working there? You don't seem happy, while all of your posts about your competition seems positive. Really strange.",AMD,2026-01-06 15:05:45,1
AMD,nxzbxgb,what about the Mantle that changed the game for APIs?,AMD,2026-01-06 09:45:55,1
AMD,ny0vm8e,Using LM Studio on windows because I didn't feel like going through the linux config.   I have the following models:  glm-4.7 q2  mistralai small 2512  mistralai 3 14B  Llama 3.3 70b  granite 4h tiny  gemma 3 27b  qwen3 4b  gpt-oss 120b  qwen3 coder 30b  qwen3 4b thinking  deepseek 8b,AMD,2026-01-06 15:46:56,4
AMD,nyma6j7,Networking.,AMD,2026-01-09 16:15:03,1
AMD,nyo0gdp,"Trying to centralize my model storage, 2.5G takes a while to load the bigger models.   Also, looks like newer engines are implementing RDMA which will allow more distributed execution and it's not possible to do over the existing nics",AMD,2026-01-09 20:56:05,1
AMD,nxzwdwj,"Yeah, all correct, signal integrity when using DIMM memory is a bitch. Not to mention the complexity of the resulting board itself would be pain, you need 4 slots at least. Yikes.",AMD,2026-01-06 12:35:09,3
AMD,ny3em3j,Because the competition is launching more exciting products that I wish my company would launch. To me laptops are really important segment as I can use it to explain to people what I used to do at work easily but eith Strix Point/Halo being so damn expensive and uncompetitive it just brings me a tinge of sadness.,AMD,2026-01-06 22:42:54,1
AMD,nymczw2,"If you don't mind asking, which split specifically are you using for *glm-4.7 q2*?",AMD,2026-01-09 16:27:35,1
AMD,nymeqex,"I mean what would you need it for, in your use case? Is it for Jellyfin to stream faster?",AMD,2026-01-09 16:35:22,1
AMD,nyneppu,"No, jellyfin does not need that much bandwidth; many tvs come with only 100 Mbit ethernet, and the streaming platform with the highest quality streams (Apple TV) top at about 50 Mbit.  The GP gave a hint: centrally storing LLM models. These files can be huge (e.g. gpt-oss-120b has ~64 GB, llama-3.3-70b has ~37 GB, glm-4.7 depending on quantization can have 100-250 GB), so you need as much bandwidth as possible when loading them over network.",AMD,2026-01-09 19:15:55,1
AMD,nzhc7m9,Takes real effort and time to make sure the software correctly reads strix point silicon shipped after X date as 400 series and selected skus have +100mhz boost clock,AMD,2026-01-14 04:02:20,2
AMD,nz9llfo,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-13 00:46:37,1
AMD,nza6dpc,Should've called it the 9969X3D.,AMD,2026-01-13 02:39:03,96
AMD,nza2kbn,Just a higher clocked 9950X3D...?,AMD,2026-01-13 02:18:45,31
AMD,nzaehe7,Ok 9999x3d might not be that crazy a prediction anymore,AMD,2026-01-13 03:22:45,20
AMD,nzaojds,Can't wait for the Ryzen 9 9999x3D.,AMD,2026-01-13 04:19:25,10
AMD,nzabde5,xXxPROslayer9969x3dxXx,AMD,2026-01-13 03:05:54,7
AMD,nzfq9pj,Son muchos muchos nÃºcleos pro,AMD,2026-01-13 22:38:44,1
AMD,nzawie9,"Is V-Cache useful in stuff other than gaming? Because if it's still not used, then what's the point (different than money)?",AMD,2026-01-13 05:12:10,1
AMD,nzbkhzr,Better name it Ryzen 9 9967X3D,AMD,2026-01-13 08:36:14,1
AMD,nzcu6cx,I can't take these seriously anymore after people managed to glorbo the 9700x3d,AMD,2026-01-13 14:22:49,0
AMD,nza6kvn,XXX6969X3DXXX,AMD,2026-01-13 02:40:06,36
AMD,nzaupa1,Ryzen 69 420X3D,AMD,2026-01-13 04:59:35,6
AMD,nzgzlku,The perfect match for when i sell a kidney and buy a 6090.,AMD,2026-01-14 02:47:12,1
AMD,nzgzy6r,The perfect match for when i sell a kidney and buy a 6090.,AMD,2026-01-14 02:49:09,1
AMD,nzatj07,9967X3D,AMD,2026-01-13 04:51:38,-8
AMD,nzas3sn,+15 higher mhz,AMD,2026-01-13 04:42:09,13
AMD,nzbsuia,Just like the 9850x3d. Just marketing,AMD,2026-01-13 09:57:31,7
AMD,nzbo0ji,Better binned 9950x3d...,AMD,2026-01-13 09:10:27,2
AMD,nzb0l3n,"Honestly, 9999X3D as the most powerful Zen5 desktop chip to commensurate an amazing turnaround for Ryzen is nice.",AMD,2026-01-13 05:41:56,13
AMD,nzazis8,9999.99x3D,AMD,2026-01-13 05:33:59,4
AMD,nzauip4,Ryzen 9 9999x3D Gen 2x2 Pro Max Ultra AI,AMD,2026-01-13 04:58:20,1
AMD,nzbbmbf,It was literally made for Epyc CPUs first to accelerate database and stuff.,AMD,2026-01-13 07:13:22,19
AMD,nzb03hr,"There are some applications that benefit, but the majority of productivity doesn't care at all. 9950x3d is basically just a 9950x in almost everything.  If you're a heavy gamer \*and\* use the same computer for heavy productivity its useful, otherwise its quite niche.     They're basically just rebadging different bins of existing products. Not unusual.",AMD,2026-01-13 05:38:15,9
AMD,nzbo9jn,"It is great forheavy compilations,simulations, databases, etc. Basicly anything tjat is bottlemecked by cache misses.",AMD,2026-01-13 09:12:54,1
AMD,nzbtowt,"Most using apps for work etc..also game in some capacity.   they might be binned better so overall a bonus as chips today are fast but there are many various niche user cases that chose the little edge  or in my case, upgrading for 7800x3d to 9800x3d made a small difference but most wouldn't value that difference but I do",AMD,2026-01-13 10:05:32,1
AMD,nzct79i,"It's really good for applications that have lots of moving parts and tons of context switching so data processing tasks, databases and such get a massive performance boost from more cache.  Most other things don't benefit as much as the lower level caches are plenty big enough for most productivity use cases.",AMD,2026-01-13 14:17:42,1
AMD,nzeavsk,"it is, just the percentages of games that benefit from vcache is much higher than the percentage of production applications that benefit from vcache. At a production level, most would benefit more from additional cores than spending that money on more cache. Gaming is the opposite in that case.",AMD,2026-01-13 18:40:26,1
AMD,nzaelq9,This reminds me of the crazy names XFX used yo come up with for their cards lol. I still remember the XFX XXX Thicc cards or whatever they were called.,AMD,2026-01-13 03:23:25,20
AMD,nzea873,xd,AMD,2026-01-13 18:37:33,1
AMD,nzh5jfg,ATI X850XT PE vibes (owned this card in 2005),AMD,2026-01-14 03:21:25,1
AMD,nzayomf,![gif](giphy|t4HFdwzbg4x8XNZF8I|downsized),AMD,2026-01-13 05:27:45,13
AMD,nzcv0t1,67 bad 69 good ðŸ˜¡ðŸ˜¡ðŸ˜¡ðŸ˜¡,AMD,2026-01-13 14:27:11,2
AMD,nzb5scq,"From 1800X to 9999X3D, it would be a cool way to end this naming scheme (before they inevitably go to something more confusing).",AMD,2026-01-13 06:23:20,4
AMD,nzhb8yw,Commensurate? Don't think I've ever seen someone try to use that as a verb before,AMD,2026-01-14 03:56:12,1
AMD,nzhdugz,"Yes, but interestingly enough zen 5 Turin-X doesnâ€™t exist.",AMD,2026-01-14 04:13:01,1
AMD,nzb18r6,"The 9950X3D shows a ~4% improvement over the 9950X in DaVinci Resolve (popular pro video editing software), a ~6% improvement in unreal engine code compilation (for game devs), etc according to Puget which reviews these things very well.   https://www.pugetsystems.com/labs/articles/amd-ryzen-9-9950x3d-and-9900x3d-content-creation-review/  It is small, but itâ€™s a nice bonus. The main reason why you should buy a X3D is still gaming, but it will beat a non-X3D marginally in productively too. It doesnâ€™t really hurt.",AMD,2026-01-13 05:46:57,13
AMD,nzaqpon,my xfx rx 590 fatboy did an incredible job when i had it mostly playing cs go and league   ![gif](giphy|bsWDUSFUmJCOk),AMD,2026-01-13 04:33:04,12
AMD,nzauzgs,[Asus V9999 Gamer Edition](https://www.techpowerup.com/gpu-specs/asus-v9999-gamer-edition.b1796),AMD,2026-01-13 05:01:31,2
AMD,nzhdpn1,r/ redditmoment,AMD,2026-01-14 04:12:07,1
AMD,nzhdnlz,"Honestly, peak Reddit moment right here.Â   69420 = totally funny and not overused to death unlike 67",AMD,2026-01-14 04:11:45,1
AMD,nzb9b9o,Well first due to the ram crisis we also gotta do 100-1700X for PCs people can actually afford,AMD,2026-01-13 06:53:19,2
AMD,nzbuex9,I can't wait for zen6 where they will probably rename the desktop parts to Ryzen AI 500 and the lowest end AI 521HX-- will actually be a Zen2 part with Vega graphics being sold in 2027,AMD,2026-01-13 10:12:13,1
AMD,nzb534q,"I did say there are some applications that benefit.  I don't think a 4% improvement in a couple apps is worth an extra 100 ish USD at the moment though, or enough to really call them notably different. (Especially Davinci which has rather limited CPU reliant features, which Pugent seems to agree with.)",AMD,2026-01-13 06:17:33,-4
AMD,nzee93f,LOLLL that's awesome. Too bad it's AGP otherwise I'd pick one up for shits & giggles as a display output.,AMD,2026-01-13 18:55:21,1
AMD,nzb92ok,"Time is money for productivity use cases tho. Itâ€™s situational, I donâ€™t game much but it was easy for me to go with the 9950x3d for just an extra $100, especially when youâ€™re likely pairing it with a 5090 if not 5080 as a top spec video editing rig.   A 4% improvement to a 4 hour render is 10 minutes saved, so it does add up quickly.",AMD,2026-01-13 06:51:16,11
AMD,nzdx9xf,"Itâ€™s not a 4% improvement to the render, that was part of the point of that. Itâ€™s 4% to specific workloads within the program that are not a sizeable chunk of program time.  Your render time is wholly GPU driven in Davinci. Most of the actions done on the timeline and in color grading are also wholly GPU driven. The chip wonâ€™t save 4% of your time unless those specific actions are all youâ€™re doing for some reason.  If youâ€™re spending a lot of time in Unreal on tasks like shader compilations? Sure, thatâ€™s gonna save time and arguments could be made for it. Particularly since that line of work also often ends up running said unreal project, which itself benefits from the chip.  â€œdonâ€™t find the additional gaming performance worth the price increase over the 9950X and 9900Xâ€ is a literal quote from the article right after saying they canâ€™t recommend the chips for every workflow, among a couple others, as a reason not to spend the extra money and the article is assuming MSRP that only puts them 50 apart.  Edit: 4% is within Pugentâ€™s margin of error, as an aside.  Edit2: since people get territorial over this kind of stuff sometimes. I *have* an x3d chip. As I mentioned originally thereâ€™s essentially no difference in most workloads, itâ€™s primarily a gaming chip. If you need the best option for gaming get one. If you need the best option for productivity specifically then thereâ€™s a big olâ€™ pause there with a lot of asterisks.",AMD,2026-01-13 17:40:04,2
AMD,nyvaaqm,"If AMD ever gets it together, the NPU could be used to offload AMD noise reduction and can also cleanup live webcam images too.  Otherwise, it has better uses in a more integrated system like a laptop.   Just wait until dGPUs have a type of NPU (much smaller and tailored to a very specific workload) directly integrated into the display engines to analyze images/frames (could support multi-frame accumulation and also denoise that) on the fly and apply transparent (to game/app) post-processing. Look! They're ""neural"" display engines now. So smrt! -_-",AMD,2026-01-10 22:47:07,20
AMD,nyu61bg,And just like thatâ€¦ not a f\*ck was given.  ![gif](giphy|RVW5PilbP2tLG),AMD,2026-01-10 19:26:27,85
AMD,nysrc1v,"Could make for a pretty decent light, power efficient home server. If the ram prices come back down :(   RDNA 3.5 might be decent for transcoding for Plex?",AMD,2026-01-10 15:25:03,25
AMD,nysshua,"they should just name it a 9700g and call it a day.  anyone who wants this for ""AI"" instead of gaming will know to buy it.  this 400 nonsense on desktops just causes more confusion for pc gaming builders.  am5 socket is only half the story, as we still don't know what bios will backwards support this igpu.  i mean if i have to get a new mobo simply for the bios change, what difference does it make to me what socket it's using.  bottom line for me is; if my b650 ever gets updated to support this ""9700g"", i would get it, even if it means i have to change my ram stick timing to make it work (sell my current 6000 sticks for something with some other cas timing etc).  **update;** most likely we'll need a 800-Series mobo to support this am5 desktop chip (so my b650 is out of luck).",AMD,2026-01-10 15:31:00,27
AMD,nys6dcb,"I wonder if the ""AI"" cpus are gonna support something like ""smart access video"" or ""smart access memory"" -type processing with the npu...",AMD,2026-01-10 13:26:10,25
AMD,nywvq8n,curious what the pcie total/use will be.,AMD,2026-01-11 03:56:16,2
AMD,nyxmkmq,Is that supposed to be an Instinct CPU?,AMD,2026-01-11 07:03:41,1
AMD,nyzip1b,Wait,AMD,2026-01-11 15:39:15,1
AMD,nys78dn,A laptop CPU for a desktop motherboard? Why?,AMD,2026-01-10 13:31:33,-6
AMD,nyt3kdt,No chance to buy anything with AI integrated in it.,AMD,2026-01-10 16:25:03,-6
AMD,nz4ttmv,"I beg to differ.  At least one was given, in the â€œno f*cking way am I buying thisâ€ column.",AMD,2026-01-12 09:13:14,2
AMD,nyz8r0x,"sadly like how the insane gpu prices stayed post covid, I am not holding my breath for ram prices to go back to normal either  these corps have too big of a choke on the market, there is no real competition",AMD,2026-01-11 14:48:25,3
AMD,nysw3dq,"The â€œAIâ€ branding refers to the built-in NPU, which is required to get Microsoft Copilot certification. No current AMD desktop CPU has a NPU, but pretty much all models have at least a basic GPU, which makes the old â€œGâ€ branding a bit redundant and confusing.",AMD,2026-01-10 15:49:08,26
AMD,nytib60,"I wish they'd have any kind of consistent naming scheme instead of three concurrent separate ones, and not make a ""8000"" series that's basically worse than the 7000.",AMD,2026-01-10 17:35:09,8
AMD,nystg8p,"It won't even be good for AI, the mobile Strix Halo chips are good for AI because of the wide memory bus and unified memory which this won't have.",AMD,2026-01-10 15:35:53,3
AMD,nyswkm6,>they should just name it a 9700g and call it a day.   You're deluding yourself if you think that the desktop SKUs won't switch to the new naming convention once they have a CoPilot+ certified NPU in them. That's the only reason Zen 5 desktop isn't using the same naming scheme.,AMD,2026-01-10 15:51:29,1
AMD,nysdnd9,It doesnâ€™t support anything special on the laptops so I donâ€™t expect it will anything on desktop either.,AMD,2026-01-10 14:09:30,50
AMD,nysofqz,Not really. Itâ€™s just more efficient to do tasks that benefit from an NPU. Desktop CPU + external GPU will always be better.,AMD,2026-01-10 15:09:42,4
AMD,nysdo0v,what the hell are you talking about,AMD,2026-01-10 14:09:37,0
AMD,nysdp7a,Theyâ€™ve always done that for every generation.,AMD,2026-01-10 14:09:48,21
AMD,nysmrog,Every G series chip from AMD is a laptop cpu for a desktop socket,AMD,2026-01-10 15:00:44,16
AMD,nysxv5l,"I actually want them to make the R9 7945HX laptop cpu for desktop, as it's very cheap and has 16 cores 32 threads which I'd like for compiling code and it's good enough for mid range gaming.",AMD,2026-01-10 15:57:48,3
AMD,nysnrq5,"Itâ€™s basically 10000-series, an upgrade to the 8000-series.",AMD,2026-01-10 15:06:06,0
AMD,nyu5rwi,"If you buy a computer at all it can run AI models on the cpu, GPU, or an npu/tpu.",AMD,2026-01-10 19:25:10,3
AMD,nyto4oa,"It's called that because it has an NPU, if your computer has a graphics card in it you bought something with AI integrated by this standard",AMD,2026-01-10 18:02:33,2
AMD,nyuhjrm,> which makes the old â€œGâ€ branding a bit redundant and confusing.  confusing branding? Isn't that the whole idea behind their naming scheme (and intels aswell)? :D,AMD,2026-01-10 20:23:34,13
AMD,nyx9rq4,Don't the 8000 series have a NPU as well? At least it shows up for my wives 8600G in the task manager.,AMD,2026-01-11 05:23:01,8
AMD,nyt0op5,thanks for reiterating the npu to gpu distinction.,AMD,2026-01-10 16:11:18,3
AMD,nysulm7,"gtk, in my case i care nothing for AI   i'm just willing to upgrade in this iDirection   to avoid buying a gpu card.   the 10yo games i play now seem to manage   just fine on my 7600 at 2k30fps.",AMD,2026-01-10 15:41:44,7
AMD,nyskfgg,They probably think the NPU on an SoC can help with gaming like the tensor cores in an Nvidia gpu.,AMD,2026-01-10 14:47:56,13
AMD,nysoywu,Isn't the G series CPU basically a cut down desktop die so that they can fit a GPU die in it?,AMD,2026-01-10 15:12:32,-7
AMD,nyt82d8,There are some mobos that they make with the 7945hx soldered in.,AMD,2026-01-10 16:46:11,5
AMD,nytan1q,"They do make the 7950X... Not sure how much they go for, but they're very similar CPUs with different TDPs.",AMD,2026-01-10 16:58:18,1
AMD,nyxjmgb,â€œ9700NGâ€ seemed too hard,AMD,2026-01-11 06:38:32,13
AMD,nyybu81,"It does indeed, I had no idea. The news article I read about the AI 400 claimed it was the first. Or maybe it's just the first to qualify for the Copilot+ sticker, as the 8000G NPU seems to only do 16TOPS, while Microsoft mandates at least 40TOPS.",AMD,2026-01-11 10:55:15,7
AMD,nytdp25,"Fair enough. I think you work with what you've got. When I was in university I crashed my car and sold my GTX 1080 and played games on a 5770 for 6 months or so and tbh it was fine, I just accepted that it'd be low settings and older games. There's lots of games out there that will run on anything.",AMD,2026-01-10 17:12:55,5
AMD,nyskl7r,"The hell does SAM, or whatever ""smart access video"" is, have to do with that???",AMD,2026-01-10 14:48:49,2
AMD,nysmxyu,No. We are asking if it could have something similar processing some day..,AMD,2026-01-10 15:01:41,1
AMD,nythegn,Like I told the other guy Microsoft has already hinted that **DirectML + NPUs** will enable *hybrid GPU/NPU* upscaling pipelines.  So yes there will be some similar type processing.,AMD,2026-01-10 17:30:46,0
AMD,nyte03c,I've seen people do framegen on the iGPU so as not to burden the discrete gpu with it.   Whether it's actually worth it's anyone's guess.,AMD,2026-01-10 17:14:23,0
AMD,nysujtn,"No. That would just be a separate engineered die.   Ryzen desktop series are chiplet based. Thereâ€™s a cpu chiplet connected to an i/o die chiplet on an am4/am5 socket. Note that zen4/5 include a small 2 CU gpu in the cpu die chiplet for basic video output purposes.   Ryzen mobile series are monolithic, no chiplets. Its just one single chip with up to a 12 CU gpu in it. All G series desktop chips are just mobile chips repurposed on a desktop socket.   https://en.wikipedia.org/wiki/Zen_4  You can see it in the wiki. The G series share the same ryzen product code name as mobile. â€œPhoenixâ€",AMD,2026-01-10 15:41:30,8
AMD,nysr3vb,"No, it's a laptop die that they put on a sockst substrate and repackaged. 3400G = 3700U, 4700G = 4800U, 5700G = 5800U, 8700G = 8840U",AMD,2026-01-10 15:23:51,13
AMD,nyt6jcq,"No, though they typically have half of the L3 cache of their desktop counterparts.",AMD,2026-01-10 16:38:58,3
AMD,nyt9pha,"Yep I've considered those, but I really wanted something that goes into an AM5 socket so I can upgrade the CPU in future.",AMD,2026-01-10 16:53:53,2
AMD,nytd5b6,The 7950X cpu is relatively expensive (~Â£475) it's also end of life so none of the reliable retailers have stock it's all 3rd party sellers. The 7945HX (cpu+mobo) are about Â£350 from minisforum.,AMD,2026-01-10 17:10:19,1
AMD,nz14igd,"That's correct. I've got a 8700G and that was touted as 'the first desktop CPU with a NPU' back at launch (8600G is the weaker sibling, also has a NPU and was launched at the same date). Combined TOPS of 39, which is suspiciously just 1 short to qualify for Copilot+... Doesn't matter though, since I'm using Linux anyway ðŸ™‚",AMD,2026-01-11 20:05:02,2
AMD,nysmgxx,nothing at all. They're just confused,AMD,2026-01-10 14:59:08,16
AMD,nyt977s,"Good lord man, its too early on a Saturday morning to be beefing on reddit over nothing",AMD,2026-01-10 16:51:29,3
AMD,nysu8h7,"Smart Access Memory is AMD's term for Resizable BAR,  which allows the CPU to access the GPU's VRAM directly.  What they're probably question is whether the igpu on whathever this Gorgon Point/Halo series of APU can access the dgpu's RAM on top of the DIMMs available to the CPU socket.",AMD,2026-01-10 15:39:53,3
AMD,nytkizg,Similar in what way,AMD,2026-01-10 17:45:41,2
AMD,nytevg0,Itâ€™s still not handled by the NPU tho,AMD,2026-01-10 17:18:36,5
AMD,nzec55s,"Great explanation. Especially the monolithic vs. chiplet design is important to note, physically they are totally different.  I've got a passively cooled desktop system and for that purpose these G parts are also a great fit. Due to the monolithic die the idle power consumption is notably lower, which is what you want when you need maximum efficiency for limited cooling capacity.",AMD,2026-01-13 18:46:01,1
AMD,nysra9j,"Oh wow, nice to know",AMD,2026-01-10 15:24:47,3
AMD,nytvnhq,interesting  I did not know,AMD,2026-01-10 18:37:19,1
AMD,nytipiv,smart access video is hybrid processing..,AMD,2026-01-10 17:37:04,-5
AMD,nyt9jb6,"let me have my fun, it's night time",AMD,2026-01-10 16:53:04,3
AMD,nysvdgi,"No it just allows the buffer to be **resizeable** the access remains the same.  And no they did not mention the iGPU once, they said NPU. And even if it could, what would the benefit be? Games render using one GPU or the other, not both. So why access an inactive GPU's VRAM data? Even if you could write to it like extra memory, that defeats the entire point of VRAM, it's just far far too slow, it would kill performance.  So again what are THEY talking about?",AMD,2026-01-10 15:45:34,5
AMD,nyti1nf,"BAR = Base Address Register.  In short, it's the address range of system memory that gets mapped to graphics memory.  Which part of graphics memory it points to is changed as required.  The size of the register is relatively small by default, because the system address range it occupies is no longer accessible as normal RAM, and with a 32-bit system that only has 4GB of address space, reserving a large chunk of that space for mapped graphics memory will impact system performance.  Resizable BAR just means you can change the size of that register, so that you can write to more of the graphics memory at once, without changing the mapping.  With a 64-bit operating system, you have plenty of address space that doesn't go anywhere near physical memory to work with, so you can map the entire size of the graphics memory to a system address without causing any issues.  AMD's SAM *uses* resizable BAR functionality, but is not simply their label for it.  There's a bit more going on in what they do with it, which is why AMD cards designed with SAM in mind gain more than other cards that merely encounter a larger BAR.  An exception can be made for modern Intel cards, which were designed with a large BAR in mind, and are actually crippled with a small one.  This is more them being lazy on implementing VRAM access properly for a small BAR than them gaining performance from a large BAR.  In all cases, the graphics memory is accessed ""directly"".  It's only a question of how large the mapped address area is, and where in VRAM it's pointing at any given moment.",AMD,2026-01-10 17:33:52,2
AMD,nytjl7w,"Not yÃ©t, however DirectML will supposedly enable this.",AMD,2026-01-10 17:41:13,1
AMD,nytjuly,Between a dGPU and an iGPU specifically their hardware video encoding/decoding  nothing to do with ai or games or the npu,AMD,2026-01-10 17:42:26,6
AMD,nywbuq2,Wtf does that even mean? CPU is already packed with silicon dedicated for hardware decoding.,AMD,2026-01-11 02:06:10,1
AMD,nysyg3t,"Everything that can benefit from ai, like upscaling. Microsoft has already hinted something like this NPU+GPU collaboration.",AMD,2026-01-10 16:00:37,-3
AMD,nytkhds,So nothing to do with your original comment then?,AMD,2026-01-10 17:45:28,4
AMD,nytdd54,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-10 17:11:21,1
AMD,nysz930,"Can you be lucid for one second and explain what this has to do with the entire discussion or ""smart access whatever"" is?",AMD,2026-01-10 16:04:31,2
AMD,nytl2z5,Could you describe in your own words what 'hybrid processing' means?,AMD,2026-01-10 17:48:18,1
AMD,nyt49zo,Eh you are asking to be lucid to something that you do not know whatever is?,AMD,2026-01-10 16:28:24,-2
AMD,nytl68d,describe in my own words what....?,AMD,2026-01-10 17:48:44,3
AMD,nyt5fx8,Dude you are not writing coherently,AMD,2026-01-10 16:33:49,2
AMD,nytlfa1,what 'hybrid processing' means.,AMD,2026-01-10 17:49:55,2
AMD,ny3pvyi,That aside what happened to the rumored 9950X3D2? That did not materialize correct? Just the 9850X3D?,AMD,2026-01-06 23:40:14,102
AMD,ny3teoh,amd trying to not have a dogshit naming scheme challenge (impossible),AMD,2026-01-06 23:58:34,121
AMD,ny3gk6z,Another year another AMD Marketing fuck up,AMD,2026-01-06 22:52:29,75
AMD,ny3q9zd,So the G series APU's are dead?,AMD,2026-01-06 23:42:17,29
AMD,ny42lt1,"AI laptops... AI processors... urgh, make it end already. :(",AMD,2026-01-07 00:45:58,33
AMD,ny5n8cf,Just wait until Zen 6 and we get the Ryxen AI 7 580X3D.,AMD,2026-01-07 06:29:58,8
AMD,ny4mq5q,So it sounds like the 400G (formerly thought to be 9700G) is basically a refresh of the HX370 mobile chip with a little more than double the default TDP.  I wonder if that means an increased GPU frequency (both 8700G and HX370 are at 2900MHz).,AMD,2026-01-07 02:34:43,6
AMD,ny685xb,i want AMD CPU which has 16 core with at least 50 TOPS NPU and 128MB 3D V cache,AMD,2026-01-07 09:38:03,2
AMD,ny6o1c8,Iâ€™m a fan of the idea of edge aiâ€¦ but I feel like no one has actually made anything official for it. Why bother with the hardware when the software rn is basically webcam stuff,AMD,2026-01-07 11:53:48,2
AMD,ny5o0bx,Ryzen AI for desktop is nice. I would love a CPU with both 3d vcache and an NPU.,AMD,2026-01-07 06:36:24,1
AMD,ny6rqpy,"Wow, the processor literally no one asked for.. Thanks AMD..",AMD,2026-01-07 12:20:33,1
AMD,ny96w55,"Words cannot describe how much I hate the fact we're being forced into having this garbage in every piece of software and on every piece of hardware. I don't want copilot in my word processor, I don't want Amazon Q in my IDE, I don't want Gemini in my search results, I don't want AFMF or FSR in my drivers, and I don't want NPTs in my processors.",AMD,2026-01-07 19:25:58,1
AMD,nya3331,Title should be:   There will be no AM5 option for a Zen5 Apu.   Because AI400 is FPU8 only,AMD,2026-01-07 21:45:00,1
AMD,nyc8xf2,"darn, i was really hoping to get a better 7 8700G type cpu   now with ddr5 and graphic cards getting so insanely priced.    if this 400G will not run on most 7000/8000/9000 mobos   then what desktop pc will bother hosting such a chip?     personal AI makes no sense for consumers, as it won't   buy you more privacy, and performance will always be   cheaper by cloud subscription versus buying hardware.    https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/    i think AMD has misjudged where market needs will go.   there will be too many PC buyers who can't afford both    ddr5 ram and a mid range GPU card with 16gb ability,     so if you gave them a CPU+iGPU at a premium price,     it saves on the card buy, so they will buy more  ram,   and bonus; that PC will consume less energy (less co2).    amd needed to give us an 9700G - and they blew it.",AMD,2026-01-08 04:23:29,1
AMD,nyem6nr,So what was MSI up to?    [https://videocardz.com/pixel/msi-releases-agesa-pre-1-3-0-0-with-microcode-update-for-upcoming-ryzen-9000g](https://videocardz.com/pixel/msi-releases-agesa-pre-1-3-0-0-with-microcode-update-for-upcoming-ryzen-9000g),AMD,2026-01-08 14:45:22,1
AMD,nygk3bf,This means that the other cpu's are gonna be cheaper and available? Right? Right?,AMD,2026-01-08 19:54:16,1
AMD,nyh4gm7,![gif](giphy|g6Oox9mVXakOCWSkf9)  Copilot+ focused processors,AMD,2026-01-08 21:24:55,1
AMD,ny4abhg,Yokatta i didnt wait for it,AMD,2026-01-07 01:27:25,-1
AMD,ny3xu0w,Probably canceled to two limited to no competition there's no need to release it.,AMD,2026-01-07 00:21:20,65
AMD,ny41jgi,According to computerbase.de AMD comfirmed it: https://www-computerbase-de.translate.goog/news/prozessoren/x3d2-bestaetigt-der-amd-ryzen-9-9950x3d2-mit-doppeltem-3d-v-cache-kommt.95665/?_x_tr_sl=de&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp,AMD,2026-01-07 00:40:26,11
AMD,ny50gqt,They just didn't announce it.   They probably want a softer announcement of the 9950x3D2. Because it won't make games run faster.,AMD,2026-01-07 03:51:54,5
AMD,ny3q2mg,Moore's Law is Dead channel lol,AMD,2026-01-06 23:41:12,12
AMD,ny5hact,[Stay tuned.](https://www.pcworld.com/article/3024786/amd-qa-at-ces-2026-we-talk-ryzen-and-more.html),AMD,2026-01-07 05:43:33,2
AMD,ny443xp,"If I were to guess, why charge a lower amount now when AMD can charge a higher amount later. And probably, even the 7800x3d is faster than the competition in gaming so AMD just doesn't have competition when it comes to gaming.",AMD,2026-01-07 00:53:49,1
AMD,ny7cw6f,9950X3D2 will be a weird product. It will have same gaming performance and need CCD parking just like 9950X3D. Adding cache to the second CCD does not help the cross CCD latency.  It only does CPU render and other ram bottlenecked compute workloads faster. And those workloads runs much better with a threadripper or EPYC.,AMD,2026-01-07 14:25:13,-2
AMD,ny4ekl9,"It's basically the whole industry right now, intel 300 pantherlake series cpus are just as confusing as amd' cpus",AMD,2026-01-07 01:50:39,33
AMD,ny4xqyj,Is it? This is successor to the APU 300 series. 400 seems like a rather logical decision. This isnâ€™t meant succeed the CPU 9000 series.,AMD,2026-01-07 03:35:51,5
AMD,ny3wd2j,not for them its not. now they can charge more,AMD,2026-01-07 00:13:47,-7
AMD,ny3st6q,"Naming scheme yes, CPUs probably not.   Remember, it's AMD. Their marketing people generally cannot allow themselves to have a consistent naming scheme for more than 2 generations. Plus there probably was an order from the top to integrate ""AI"" somehow so it's even more visible.",AMD,2026-01-06 23:55:30,55
AMD,ny3vv0t,"Aren't they just moving to most everything having an iGPU, like Intel?   That was kinda the whole reason for the G, I thought...",AMD,2026-01-07 00:11:12,3
AMD,ny5x3bf,When AMD puts laptop apus onto a am5 socket they will just use the laptop chip name instead of making up a new half generation line. Last generation they had 7000 and 8000g. It looks like they ran out of numbers and could not do 10000g after ryzen 9000,AMD,2026-01-07 07:54:54,1
AMD,nycew2i,"AI just means it has an NPU, there will likely still be budget SKUs without.",AMD,2026-01-08 05:01:34,1
AMD,nyd24cl,"I'm currently using a 3400G Vega 11. My next upgrade, once RAM is cheaper, would be the top-of-the-line G series. Are integrated graphics going to disappear? I'm confused.",AMD,2026-01-08 08:01:53,1
AMD,ny4a1wi,Who's going to buy them when the RAM costs more than the CPU?,AMD,2026-01-07 01:25:59,15
AMD,ny6ct3h,"Funny thing is [Dell of all companies have started pointed out how marketing PCs as AI is backfiring](https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/).  The problem is these brandings aren't for consumers, they are for shareholders - because shareholders, despite having a lot of money, are all monkey brained idiots who get dopamine hits every time they see the word AI.",AMD,2026-01-07 10:20:36,6
AMD,ny6bvhz,"Ryzen has been around for too long, AMD has to find a new name for Zen 6 !",AMD,2026-01-07 10:12:05,1
AMD,ny6ni86,"The only AMD chip that comes to my mind that would fit those requirements (according to rumours) is Medusa Halo, expected to be released in 2027.",AMD,2026-01-07 11:49:53,2
AMD,ny5rwpf,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-07 07:08:56,0
AMD,nycnni6,"they dont care, and they wont waste a waffle to make low market chip.",AMD,2026-01-08 06:02:45,1
AMD,ny4mt2i,Maybe it will be another Microcenter exclusive.  :-),AMD,2026-01-07 02:35:10,21
AMD,ny4w6o5,It's a bizarre decision to cancel something like that because of competition when there wasn't competition in the first place.,AMD,2026-01-07 03:26:53,9
AMD,ny44u7y,This is highly likely. AMD simply is too good and they are waiting and delaying and milking current X3D options. The minute Intel has something remotely close youâ€™ll see new releases.,AMD,2026-01-07 00:57:44,14
AMD,ny3t8ks,"the existance has been known for awhile. private overclockers have been known to have had engineering samples of them i think reported on GN, as well as the cpu had actual benchmark leaks for an [ID existance](https://overclock3d.net/news/cpu_mainboard/amd-ryzen-9-9950x3d2-cpu-benchmark-results-leak/).  Not all products are released, but its hard to deny they didn't exist.",AMD,2026-01-06 23:57:42,28
AMD,ny4wgoq,"With how inaccessible RAM is becoming to consumers, I'd sooner believe it's canceled because it's a niche, expensive product that people are going to be priced out of. They can't sell it for more later if RAM keeps climbing to a point where no one considers buying DDR5-based PCs worth the cost.",AMD,2026-01-07 03:28:29,2
AMD,ny8mjek,"Yes and no  Intel is making NEW chips.  AMD is, as per usual, literally just renaming their old shit",AMD,2026-01-07 17:57:25,8
AMD,ny6uetr,"Because they named the previously mobile to desktop series the 8000 lineup.  I'm guessing this is also how AMD is going to rename the next Zen6 desktop series then, making it the 500 AI series.",AMD,2026-01-07 12:38:53,8
AMD,ny4ctrb,"Yeah end-users and consumers sure love... ""AI"" plastered all over everything. Especially when OpenAI and co. are why everything costs too damn much.",AMD,2026-01-07 01:41:10,15
AMD,ny3vtf6,"Job security for their marketing team lol, needless and confusing name scheme changes every so often ðŸ˜…",AMD,2026-01-07 00:10:58,17
AMD,ny6gkcb,">  Plus there probably was an order from the top to integrate ""AI"" somehow so it's even more visible.  *AIMD RAIDEON 9 AI MAIX x400xt (AI+ edtion)*",AMD,2026-01-07 10:53:42,8
AMD,ny79n0u,"Tbf the strix point architecture is so leaps and bounds ahead of the 8000s laptop APUs they might as well rebrand, 8060S has frankly ridiculous performance when compared to previous gen iGPUs",AMD,2026-01-07 14:07:59,2
AMD,ny3wr9o,"The G naming scheme has existed since zen+, so I dunno wtf youâ€™re yapping about. Ryzen AI has also been a thing for years now.",AMD,2026-01-07 00:15:49,2
AMD,ny6ziwr,Isn't this almost every computer part/peripheral manufacturer and in general a lot of companies that do this to confuse the consumer to make them take bad choices.,AMD,2026-01-07 13:11:04,1
AMD,ny4iozh,"G is supposed to be a much better iGPU, the main 7000 series desktop has 2CU RDNA2 while the 780M in the 8700G is 12CU at a higher boost clock. They're usually refered to as APUs rather than iGPUs.",AMD,2026-01-07 02:12:53,21
AMD,ny5coz1,"G = gaming, otherwise the non-g 7000 processors would also be G because they have graphics",AMD,2026-01-07 05:10:33,1
AMD,nyfjbuf,you'll be lucky to get a 5600GT if AMD doesn't start producing new AMD Renoir processors (mainly used for laptops),AMD,2026-01-08 17:14:40,2
AMD,ny4ao9k,"Individuals no, but businesses on fixed upgrade cycles and people who desperately need one would just pay the price anyway.",AMD,2026-01-07 01:29:21,8
AMD,ny5nrc3,"I will, I bought RAM before the price hike. The only thing left is to either get the 8700G or this 400G. I'm building a living room PC in a 21x21x11cm case and I want to be able to play games locally while also be able to stream more demanding games onto it from my actual gaming PC",AMD,2026-01-07 06:34:20,1
AMD,nya87hi,If it is just shows they couldn't produce them in any type of quantity.   Counter to the reddit hiveminds beliefs Microcenter doesn't even serve 1/4th of the United States.   The fact they don't ship makes it worse.,AMD,2026-01-07 22:07:30,3
AMD,ny4qtia,"Microcenter my beloved, just picked up an open box 7800X3D from them today.",AMD,2026-01-07 02:56:54,5
AMD,ny79dxe,self-competition is a real thing. If you sell 8 variations of the same thing you're increasing your overhead by managing a bunch of SKUs and reducing sales by confusing customers,AMD,2026-01-07 14:06:36,3
AMD,ny43wyg,So many people don't realize this. Remember when the roadmap for an Nvidia board partner leaked shortly after the 30 series launch and the 20gb 3080 was on it? It never actually materialized but recently someone posted an engineering sample of it.,AMD,2026-01-07 00:52:48,16
AMD,ny5nm6q,There are plenty of AM5 owners who already own what they need and will upgrade,AMD,2026-01-07 06:33:08,1
AMD,ny7dkxq,We will see people upgrading from Ryzen 7 5800X3D to Ryzen AI 580X3D.,AMD,2026-01-07 14:28:49,5
AMD,ny8mwrm,7000 -> 8000 -> 200  renaming bullshit to trick consumers    Now 300 -> 400 more renaming bullshit.   AMD isn't releasing anything relevant in the mobile space so they just keep renaming shit. There is small wonder intel is simply superior in every aspect of mobile computing   ... now if only they can put out a game focus desktop chip they will stop losing buyers there,AMD,2026-01-07 17:59:01,2
AMD,ny4kcnm,"I bought 32 GB DDR4 ram in 9th of June last year for 101 e. Just checked the price of that exact same kit from where I bought it and it's now 303 e. Really drove home how insane the prices are getting.  Also looking at price charts, it seems my timing was pretty good, since price for DDR4 shot up in mid June and has kept rising since. So, I probably should also mention, I ordered a new GPU on Sunday.",AMD,2026-01-07 02:21:48,6
AMD,ny74k9o,Getting close to going with an actual r/ayymd CPU name,AMD,2026-01-07 13:40:06,3
AMD,ny6d181,The thing is standard businesses aren't buying products *because* they have AI branding. AI branding is just tech bro circlejerk bs designed to give the venture capitalists their dopamine hit so they give the company more money to continue this charade.,AMD,2026-01-07 10:22:39,4
AMD,nyjlhnl,"However, that's also what allows them to avoid the scammers when new products launch and to give their deals.",AMD,2026-01-09 05:10:16,1
AMD,nyaf37g,All of which I'm OK with.,AMD,2026-01-07 22:38:49,-1
AMD,ny4uk73,"I got a 7600X3D the other week for my living room gaming box.  It will no doubt still be GPU limited, but it was pretty inexpensive for an AM5 chip.",AMD,2026-01-07 03:17:42,3
AMD,ny7nysf,Managing a bunch of SKUs and confusing customers has basically been AMD's entire mobile strategy.,AMD,2026-01-07 15:20:40,3
AMD,nybzd8d,and pull out their rx 580 for memes,AMD,2026-01-08 03:27:44,2
AMD,ny4m8hu,"Redid my comp just this past week (9800x3D, 32GB RAM, etc.) and helped some family on that front with redoing hardware or picking out new laptops. Prices are higher than I like but some of the builds were going on their 8-9th year of operation. Better to eat the cost of DDR5 now when it's at local stores for about 200-300, than be forced to eat it later in the year when stuffs more apocalyptic. The 5800x3D I swapped out in my build already sells for more than a new 9800x3D on ebay judging by listings.   Some friends are also swapping their GPUs while they are still MSRP.",AMD,2026-01-07 02:32:03,2
AMD,nykcgww,Not necessarily a bad thing. I tried playing with the power limit set to the minimum on my 9070XT and I was surprised how little performance it lost. And the card was dead quiet while only drawing 200W or so.   Hopefully they will put a decent heatsink on it so that it will be silent.,AMD,2026-01-09 08:51:40,22
AMD,nyjlzkk,"9070XT with a 9070 BIOS or just 9070, -20% power limit, put in an x4 slot/mode",AMD,2026-01-09 05:13:38,30
AMD,nyjo057,Why not just use a 9070 instead?,AMD,2026-01-09 05:27:36,32
AMD,nylmjul,"My 9070XT is amazing at -25% PL and -40 undervolt. Like, almost dead quiet at arms length during heavy gameplay. That's a 3 fan model, tho.  I wonder what a skinny 2 fan can do at 180w. Probably not even close to a full TDP 9070.   Can't help but wonder if they can put this GPU in a steamachine like desktop thingy.",AMD,2026-01-09 14:24:21,7
AMD,nykeg7d,Meanwhile I want a 500W+ BIOS bc I'm on water.,AMD,2026-01-09 09:09:48,9
AMD,nyn1ce8,Yeah looks like a size constrained limitation  Theres probably no AC to 12V 300W PSU in that size range,AMD,2026-01-09 18:16:43,1
AMD,nyjpbkb,"I'm guessing you could get slightly better performance with the ""more slower cores"" approach than ""faster but fewer cores""?.  Sounds like a waste of a premium chip, though.",AMD,2026-01-09 05:37:06,34
AMD,nysykf5,"My Asus ROG NUC 2025 houses a 140W 5070 TI mobile that handles it fine. Within it is also a CPU that can boost up to 120W as necessary.   GPU's don't need to be as large as they are, there's more than enough space for heatsinks to run that GPU at 180w.   I doubt it'll be worth the price, the use cases for these things are the niche of niches at such a price point.",AMD,2026-01-10 16:01:12,3
AMD,nyoo98n,you got a boat? thatâ€™s sick bro I want to be on the water too,AMD,2026-01-09 22:48:21,6
AMD,nynyis9,This. I'm really hoping someone can figure out how to modify existing 9070 XT BIOSes to increase the power limit more than 10%.,AMD,2026-01-09 20:47:10,3
AMD,nyroi35,https://www.overclock.net/threads/increasing-rdna3-rdna4-desktop-class-power-limits-and-adding-vid-offsets.1816083/unread,AMD,2026-01-10 11:07:07,2
AMD,nyk2wy9,"Yup, you are correct on both fronts.",AMD,2026-01-09 07:26:16,14
AMD,nyl8jtu,9800X3D + some mhz or whatever.  Good lord hardware releases are boring right now.,AMD,2026-01-09 13:08:32,30
AMD,nyo8nvs,"I imagine it looks a lot like a 9800X3D, except it says 9850X3D instead, since you know, it's exactly the same CPU, just binned. What even is this? What happened to journalism?",AMD,2026-01-09 21:34:13,10
AMD,nyzf1nd,Iâ€™m still waiting to hear if the 9850X3D will support PBO and/or overclocking. If no then the gap to the 9800X3D is effectively even less.,AMD,2026-01-11 15:21:13,1
AMD,nylwomx,"Do new sites even do basic thinking these days? Fake as hell, there's no way the IHS says 2024, considering AMD transitions that date super fast, with the 9950X3D, manufactured in the second week of January 2025, already having a 2025 date on the IHS. The real 9850X3D will have a 2025 date at least, if not a 2026 one.",AMD,2026-01-09 15:13:40,0
AMD,nyfbcpw,"Bro thinks hes god, what a loser.",AMD,2026-01-08 16:40:07,-35
AMD,nyoujjj,"That's what happen when one company dominate a certain market. Just like when Intel released their quad cores CPU years after years. Now it's AMD being successful with the CPU market, Intel really lost their customers after multiple CPU issues.",AMD,2026-01-09 23:20:54,11
AMD,nyu686v,Its how the CPU Market was from like 2011 till Ryzen launched in 2016.,AMD,2026-01-10 19:27:21,5
AMD,nz6tzjo,Strangling enough it also looks like a 9600x,AMD,2026-01-12 16:50:10,1
AMD,nz4yzg3,why wouldnt it,AMD,2026-01-12 10:03:27,1
AMD,nyn8qns,?  All of the retail samples of the 9950X3D have a 2024 year date printed on them.,AMD,2026-01-09 18:49:13,1
AMD,nz50yj3,If they have pushed the silicon to the absolute limit already it may have no headroom left.,AMD,2026-01-12 10:21:53,2
AMD,nytqdlh,"Exactly, it's not the manufacturing date but the trademark date.",AMD,2026-01-10 18:12:58,1
AMD,nz2uik4,A lot of cheaper prebuilt desktops are actually reused laptop boards in a desktop case,AMD,2026-01-12 01:06:08,7
AMD,nyz40g8,"I really wish everyone would just stop buying this bullshit entirely because that would make it blow over quicker, but Lord knows someone will",AMD,2026-01-11 14:22:20,11
AMD,nzarn7k,"Close enough, welcome back R7 260",AMD,2026-01-13 04:39:08,3
AMD,nzc1j5o,"LOL, I guess Asus is literally bringing MoDT back",AMD,2026-01-13 11:15:37,2
AMD,nzcd6i8,"I really like this PC it is a PS5 Pro but better especially the CPU being Zen 4 instead of Zen 2, the number if USB ports is neat to.  There is of course some down side such as non standard PSU and motherboard form factor, the PCI gen 4 by x8 could be a problem considering the PSU limits you to something like XX60 series GPUs?  Future GPUs will be on gen 6 or gen 7 by x8 will probably be best match for it and that could be a huge ask!  Upgradeability is storage, RAM and GPU barely...",AMD,2026-01-13 12:44:54,0
AMD,nz07zgb,You can buy something much worse than this easily,AMD,2026-01-11 17:39:40,1
AMD,nyjbqg6,"So they tried to NOT release it for desktops to save their video card business and then the Chinese took the Strix Halo and put it into mini pcs which meant the big computer companies lost out to selling them to consumers. Â Now they are making desktop chips because everyone is upset with them. At least it will decrease the current rip off prices for the Strix halo mini pcs in the future, albeit distant futureâ€¦",AMD,2026-01-09 04:09:08,-11
AMD,nyjh5yx,I donâ€™t think they refused to sell to mainstream oems. I think nobody wanted it for the price they wanted to sell them for. Hence they had to add the 388 and 392 to make it cheaper but still 40CU. It also canâ€™t be sold as an AM5 cpu because itâ€™s not compatible.,AMD,2026-01-09 04:41:55,23
AMD,nykc4w7,Strix Halo can't work on the desktop because it has a 4 channel memory controller. These are just embedded parts with ECC and additional certification and testing.,AMD,2026-01-09 08:48:38,9
AMD,nyc130l,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-08 03:37:21,1
AMD,nyd2351,"""Confirmed""  ""post flaired as a rumor""",AMD,2026-01-08 08:01:35,191
AMD,nycenjb,Someone educate me but does have dual 3d v cache actually improve gaming performance or is it just for productivity use?,AMD,2026-01-08 05:00:04,45
AMD,nyc9lxc,Excellent,AMD,2026-01-08 04:27:45,15
AMD,nycb2mc,Wouldnâ€™t the latency across the two CCDs outweigh any benefits from being able to use all 16 cores in games?,AMD,2026-01-08 04:37:06,26
AMD,nz0vxqr,"It is not coming.  Turns out it was an engineering prototype undergoing testing. They concluded it was not worth the extra costs because of very little to no improvement in performance.   But we could expect a dual CCD cache CPU in Zen 6 when they fix the latency between CCD's. In other words, it is not real.",AMD,2026-01-11 19:26:30,2
AMD,nyd2nhp,ill be honest. I think we are not gonna see any difference in games. you want everything one die/x3d cache anyways since the latency between the two ccd would hurt fps if you did end up using cores or cache from both.,AMD,2026-01-08 08:06:40,2
AMD,nycqmo1,This is the last piece I'm waiting for to complete my new PC. It's going to be a large jump from R5 5600 for my simulation workloads for sure.,AMD,2026-01-08 06:25:26,2
AMD,nydefzz,Didn't AMD tell people at CES this IS NOT happening when they asked.#  I believe AMD more.,AMD,2026-01-08 09:54:43,1
AMD,nyhtnwu,"Iâ€™m stupid, can anyone tell me if this is any reason to immediately ditch my 9800x3D?",AMD,2026-01-08 23:20:40,1
AMD,nyi7wm0,"Id like to see a test of a game running on one ccd, and then split it across both ccds",AMD,2026-01-09 00:33:54,1
AMD,nyksc3s,i bought 9950x3d . did I waste my money?,AMD,2026-01-09 11:13:57,1
AMD,nyz5o4f,This confirming is just as weak or even weaker than the Intel B770. It makes no sense. The fact they did not announce it on CES should tell us that the 9950X3D2 is not real.,AMD,2026-01-11 14:31:37,1
AMD,nz9fjoc,And AM5 issue with the X3D isn't fixed yet...,AMD,2026-01-13 00:14:24,1
AMD,nz9hk0d,"I have a 9800X3D, and i want one of these 9950X3D2's so bad.  I know it will make barely if any difference to games, and i dont really need it at all.   But i still want it so bad. 16 cores, all that cache.",AMD,2026-01-13 00:25:07,1
AMD,nyd3334,"Can AMD just release dual x3D right away at launch instead of trying to milk us while creating potentially CPU shortage with AI boom wasting resources thanks, alto i am glad i did not wait and got 64 GB of DDR5 for only 239 however i am not looking forward to the potential of more hardware shortages in the future especially CPU shortage.",AMD,2026-01-08 08:10:30,1
AMD,nydjrb6,"Wouldn't this be limited by the infinity fabric bandwidth and latency though? It will not be worth it for one CCD to try and access the L3 cache of another CCD. They are solving this for Zen 6 though, apparently, so that's something to look out for.",AMD,2026-01-08 10:41:45,1
AMD,nychsst,I'm genuinely not expecting any serious performance improvements,AMD,2026-01-08 05:20:56,-3
AMD,nycwx6t,I'm waiting on 4k performance benchmark with this and potentially the 12 core ccd....  I'd be willing to buy the new chip and have the 9800x3d become my livingroom build lol,AMD,2026-01-08 07:16:35,-1
AMD,nye32zf,So it's 6D then?,AMD,2026-01-08 13:03:13,-1
AMD,nyd0zyf,Now I just need to win the lottery so I can afford the DDR5 ram,AMD,2026-01-08 07:52:08,0
AMD,nyeq8up,"Does this CPU have a quad-channel DIM/Memory Controller? That's my main very luxury problem right now. I have 4x32GB DDR5 ram, and it's hard to get it stable at the target speeds with my current motherboard/CPU(Z790, i713700k). From what I've read AM5 has the same problem.",AMD,2026-01-08 15:04:57,0
AMD,nydd9qi,How do people not understand that this doesnâ€™t fix any gaming related problems? The best case scenario will be to turn off the worse CCD.,AMD,2026-01-08 09:44:02,-2
AMD,nyd2zj7,confirmed to be a rumour,AMD,2026-01-08 08:09:38,72
AMD,nyxd9pu,Benchmarks already made.  [https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+9950X3D2&id=7115](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+9950X3D2&id=7115),AMD,2026-01-11 05:48:21,1
AMD,nyd3pxz,"It will improve, but not by an appreciable amount, and wonâ€™t perform significantly better than the 8core X3D part.   As for productivity, some benchmarks might see some gains. But youâ€™ll find a few edge cases where the performance would skyrocket just because some memory bound dataset dealing with random data happens to get an unexpectedly high hit rate on the L3.   The majority of them will however perform near identically. Games are typically â€œLow IPCâ€ workloads. Not much stuff being done, but it needs to react to things fast.  But productivity software is â€œHigh IPCâ€ and the optimization targets in the software and even the compilers simply arenâ€™t designing for CPUs that have this much L3 cache.   They know what a normal CPUs latencies are, and ensure their software is always able to keep the average CPU engine fully fed never waiting for data.  AMD offers some X3D server parts for certain workloads. And they have X3D on all the CCDs. But they arenâ€™t nearly as popular as the classic ones.",AMD,2026-01-08 08:16:12,31
AMD,nychjrt,We'll have to wait and see in the benchmarks. My initial guess is there _may_ be improvements at 1080p if there are games that can benefit from that many cores but for 4k I doubt it'll matter since we're always GPU limited.,AMD,2026-01-08 05:19:13,12
AMD,nyez71j,It will not improve gaming performance unless you are talking about city skylines 2.  This is a bandaid product for productivity as Zen 5 MSDT were all handicapped by the limited RAM bandwidth.,AMD,2026-01-08 15:46:11,2
AMD,nye4gnb,"One thing seems fairly certain about dual x3d CCDs, its going to be a toasty boi and (less certain but highly probable) wont be as fast for gaming as the 9850x3d",AMD,2026-01-08 13:11:29,1
AMD,nyfsf40,"In theory yes, in ""real"" situations probably not.  For gaming, you already have enough cores with 3d cache on a 9950x3d, same for (general) productive work, we don't know about the latency between both cdcs yet.  What will gain a massive amount of performance is stuff that is built to use the L3 cache because now you have double the amount of cdc then before.  Still probably better to wait for ZEN6.",AMD,2026-01-08 17:54:29,1
AMD,nyi893p,"It will improve gaming performance in all of the games where, at stock, the 9800X3D out-performed the 9950X3D.  The ""at stock"" means in the absence of something like *Process Lasso* to ensure the game only ran on the CCD with V-cache, which erases the deficit already.  Outside of that scenario, there are few games which will run faster on the 9950X3D2 over a properly scheduled 9950X3D.  At least right now.",AMD,2026-01-09 00:35:40,1
AMD,nykifxt,"Some games that can utilize it will improve significantly, something like Star Citizen I imagine would love the extra cache.  Other games will improve immensely compared to the 50/50 x3d-chips like the 7900x3d simply because their OS can't mess up the core-parking since both chiplets are x3d now.  Still other games will have minor improvements from the small uplift in boost clock.  If you're into pure productivity use, a non x3d-chip is probably the way to go as it clocks higher and will chew through serial tasks faster. Maybe it makes a difference if you're doing 3d-art stuff, not sure.",AMD,2026-01-09 09:46:56,1
AMD,nycufzw,It's for nothing basically.. just for some niche apps,AMD,2026-01-08 06:55:55,-7
AMD,nycdp8d,"It will be interesting to see if there are any gaming benefits, but I'm sure there are applications that will use all that sweet cache. Probably rendering and handbrake for a start. 9800x3d, or rather 9850x3d now might still be the best gaming cpu.",AMD,2026-01-08 04:53:53,14
AMD,nycdyck,"Yes, but there's a couple ways to get around it. Besides there's some people that keep asking for it, and it will make an interesting baseline for the next couple CPU gens.   Rumors exist future architectures may package the CCDs closer together to mitigate the latency. Plus we are getting more cores per CCD.",AMD,2026-01-08 04:55:33,8
AMD,nydqrhk,"Yes. This is only a problem because people talk as if X3D is black magic. It just extra L3 cache. The vertically stacked ""3D"" part is just how they're able to squeeze that more cache onto the die.  L3 cache is orders of magnitude faster than even RAM, so any time the CPU can get the data it needs from cache instead of even having to go out to RAM, you get better performance. This applies to everything a CPU does, as well, not just gaming. Gaming simply benefits greatly because, as a workload, it tends to use a lot of the same data over and over again, which means you get a lot of cache hits. A bigger cache means more data can be stored and, the chance of a cache hit versus a miss is greater, as a result. That's it. The entire secret sauce to X3D.  The reason already that the second CCD is disabled is because of the two pools of cache are separate. If a thread on CCD0 requests data, it's added to the the cache on CCD0. If a thread on CCD1 requests the same data, that's not in *its* cache, so you either call it a miss and move on or cross over the fabric to check the cache on CCD0. That instantly slows it down, though, to the point where you might as well have pulled it fresh from memory. Therefore, the way to extract the most performance is to make sure all the threads that are accessing the same cached data are using the same pool of cache, and the way you do that on a multi CCD design is to disable the other CCD.  Adding more cache to the second CCD doesn't solve this problem. The existing cache on the second CCD could already be utilized, if it made any sense to, and all that mattered was more cache overall. It just makes it so that any thread, regardless of which CCD it's on, has access to a large pool of cache. But, you would still want to confine the work for a single application entirely to one CCD to extract the most benefit. It doesn't help gaming at all. It could have some limited productivity benefits.",AMD,2026-01-08 11:39:19,7
AMD,nycl5au,"Maybe for gaming, but if you use these for compute intensive work and know how to nice your processes, they will go lightning fast.",AMD,2026-01-08 05:44:20,6
AMD,nycnd30,"I think it was not a useful change since each CCD has separate L3 cache so going across the infinity fabric means not using the same cache and adding latency. AMD said they didn't think it would be a useful configuration.  But ""give the people what they want"" is a saying for a reason and people keep begging for both CCDs to be 3D cache",AMD,2026-01-08 06:00:33,5
AMD,nycd8kt,"No. Cache is not something that adding more of can hurt. The rest of the die is rumored to be the same, just now both dies have 96MB L3 cache instead of just the one.   Will some workloads *not benefit*? Maybe. But this change doesn't *add* latency. The latency you speak of is already there with your 9950x3d. (Ok.. +2ns to 3DV cache vs the standard L3 layer).",AMD,2026-01-08 04:50:55,20
AMD,nycd9p5,"I think the idea is that the games will still only run on one ccd as per usual, but now you donâ€™t have to worry about making sure the game is on the correct one since they both have the 3D cache",AMD,2026-01-08 04:51:07,7
AMD,nycd8oj,"Lots of apps can deal with the latency and are designed multiple CPUâ€™s with NUMAâ€¦ gaming is a different story usually. But the writing for intel also, same with Nvidia graphicsâ€¦. But if a chicken or egg first.",AMD,2026-01-08 04:50:56,1
AMD,nyyqgeg,There is no benefits of having 16 cores in games anyways. But CPUs are used for more than just gaming.,AMD,2026-01-11 12:57:22,1
AMD,nychfkm,"Doesn't mean people won't buy it anyway. But I think you're right, I assume it will use core parking the same as the 9950x3d I really don't see the point of having an extra v-cache CCD that's doing nothing while gaming.  I hope I'm wrong and it does turn out to be really cool.",AMD,2026-01-08 05:18:26,1
AMD,nycuk52,"Absolutely, there won't be any gains for gaming",AMD,2026-01-08 06:56:51,1
AMD,nydopoe,"it improves performance by an appreciable amount due to the homogeneous configuration in specific scenarios.  the inter CCD latency would be more predictable because the core-cache config is symmetric  games? forget project lasso  productivity? memory intensive software like WinRAR should see a considerable performance increase due to having stored the all the dictionary or a bigger portion of it inside the on-die cache.  the performance increase is noticeable, but in very specific scenarios",AMD,2026-01-08 11:23:01,0
AMD,nyi8if2,"No, because having the same amount of cache on both CCD's makes that latency essentially irrelevant.",AMD,2026-01-09 00:37:00,0
AMD,nyf50td,the opposite,AMD,2026-01-08 16:12:25,2
AMD,nyi0ati,It's not.,AMD,2026-01-08 23:54:56,1
AMD,nz0w1m4,It is not coming.  Turns out it was an engineering prototype undergoing testing. They concluded it was not worth the extra costs because of very little to no improvement in performance.,AMD,2026-01-11 19:27:01,2
AMD,nyi8v71,"It's exactly the opposite.  If you run a game on both the V-cache CCD and normal CCD of a 9950X3D, that's when you'd have a lot of cross-CCD traffic to consider.  With both CCD's having V-cache, that scenario goes away, because running code on both CCD's will quickly end up having the same data in cache on each.",AMD,2026-01-09 00:38:47,1
AMD,nydhivm,"Remember, most PCs aren't used for gamers.",AMD,2026-01-08 10:22:06,2
AMD,nycjgon,You will buy it because you have to have the best. Report back when you do :),AMD,2026-01-08 05:32:30,7
AMD,nycjjzq,"If you currently have a 5700x3d and you are going to upgrade to AM5, would you get a 9950x3d or a 9950x3d2?",AMD,2026-01-08 05:33:09,4
AMD,nycngnx,10 series is already gonna be expensive just because tsmc raised wafer costs of 2nm. That and AMD still having no competition. I'd rather just safe up for that instead of double dipping on the same generation with a slight mhz bump.,AMD,2026-01-08 06:01:18,1
AMD,nye7cy4,I can confirm I read this post that confirms a rumour,AMD,2026-01-08 13:28:00,12
AMD,nygu0xi,Is this like a trailer for a trailer?,AMD,2026-01-08 20:38:48,2
AMD,nyi3j1v,Rumored to be confirmed,AMD,2026-01-09 00:11:34,2
AMD,nyz5vo7,The benchmarks are fake sadly.,AMD,2026-01-11 14:32:48,1
AMD,nydlo8s,"Some of those server SKUs are crazy. I remember there was a cache optimized EPYC that had a full 8 compute dies with 3D vcache, but had only 2 cores active for each die.  I only learned about it because we were deploying SQL Server for a project and the per core licensing cost can get crazy, so we had to study the best way to throw hardware at the problem without increasing the software cost.",AMD,2026-01-08 10:57:47,14
AMD,nyew9k3,Cities skylines 2 maybe?,AMD,2026-01-08 15:32:56,6
AMD,nydogmg,"it improves by an appreciable amount due to the homogeneous configuration.   the inter CCD latency would be more predictable because the core-cache config is symmetric   games? forget project lasso   productivity? WinRAR should see a considerable performance increase due to having stored the all the dictionary or a bigger portion of it inside the on-die cache.   the performance increase is noticeable, but in very specific scenarios",AMD,2026-01-08 11:20:59,3
AMD,nyvmh5e,Cannot upvote you enough on this.,AMD,2026-01-10 23:51:27,1
AMD,nye7ih3,What threadripper has x3d?,AMD,2026-01-08 13:28:51,0
AMD,nycszvk,Not always gpu limited for 4k gaming. Went from 5900x + 4090 for 4k gaming to 9800x3d + 4090 and it was extremely noticeable in games like escape from tarkov and other cpu intensive games.,AMD,2026-01-08 06:44:10,29
AMD,nycvv3i,"If you look at average FPS yes, for the 0.1% and 1% a better processor does help.",AMD,2026-01-08 07:07:38,4
AMD,nydo4zd,"No, because the core problems remain. The reason the second CCD is disabled for the game is to keep all the game threads on the same CCD using the same pool of cache. The X3D CCD is chosen, obviously, because of the higher pool of cache, but either crossing CCDs and/or using two separate pools of cache would slow down performance, and that's still going to be a problem, regardless of if you added more cache to the second CCD. You could potentially play two games at the same time, both with the performance of a 9800X3D, each with their own CCD, but that would obviously be a fairly niche use case.",AMD,2026-01-08 11:18:20,9
AMD,nydmw96,"Never had to turn off anything on my 9950X3D. For those who have this CPU, set the Cache preference in the BIOS to Driver and install the latest Chipset drivers and you are golden.",AMD,2026-01-08 11:08:00,2
AMD,nycp8d5,What we really need is 12 core CCDs on the future 10800X3D and 10950X3D. That would be a real upgrade.,AMD,2026-01-08 06:14:41,6
AMD,nycq1ha,"It's not about the latency that's added due to the large cache he's talking about, it's about cross talk CCD latency aka infinity fabric and I assume also specifically for games.  Most games only use a few threads and we already park the 2nd CCD due to it missing the cache + cross talk latency, so it begs the question how much does adding a second 3d cache really benefit that? If we stop parking the cores we get the cross talk latency which is probably gonna be worse in that scenario, especially because games are very latency sensitive.",AMD,2026-01-08 06:20:50,21
AMD,nycuj35,You didn't get his question,AMD,2026-01-08 06:56:36,8
AMD,nygfa8a,"Actually it can hurt. More cache, more area, more latency. There will be a point on the cost benefit curve when it crosses from a positive into a negative.  At 5ghz there is only enough time for the signal to travel about 3-4 centimeters before the next cycle begins. You need things to be physical close, or you have to wait cycles in between requests. The more area you add, the more cycles you need to wait.  Just think about it logically, take the supposition to the extreme; imagine an infinite cache, the latency would be infinite.  -----  In this case its not adding more cache to the same ccd, but to the other ccd that lacks it. There wont be any additional latency concerns, so it should be a net positive. However, for anything that sits on one ccd, its not going to help at all. On ryzen, each ccd can only write to its own caches. One ccd can not go and write to the other ccd. It can however snoop the cache of the other ccd(and it must to ensure memory coherency), but this adds cross talk over the infinity fabric. Too much cross talk can likewise hurt performance instead of help. Again i dont think this will be an issue in this case. Its just there is such a thing as too much cache.",AMD,2026-01-08 19:33:04,0
AMD,nyd3o6f,Unfortunately you still need to worry about making sure the game is only running on 1 CCD. I have a dual CCD CPU (7900) and every time a game is split between CCDs it stutters. It will do the same even if both CCDs have V-Cache.,AMD,2026-01-08 08:15:45,4
AMD,nyvljb1,"i thought both don't get 3d cache  they would just doubling the size of the v cache on one ccd ,so core parking is still a issue , i have a 9950x3d and id never upgrade other then finding a way to give both ccds v cache to make core parking a thing of the past but gaining a few Fps due to a double sized vcache on one CCD is just a money grabbing refresh",AMD,2026-01-10 23:46:31,1
AMD,nye0bfa,I see this as more of a proof of concept SKU...not really all that useful but its a step forward in some ways and people that just want the best will buy it. Future SKUs will be what you should be watching to see if they can make this better in a way that is more meaningful.,AMD,2026-01-08 12:46:08,1
AMD,nydhydb,A gaming focused CPU will most likely be used that way.,AMD,2026-01-08 10:25:54,1
AMD,nyg8w56,"That is fine journalism, i think you are qualified enough to start a newspaper now.",AMD,2026-01-08 19:05:03,6
AMD,nyjiegm,"This just in - BREAKING NEWS FROM SWIM - Complete confirmation of a rumor being a rumor comprised of other rumors for other rumors and from a rumor, let the rumors begin to take multiple and drastic directions!   PS This is just a rumor!   pheeew I'm rumored out already! lol.. Sure wish I got to go to CES so I'd have a more accurate glympse of the year to come in tech! Obviously I was joking on a rumor train but I actually have heard that we are in for a very technically advanced journey! Pretty soon they will have to start having a q1 and q3 CES....",AMD,2026-01-09 04:49:48,1
AMD,nyhsw2h,an open beta for a game that is still riddled with bugs on release,AMD,2026-01-08 23:16:45,1
AMD,nyedhf0,"Another thing that makes my blood boil to unreasonable levels. ""We"" are forced to develop and buy weird hardware simply to circumvent the greed of some software companies. There's no actual reason why having more cores should result in a license increase besides pure greed",AMD,2026-01-08 14:00:53,14
AMD,nyjm0tl,"I've been deep inside many different worlds and ok my first confession on this,  here it goes, Hi I'm John and I have built #231 Gaming PCs and Workstation/Gaming combos, and I never even considered AMD as an option. Until recently. I have built old school AMD PCs but it wasn't until recently I decided my homelab needed a big updated NAS/Workhorse from my truenas i5 6500 Truenas server, running on a Biostat B250 BTC PRO 12 GPU (PCI x1 slots, well 1x16 and 11x1 slots.. It works great tbh for what it is, all my little x1 sata breakouts fit and more, no shortage of 2.5g nice either... But I couldn't have a 10Gig NIC AND. dedicated GPU (unless I wanted to run it off a riser... lol...)..   My point is that you are very right about the crazy SKUs AMD has on their CPUs, I got lost learning them... Some are nearly identical but not so much on the inside and one or two subtle differences, It's like they have a CPU to fit EVERY NEED and even more!   This is why I am building my first AMD server, which will be highly stress tested and pushed to its absolute limits until I truly KNOW I can trust a particular setup with my mission critical data!; For me it was the moment I paid for 2 years of web hosting on an Unmanaged VPS ran some commands showed me quickly I was running off a newer EYPC and it's extremely fast and responsive! . But my head was spinning for a few hours cross referencing CPUs of the same model and different price different SKU...   And some just a letter off..",AMD,2026-01-09 05:13:53,1
AMD,nz81fx7,I have a 5090 and 9950x3d and still runs like hot garbage,AMD,2026-01-12 20:07:59,2
AMD,nye82al,"You still won't want game threads crossing CCDs. The SerDes interconnect is still not fast enough, nor does it offer enough throughput for cache-cache transfers (and doesn't have this capability). All data will hit system memory if it needs to traverse another CCD. This is a significant latency hit vs V-cache, at least for first transfers. After that, data should be in both CCDs' caches, but there's not really a good way to control workloads with dependencies without locking to a single CCD. Otherwise, threads on both CCDs will stall as they wait for data from one another, traversing system memory, as all cached data is present in RAM.  Parallel ops with independent workloads can task both CCDs without much issue.  EDIT: There might be a way to optimize traffic in software, if it's intelligent and catches op dependencies from traversing CCDs. But, as usual, this has overhead and can still allow some edge cases to cross. Better to have some type of data traffic QoS than nothing though or we're back to parking the other CCD, which seems wasteful to me. Background tasks should operate on the secondary CCD at minimum. I see Linux moving faster with intelligent scheduling. Windows is too much of a mess.",AMD,2026-01-08 13:31:54,7
AMD,nydqd0c,So glad I /r/paidforwinrar,AMD,2026-01-08 11:36:11,2
AMD,nyf9sdt,"Epyc, not TR",AMD,2026-01-08 16:33:20,2
AMD,nycy7m0,I did this exact same upgrade and the games definitely became much smoother with way better 1% and 0.1% lows.,AMD,2026-01-08 07:27:40,14
AMD,nydybwz,"I mean thatâ€™s a massive jump so Iâ€™d expect to see a big improvement.  Youâ€™ also moved from a non-X3D part to an X3D part, so itâ€™s not a fair comparison.",AMD,2026-01-08 12:33:13,1
AMD,nyeaakr,This,AMD,2026-01-08 13:44:04,3
AMD,nydbre9,This guy gets it. ðŸ‘,AMD,2026-01-08 09:30:02,3
AMD,nydn8jv,Is a 16c part likely to be a gamer CPU?,AMD,2026-01-08 11:10:51,2
AMD,nyf13n6,"Itâ€™s the same philosophy of taxation.  Extract the money from people whom can afford it and are most benefitted from the government so you can subsidize those who canâ€™t.   Is it feasible or even fair that a company like Apple Pay the same licensing cost to use enterprise software as a single employment entrepreneur?   If Iâ€™m a software company whoâ€™s enabling you to make MILLIONS off of my product, I want a few thousands of that.  But I still want to sell my product to the single person startup, so I charge him less.   And for educational institutions, personal, or non-profit use, I give the licences for free or heavily discounted.",AMD,2026-01-08 15:54:42,11
AMD,nye8lez,"thank you for your response, the SerDes is perhaps the main bottleneck of the current ryzen tech, you're completely right.   do you think that the sea of wires implementation from strix halo would solve this?",AMD,2026-01-08 13:34:50,1
AMD,nyia7cr,"Fair. I never paid for Winrar, but I did donate to 7-Zip twice.",AMD,2026-01-09 00:45:37,1
AMD,nydydnj,3D V-cache helps a bunch with lows.,AMD,2026-01-08 12:33:33,3
AMD,nydq7ny,">Maybe they have managed to decrease CCD latency  Unlikely since its simply the infinity fabric not being very good, its the biggest bottleneck in zen 4/5, after that the memory controller.  >or have figured out new way of scheduling tasks?  That would be the most likely thing altough im not confident seeing how much trouble they have even getting parking to work well, lol, but its probably a good option.  >Also with 9950X3D the performance CCD is preferred  not quiet sure what you mean here? For games cache CCD should be the one it uses.",AMD,2026-01-08 11:35:01,3
AMD,nydogz2,Who else is buying X3D CPUs?,AMD,2026-01-08 11:21:04,1
AMD,nyf4k8d,"...I mean. Okay, fine. That makes sense. I'll concede.",AMD,2026-01-08 16:10:22,4
AMD,nyh88c6,Which south american country was it that was just torching the poor when the olympics came by?,AMD,2026-01-08 21:41:18,0
AMD,nyed9qp,"Hmm. I suppose it depends on how that's implemented. I want to see AMD link CCDs together via direct communication, instead of having to hit RAM for every data sync between CCDs. Fanout/sea-of-wires can certainly move in this direction, but honestly, an expensive active interposer or active bridge die might be the only way to get more benefits. A lot of hardware logic is actually needed to bridge CCDs, so maybe AMD doesn't feel it's worth the transistor cost, as each design has strict transistor budgets. Those budgets are often used to further compute performance.  A full 16-core CCD is likely what will be needed without all of the expensive silicon. That's just how it goes, I think.",AMD,2026-01-08 13:59:45,2
AMD,nyedxoc,I'm here crossing my fingers that we will get AM5 CPU's with that. I would be very happy if I could go from my 7800X3D to another X3D with 12+ cores if rumors are to be believed. Planning on using my AM5 motherboard as far as possible.,AMD,2026-01-08 14:03:15,1
AMD,nyhfx6k,???,AMD,2026-01-08 22:14:51,0
AMD,nyfw4n7,"perhaps the next step is including cache on the IO die to reduce inter CCD latency and low power cores to reduce idle power draw.   zen 5 chiplet design it's an improved zen 2 layout, that layout was good for increase core count for cheap (3950X), however SerDes interface is showing its limitations.",AMD,2026-01-08 18:10:27,2
AMD,nyqywxo,"i think it was brazil, everytime the olympics would come through theyd burn the favellas.",AMD,2026-01-10 07:11:33,1
AMD,ny3n7mq,Intel moment.,AMD,2026-01-06 23:26:08,152
AMD,ny3ps9y,Wouldn't trust a CPU with AI in it's name,AMD,2026-01-06 23:39:42,139
AMD,ny4yfb0,Wow that website is abhorrent. privacy invading commercial trackers must be allowed or you have to pay up?  At least reader mode bypasses it.,AMD,2026-01-07 03:39:49,32
AMD,ny4yyre,"Looking at the actual content of the article, they're basing everything off of ghz specifications, and it's really only mid range and lower mobile cpus. This is a refresh cycle, not a new generation.  The higher end units have more graphics stuff and no lesser ghz... and there's no benchmark data. It's all assumptions based on ghz specifications only.",AMD,2026-01-07 03:42:58,30
AMD,ny4cu1g,"Me initially: Well the Ryzen AI 300 series is also stupid already, the AI 7 350 has 2/3 the GPU cores of the R7 7840U while the AI 5 340 has HALF the GPU of the R5 7640U, not to mention the AI 5 330 having only FOUR cores despite the ""5"" in there, what else could they possibly do?  Me after looking at the article: Wow, they're actually reinforcing my decision to go for Intel with my last laptop purchase last year, once again AMD never fails to disappoint, what the fuck is that AI 7 445 doing there with only 6 cores and HALF of the AI 7 350's already anemic GPU (relative to the R7 7840U)? Not even going to look at the other models, I'll probably pop a blood vessel somewhere if I do.",AMD,2026-01-07 01:41:12,23
AMD,ny4b4ga,Not sure how they deduced that there are fewer p-cores,AMD,2026-01-07 01:31:49,7
AMD,nyam8d2,"I don't mind having fp8 calc in a cpu, but it only makes sense when you don't have a gpu",AMD,2026-01-07 23:13:08,1
AMD,nysrxva,I thought the time an Ryzen 7 CPU would have less than 8 cores was behind us. The AMD Ryzen AI 7 450 is decent but the AMD Ryzen AI 7 445? Oh nope nope nope.,AMD,2026-01-10 15:28:10,1
AMD,ny68cie,"Funnily enough, Intel Panther Lake looks quite promising.",AMD,2026-01-07 09:39:47,54
AMD,ny5wp56,actually worst than Intel.  Intel at least change/bump the spec after refresh.,AMD,2026-01-07 07:51:21,28
AMD,ny3qvmr,"Not that I don't trust them, but I don't need them. I've got a handful of GPUs that I can do my AI goonery on.  An NPU or high powered iGPU is a waste of silicon for me. I hope they come out with a crop of decent regular desktop CPUs next gen.",AMD,2026-01-06 23:45:26,45
AMD,ny6ylb6,I have a AMD CPU with NPU and I gave up on finding an use for te NPU.,AMD,2026-01-07 13:05:23,9
AMD,nycjc3n,Nobody gives a shit about the gdpr because the EU is too much of a pussified institution to actually enforce its laws.,AMD,2026-01-08 05:31:34,4
AMD,nyb6vnv,"If you use Reader Mode, you can bypass all of it.",AMD,2026-01-08 00:57:18,1
AMD,ny5zk2f,It's just annoying that they mix and match different generation cores for GPU and CPU in every new lineup just to have new numbers for the same product s. Intentionally making it difficult for a normal consumer to know what they are buying.,AMD,2026-01-07 08:17:16,7
AMD,ny4oa0e,"AMD has always gained traction in the mobile market, and then do something stupid to kill it.  Fucking 20yr cycle at this point.",AMD,2026-01-07 02:43:09,33
AMD,ny52jn4,To be honest the clock for those iGPUs are bumped a lot and they will be RAM bottlenecked anyway. Cutting down WGPs is an efficient way to save some cost.,AMD,2026-01-07 04:04:32,5
AMD,ny40jpe,I'm still using a 1080 Ti gpu ;(,AMD,2026-01-07 00:35:20,11
AMD,ny4uj21,You don't have a handful of GPUs in a notebook though. The point of these chips is for power efficient devices where getting 50-60 trillion operations a second out of a few watts does have a lot of value.,AMD,2026-01-07 03:17:30,6
AMD,ny7qm0p,"I tried so many apps yet only ONE used the npu, basically wasted sand that could've been used for a bigger iGPU or more cores",AMD,2026-01-07 15:33:15,5
AMD,ny9it1w,"It sucks but everybody does it.  If you buy a civic si today it doesn't have a 200HP NA engine anymore like it did a decade ago, now it has a tiny dinky engine with a turbo getting it to 200hp... no adding a big ass turbo to the NA version for 320-500+HP.  A lot of people complain about modern oled TVs compared to the ones 3-4 years ago. I think the only thing these tv companies get better at is their spyware.   I can go on basically forever.",AMD,2026-01-07 20:17:56,4
AMD,ny5trz8,"When was the last cycle like this in the mobile market? I don't recall AMD having gained much traction like right now. I remember a laptop I had with a socketed Athlon II CPU, but with its anemic GPU, it wasn't top of the line or anything.       Maybe the A-series chips were decent in laptops? From my hazy memory, I associate those older AMD laptops with very cheap Compaq and HP laptops. I don't remember how many higher-quality laptops used AMD back then.",AMD,2026-01-07 07:25:17,2
AMD,ny56pt9,"Which would be cool if they're cheaper for the user, but they're not, certainly not for the Framework 13 which is what I was looking at, going from the 7640U to the AI 5 340 costs $130 extra, and for that I get... what, half the CPU cores dropped to compact versions AND the GPU chopped in half? It's not like the clockspeed of the GPU doubled (not even close, 2600 vs 2900 MHz) either so ain't no way that half assed version is going to totally match the old one, nevermind *improve* as one might expect when buying something newer.  So in my case since at the time the 7640U model was constantly going in and out of stock while the AI 5 340 just costs more for less I ended up splitting the difference and getting the Core Ultra 5 125H model instead, at least Intel only cut 12.5% of the GPU cores instead of 50% when dropping from the 7 tier to the 5 tier.",AMD,2026-01-07 04:30:42,6
AMD,ny436mn,"I've got one too, a real workhorse. I haven't put it out to pasture yet.  I'm on Linux, so I'm going to build a Windows 7 ""console"" for all my old games. That's where the 1080ti will find its home.",AMD,2026-01-07 00:49:00,13
AMD,ny7n29i,Heck im still using a 1070,AMD,2026-01-07 15:16:22,2
AMD,nygvyoo,"I gifted my old EVGA 1080Ti to my brother, he's running it with a 5800x3D",AMD,2026-01-08 20:47:27,2
AMD,ny7set6,"Iâ€™ve never seen the NPU core in these new AI laptops even do anything, itâ€™s all happening in the cloud",AMD,2026-01-07 15:41:45,5
AMD,nygemkf,"I don't mind getting what I'm paying for (CPU's or car engines that don't have headroom for more performance).   I do mind companies obfuscating their labels so it's almost impossible to know what generation/technology I'm actually buying, and AMD is really getting on my nerves with their mobile naming schemes, mixing older and newer tech so a latest gen system could be worse than a older one while having a bigger number.",AMD,2026-01-08 19:30:09,2
AMD,ny5wgbl,"You have to remember that before Centrino and Core, AMD's competition was the godforsaken Intel Mobile Pentium 4. Any Athlon 64 (later Turion) laptop at the time was thinner, lighter, faster and quieter. OEMs were still selling Mobile Pentium III laptops for anyone who wanted more than 4 hours on a charge, that is how shite the P4 was for mobile.  Intel backtracked on the entire Netburst pipeline and went with a P3-based design for Centrino, which begat Core and the beginning of Intel's 15 years or so of absolute dominance over both mobile and desktop.",AMD,2026-01-07 07:49:10,17
AMD,ny4kaat,"My main machine has the 1080ti with a 7970x, on 8.1     Maybe someday I can afford to buy a 3090 Ti, to be used on Windows 8.1",AMD,2026-01-07 02:21:27,3
AMD,nyhcjk8,Threadripper 7970x and Evga ftw3 1080 Ti and Windows 8.1 Enterprise x64,AMD,2026-01-08 21:59:55,1
AMD,nybt95b,">Iâ€™ve never seen the NPU core in these new AI laptops even do anything  That you haven't looked into the uses or see the potential doesn't reflect on the hardware. We also have to accept that you first have to make the chips and get them into the market before the can evolve. And we shouldn't assume our personal use-cases are the only use-cases.  Adobe already supports the NPU for a lot of tasks including automatic transcription, scene cut detection, and speech enhancement.   Microsoft supports it for biometrics and video conferencing features.  AI programs like LM Stuio and Amuse can leverage it.  \-- [https://www.pcworld.com/article/2905178/ai-on-the-notebook-these-tools-already-use-the-new-npu-technology.html](https://www.pcworld.com/article/2905178/ai-on-the-notebook-these-tools-already-use-the-new-npu-technology.html)  Almost any AI task that you want to run locally and in low power could benefit and I think there's a lot of potential for that.",AMD,2026-01-08 02:54:49,0
AMD,ny6vq38,"Bingo.  Pre-Pentiun M (Pentiun M + Intell wifi adapter = Centrino, for the youngsters) AMD had a superior mobile platform.  Those Pentium Mâ€™s were the shit, though.",AMD,2026-01-07 12:47:28,4
AMD,ny7nt3p,I really hope you have that machine airgapped,AMD,2026-01-07 15:19:56,7
AMD,ny71kzg,Jesus Christ that CPU is going to want to kill itself man it's being so limited by that ancient OS and the GPU from 10 years ago.  What are you doing just use Linux man if you hate Windows 10 and 11 so much I wouldn't recommend 10 anymore because tens no longer supported you're literally running a security hole OS you really need to just throw that operating system in the garbage.    Install Linux like a normal human being and get a 5070 TI,AMD,2026-01-07 13:23:08,2
AMD,nybuexq,"I hear you but these are business laptops that are marketed as Copilot ready, Copilot or any business case AI tool will run in the cloud. Heck even the Copilot desktop app doesnâ€™t leverage the NPU",AMD,2026-01-08 03:00:53,4
AMD,ny7z01h,Go away,AMD,2026-01-07 16:11:48,1
AMD,nyckyes,"You cannot do biometrics on the cloud. For security reasons, for latency reasons, and you probably want to be able to login when there's no signal.  And think of all the other business cases where you don't want data travelling to a cloud provider. Something people already have access to is live translation and captioning. Difficult with a cloud based service and you might not want meeting conversations going up to the cloud.  Microsoft's controversial 'recall' function is another example. You don't want your documents and screenshots sent to a cloud. Microsoft states *Recall* ""operates directly on your device, utilizing the *NPU*"".   Privacy aside you can't do basic things like background blur in real-time with a cloud based model and you could do that on a GPU but why run a GPU for an hour when you can run the NPU in just a few watts.  You do not want to spin up a power hungry GPU for every AI task a consumer might need and that's why NPUs exist and will continue to exist.  If you don't see a use for an NPU it is only a failure of imagination where you have not been able to think of a continuous background AI task that might be useful. But those tasks do in fact exist.",AMD,2026-01-08 05:43:00,1
AMD,nybvyk4,Woo?,AMD,2026-01-08 03:09:10,1
AMD,nycd4x9,"Iâ€™m currently in the process of building a PC. Last part to buy is a CPU, I was going to get 9800x3d. I wonder if I should just wait for this if it ends up being the same MSRP",AMD,2026-01-08 04:50:16,1
AMD,nyc31sw,"Something like this is obviously going to make more sense to upsell in a prebuilt bc it wonâ€™t game any faster than single X3D CCD, but Iâ€™d be very surprised if it is OEM only bc some enthusiast will drop 1k on one of these.",AMD,2026-01-08 03:48:28,13
AMD,nyca81d,Maybe this can finally hit 100+ FPS and 60 0.1% FPS in Microsoft Flighht Simulator? 5090 + 9950X3D is around 90 average FPS and 50 0.1% FPS in 4k Native DLAA. /s,AMD,2026-01-08 04:31:39,3
AMD,nyd410j,"CES would've been the perfect time to tease this, rather than the business- and corporate-oriented announcements AMD gave us.",AMD,2026-01-08 08:18:57,2
AMD,nycuzn2,"Sytronix also lists it in their upcoming workstation system, which makes a lot of sense for certain workloads.",AMD,2026-01-08 07:00:21,1
AMD,nz0ujbs,Other news sites now say that it was not correct ant that Alienware was talking about the 9850X3D and not the 9950X3D2. For now it still looks like it is not going to be a real CPU.,AMD,2026-01-11 19:20:09,1
AMD,nycutpn,The Sytronix one (i.e. workstation use) makes a lot more sense.,AMD,2026-01-08 06:59:02,2
AMD,nyc39gd,Potentially less stuttering in games with more cores for on the fly shader compilationÂ    Also less time pre compiling shaders,AMD,2026-01-08 03:49:41,2
AMD,nydvr3v,Does msfs2024 use all 16 cores?,AMD,2026-01-08 12:15:39,2
AMD,nyi9aw0,"The executives of all of these companies, AMD very much included, are foaming at the mouth from the ""AI"" virus.  They cannot make rational decisions about anything that doesn't directly contribute to milking that bubble.",AMD,2026-01-09 00:41:00,3
AMD,nyvqx65,Link?,AMD,2026-01-11 00:14:39,1
AMD,nz0u9nw,I think it is more likely they made an error creating the page. There was no CES announcement on it and no other credible source. This sounds very much like the Intel B770 type news which god knows when that will show up if at all.,AMD,2026-01-11 19:18:57,1
AMD,nydhm4e,Which workstation use case will make use of that but not warranty more memory channels in a threadripper? Itâ€™s interesting they donâ€™t even make a zen 5 epyc with x3d (no Turin-X) this gen.,AMD,2026-01-08 10:22:55,1
AMD,nyc3d6y,I doubt it will affect anything due to massive cross ccd latency anyways.,AMD,2026-01-08 03:50:15,-2
AMD,nyvrnw5,It's also in the OP,AMD,2026-01-11 00:18:37,1
AMD,nyda7pr,"The cross CCD latency is surely a factor, but itâ€™s definitely not massive. And the immense cache can also mean that threads can stay longer in the same CCD.",AMD,2026-01-08 09:15:38,0
AMD,nycjqtk,The cross ccd latency isn't any worse with X3D2 against X3D.,AMD,2026-01-08 05:34:30,-1
AMD,nydhzyc,And the difference with the 8 core model beingâ€¦? That also has the same number of cores and same amount of cache _per_ CCD.,AMD,2026-01-08 10:26:16,2
AMD,nydeepa,But also not any better.  Best case scenario will be turning off one CCD,AMD,2026-01-08 09:54:25,3
AMD,nycmo7f,It isnâ€™t. Thatâ€™s why said it â€œwonâ€™t game any fasterâ€ as in it will be the same. Not slower. I did NOT in any way imply it will be any worse.,AMD,2026-01-08 05:55:26,4
AMD,nzgp17q,"Playing at 4K (7900xtx, 5800x3D) my bottleneck is not my cpu :-) .   Ill wait for AM6.",AMD,2026-01-14 01:47:42,48
AMD,nzgo9sk,"Got onto AM4 earlier in 2025 to find 5800X3D already being sold out, and snagged 5700X3D instead.  Really want to move to AM5, but might be holding onto this for a long time.",AMD,2026-01-14 01:43:23,21
AMD,nzgncgv,I had forgotten that the gap between that and the 9800X3D was so huge,AMD,2026-01-14 01:38:08,14
AMD,nzgjced,I was under the impression that even older RAM sticks now are already skyrocketing as well.,AMD,2026-01-14 01:15:05,18
AMD,nzggsbs,"Yeah people are so desperate for 5000 x3ds i sold mine for $450, gonna get a 9800x3d.",AMD,2026-01-14 01:00:29,21
AMD,nzh2jdb,Upgrade GPU to 5070ti...paired with my 5800X3D....seems like a good combo ..will skip Zen 5 for now..,AMD,2026-01-14 03:03:51,3
AMD,nzh48ge,"man, if I could just jump to a 5700x3d, it would be a huge upgrade. 16MB L3 cache in the 5500 is a huge bottleneck for some games (like Cyberpunk)  in Brazil you can't find these chips for less than 800 dollars lol.",AMD,2026-01-14 03:13:42,3
AMD,nzgr1ps,Back? Never left for me lol.,AMD,2026-01-14 01:59:03,6
AMD,nzginve,if amd's marketing didn't come out of stupidtown they would have already had 12 and 16 core zen 3D on the market along 8 and 6 core variants,AMD,2026-01-14 01:11:12,5
AMD,nzggz4w,Best CPU of 2022 against best CPUs of 2023/2024.    A result you could have seen by just... checking the reviews of said CPUs in 2024.,AMD,2026-01-14 01:01:33,1
AMD,nzgt5p8,Or just crank the graphics settings until the CPU isnâ€™t the bottleneck. This shouldnâ€™t be too hard. The Venn Diagram of 5080/9070XT users and non-3D AM4 CPU users has got to be pretty slim.,AMD,2026-01-14 02:10:57,1
AMD,nzhajb0,I guess 5090 owners are not interested in 5800x3d,AMD,2026-01-14 03:51:42,1
AMD,nzhbg6n,5800X3D & 9070 XT ðŸ’ª,AMD,2026-01-14 03:57:28,1
AMD,nzhl5qk,"Started on AM4 with a 2700x, now sitting with a 5800x3d and the same board. I have zero reason to move off this chip in the foreseeable future.   I also started with a GTX 1070, and moved to a 6900XT at release. I'm confident it can take another GPU upgrade down the road still.",AMD,2026-01-14 05:03:16,1
AMD,nzgv6fl,Ditto,AMD,2026-01-14 02:22:20,6
AMD,nzhh2nj,"I got my 9800X3D about a year ago, I can't wait to replace it in 5-7 years.",AMD,2026-01-14 04:34:47,1
AMD,nzgy0v7,Yeah because your gpu is not fast. My old 5800x3d bottlenecked the shit out of my 5090 in many games even at 4k,AMD,2026-01-14 02:38:23,-16
AMD,nzh0iny,Why did you go AM4 so late and not just AM5 in 2025?,AMD,2026-01-14 02:52:18,3
AMD,nzh0ppc,yes I did this upgrade in April 5800X3D to 9800X3D is a huge jump.,AMD,2026-01-14 02:53:24,9
AMD,nzhenxr,Depends on the resolution and GPU I suppose. 1080p on a 5090 shows the gap. Say 4k on a 9070XT and the gap would be a lot smaller.,AMD,2026-01-14 04:18:27,4
AMD,nzgn74u,obligatory this is a product for people already on AM4 not new computers,AMD,2026-01-14 01:37:17,28
AMD,nzgsi38,"Yeah, bought 32 GB last June for 101 eur, checked the price of the exact same kit from the same store and it was 303 eur. Prices had started rising mid June. According to the chart I saw, DDR4 prices went up before DDR5, which started rising in the Fall.",AMD,2026-01-14 02:07:17,5
AMD,nzh9xkb,Yeah it is nuts. I ordered 2x32GB DDR4 sticks back in 2023 and their price has almost tripled based on Amazon pricing.,AMD,2026-01-14 03:47:58,2
AMD,nzgug02,Have fun buying DDR5.,AMD,2026-01-14 02:18:10,20
AMD,nzh7awt,"I recently picked up the AM5 bundle at micro center (9800X3D, 32GB DDR5, B650E-E) for $680 with the main justification being that Iâ€™ll make most of if not all my money back from selling my AM4 parts because holy shit 5700X3Ds go for a stupid amount these days.",AMD,2026-01-14 03:31:56,2
AMD,nzgo9mz,"Those already exist?  The 7 nm prototypes never made it to market, likely for performance reasons they were not measurably better than this chip.  And I mean, they already solved these issues with the 7900X3D which was probably designed \~4.5 years ago.  Their CPU design teams are likely working on Zen 7 and Zen 8 at this point. They're not going to go back to a Zen 3 chip to try to workaround AM4 limitations to deliver a real 5950X3D in 2028 that would require a major BIOS update, be limited to very specific boards and high-end memory.",AMD,2026-01-14 01:43:22,7
AMD,nzghsag,Sure? But it's easier this way,AMD,2026-01-14 01:06:11,10
AMD,nzgnwbi,Its not vs its about 5700x3d and 5800x3d in modern games 9800x3d is just for scale,AMD,2026-01-14 01:41:16,8
AMD,nzgp7t1,The gaps have actually gotten wider,AMD,2026-01-14 01:48:45,2
AMD,nzgwffn,"5900X with a new 9070XT here and for my 3440x1440 @ 160HZ purposes with I'm still really well served, considering myself a heavy user. Been playing through some RT titles like Cyberpunk and GTAV:E, while having good productivity for my coding needs.  I wouldn't know what to upgrade for at the moment, to me it's just not worth the cost. I'd e.g. have more value from going OLED.",AMD,2026-01-14 02:29:21,1
AMD,nzgzolk,"Why would you have a 5090 without the best CPU you could get, which is 1/5 the price of the 5090.",AMD,2026-01-14 02:47:40,16
AMD,nzh03vh,no doubt I had a 5800X3D + 7900XTX and at 1440UW there was a small amount of bottlenecking. Even a 4090 is bottlenecked alot by a 5800X3D with a 5090 forget about it.  Below 4k a 5800X3D + 4070 Ti is a good combo anything faster than that you will to limit the gpu.,AMD,2026-01-14 02:50:01,1
AMD,nzh9g4m,Lol the driver overhead from your 5090 certainly contributed to that,AMD,2026-01-14 03:45:01,1
AMD,nzhbsxf,Yes that's what i meant. For 4K I probably need to replace my GPU before my CPU ðŸ˜€ . I'm mostly sim racing so my cpu is pretty strong still. Doubt ill see too much dif on a 9800x3d with the same gpu .   I sold my 1 kidney already to buy the 7900xtx on launch so selling the other one  for AM5 is not an option !,AMD,2026-01-14 03:59:42,1
AMD,nzhklm6,"Idk why youâ€™re being downvoted, I upgraded from 5800x3d to a 9800x3d with the same 3080ti and everything is way smoother. star citizen got a big bump in frames and itâ€™s so much smoother.",AMD,2026-01-14 04:59:15,1
AMD,nzh49ix,"Freebie!  Was on 9th-gen Intel, and this was a bit of a bump in performance.",AMD,2026-01-14 03:13:52,8
AMD,nzhgsd9,"Same, upgraded a few months ago right when ram prices started to climb. Was a crazy big jump for a cpu upgrade",AMD,2026-01-14 04:32:50,1
AMD,nzh7kum,If only Microcenter is available around the world.,AMD,2026-01-14 03:33:38,3
AMD,nzh0q96,I got a 9950x3d now but at first I had my old 5800x3d,AMD,2026-01-14 02:53:29,3
AMD,nzhh3yi,no doubt a 4090 is bottlenecked on a 5800X3D.,AMD,2026-01-14 04:35:02,1
AMD,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,5
AMD,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,11
AMD,nz80svg,I would love an ITX motherboard Strix Halo Competitor,Intel,2026-01-12 20:04:58,2
AMD,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,2
AMD,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,1
AMD,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,3
AMD,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,2
AMD,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,10
AMD,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
AMD,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,14
AMD,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,1
AMD,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,1
AMD,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
AMD,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
AMD,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,4
AMD,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
AMD,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,10
AMD,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
AMD,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,4
AMD,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,1
AMD,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
AMD,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,1
AMD,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,1
AMD,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
AMD,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
AMD,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
AMD,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,1
AMD,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
AMD,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
AMD,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
AMD,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,1
AMD,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,4
AMD,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,33
AMD,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,34
AMD,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,32
AMD,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,3
AMD,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,9
AMD,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,2
AMD,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,2
AMD,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
AMD,ny3a7m6,"Iâ€™ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, Iâ€™ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
AMD,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
AMD,ny5mck0,"Can anyone say real life performance diff, and how much increase in battery life in real laptops, Because I feel many times those ppt numbers don't nearly match real usage (especially when ppt numbers are huge).   Can I get Mac like battery(or atleast 7hrs with no performance drop) and how much does it compare to Mac m1/a18 air performance with them on a $600 laptop.(Assuming fedora/mint as OS)",Intel,2026-01-07 06:22:50,1
AMD,ny68bvw,"I don't understand, we need to see the price of this thing, because otherwise we have to compare it to the AMD 8060s which will be more powerful, but even the cheapest machine with that costs â‚¬1500/â‚¬2000. We just need to see what price point this chip will be offered at.",Intel,2026-01-07 09:39:36,1
AMD,ny87s84,"On just 2 channel / 128 bit RAM, well done Intel!",Intel,2026-01-07 16:51:19,1
AMD,nyaqr0z,An ancient Chinese proverb (roughly) states: *'Talk...* does not make rice...' ðŸ¤”,Intel,2026-01-07 23:35:58,1
AMD,nybohxz,Steam deck 2??? Take my money gaben.,Intel,2026-01-08 02:30:03,1
AMD,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-19
AMD,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,13
AMD,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,4
AMD,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,23
AMD,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,5
AMD,nylh0bq,"A quick Google says 9 TFLOPS or the equivalent to an RTX 1080, 2070, 3060, 4050 give or take.",Intel,2026-01-09 13:55:48,1
AMD,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,4
AMD,ny0hpkm,82% faster than 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,3
AMD,ny7zxvb,>77% faster while using 80% more power.  Are you following CES at all?  The top feature of that architecture so far has been power reduction,Intel,2026-01-07 16:16:03,1
AMD,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,4
AMD,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,2
AMD,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,1
AMD,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,15
AMD,ny6mhyg,Mfg on or off? The article wasnâ€™t clear on that.,Intel,2026-01-07 11:42:22,1
AMD,ny5jwr8,their Zen 5 desktop iGPU still use RDNA2; a 5 yr old architecture let that sink in...,Intel,2026-01-07 06:03:30,5
AMD,ny89039,"AMD didn't have money when they were using Vega iGPUs, and they were still the best iGPUs around",Intel,2026-01-07 16:56:45,2
AMD,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,2
AMD,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,0
AMD,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,5
AMD,ny8apbn,native rendering,Intel,2026-01-07 17:04:31,1
AMD,nz2dezh,Those weren't good though.  They didn't get close to the 1050ish equivalent that's a decent min spec card until the steam deck.,Intel,2026-01-11 23:40:06,1
AMD,nya3a7m,"High with XESS maybe at â€˜okayâ€™ frame rates. Still, crazy.",Intel,2026-01-07 21:45:51,1
AMD,nylfre3,"While I agree their naming scheme is a mess, yours is far worse.",Intel,2026-01-09 13:49:14,16
AMD,nykrwmo,"TBH, as long as the Ultra 5 338H is actually called an Ultra X5, it'd make the entire thing a lot more consistent  as in now X always means ""the one with the good GPU""",Intel,2026-01-09 11:10:20,19
AMD,nykqc9r,"You have no understanding of Intel's business and thus are not qualified to advise them what to do   Intel doesn't sell these CPUs to the end consumer, they sell them to their customers - the PC manufacturers. And that is the reason why there is so much choice, because the PC manufacturers want it.   Also, you clearly have never heard of vPro.",Intel,2026-01-09 10:57:07,36
AMD,nylcqp0,"The SKU count is roughly doubled because you have each step with/without vPro - these get a 100MHz max turbo frequency bump, but the main benefit is you can run the corporate firmware with vPro support, so you get additional security and manageability features. Exception is the Ultra 9 where they just do it with vPro support as standard.  These have a higher cost because you are getting more features.  You could make it so you just have one CPU and then the manufacturer pays a license for corporate firmware per device, but that's more work to then ensure manufacturers are licensing machines correctly, and more confusing for end-users where now if you're buying a corporate device with vPro support you know you are looking at Core Ultra 236V and 268V for vPro support whilst 226V and 258V don't have it.",Intel,2026-01-09 13:32:44,5
AMD,nykrcxa,"Because intelâ€™s customers work with thin margins and want the wide product stack with lots of performance and price steps. For them it matters if they get 4.4ghz for $300 or 4.6ghz for $350. You are not Intelâ€™s customer unless you ordered a pallet of 1000 CPUs, which I doubt.",Intel,2026-01-09 11:05:44,11
AMD,nyo7z62,Apple really isn't better. They leave out lots of the important performance information. They just don't tell you at all.,Intel,2026-01-09 21:31:03,3
AMD,nykykh6,I agree for those cpu that have no alphabet denomination at the back as that just looks like how desktop cpu is.  But for X7 and X9 is just even easier CPU differentiation,Intel,2026-01-09 12:02:34,2
AMD,nywwcvg,I only agree with the title. Your naming scheme is much worse lol,Intel,2026-01-11 03:59:55,2
AMD,nylenad,"At this point even S3, S3 Pro and S3 Pro Max would be a great improvement.",Intel,2026-01-09 13:43:16,1
AMD,nylwbke,"Totally donâ€™t get it, miss the 13900k 14600k type names.",Intel,2026-01-09 15:11:59,1
AMD,nyneqru,"Brah I don't care about the naming conventions, it is what it is.  It is petty to argue about all of this.  I need the B770 and C880 to be released.  I need more Intel Arc Pro cards to be released, there is no hope for humanity otherwise.  I need Battlemage and Battlematrix everywhere but TSMC is the bottleneck.  Hopefully there is more for 2026 where Intel IFS shines.  God help us all!",Intel,2026-01-09 19:16:03,1
AMD,nyo686v,Are you saying they are all locked???,Intel,2026-01-09 21:22:59,1
AMD,nyqfh80,I only care about the top level sku so the names don't matter.,Intel,2026-01-10 04:43:17,1
AMD,nykt7v1,Samaung Galaxy S3,Intel,2026-01-09 11:21:10,1
AMD,nyky6jo,Intel to $100 guaranteed,Intel,2026-01-09 11:59:46,1
AMD,nymfe1x,Just Josh presses them hard on this issue at about 7:30 in this videoÂ https://youtu.be/AzGFbkKZE7A?si=yq1pmpRv7exQ-7i5,Intel,2026-01-09 16:38:15,0
AMD,nyo3uhv,"Check the Just Josh interview! Dude criticizes exactly that to an intel executive... For me, they should drop the ultra naming scheme altogether... it hasn't stuck yet... they should go back to de i3/i5/i7/i9...use the X for the B390... and an S for the 16 core variants...",Intel,2026-01-09 21:11:54,0
AMD,nyps0n6,Apple always nails the small stuff,Intel,2026-01-10 02:21:54,0
AMD,nyn9p1s,"Just because it sounds similar to Apple does not make it ""bad"".  I chose ""S3"" because they literally market them as ""Series 3"" (sounds awfully similar to M-series ðŸ¤¨). Could fiddle with it but my idea stands:  Different core count = different name, Better gpu = add X",Intel,2026-01-09 18:53:22,-6
AMD,nyoegef,"Nope, its the Ultra 5 338H: [https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html](https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html)  The Ultra X7 and X9 are listed as such on Ark: [https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html](https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html)",Intel,2026-01-09 22:00:46,3
AMD,nz4b2zo,"the 338H doesn't have ""the"" good GPU, it has a B370, with 10 cores",Intel,2026-01-12 06:22:28,1
AMD,nylk5nn,"Btw, OEMs love the fact that the Ultra 5 336H and 338H are vastly different products with hugely different performance when it comes to graphics. Why? Cause they can market the 338H to you, and then sell you the 336H at a fraction of the cost, and if you are not very tech-savvy, well, that's too bad for you.",Intel,2026-01-09 14:12:07,17
AMD,nynb0an,"Yes because PC manufacturers want a ""choice"" to get a CPU with 100MHz higher clock speed as if that will make a difference in a mobile device at all.  If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.",Intel,2026-01-09 18:59:08,-7
AMD,nyn93or,"I see, that does complicate things.",Intel,2026-01-09 18:50:47,-2
AMD,nz8mprz,"It still should have the X IMO. They're still bothering to call it a B3xx chip rather than just ""Intel Graphics"" like the <=4 Xe chips. The handheld chips running downclocked GPUs get the B360 and B380 names as well. Those should all be Core Ultra X.  As of right now only the ending 8 differentiates the 338H from the small-GPU SKUs, which IMO is not clear enough. Also, if the X became the standard for all big-GPU chips, that ending digit can be used for something else, such as noting the actual GPU performance within the stack. Perhaps just using the 6-9 from B360-390 or something like that.",Intel,2026-01-12 21:47:30,1
AMD,nynhyn2,"You are placing way more importance into the naming than any normal customer would.  The names that matter to normal customers are Core Ultra 5, Core Ultra 7, Core Ultra X7, which is what you will also find on the stickers that Intel has the PC manufacturers put on the device. The model numbers are just for the PC manufacturers and customers who want the exact SKU.  Seriously, your obsession with this is weird. Just accept that the naming is not meant for you and move on. Not everything has to be like Apple.  > If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.  Yes, because vPro doesn't matter for normal consumers, but only for big enterprise customers.",Intel,2026-01-09 19:30:44,5
AMD,nymf8n3,Itâ€™s too hard to be a smart consumer and Google the names of the processor(s) and compare?,Intel,2026-01-09 16:37:35,4
AMD,nyo54xn,"""Obsession"" as if multiple YouTubers, some with millions of subscribers, haven't said the exact same thing I did",Intel,2026-01-09 21:17:55,-2
AMD,nynq0up,If you're a smart consumer you aren't buying windows laptops.,Intel,2026-01-09 20:07:44,-3
AMD,nyou0lx,"Right, those guys are surely the ultimate authority on anything and not just engagement driven outrage machines /s",Intel,2026-01-09 23:18:05,4
AMD,nyviuhi,That's some serious credentials you're bringing up,Intel,2026-01-10 23:32:07,4
AMD,nypm046,Ah I should instead buy Apple laptops and/or Arm laptops that don't work with my programs. Genius!,Intel,2026-01-10 01:49:37,5
AMD,nxtihn4,The biggest issue was that it was crippled by the ported meteor lake memory controller dies its that simple,Intel,2026-01-05 14:12:18,29
AMD,nxtjtfv,"Very good explanation, I own a 285k and I can say the stock experience is average, but the platform is great and coming from 14900k, the temps and power efficiency are impressive. Once fully tuned, 9000c38 A-die, 36 d2d and 34 ngu, gaming is on par with 14900k, but more efficient. I think nova lake will be amazing.",Intel,2026-01-05 14:19:43,30
AMD,nxvnqr5,">if you judge Arrow Lake solely by the frame rate counter in Cyberpunk 2077 at 1080p  Am I allowed to take into account that Intel went all the way from ""7"" to ""3"" lithography which is more than 2x improvement to achieve almost nothing?",Intel,2026-01-05 20:16:32,8
AMD,nxugtl3,">We need to stop looking at the Core Ultra 9 285K through the lens of a typical generational refresh  That's all what consumers care about. They don't care if ARL on paper or on theory is some great reset. Perf, power, and cost is what's important.   >he 285K is suffering from the acute growing pains of decoupling the compute complex from the uncore in a way that creates a distinct latency penalty that enthusiasts are mistaking for regression.  It's not being ""mistaken"" for a regression, it quite literally is one.   The problem is also that AMD also has disaggregated their compute from their IMC, and yet has *better* latency on *less advanced* packaging.   >The controversy here isn't that Intel failed to push frequency; it is that they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.  Nothing about ARL's current design prioritizes ppw or area efficiency over RPL's design from a chiplets vs monolithic perspective. ARL isn't enabling higher core counts from going chiplets, that seems to be left to NVL according to rumors. And chiplets carries an area penalty over monolithic designs anyway.   >The removal of Hyper-Threading from the Lion Cove P-cores is the most contentious yet logically sound decision engineers could have made given the thermal constraints of modern silicon.Â   This makes no sense   >By removing the simultaneous multithreading logic, specifically the duplication of architectural state and the complexity required in the reorder buffers and schedulers to handle two threads, Intel was able to physically widen the core and increase the L2 cache per core to 3MB without blowing up the die size  SMT costs Zen 5 less than 5% in area btw. Just throwing that out there.   >The result is a P-core with significantly higher IPC than Raptor Cove  It's not though. This has been a significantly worse ""tock"" in terms of IPC uplift compared to something like SNC or GLC.   >but this raw single-threaded throughput is being masked by the interconnect latency.  Maybe gaming or some benchmarks are, but for the large part, no.   You can see this two ways, LNC's structural gains (core width, ROB capacity, etc etc) have smaller gains, percentage wise, over their predecessor versus something you would see in GLC vs SNC, or SNC vs SKL.  And also LNL's uncore is dramatically improved over ARL, and yet you see the same unimpressive IPC gains over MTL/RWC (which is the basis for ARL's mid mem fabric).   >the architectural overhead of the Foveros packaging means that ring bus latency is higher.  No? The ring runs at a different frequency than the D2D?",Intel,2026-01-05 17:00:52,12
AMD,nxti8go,Itâ€™s not necessary for consumers to buy an inferior product from a multibillion dollar company now backed by the global superpowerâ€™s government.,Intel,2026-01-05 14:10:52,36
AMD,nxtflnr,"https://chipsandcheese.com/p/skymont-in-gaming-workloads  None of the youtubers mentioned about core-to-core latency, improvements on the schedulers and execution ports setup.  The e-cores are really great in a 4 group cluster.",Intel,2026-01-05 13:55:51,19
AMD,nxv9k2w,Designing a consumer product line around the niche of top of the line workstation is not a good decision for the average consumer.  I only once met a person with video production workstation that has more than an I7 or R7.  Previous gen I5 were amazing combo of multi core and single core . They rivaled amd r7 in preformance . Now a person wanting mid level cpu's would pay preformance tax due to it being made for the few people needing extreme amount of cores since those people would not buy dies that actually were designed for it like Xeon.,Intel,2026-01-05 19:10:59,5
AMD,nxtg5aw,"this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)   i would agree this architecture is very much limited by d2d and without 200s or just pushing d2d can be kinda underwhelming in performance but for sure as first gen product is very solid and makes me personally excited for nova lake as it seems they plan to fix and improve on their current architecture  also i believe clockspeed difference was merely responsible to 13/14th gen failures which were caused by excessive idle voltage.  i would say adding DLVR was kinda smart as well as it reduces your power consumption significantly at idle especially with proper tuning.  Edit: fixed typos, autocorrect being silly with me",Intel,2026-01-05 13:58:56,10
AMD,nxtm1ov,as a person who made an upgrade from lga1700 13950hx ES laptop mutant to 265kf this cpu feels soooo smooth in win 11 despite on a huge 75 NS the ram latency and not ability to reduce latency this new E core with 800 score at 5 ghz in cpuz single core make using the pc so nice and ofc in single core game like cs2 I've loose a lot of performance with 13950hx at 5.6ghz I've had 980fps in dust 2 fps benchmark and on 265kf only 800... BUT in a multi core load game I got fps improvement but any way this upgrade is worth it for ppl who is not a pc enthusiast and don't want to tune the lga1700 CPUs,Intel,2026-01-05 14:32:00,3
AMD,ny3llfj,That's one big wall of excuses,Intel,2026-01-06 23:17:53,3
AMD,nxto7vg,"It smells like slop in here. Shit post rather than a shitpost, congratulations.  I like my 265K. It performs well for my purposes and has reduced my personal power consumption considerably vs AM4. It is behind AM5 in my testing for broad term ""gaming"" when specifically chasing framerates, but compared to a 9700X or 9800X3D it is absolutely stellar at doing stuff while doing other stuff.",Intel,2026-01-05 14:43:46,4
AMD,nxtkrwq,Yea with a z bord and the 200s boost and fast ram 15th gen is finally matching or supasing 14th gen,Intel,2026-01-05 14:25:03,4
AMD,nxwaygs,Wall of slop.  Make your points in a more concise manner.,Intel,2026-01-05 22:04:26,4
AMD,nxtzro3,"AMD had a very similar experience with their first generation of Ryzen CPUs. One of the differences here is Intel had a competitive product prior to their architectural shakeup. Had Intel totally croaked for years and not been competitive in the CPU space the narrative would be completely different and everyone would be singing their praises.  Aside from that I think the biggest problem with this new architecture is simply there's little reason to buy in. It was only recently (if memory serves, I could be wrong on this) announced that the next generation of CPUs will share this platform and later ones will be on a new socket. When AM4 was announced we knew that it would persist for multiple generations and now with AM5 - why would you buy a board that will be obsolete when its time to upgrade when you could buy into a platform that will support your next 1-2 CPUs? Especially with the old intel socket performing just as well on a more mature platform, for most end users this first core ultra series just isn't worth investing.",Intel,2026-01-05 15:41:28,2
AMD,nxua32l,"""they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.""  That good for them, but we don't want that. I would take better latency over improvements that pretty much only save money to the companies.",Intel,2026-01-05 16:29:42,5
AMD,nxu3n2t,Love my 285K rig.,Intel,2026-01-05 15:59:37,3
AMD,nxu27tg,"IMO corrupt tech sites and tech YouTubers are behind ARLâ€™s failure for two reasons.   The first: AMD MCM processors without 3D cache are bad for gaming, and itâ€™s hard to find this information in 90% of charts to warn intel about the consequences of going down this path, even though they should have done their own tests and experiments.   The second: in my own tests, the 265K was 15% faster than the 9700X on launch BIOS. Through BIOS updates, that gap extended to 20%, making it only 7%-10% slower than the 14700K. Yet on some very questionable charts, the 265K is shown as slower than the 9700X.   For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off and undervolting. For inexperienced users, ARL is better, as it runs 20Â°C cooler out of the box.   I believe that in 2026 we deserve raw, unedited video benchmarks that start from the desktop, show full system specs, then enter game-by-game benchmarkingâ€”no more charts with zero evidence to back them up.   When I tried to confront HUB with my benchmarks, I got blocked to cover up their lies and corruption. If they truly cared about which CPU is faster, they would share their in-game benchmark scores and discuss them in a scientific way, but thatâ€™s not their goal. The numbers go up and down for the highest bidders.   Lastly, I believe Nova Lake, with its expected 144 MB cache, will be faster than Zen 6 X3D by 10â€“20% as 9700x in real world is 20% slower than 265k and if both got the same IPC uplift NL will end up on top.   My own testsÂ    14700k vs 9700x 30% faster https://youtu.be/1f6W6nkDS4o?si=chFUAeBWzybQopaL   265k vs 9700x 23% faster https://youtu.be/PuB0Dg-Jvyk?si=SmGJUFtYj-OjjpQh   Tech sites got same results that clearly show RPL and ARL CPUs are only slower than 9800x3d and faster than everything else from AMDÂ    https://www.pcgameshardware.de/Ryzen-9-9950X3D-CPU-281025/Tests/Benchmark-Release-Preis-vs-9800X3D-1467485/2/     https://www.purepc.pl/amd-ryzen-9-9950x3d-test-recenzja-opinia-cena-wydajnosc-gry-programy?page=0,55",Intel,2026-01-05 15:52:58,6
AMD,nxts3mv,"For me.   I will go with Intel because of reliability (I know about chip degradation) but for me I bought 13900K from first day I undervolted using offset and but Max turbo frequency to 5.5Ghz . so my chip never tried to boost 5.9Ghz with crazy voltage.  Next is the most important is Everything just works. The boot is faster. wakes from sleep. its a more mature. I have another amd pc with r5 2600, where I found some stability issue.  Another one that is important to me. is idle power consumption. My 13900K can idle at 6 watts. Imagine 24 cores idling at 6 watts. where as 6 core zen idle at 15-20watts.(and that is low side many user reported 25+watt). its all because of chiplet.  I didn't test the new Arrow Lake as this uses tile. so i cant comment on idle power draw. if anybody has test, let me know.",Intel,2026-01-05 15:03:58,4
AMD,nxwilo7,"Intel's latency problems have been around for a while now. Arrow Lake just threw gasoline on a fire that was already burning. [The 14900K had a latency of about 90ns for memory access, which is awful compared to the 10900K's 66ns and the 3950X's 73ns latency.](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the). The 285K sits at 106ns.  The 9900X sits at 82.43ns, so it seems like latency is going up across the board in general.",Intel,2026-01-05 22:41:53,2
AMD,nxmkdsp,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2026-01-04 14:07:07,1
AMD,nxxn31i,I compare Arrow Lake to Zen 1 or the first iterations of Ryzen. It is quite the techinal hurdle but it allows for future generations to bake very well. Intel was just playing catchup from having 14NM^6 nodes lmao.,Intel,2026-01-06 02:15:07,1
AMD,nxyburv,the problem for me the platform cost value over lga1700 isn't where it needs to be. namely I'm not interested in trading 32GB of DDR4 for just 8GB of DDR5.,Intel,2026-01-06 04:39:27,1
AMD,ny7t3re,"Since you are talking about latencies, best intel gaming cpu, so far, is 14700k. Excellent gaming performance and also great cpu for productivity.  Second best, is 14900k, same as 14700k, but cause of price it gets to second place.  Third place, you name it.  Period.  PS: I'm with intel since ..... Pentium 3 at 800MHz. Also have AMD cpu.",Intel,2026-01-07 15:44:56,1
AMD,nyabjt5,"End-user workloads at midsized businesses are almost entirely single-threaded. Even when the applications are multi-threaded, they often become single-threaded when they get shuffled through corporate antimalware. While AMD chases gamers and Apple chases artists, Intel is hitting a sweet spot here not perfectly served by any other single-user processor.",Intel,2026-01-07 22:22:37,1
AMD,nyduqy4,"I've been saying similar in a couple of PCMR threads.  Arrow lakes problem is chiplet to chiplet latency. Which is understandable as this was a major departure from monolithic dies this generation. This explains both why games particularly suffer, and why very high speed memory can mitigate the issue somewhat.  Does this mean they aren't bad chips? No, they are (or at least for gaming performance they are). But it's quite exciting in the sense that it's a specific issue holding them back that can be solved. The rest of the architecture has a lot of potential.  I think of these chips as in a similar place to AMD Zen 1 technology wise. A huge shift, not actually delivering much performance boost yet, but with a whole lot of potential for updates to make big improvements.",Intel,2026-01-08 12:08:37,1
AMD,nyf32oe,"Yes yes, I remember saying similar things about my FX-8350.",Intel,2026-01-08 16:03:36,1
AMD,nysqs8m,"285K seems to have fantastic workstation performance, I didn't think much before selecting Intel over AMD parts for work to be honest, considering the efficiency gains. We landed on 265K, it benches very favorably compared to AM5 parts in nearly every workload except for gaming, and even then, that's mostly the X3D parts, and even then, that's when there is virtually no GPU bottleneck.   Most initial ""gaming"" reviews were done exclusively with 5090s, which is extremely unrealistic for most gamers and gave an extremely false impression that you would see a massive uplift by buying this part compared with the alternative CPU parts in gaming.   A lot of reviewers such as HUB have done a lot of work to correct this impression with a variety of testing scenarios and explaining their reasoning behind removing the GPU limiting factor as much as possible in benchmarks. Incidentally, these reviews typically showed that anything less than a 5080 sees virtually no benefit with a 9800X3D versus, say, a 9700X, and incidentally the two parts that saw no discernible benefits were the Radeon 9070XT and the 9070.  Obviously, if you have a 5090 and an unlimited budget with price being no factor, your best bet for top tier performance in all categories is a 9950X3D, but that just isn't what most people are looking at.  I myself bought into the future proofing mentality with my CPU and bought a 9800X3D, and ironically bought another AMD Radeon 9070XT to pair it with, a GPU that seems literally no performance gain compared to a 9700X, let alone a 285K. The same 285K that takes a dump all over the 9800X3D in nearly every other productivity benchmark.  I guess the good news is that when I upgrade my GPU in 2-4 years, at least I'll take advantage of the 9800X3D in games, lol. But considering it cost me over $900 for CPU, RAM, and motherboard, that's cold comfort. Ironically I probably would have made out a lot better buying a 5700X3D or a 5800X3D and the 9070XT and keeping my AM4 board and DDR4 RAM and saving $600.",Intel,2026-01-10 15:22:09,1
AMD,nxtna7j,"You are correct that they needed to make a big architectural change as the 14th gen was clearly having issues and was being pushed too hard to make up for them, and ARL is the first step to that reset.  From their engineering perspective, it makes sense.  I wouldn't buy it though.  NVL... different story.",Intel,2026-01-05 14:38:45,1
AMD,nxvbwve,"No 285k is not shaving off 80-100W from 14900k, Set 100W PL1=PL2 on both and then compare. 285k is not even that much better than 9950x in MT. No amount of words can explain ARL flop, it took intel 3 gens after rocket lake fiasco to catch up, just to waste all that effort on ARL. Show me any other silicon design with advantage of 2 nodes and a new architecture just to be slower than the predcessor.",Intel,2026-01-05 19:21:42,1
AMD,nxwb6x7,"""If you are buying a 285K solely for (1080P) gaming, you are buying the wrong product for the wrong reason.Â ""  Fixed it for you. I game in 4K and I didn't buy a Ultra 9 285K to play games in 1080P to get high(er) framerate at the expense of stuttering. The ultras are outstanding CPUs.",Intel,2026-01-05 22:05:33,1
AMD,nxtfm4y,I guess.,Intel,2026-01-05 13:55:55,0
AMD,nxte2yd,No one has time to read all that,Intel,2026-01-05 13:47:04,-11
AMD,nxvicu7,This is a very long way of saying Intel has chose to prioritize competing with Appleâ€™s SoC and ignoring gamerâ€™s and PC enthusiastâ€™s wants/needs.,Intel,2026-01-05 19:51:27,0
AMD,nxuwzmt,"Iâ€™m just waiting for Nova Lake and if it gets delayed into 2027 and if TSMC does the packaging for some chips there may be issues  with supply. TSMC does not like that Intel has IFS and they play dirty, real dirty.  Nova Lake flagship could be at or above $1000, I may go with Ultra 9 285K with the fastest pcie5 nvme and ram, by then hopefully ddr5 is accessible.",Intel,2026-01-05 18:14:53,0
AMD,nxtrgru,My same experience with both of my 265k systems. They have been extremely stable for me and very efficient. Never have to worry about temps and they perform well with a 5080 and 9070XT. Contrary to all the media rhetoric I enjoy gaming with them.,Intel,2026-01-05 15:00:40,16
AMD,nxu6pg9,Basically the same or even less FPS than a 9800X3D consuming 80 watts,Intel,2026-01-05 16:13:55,3
AMD,nxvj13w,"Hi I'm sorry to ask but what does this mean?     9000c38 A-die, 36 d2d and 34 ngu     is that an app or something?",Intel,2026-01-05 19:54:33,2
AMD,nxuzcbk,"Exactly, tuned 285k is just 2-3% away from max tuned 14900KS + DDR4 at 4300MHz CL16.  All Z890 boards are so cheap so I grabbed an APEX with a 265K. Only costed me $600",Intel,2026-01-05 18:25:21,3
AMD,nxvtdhm,"that is exactly the point, if it was just architecture we could live with it and consider it an scurve of innovation, but the process proves that this design is going nowhere",Intel,2026-01-05 20:42:56,6
AMD,nxwwv5w,"New process nodes donâ€™t improve  performance on desktop processors when clock speeds and core counts stay the same. A 10nm monolithic ARL could have performed better and cost them less, although the newer process does improve power consumption which is essential for laptops.",Intel,2026-01-05 23:55:52,1
AMD,nxuiree,">In highly parallelized rendering workloads like Blender or Cinebench, the 24-thread Arrow Lake design is often matching or beating the 32-thread Raptor Lake parts, which proves that the removal of Hyper-Threading was not a net loss for total throughput  So matching perf with a last gen part, after you hit a double node shrink **and** a massive E-core IPC gain and a P-core tock too is fine?   >The ""rent"" paid in silicon area for HT was no longer worth the ""yield"" in multithreaded performance,  This was a mistake according to LBT himself.   >This implies that Intelâ€™s next step must be an aggressive overhaul of the interconnect topology, perhaps moving towards a mesh or a more direct active interposer solution for desktop parts if they want to reclaim the gaming crown from AMDâ€™s X3D parts  Moving to a mesh wouldn't help much, and Intel's mesh's have a reputation for being insanely slow on their Xeon parts.   How much more advanced packaging does Intel have to use to match the latency of AMD using iFOP?   >Â But if you analyze the architecture, the Lion Cove P-core is a marvel of width and prediction capability that is simply being strangled by the packaging logistics  It's not. LNC is both not that all that wide, all the ARM cores beat it in that metric, and the prediction capabilities of LNC is bad- it's a literal regression vs RWC (last gen) in accuracy. It's worse than the E-cores branch prediction accuracy. And it's much worse than Zen 5's as well.   >and the floating-point performance is stellar.  This specifically is not the case. While in previous generations Intel was very competitive with AMD in spec 2017 FP, with ARL vs Zen 5 we see an almost 15% gap.   >The 285K is the cooler, more efficient, strictly professional grown-up in the room that unfortunately forgot how to play games because itâ€™s too busy trying to figure out how to talk to its own memory controller across a microscopic bridge.  Idk why you are trying to trivialize gaming when it pushes a huge percentage of the market, and is why Intel has been repeatedly telling investors they have lost the high end DT market.",Intel,2026-01-05 17:09:57,6
AMD,nxum4ri,"And nobody was saying you're required to. The entire point of the post was to say regardless of your thoughts and purchasing habits, ARL was deliberate, and even smart. Those who look for single metrics by which to judge ARL are missing the point.   Yes, if that single metric is all you care about, by all means go spend your money elsewhere, but ARL is a step sideways so future generations can take leaps forward.",Intel,2026-01-05 17:25:38,8
AMD,nxtkrhd,"This is way too true. I want a healthy Intel and AMD, but I'm also not going to act like Intel being now backed by the US Government and MAGA, as well as securing a well funded partnership with nVidia, doesn't make me feel a lot less like they need consumers to pity buy things from them to encourage innovation.",Intel,2026-01-05 14:24:59,5
AMD,nxtic59,Yes because they do core to cores transfers via their shared l2 rather than the l3/ring. It's sick. Skymont in general is so underrated for how enormous of a performance gain it was. They literally fixed all the ecore issues it's the pcores that flopped,Intel,2026-01-05 14:11:26,17
AMD,nxu08ua,"As someone who â€œwrites like AIâ€ in part because of a learning disability that made it hard for me to write, I tend to organize my thoughts very deliberately. Using lots of punctuation, dashes, etc is now often interpreted as having used AI, although I have no idea whether OP did or didnâ€™t.",Intel,2026-01-05 15:43:44,9
AMD,nxviton,">this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)  It doesn't point out a bunch of BS, it introduces a bunch of new BS that is just straight up, factually wrong.   >i would agree this architecture is very much limited by d2d  Not single core performance like he is implying it is.   Just look at ARL-H vs MTL-H for example.",Intel,2026-01-05 19:53:36,3
AMD,nxv2i2x,"Soon you will not be able to tell what is AI and what was before, where you were living in the Stone Age. Fooz better buckle their seat belts and get rekt, itâ€™s about to get a Bit-Funky.",Intel,2026-01-05 18:39:29,2
AMD,nyi3r20,"\> For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off     \> 265k vs 9700x 23% faster   \> 5 games    You are cute.",Intel,2026-01-09 00:12:42,1
AMD,nxzan7c,AMD Unboxed are not serious and should be ignored at every opportunity. They have a long history of doing what you've said they've done to you. They get in childish arguments on twitter when they aren't blocking people who have data that disagrees with their clear bias.,Intel,2026-01-06 09:33:39,1
AMD,nxu6lhk,I have one 13900k since day one... But like all enthusiastic guys I did some benchmarks and the voltages were a concern. So I tuned the bios after just a few hours of working.  3 years have passed and I have 0 problems. It's like a rocket ðŸš€ very fast and reliable,Intel,2026-01-05 16:13:24,7
AMD,nxzllb5,"Very nice, do you also use a contact frame?",Intel,2026-01-06 11:12:32,1
AMD,nyi50z0,> so my chip never tried to boost 5.9Ghz with crazy voltage.  You have no idea what cause degradation and undervolting does not save CPU.,Intel,2026-01-09 00:19:10,-1
AMD,nxwmvqc,For sure.  I still use 10900k which benefits massively from memory frequency. When running 4500 cl16 it sits in 33-36ns territory. Lowest latency cpu around was 2020.  Doesnt hold up vs today's cpus but the user experience is great.,Intel,2026-01-05 23:03:28,1
AMD,nxxq0wj,"Really, 90ns on DDR5-3600, what else can go wrong here...",Intel,2026-01-06 02:31:08,1
AMD,nxx7vlh,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:14,1
AMD,nyf7a0y,"""Wait till games leverage multicore"".",Intel,2026-01-08 16:22:24,1
AMD,nxz46az,"Could also add 1440p into the mix, at max settings with ray tracing, the CPU is going to matter less and less.",Intel,2026-01-06 08:31:05,1
AMD,nydtdxm,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-08 11:58:45,1
AMD,nxuz1x1,"Are you using a B580, Iâ€™m wondering how it performs.  I have a 5070 OC and an army of Alchemist and Battlemage cards.  I may try to get a 9070 this year and OC / solder it to XT performance. There is a great deal of fake hype around the 9070 XT, Lisa and her army of gawkers really pulled out the hype train for the 9070 XT.  Many influencers (somebody who can post a video to snoozetube) received 9070 XTs for free so they could gimp primp them out to the masses. It has left a stale, rotting smell in my loins.",Intel,2026-01-05 18:24:04,2
AMD,nxwegrj,"Did you used just ""stock"" profile ie NGU and D2D at auto and Intels default profile? Or Intel 200s boost?",Intel,2026-01-05 22:21:22,1
AMD,nxuzm9f,"Both 14900K and 285K won't get frametime spike when the L3 Cache is maxed and no random stutter/issue.  If I want a good gaming experience, I would go for the 14600k instead of the 9800X3D. Way cheaper and works way better.",Intel,2026-01-05 18:26:34,0
AMD,nxvwkhz,"These are the bios overclocking settings, ram speed is 9000mhz, d2d die2die is 3600mhz, ngu chip fabric 3400mhz fully manually tuned",Intel,2026-01-05 20:57:48,5
AMD,ny1yrwf,"> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  This is correct. The node shrink itself does not give IPC gains, what it gives is energy efficiency and area. The reason why people associate node shrinks with better performance is because when they switch a processor to a newer node they typically increase cache sizes and/or improve base/turbo clocks.",Intel,2026-01-06 18:43:59,2
AMD,ny0mjxi,> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  Netburst was awesome!,Intel,2026-01-06 15:03:34,1
AMD,nxvi8ry,The problem is that MTL for all means and purposes should have been that test bed product. Or even lakefield tbh.,Intel,2026-01-05 19:50:56,6
AMD,nxxqpo4,"Nothing smart about it.. they just couldn't do any better than release a half unfinished product because the company is corrupt as hell, fully relying on US gvt injecting  tons of money that ends up in a few top manager pockets instead of being used for restarting the core architecture from scratch.",Intel,2026-01-06 02:34:57,1
AMD,nxubozd,"We were backed strongly by Biden admin too, the CHIPS act money was what Trump gave us, but demanded the stock in return instead.   Which I think is ultimately good for us American citizens. Too many times companies just got hand outs, even Bernie Sanders approved the Intel stock deal. https://www.reuters.com/world/us/us-senator-sanders-favors-trump-plan-take-stake-intel-other-chipmakers-2025-08-20/",Intel,2026-01-05 16:37:08,13
AMD,nxu3xx9,Brotha everyone is tryina get a piece of that maga pie. Or vice versa. It would probably be unlawful to go against maga on fiduciary responsibility alone,Intel,2026-01-05 16:01:00,2
AMD,nxtmb1f,This!,Intel,2026-01-05 14:33:25,5
AMD,nxv74ph,"On my 13600k I also see a shared L2 for each 4-core E-core cluster. Was there a regression between then and now that they've resolved? Or is there some hidden behavior where this shared L2 couldn't actually be used to core-to-core communication without going through the ring?  If you have a source with more info, I'd greatly appreciate it.",Intel,2026-01-05 19:00:02,2
AMD,nxvzfjz,"AI is the aggregation of all the rules and examples fed to it.  You write according to proper ""rules"" or clear organization (which is very much not ""vernacular"" level writing), then boom, you and AI aren't all that different.  It's bloody annoying to try organizing thoughts or points to be easily digested instead of a wall-of-text like you've been doing since Mavis Beacon taught you typing only to have people bitch about the style and ignore the content.",Intel,2026-01-05 21:11:12,3
AMD,nxv1yn0,There are so many of us lol,Intel,2026-01-05 18:37:05,0
AMD,nylpv7p,5 CPU bound games enough.,Intel,2026-01-09 14:41:06,1
AMD,nxzllwo,Do you use contact frame for the cpu?,Intel,2026-01-06 11:12:41,1
AMD,nxzluhl,"yes, i do",Intel,2026-01-06 11:14:44,1
AMD,nyi5ufb,"Please, High life form, what causes degradation? enlighten us, mere mortal",Intel,2026-01-09 00:23:20,2
AMD,nxv14y0,I do have a B580 but haven't paired it with the 265k cpus yet. The B580 is currently in a i5-13600 system.,Intel,2026-01-05 18:33:23,2
AMD,nya2yya,I have a 265k and b580. Had it about 6 months. First desktop I've had in about 20 years. Works flawless for everything I need. Very stable. Very fast for my needs.. I pretty much only play warcraft though.,Intel,2026-01-07 21:44:30,1
AMD,nxwoxfl,"Stock. One system uses a Z890 mb with 8200mhz cudimm and the other uses a B860 MB with 8000mhz cudimm. I tried 200S boost on the Z890 and it didn't benchmark much faster at all for the games I play, plus I had occasional lockups. It wasn't worth leaving it on for me so everything is at the Intel default profile.",Intel,2026-01-05 23:13:58,1
AMD,nxzc32p,Lmao,Intel,2026-01-06 09:47:24,5
AMD,nxv0qyc,"Lol, Userbenchmark guy making things up.",Intel,2026-01-05 18:31:40,13
AMD,nxw9ntc,thank you,Intel,2026-01-05 21:58:18,2
AMD,ny41irt,"Clock speeds havenâ€™t meaningfully improved since 32 nm Sandy Bridge. The 2500K and 2600K could overclock to around 5.0 GHz, and modern CPUs still run at roughly the same frequencies. Core counts are also similarâ€”Intelâ€™s 14 nm 7980XE had 18 Skylake cores. Cache increases are possible on older nodes as well, so newer process nodes mainly improve efficiency these days, which is contrary to what most expect of them.  A 10nm monolithic ARL could have performed better at least in gaming on desktop.",Intel,2026-01-07 00:40:20,1
AMD,nydvpul,Apart from straight up increasing frequency improved nodes also make improvements to CPU possible even at the same clock.,Intel,2026-01-08 12:15:24,1
AMD,nxvyiba,"I suspect they just didn't have enough time to change anything. MTL releasing in Dec 2023, internal testing I'm sure yielding some set of data, and external reviews giving other feedback, even by the release of MTL, ARL has probably been in the pipeline for years and probably locked into certain design choices regardless of the feedback and testing.  Additionally I suspect that on mobile chips/laptop gaming rigs there's less focus on each individual part because  a) few people hardcore game on laptops b) the latency can be blamed on other things since laptops are a prepackaged consortium of parts and it's harder to isolate, and  c) therefore laptops tend to be evaluated as a whole rather than the individual pieces that comprise them.   Therefore the ""latency issues"" only became a massive kerfuffle when the offending cpu could be isolated and tested alone, and reviewers needed something to complain about.   That's even if Intel was considering latency as the issue everyone made it out to be. Intel could have looked at it and considered the latency worth the cost to improve in other areas and serve as intels seminal desktop entry utilizing disaggregated silicon. Then gamers came and lost their shit that the newest Intel chip deprioritized something that impacted their precious fps- even though the impact was ""the new one is approximately the same to maybe a little worse as last gen in most games"".   Notwithstanding the fact that the 200 series chips retain healthy gen-to-gen perf uplift and a _massive efficiency improvement_ in productivity and general computing, and boosting the NGU and D2D clocks (which are _very_ low from the factory, and can be done with the app that Intel _has specifically designed for tuning their chips_ and is freely available (XTU)) brings the gaming performance to ""approximately the same to a little better in most games"". Contrary to what some people may think these chips are not solely or even majority used for gaming and there are other use cases Intel can to think about/chose to prioritize.   To be fair, gamers and tech enthusiasts are the ones who will care the most and therefore have a disproportionately large impact on the kind of publicity the chip will get. So, was this intels smartest PR decision? Maybe not. But I think it was still a sound engineering decision, regardless of whether or not they had feedback or data on the issue, even if it didn't go over very well with their loudest customer base.  And this is all overshadowed by the fact that if you stuck even the most hardcore of gamers on a blind trial and told them to identify what kind of chip they were gaming on, I have a hard time believing any of them would be able to tell with any certainty which is which.   Once you get to 60 fps, the vast majority of people stop caring. Whats five fps going to do to radically change your gaming experience so much that it's unplayable? Which brings me back to one of my original points: If you care _that much_, you can go spend your money _elsewhere_.",Intel,2026-01-05 21:06:54,3
AMD,nxun1ug,"I'm aware of CHIPS.  Like I said, I want Intel to be competitive. I hope if their new layouts mature into something that can retake marketshare that the same care will be taken to preserve AMD. AFAIK, no one came to lend them money or sign large multifaceted partnerships at that time. This ""must save Intel,"" movement is something that is borne out of how big Intel is. They're too big to fail, it would have too many implications for the economy. It's not tit for tat, but as they are both American companies and AMD has significantly more invested in my country, I don't feel particularly motivated to help bail out Intel when they have the equivalent of the iron rice bowl rolled out for them at the moment. I'll buy what works best for my needs at the time I'm buying, as always, and if Intel can make a better gaming focused, with some imagery stitching on the side, processor than AMD can at the time I'm buying, that's the direction I'll go.  Ultimately the 200 series is what the Ryzen 1000 series was, but more stable as Intel has always had better support both internally and from vendors. I have no doubt Intel may catch AMD. My worry will be, what happens when they put their boots on AMDs throat, especially now that nVidia has more control over the situation than ever before?",Intel,2026-01-05 17:29:53,1
AMD,nxxrp83,Pouring free gvt money into Intel is only making the company high managers less interested in restructuring and improving the company when instead they can just keep going at snails pace while cashing in. Gvt money is only supporting growth of corruption inside the company and stalling progress.,Intel,2026-01-06 02:40:23,1
AMD,nxv6i3j,"It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor. It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.",Intel,2026-01-05 18:57:13,1
AMD,nxty6pw,I upped the tREFI of my memory in my MSI Vector with Intel Ultra 9 275HX and got some fps gains in Fortnite/Valorant/Hogwarts Legacy at 1200p,Intel,2026-01-05 15:33:56,2
AMD,nxv7w1f,Yeah pre arrow lake it was particularly shit as it would instead go through BOTH l2 and then to l3 to communicate between cores like a shitty ISP route to a game server   That is obviously worse than normal but core to core communication being done through even shared l2 is very rare so even without that quirk it's not expected  Go to chips and cheese.com they have a ton of information about this stuff. Like their skymont article in this case it details all the huge upgrades,Intel,2026-01-05 19:03:27,3
AMD,nxx7gwg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:51:03,1
AMD,nylunna,Every respectable benchmark is CPU-bound (including HUB whom you try to diss) because every sane reviewer uses a fat 5090 and 1080p to show CPU differences.  Even if you did not cherry-pick the games the review with fast GPU and more games is much more indicative of CPU performance than your benchmarks.,Intel,2026-01-09 15:04:11,1
AMD,ny47bmi,No I don't need it... You need a 360mm AIO that's all... After that it's all about bios settings.,Intel,2026-01-07 01:11:04,1
AMD,nxzm8r1,which brand do you use?,Intel,2026-01-06 11:18:02,1
AMD,nyi87lr,"1. tvb working incorrectly 2. unlimited current set by motherboards which gets only higher when undervolted because same power limit at lower voltage necessarily means more current   Motherboards were undervolting CPUs with wrong LLC calibration intentionally   4. ~~motherboards having wrong LLC calibration~~ EDIT: Intel CPUs themselves requesting abnormal voltage in anticipation of frequency boost resulting in abnormal idle voltage (just remembered it correctly) 5. motherboards having wrong LLC calibration resulting in abnormal load voltage  Not a single smart ass on the planet could have predicted that all of the above can happen with ""stock"" settings and foresee all of this.  No frequency limit will save CPU when it dies in idle state.",Intel,2026-01-09 00:35:27,-1
AMD,nxyej0v,"Thanks so in short:  You use only ""intel default profile"" and enabled XMP on your CUDIMM's right?   No further manual tweaks under NGU or D2D and RING values or any other critical tweaks pertaining to voltages no?",Intel,2026-01-06 04:57:20,2
AMD,ny5j3g0,"Bruh my 2600k couldn't hit 5ghz. I struggled to get to 4.4. same with my 5820k, which would do 4.2, both with voltage bumps and good cooling.   Modern processors definitely run faster. The 12900k in my media pc will happily sit at 5.2 on lightly threaded loads   Tho you are right about the core counts. High core numbers have been around for eons, they were just far too expensive for mainstream use",Intel,2026-01-07 05:57:02,1
AMD,nxuzom9,"It's because AMD doesn't manufacture chips, we do. That's what the funding was for, to revive manufacturing leadership in the US not to save failing architectures.  And yes as a consumer buy what works best for your needs and budget. That's the best thing about Ryzen and AMD's resurgence.  I don't get your ""boots on throat"" comment, but to think AMD hasn't done anything mischievous in the past is, well a lot has happened between the two companies in 40 years.",Intel,2026-01-05 18:26:52,5
AMD,nxv19d7,"A strong Intel IFS is a strong US. Many people get disillusioned and deceived through all the cognitive dissonance on social media, especially gamers. Unfortunately they are easy to manipulate.  Many people are buying INTC in the US to retire on, we will see this more and more as we approach 2030. INTEL IFS has to succeed otherwise the US will be doomed in this century. Even India is getting into the semiconductor industry now and Intel is working with them. We need IFS to be on top, cream of the crop, I need a taste.  You are defeating yourself by getting wrapped up with all the geopolitical propaganda, go take a walk in the woods and get away from it all.  Checks are in the mail.",Intel,2026-01-05 18:33:57,0
AMD,nxypf2c,"What? The deal under Trump was for 10% of the stake in shares. The government can sell the shares at any time to get a return. Since the stock has doubled since, it looks like it was a great deal for tax payers.",Intel,2026-01-06 06:19:05,2
AMD,nxv1ubv,Which way is up...?,Intel,2026-01-05 18:36:33,1
AMD,nxw36zg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.  Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2026-01-05 21:28:35,1
AMD,nymqb9s,"Here are few more games  [https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U\_](https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U_)  [https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI](https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI)  [https://youtu.be/mQ80rNg0k3c?si=sgXQJ\_kh0Nb22BIM](https://youtu.be/mQ80rNg0k3c?si=sgXQJ_kh0Nb22BIM)  [https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU](https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU)  Non 3ds are bad for gaming, thats a fact.  Here is my latest test for 7800x3d vs 14700k  [https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2](https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2)",Intel,2026-01-09 17:27:05,1
AMD,ny1tgj8,thermalright.   Any frame will do. get whatever is cheap,Intel,2026-01-06 18:20:04,1
AMD,nyimhf2,"Let me explain why it will not degrade my CPU.  1. **TVB (Thermal Velocity Boost) is disabled when I set the turbo limit.** 2. **Current is not the issue.**Â First, even Intel chips were failing at idle. Second, you stated that ""the same power limit at lower voltage necessarily means more current."" That is incorrect, if it drew more current, then why does lowering voltage reduce power consumption? Have you missed basic physics? 3. **CPUs follow a voltage-frequency (V-F) curve.**Â The main issue was that either the CPU or the motherboard was supplying excessive voltage, or the CPU was requesting too much. Higher voltage is required for extreme single-core boosts, such as 5.9 GHz. This is why i3 and i5 CPUs were not affected. When I limit my boost and apply an undervolt, the CPU no longer requests high voltage. It might request slightly higher voltage for 5.5 GHz, but that is nowhere near the voltage required for a 5.9 GHz boost. 4. **The same principle applies.**  Its seems you are a failure of high life form.   Maybe read here more. it was already to high voltage   [https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239](https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239)  Maybe you havent read much. if you undervolted only using AC/DC loadline and didnt limit max frequency. Then your CPU could have degraded.   While I used offset and set the max turbo limit.  I know how silicon works. I tune every CPU and GPU I buy.",Intel,2026-01-09 01:50:53,2
AMD,nyx4wv5,"Yeah, I do use XMP to get 8000/8200hz but no tweaks and everything is Intel default.",Intel,2026-01-11 04:51:00,1
AMD,nxx7x5q,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:28,1
AMD,ny6b0of,"Probably just bad chips. In almost every review, 4.7â€“4.8 GHz was achievable on the 2600K, and 4.6â€“5.0 GHz on Haswell-E. 5.0ghz wasn't hard to do on 8700k-9900k-10900k. Also, ARL runs at lower clock speeds than RPL, so in some cases newer process nodes actually clock lower.   this happened many times before like 32nm SNB OC better than 22nm IVB and haswell or even 14nm 6000 series skylake, so no in most cases new process nodes don't improve clock speeds or at least not on desktop.",Intel,2026-01-07 10:04:24,0
AMD,nxv4op7,"Okay, so we've gone right off the rails of a logical discussion. I fail to see how we went from talking about a failing architecture, which I don't think of Intel's product line as, to the whole bailout discussion. Maybe because I said AMD didn't get bailed out? I think maybe you forget that AMD had to offload global foundries, I don't think anyone even blinked when that happened and it's likely because that was a different era.   My post had nothing to do with the bailout versus architecture (or saving it) , apart from mentioning that I don't believe as a consumer I should feel motivated to buy Intel over AMD, or any other American firm, at the moment.   To be very clear, as this is personal for you, I do own Intel equipment including an Arc graphics setup for one of my children. I'm not anti-Intel or anti-American at all.   I know the history between AMD and Intel fairly well. Oversimplifying, AMD as it is doesn't exist without Intel. AMD only grew because they were the most successful second source producer for Intel, and the most successful at riding the thin grey line between patient infringement and unique implementation of similar IP that kept them alive while everyone else in the X86 space either died or became irrelevant to the consumer or enterprise space. After Itanium, the story levels out with both companies becoming effectively unwilling AMD64 codependents, and I'm saying that humorously.   Intel would gladly own the X86 market outright. So would AMD. At the end of the day, we need the two driving innovation through competition. Even if the CHIPS Act, the government stock acquisition, and the nVidia partnership are solely aimed at bringing more, needed, chip manufacturing to the US, it could create a situation where that amount of leverage puts Intel into a hyper dominant position again in the near future. Honestly, I hope I'm wrong.",Intel,2026-01-05 18:49:13,2
AMD,nxwdvf9,"Hey sorry to interject like that, can you ask around what ppl in the team think the safe VCCSA voltage for raptor lake is beyond standard spec? You can dm the answer if u want. I'm from Russia so it's not like I'll go run RMA'ing this stuff just because you told me that info",Intel,2026-01-05 22:18:29,1
AMD,nxv1r9g,"Did you even read my post, or are you a bot? Seriously asking. I didn't bring up geopolitics. I live in Canada my dude. AMD has their graphics office still right beside the TO airport, and that's only a side comment. AMD is American too. My concern isn't about anything you just said lol.",Intel,2026-01-05 18:36:10,0
AMD,nxv9flb,65535,Intel,2026-01-05 19:10:25,1
AMD,nyk009z,"\>then why does lowering voltage reduce power consumption?  I have no idea what is the workload you are using which is guaranteed to never hit the limits. Solitaire?   Undervolting does not guarantee lower power usage with modern boosting CPUs. It very obviously (to any sane person) depends on workload.  \>Maybe read here more.   \>Thomas\_Hannaford, Employee   â€Ž You are so cute.  \>Motherboard power delivery settings exceeding Intel power guidance.  Literally what I said in 2, 3, 4 but in stupid terms.  \>Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity.  Literally what I said.  \>IntelÂ® reaffirms that both **IntelÂ® Coreâ„¢ 13th and 14th Gen mobile processors** and future client product families â€“ including the codename Lunar Lake and Arrow Lake families - are unaffected by the Vmin Shift Instability issue.  This is verifiably bullshit because nothing about mobile processors protects them from wrong voltage received because of wrong LLC settings.  \>CPUs follow a voltage-frequency (V-F) curve  If you do not know that Intel is requesting high voltage to avoid insufficient voltage before frequency boost you do not know jack shit. It is literally what happens under description ""**Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity**"" and there is a video proof of that with an oscilloscope.  No settings which you mentioned are preventing irreversible damage.   Claiming that you use Intel for reliability after you did not wish to use your own Intel at stock settings is laughable.",Intel,2026-01-09 07:01:30,0
AMD,nyjz7xj,"You skipped the 8600K, it also did 5ghz",Intel,2026-01-09 06:54:55,1
AMD,nxv805h,"My initial comments were on, ""I'm also not going to act like Intel being now backed by the US Government and MAGA"" and what the funding was for.  Edit: and my response was about who also backed the funding.",Intel,2026-01-05 19:03:57,1
AMD,nxwhiw8,"All I can recommend and say is follow whatever guidelines you're given officially and make sure your BIOS is updated. All those teams work to make sure it's delivered to the customer. Otherwise it's all random, some parts can handle higher voltage, some can't.  The term silicon lottery is real and just a nature of small scale manufacturing, EM and quantum effects these days.  I will say, with our new CEO a lot of these customer issues are now streamlined internally. Used to be layers between engineering and customer interaction, so I expect better responses and reliability than before.",Intel,2026-01-05 22:36:31,1
AMD,nxv9vyk,"Ah, so you mean *lower* the TREFI then, from what JEDEC or XMP specified?",Intel,2026-01-05 19:12:28,1
AMD,nxv944s,"On that side, it'll depend on what the US does with the stock it has. Canada bought GM stock during the 2008 crash, and then quietly sold it off as GM recovered. If the US does that, I don't really see an issue.  The MAGA part was a half hearted comment, made to follow the weirdness of the whole situation and comment I was replying on as well. As I said in a separate post:  ""It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor (speaking about how decisions beyond national security are now driven by politics and trying to be on the right side at any moment when it matters). It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.""  I'm going to add, light heartedly, I do hope we see AMD/nVidia/Qualcomm/etc manufactured and final packaged products coming out of Intel Foundries one day. I'm not sure how or if it will work, but the situation seems dead set not to allow significant further nodes beyond 2 nm or packaging to occur outside of North America. If the US wants viable national chip production, Intel is the better option, I hope it works out in a way that maintains design level competition while meeting national security goals.",Intel,2026-01-05 19:08:59,1
AMD,nxwo9wf,"I get that. But itâ€™d be nice to have a â€œthis voltage is safe for 99% of the cpus and this voltage is the LD50 for the cpuâ€. That would probably improve customer relations but your legal team might be very, very unhappy with that lol.   Anyway cheers for the response, with the way things are looking up for intel, i might be buying some stocks soon",Intel,2026-01-05 23:10:36,1
AMD,nxvnm9c,JEDEC 5600 cl 40 kingston fury sodimm 2x 16 gb,Intel,2026-01-05 20:15:57,1
AMD,nxws02u,"I mean, you can just figure that out yourself and buy new CPUs till you get one working :P",Intel,2026-01-05 23:30:06,1
AMD,nxvpz7b,"Yeah, but what was TREFI before you lowered it?",Intel,2026-01-05 20:27:01,1
AMD,nxvvs3t,10000 ish,Intel,2026-01-05 20:54:09,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
AMD,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
AMD,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
AMD,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
AMD,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
AMD,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
AMD,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
AMD,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
AMD,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
AMD,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,4
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,17
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSDâ€™s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake wonâ€™t sell as well because of this.,Intel,2025-12-19 09:41:00,5
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,13
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,10
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,2
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nym6t8e,"Hm, we will see what happens with the stock price soon, but so far so good",Intel,2026-01-09 15:59:57,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blueâ€™s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Letâ€™s get it done. Iâ€™m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-18
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.* Â ***\[Edit:Â to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,11
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,15
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,4
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,9
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month ðŸ˜‚,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps itâ€™s better if you post it on the r/intelstock subreddit instead ðŸ¤ª",Intel,2025-12-19 05:37:45,1
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 ðŸ¤ž",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,3
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,1
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,6
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,4
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,7
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,12
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,5
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,6
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250 Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,3
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,9
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,2
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasnâ€™t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,4
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,-1
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,1
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,4
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,1
AMD,nuu7dr5,"It can expand in the future but this is a trial, itâ€™s not yet a long term commitment until the outcome of the project is known (final evaluation wonâ€™t be until 2026/2027). 14A is not part of RAMP-C, itâ€™s still in phase III trial with 18A. Thereâ€™s been no additional RAMP-C design calls via NSTXL that Iâ€™m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,2
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed ðŸ˜”,Intel,2025-12-24 15:43:21,2
AMD,nybmp7b,did you buy one? I have owned two. A A750 and now a B580. They are great cards for the price paid. I'd like to upgrade to a B60 PRO. 24GB VRAM sounds amazing especially for $600. but I can't find one in stock anywhere.,Intel,2026-01-08 02:20:39,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,52
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,9
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,8
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,4
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,3
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If itâ€™s just â€œ16% faster than 890mâ€ itâ€™s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,7
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,7
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.Â      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.Â  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.Â  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,3
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.Â  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,29
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,27
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,8
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,7
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme ðŸ˜˜,Intel,2025-12-11 20:12:02,5
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,4
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was â€œstableâ€,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,4
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I donâ€™t even try.,Intel,2025-12-11 21:24:52,14
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasnâ€™t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,5
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,5
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is â€œslowâ€ is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already â€œbeatingâ€ AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,50
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,9
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,4
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) â€¦ I got think pad 780M laptop by company I work for. Randomly display wonâ€™t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,11
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,8
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMDâ€¦.?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,1
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,0
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-14
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,8
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,9
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isnâ€™t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey Iâ€™m looking at the exact same laptop that you have. Can you tell me about the build quality and if thereâ€™s any keyboard flex when pressing down on it? Please tell me. Iâ€™m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,6
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,2
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,1
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,29
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,18
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,5
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.Â    Thinkpads are solid machines that are easy to fix.Â    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,4
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isnâ€™t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,16
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,4
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-10
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,20
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,7
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,4
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and Iâ€™m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure ðŸ˜,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go ðŸ˜‰,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer ðŸ˜,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,4
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and Iâ€™ll tear it down and prep it. Iâ€™ll see how it goes when itâ€™s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. Iâ€™ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,80
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,29
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores theyâ€™re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K arenâ€™t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointingÂ  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,3
AMD,ngiqxv3,"I havenâ€™t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30â€“40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,34
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,33
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,7
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,50
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,9
AMD,ngn0xy1,">asking others what scores theyâ€™re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K arenâ€™t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,1
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,4
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,4
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I havenâ€™t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,1
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,11
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,19
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,2
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. Iâ€™ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. Iâ€™ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but Iâ€™m still not sure. Since Iâ€™m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesnâ€™t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30â€“40% slower than the 14700K. Why does it feel like Iâ€™m the only one who knows this?  As for proof, Iâ€™ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. Iâ€™ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,1
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,Â  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,4
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-4
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,5
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. Iâ€™d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,1
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,7
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,17
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,-1
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,8
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,9
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,4
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,6
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuberâ€™s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores theyâ€™re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isnâ€™t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,7
AMD,ngqfmrw,"Did you get any more FPS when you tried it? Iâ€™m getting good frames without it, so I donâ€™t think itâ€™s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there werenâ€™t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,5
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,13
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.Â   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,4
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didnâ€™t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"Whatâ€™s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If youâ€™re comparing score screenshots to OBS-recorded videos, you shouldnâ€™t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. Iâ€™ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think theyâ€™d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since Iâ€™ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,20
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,4
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,6
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,10
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,3
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,3
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-11
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.Â   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform thatâ€™s already halfway out the door.",Intel,2025-09-26 15:54:51,39
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,12
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,4
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-8
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,19
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-8
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,4
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,7
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,7
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  Itâ€™s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, itâ€™s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,10
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,7
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what â€œcheaperâ€ is seen as.",Intel,2025-09-27 17:30:41,3
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.Â    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,1
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,1
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe Iâ€™ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so itâ€™s pretty relevant Iâ€™d say. Of course itâ€™s anecdotal but I donâ€™t think itâ€™s too uncommon,Intel,2025-09-27 01:22:31,-1
AMD,ngg1fuo,"I agree with you, itâ€™s daft, no doubt about it. But youâ€™ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and itâ€™s still holding its own. Is it on par with the 9800X3D? Of course not. But itâ€™s still a very capable bit of kit. Next time I upgrade, Iâ€™ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,11
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,4
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-3
AMD,ngc0yus,"Even accepting that as true, I donâ€™t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. Itâ€™s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet itâ€™s probably not worth their money and time to test it.Â   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,10
AMD,ngeb3z7,Isnâ€™t that just either lying or exaggerating?   I know those words donâ€™t â€œgo hardâ€ and are â€œlow keyâ€ boring.Â   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,7
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,5
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,4
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, Iâ€™m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:   Â No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {â€¦}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,nq4im71,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Intel,2025-11-22 01:57:26,1
AMD,nq9ltvl,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Intel,2025-11-22 22:42:16,1
AMD,nqoo97z,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasnâ€™t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED â€” nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, Iâ€™ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If itâ€™s too tight: no POST. If itâ€™s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85Â°C. Which is not normal at all, as I used to run the game at around 100â€“110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   Iâ€™m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Intel,2025-11-25 10:53:08,1
AMD,nr6yiuz,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Intel,2025-11-28 09:31:06,1
AMD,nsz9e66,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Intel,2025-12-08 18:41:34,1
AMD,ntcs1o8,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Intel,2025-12-10 21:00:34,1
AMD,ntn8xto,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Intel,2025-12-12 13:56:20,1
AMD,ntoe22w,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Intel,2025-12-12 17:25:39,1
AMD,nttrt4h,"Hey everyone, looking for second opinions because this behavior doesnâ€™t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25â€“30Â°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60â€“75Â°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95â€“100Â°C. When acessing the Bios, the temp shown is 78-88Â°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95â€“100Â°C  Fans ramp up correctly  Iâ€™ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W â†’ temps drop but CPU becomes extremely slow, 1Ëœ3GHZ, but still hitting 50-60ÂºC on idle and 80-90ÂºC on the rest, 88 on the bios as well.  Undervolted â€“0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Intel,2025-12-13 15:34:32,1
AMD,nupcpsu,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadnâ€™t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didnâ€™t get a reply.  So, I went into the ticketing system and didnâ€™t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if Iâ€™d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said heâ€™d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didnâ€™t see his second response in the ticket.  Is that really how this goes down? Thereâ€™s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Intel,2025-12-18 16:06:09,1
AMD,nv6u3b9,"Hi,  Iâ€™m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always â€œphone number unreachableâ€.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system canâ€™t use A2P SMS for Indonesia Region",Intel,2025-12-21 12:36:51,1
AMD,nvbb6wv,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Intel,2025-12-22 03:46:16,1
AMD,nw4g1zt,VR Support for ARC GPU? :),Intel,2025-12-27 02:03:19,1
AMD,nwenzma,"Hello, I'm experiencing an intermittent but severe system hard freezes on a prebuilt PowerSpec G441 desktop purchased from Micro Center 2 years ago. The freezes occur during gaming, mixed workloads (gaming + video playback), remote desktop usage (TeamViewer), and occasionally shortly after logging into Windows with no workload running.  When my PC hard freezes:  \- Mouse is unresponsive  \- Keyboard is unresponsive  \- No audio sound  \- No BSOD or error message     My PC does not recover and requires a manual power button reboot.  After a forced reboot, Windows loads normally and allows me to log in but the system may freeze again within 1â€“2 minutes, even at idle. Other times, the system may run normally for days before the issue reappears.  No relevant error logs are generated at the time of the freezes. Event Viewer only shows Kernel Power events related to the forced shutdown.  So far I've tried all possible solutions to resolve my issue:  \- Updated motherboard BIOS to the latest available version  \- Updated Windows 11 fully and also reinstalled Windows 11 through USB  \- Performed clean GPU driver reinstallations  \- Verified CPU and GPU temperatures (normal under load)  \- Ran hardware stress tests and diagnostics (CPU, GPU, memory, storage) and no failures detected  \- Tested each monitor individually (dual monitor setup)  \- Swapped HDMI/DisplayPort cables and ports  \- Tested my PC without background applications  \- Returned my PC to stock settings (no overclocks, no undervolts, no XMP changes beyond default)  I've also took my pc to a local repair shop, which they could not reproduce the issue but suggested it may be CPU or PSU related issue. However, when I took my PC to Micro Center, technicians ran similar diagnostics and stress tests but were also unable to reproduce the freezes. They have also tested the PSU voltages and it passed. Despite extensive testing, the issue remains unresolved and continues to occur consistently now.  Specs:  Operating System: Windows 11 Pro 25H2 (Build 26200.7462)  CPU: Intel Core i7-13700KF (stock settings)  GPU: NVIDIA GeForce RTX 3070 Ti 8GB (stock)  Motherboard: MSI PRO Z690-A WIFI (MS-7D25), BIOS v5.32  RAM: 32GB (2Ã—16GB) DDR5-5600 (G.Skill)  Storage: WD Blue SN570 1TB NVMe SSD  Cooling: NZXT Kraken 240mm AIO liquid CPU cooler  PSU: 750W 80+ Gold (PowerSpec OEM, included with prebuilt)  Display: Dual monitor setup, tested individually  I gave a call to Intel and now waiting for a call back. Thank you and I appreciate any guidance!",Intel,2025-12-28 18:50:36,1
AMD,nwjovjk,"Does my 12700f support TME?  In specs, there's no single line about TME. 12700's specs declare TME support. Processor Identification Utility does not mention TME, `hwinfo64` shows TME in gray, and `cpuid -1 | rg TME` prints `TME: Total Memory Encryption = false`. There can i check whether my cpu support TME or not?",Intel,2025-12-29 14:08:35,1
AMD,nwkfoww,"I have an i5-13600kf and a asus strix b760-i motherboard and mostly it runs fine but in specific games I get random shutdowns. Most commonly hell divers and now expedition 33.  I found the vmin instability issue and patched my bios to the latest version now but I still get it happening. Is there any suggested settings changes that have been found to help or is the only course to look at an RMA? Iâ€™d rather not be out of a PC for a while so trying to seek help to avoid that if possible.  Whatâ€™s I find odd is I never get any errors or BSOD. Just the monitor goes black and the system locks up, the fans are still running but it has to be shutdown by holding the power button.",Intel,2025-12-29 16:25:47,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with yourÂ Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [IntelÂ® Driver & Support Assistant (IntelÂ® DSA) Results in â€œSorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you donâ€™t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of IntelÂ® Coreâ„¢ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows,Â disable Turbo and run the system to see if the instability continues. Â If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version youâ€™re using and where you downloaded it fromâ€”was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560Ã—1440 at 60Hz. Ultra-wide resolutions like 3440Ã—1440 often arenâ€™t exposed because theyâ€™re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters wonâ€™t work** because HDMI and DisplayPort use different signaling. Youâ€™d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **wonâ€™t get 3440Ã—1440 at 144Hz,** maybe 3440Ã—1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playgroundâ€™s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM. Â Currently you can download the installer for discrete GPUs. Â We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind thatÂ [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html)Â for integrated GPUs, so 16GB or more of system RAM are required) Â and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of Â HDD/SDD requirements: 8GB w/o models, Â \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nqg4c7m,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit**Â ourÂ [Newsroom](https://newsroom.intel.com/)Â for the most recent announcements and news releases.",Intel,2025-11-24 00:10:02,1
AMD,nqg5tvg,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Intel,2025-11-24 00:18:36,1
AMD,nqry4gw,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturerâ€™s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Intel,2025-11-25 21:43:16,1
AMD,nrmm6fq,"u/BudgetPractical8748 Â Â Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings. Â As always, system performance is dependent on configuration and several other factors.",Intel,2025-12-01 00:19:07,1
AMD,ns3syx7,Nope got cash back,Intel,2025-12-03 18:08:27,1
AMD,nt12wk5,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HPÂ® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HPÂ®ï¸ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Intel,2025-12-09 00:22:32,1
AMD,ntk9thw,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [IntelÂ® 11th â€“ 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Intel,2025-12-12 00:44:01,1
AMD,ntyyo2o,"u/MISINFORMEDDNA Â I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Intel,2025-12-14 12:43:17,1
AMD,ntz1is6,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.Â   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Intel,2025-12-14 13:04:59,1
AMD,ntz4fy4,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[IntelÂ® Coreâ„¢ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation.Â Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Intel,2025-12-14 13:25:40,1
AMD,nurwhxx,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Intel,2025-12-19 00:03:08,1
AMD,nvbnrfy,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-12-22 05:14:39,1
AMD,nvfrjth,"u/earwig2000 Let me check this internally. From what I see, youâ€™ve already tried a lot of steps to address the game crash issue. Iâ€™ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Intel,2025-12-22 21:29:55,1
AMD,nvmgqub,"u/earwig2000 Have you tried doing a clean installation of the graphics driver usingÂ [DDUÂ ](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)and then installing the latest version from ourÂ [download center](https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html)? After that, please retest the game. If the issue still persists, could you share your PSU make, model, and wattage? Also, check if Resizable BAR (ReBAR) is enabled in your BIOS. Finally, review the Event Viewer for any error messages or crash-related events, this will help us determine whether the problem is driver-level or application-related.",Intel,2025-12-23 22:44:39,1
AMD,nwglbda,u/outlander94 Kindly check this article: [Is Virtual Reality (VR) supported on IntelÂ® Arcâ„¢ A-Series and...](https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html),Intel,2025-12-29 00:38:16,1
AMD,nwgmhqt,"u/Kai-juu We trust the technicianâ€™s diagnosis of the system. However, since the unit is a prebuilt, it is likely to have a tray processor. Based on our warranty terms and conditions, we can only replace boxed processors. For a faster turnaround time, please first verify whether the processor is tray or boxed using our website. Once you confirm the type, I can guide you through the next steps.  [Warranty Information](https://supporttickets.intel.com/s/warrantyinfo?language=en_US)  [Where to Find IntelÂ® Boxed Processor Serial Numbers (FPO and ATPO)...](https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html)  [Warranty Policy for IntelÂ® Boxed and Tray Processors](https://www.intel.com/content/www/us/en/support/articles/000024255/processors.html)",Intel,2025-12-29 00:44:27,1
AMD,nwohvpq,"u/Sk7Str1p3 I can see you've already tried several tools and are getting mixed results, which is definitely frustrating.  To help you out better, could you share what's driving this inquiry? Are you working on a security project, dealing with compliance requirements, or troubleshooting a specific feature? Understanding your use case will help me point you toward the right verification methods or suggest alternatives if needed. The more context you can provide, the better I can assist you!",Intel,2025-12-30 05:04:29,1
AMD,nwvjztq,"u/pheoxs you may try to follow this article [Computer Randomly Reboots or Shuts Down](https://www.intel.com/content/www/us/en/support/articles/000035903/processors.html) if the issue persist, disable Turbo boost If the motherboard BIOS allows and run the system to see if the instability continues. Â If the instability ceases with Turbo disabled, it is likely that the processor is affected and need a replacement.",Intel,2025-12-31 06:58:32,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nqkt81n,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Intel,2025-11-24 19:16:09,1
AMD,nz1bddl,"The issue turned out to be Bitdefender (which uses hyper-v), which I believe Killer services depend on. After uninstalling all Killer stuff, I was able to delete the Bitdefender driver and my computer is now stable.",Intel,2026-01-11 20:37:01,1
AMD,nvmmo42,"Tried running a clean driver reinstall using DDU. (I'm pretty sure I did this last install too but did it again to double check) and that didn't fix the issue.  ReBAR is enabled, it was on by default.  My PSU is the [Thermaltake Toughpower 650W Gold](https://www.thermaltake.com/toughpower-650w-gold-modular.html)  Windows event viewer did pick up the crash [(imgur link)](https://imgur.com/NhS5e6l), not sure what to make of it though.",Intel,2025-12-23 23:18:45,1
AMD,nwgvzmr,"u/Intel_Support Thank you for your guidance. My system is a prebuilt, so the processor never came in a box and is likely a tray CPU. Micro Center told me to reach out to Intel for warranty support, so I just want to confirm, should I be working with Intel directly or do you recommend contacting PowerSpec/Micro Center for the RMA? Thank you again, I look forward for the next steps!",Intel,2025-12-29 01:38:35,1
AMD,nwr1jrz,"Yes, I am information security hobbyist, I have a research project on countering attackers with physical access to the machines. Unfortunately, my current hardware is average gaming PC - i7 12700f with h670 motherboard and I cannot update hardware.",Intel,2025-12-30 16:12:50,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued IntelÂ® Killerâ„¢ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.  Â   Based on your initial description, here's what I recommend:  Â   First, please review this article-[Customer Support Options for Discontinued IntelÂ® Killerâ„¢ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html)Â and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html).Â Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.  Â   If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver  Â   For additional troubleshooting, here's another helpful articleÂ that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most IntelÂ® Killerâ„¢...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)  Â   If you've tried these steps and the issue persists, I'd recommend coordinating directly withÂ [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. Iâ€™ve reviewed case number and confirmed that itâ€™s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic IntelÂ® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see thisÂ [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nvz4tef,"u/earwig2000 ï»¿Thank you for sharing this information. I will begin investigating the issue and attempt to replicate it on our end. I'll post an update here or notify you directly once there are any developments. If I need further details, I'll reach out to you here. I appreciate your patience as I work on this matter.Â   [](javascript:void(0);)",Intel,2025-12-26 04:28:47,1
AMD,nwnx92g,"u/Kai-juu According to our warranty policy, RMAs for tray processors must be handled by the original place of purchase, as clearly stated in the article I referenced.",Intel,2025-12-30 02:59:10,1
AMD,nwvn4dm,"u/Sk7Str1p3 I see,Â let me loop in our product support engineer on this one and dig into it a bit more. I'll circle back with you once I have a clearer picture on the TME support situation.",Intel,2025-12-31 07:26:19,2
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nvz6y15,"If it helps, I'm also using   CPU - Ryzen 5 7600   RAM - Corsair Vengeance 32gb DDR5 6000mhz cl36  SSD - Crucial p3 plus 1tb  Motherboard - MSI b650m-a",Intel,2025-12-26 04:44:54,1
AMD,nx27aj0,"Hello, have you received a reply yet?",Intel,2026-01-01 10:17:28,1
AMD,nwgjs6p,u/earwig2000 Please check your inbox; Iâ€™ve sent you a message.,Intel,2025-12-29 00:30:28,1
AMD,nx7bu85,u/Sk7Str1p3 ï»¿Iâ€™m still reviewing the details to ensure I provide you with the most accurate information. Iâ€™ll update you as soon as I have a clear answer.  [](javascript:void(0);),Intel,2026-01-02 04:55:46,1
AMD,nxygejg,"u/Sk7Str1p3 I'm pleased to confirm thatÂ TME is indeed supported on your i7-12700F processor. TME is a security feature that encrypts the entire system memory using AES encryption with a hardware-generated key. The i7-12700F is part of Intel's 12th generation Alder Lake family, which includes TME support as a standard security feature.  However, TME functionality requires both processor support and proper system configuration:  Verification methods:  * BIOS/UEFI security settings * System diagnostic tools that enumerate processor security features  Important considerations:Â TME enablement depends on the motherboard BIOS implementation and may be disabled by default on many systems.  To assist you further, I need the following information:  * Make and model of your motherboard, as the TME feature may be disabled in the BIOS settings  For your reference, please find the official documentation:Â 12th Generation IntelÂ® Coreâ„¢ Processors Datasheet, Volume 1 of 2 Â [655258](https://cdrdv2.intel.com/v1/dl/getContent/655258)Â (Page 40) - This document contains the specific technical details about TME support for your processor.  Once you provide your motherboard information, I can guide you on how to check and potentially enable TME in your system's BIOS settings.  Please let me know if you have any additional questions.",Intel,2026-01-06 05:10:19,1
AMD,nxzgti7,My motherboard is Asrock H670-M Pro RS. firmware version: 18.01,Intel,2026-01-06 10:30:53,1
AMD,nz0ylbi,Hello. I want to tell you i7-12700f does NOT support TME. Thank you for your help anyway.,Intel,2026-01-11 19:38:08,1
AMD,ny5a7jq,"Your Intel i7-12700F processor does support TME (Total Memory Encryption), but the issue lies with your motherboard implementation rather than CPU capability. The ASRock H670M Pro RS with firmware 18.01 appears to either disable TME at the platform level or doesn't expose the necessary BIOS controls for consumer users. This explains why your diagnostic tools show ""TME = false"" and why HWiNFO64 displays it in gray. While you could check for TME-related options in your BIOS under Advanced â†’ CPU Configuration or Security sections, consumer-grade H670 motherboards often omit these enterprise security features from their interfaces. Your best course of action is to contact ASRock technical support directly, providing your exact motherboard model, BIOS version, and specifically asking about TME support and whether future firmware updates might include these controls.",Intel,2026-01-07 04:53:34,1
AMD,nfolbmr,"Most commenters are wrong or didnâ€™t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you havenâ€™t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intelâ€™s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,54
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,50
AMD,nfm76rf,"I donâ€™t think it will change that much overall.  Itâ€™s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I donâ€™t think weâ€™ll see big changes on the Intel side.  I donâ€™t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,3
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,7
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,15
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,0
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,-1
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-4
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-4
AMD,nfmy4sf,"Iâ€™m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days aheadâ€¦",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-9
AMD,nfma1mz,"It wonâ€™t affect either cause on parts that donâ€™t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but whatâ€™s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,5
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,22
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,8
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,13
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,3
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,4
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,6
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,5
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but whatâ€™s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,14
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,12
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,9
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,5
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,7
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,15
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,-3
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-3
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,4
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,9
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,2
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just donâ€™t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- itâ€™s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the â€œprebuiltâ€ market, with a 1-3 generation lag.  Many people who buy â€œprebuiltsâ€ have people they go to for advice (i.e. they talk to their friends/family members who DIY). When thereâ€™s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
